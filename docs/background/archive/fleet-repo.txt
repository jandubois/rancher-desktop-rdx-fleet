Directory structure:
└── rancher-fleet/
    ├── README.md
    ├── _typos.toml
    ├── CODEOWNERS
    ├── DEVELOPING.md
    ├── generate.go
    ├── charts/
    │   ├── fleet/
    │   │   ├── README.md
    │   │   ├── Chart.yaml
    │   │   ├── values.yaml
    │   │   ├── ci/
    │   │   │   ├── debug-values.yaml
    │   │   │   ├── default-values.yaml
    │   │   │   ├── nobootstrap-values.yaml
    │   │   │   ├── nodebug-values.yaml
    │   │   │   ├── nogitops-values.yaml
    │   │   │   └── nohelmops-values.yaml
    │   │   ├── templates/
    │   │   │   ├── _helpers.tpl
    │   │   │   ├── configmap.yaml
    │   │   │   ├── configmap_known_hosts.yaml
    │   │   │   ├── deployment.yaml
    │   │   │   ├── deployment_gitjob.yaml
    │   │   │   ├── deployment_helmops.yaml
    │   │   │   ├── job_cleanup_clusterregistrations.yaml
    │   │   │   ├── job_cleanup_gitrepojobs.yaml
    │   │   │   ├── rbac.yaml
    │   │   │   ├── rbac_gitjob.yaml
    │   │   │   ├── rbac_helmops.yaml
    │   │   │   ├── service.yaml
    │   │   │   ├── service_gitjob.yaml
    │   │   │   ├── serviceaccount.yaml
    │   │   │   ├── serviceaccount_gitjob.yaml
    │   │   │   └── serviceaccount_helmops.yaml
    │   │   └── tests/
    │   │       ├── affinity_test.yaml
    │   │       ├── agent-leader-election.yaml
    │   │       ├── agent_replica_count_test.yaml
    │   │       ├── extraAnnotations_test.yaml
    │   │       ├── extraLabels_test.yaml
    │   │       ├── fleet_controller_replica_count_test.yaml
    │   │       ├── gitjob_controller_replica_count_test.yaml
    │   │       ├── helmops_controller_replica_count_test.yaml
    │   │       └── resources_test.yaml
    │   ├── fleet-agent/
    │   │   ├── README.md
    │   │   ├── Chart.yaml
    │   │   ├── values.yaml
    │   │   ├── ci/
    │   │   │   └── default-values.yaml
    │   │   └── templates/
    │   │       ├── _helpers.tpl
    │   │       ├── configmap.yaml
    │   │       ├── deployment.yaml
    │   │       ├── network_policy_allow_all.yaml
    │   │       ├── rbac.yaml
    │   │       ├── secret.yaml
    │   │       ├── serviceaccount.yaml
    │   │       └── validate.yaml
    │   └── fleet-crd/
    │       ├── README.md
    │       ├── Chart.yaml
    │       ├── values.yaml
    │       └── templates/
    │           └── crds.yaml
    ├── cmd/
    │   ├── codegen/
    │   │   ├── boilerplate.go.txt
    │   │   ├── main.go
    │   │   ├── cleanup/
    │   │   │   └── main.go
    │   │   └── hack/
    │   │       └── generate_and_sort_crds.sh
    │   ├── docs/
    │   │   └── generate-cli-docs.go
    │   ├── fleetagent/
    │   │   └── main.go
    │   ├── fleetcli/
    │   │   └── main.go
    │   └── fleetcontroller/
    │       └── main.go
    ├── dev/
    │   ├── README.md
    │   ├── benchmarks.sh
    │   ├── build-fleet
    │   ├── create-secrets
    │   ├── create-zot-certs
    │   ├── env.multi-cluster-defaults
    │   ├── env.single-cluster-defaults
    │   ├── import-images-k3d
    │   ├── import-images-tests-k3d
    │   ├── k3d-clean
    │   ├── LOGGING.md
    │   ├── logs
    │   ├── remove-fleet
    │   ├── setup-cluster-config
    │   ├── setup-fleet
    │   ├── setup-fleet-managed-downstream
    │   ├── setup-fleet-multi-cluster
    │   ├── setup-k3d
    │   ├── setup-k3ds-downstream
    │   ├── setup-multi-cluster
    │   ├── setup-rancher-clusters
    │   ├── setup-single-cluster
    │   ├── update-agent-k3d
    │   ├── update-controller-k3d
    │   └── update-fleet-in-rancher-k3d
    ├── docs/
    │   ├── README.md
    │   ├── design.md
    │   ├── performance.md
    │   ├── qa_template.md
    │   └── release.md
    ├── internal/
    │   ├── bundlereader/
    │   │   ├── auth.go
    │   │   ├── auth_test.go
    │   │   ├── charturl.go
    │   │   ├── charturl_test.go
    │   │   ├── helm.go
    │   │   ├── helm_test.go
    │   │   ├── loaddirectory.go
    │   │   ├── loaddirectory_test.go
    │   │   ├── read.go
    │   │   ├── resources.go
    │   │   ├── resources_test.go
    │   │   └── style.go
    │   ├── client/
    │   │   └── client.go
    │   ├── cmd/
    │   │   ├── builder.go
    │   │   ├── debug.go
    │   │   ├── options.go
    │   │   ├── agent/
    │   │   │   ├── clusterstatus.go
    │   │   │   ├── operator.go
    │   │   │   ├── register.go
    │   │   │   ├── root.go
    │   │   │   ├── clusterstatus/
    │   │   │   │   ├── suite_test.go
    │   │   │   │   ├── ticker.go
    │   │   │   │   └── ticker_test.go
    │   │   │   ├── controller/
    │   │   │   │   ├── bundledeployment_controller.go
    │   │   │   │   └── drift_controller.go
    │   │   │   ├── deployer/
    │   │   │   │   ├── deployer.go
    │   │   │   │   ├── deployer_test.go
    │   │   │   │   ├── cleanup/
    │   │   │   │   │   ├── cleanup.go
    │   │   │   │   │   └── cleanup_test.go
    │   │   │   │   ├── data/
    │   │   │   │   │   ├── data.go
    │   │   │   │   │   ├── values.go
    │   │   │   │   │   └── convert/
    │   │   │   │   │       └── convert.go
    │   │   │   │   ├── desiredset/
    │   │   │   │   │   ├── clients.go
    │   │   │   │   │   ├── desiredset.go
    │   │   │   │   │   ├── desiredset_apply.go
    │   │   │   │   │   ├── desiredset_compare.go
    │   │   │   │   │   ├── desiredset_process.go
    │   │   │   │   │   ├── desiredset_process_test.go
    │   │   │   │   │   ├── diff.go
    │   │   │   │   │   ├── diff_test.go
    │   │   │   │   │   ├── plan.go
    │   │   │   │   │   └── style.go
    │   │   │   │   ├── driftdetect/
    │   │   │   │   │   └── driftdetect.go
    │   │   │   │   ├── internal/
    │   │   │   │   │   ├── diff/
    │   │   │   │   │   │   ├── diff.go
    │   │   │   │   │   │   ├── diff_options.go
    │   │   │   │   │   │   ├── json/
    │   │   │   │   │   │   │   └── json.go
    │   │   │   │   │   │   └── scheme/
    │   │   │   │   │   │       └── scheme.go
    │   │   │   │   │   ├── normalizers/
    │   │   │   │   │   │   ├── diff_normalizer.go
    │   │   │   │   │   │   ├── knowntypes_normalizer.go
    │   │   │   │   │   │   └── glob/
    │   │   │   │   │   │       └── glob.go
    │   │   │   │   │   └── resource/
    │   │   │   │   │       └── ignore.go
    │   │   │   │   ├── kv/
    │   │   │   │   │   └── split.go
    │   │   │   │   ├── merr/
    │   │   │   │   │   └── error.go
    │   │   │   │   ├── monitor/
    │   │   │   │   │   ├── condition.go
    │   │   │   │   │   ├── conditions_test.go
    │   │   │   │   │   ├── updatestatus.go
    │   │   │   │   │   └── updatestatus_test.go
    │   │   │   │   ├── normalizers/
    │   │   │   │   │   ├── jsonpatch.go
    │   │   │   │   │   ├── mutatingwebhook.go
    │   │   │   │   │   ├── norm.go
    │   │   │   │   │   ├── status.go
    │   │   │   │   │   ├── status_test.go
    │   │   │   │   │   └── validatingwebhook.go
    │   │   │   │   ├── objectset/
    │   │   │   │   │   ├── objectset.go
    │   │   │   │   │   ├── objectset_test.go
    │   │   │   │   │   └── stringset.go
    │   │   │   │   └── summary/
    │   │   │   │       ├── cattletypes.go
    │   │   │   │       ├── condition.go
    │   │   │   │       ├── coretypes.go
    │   │   │   │       ├── doc.go
    │   │   │   │       ├── gvk.go
    │   │   │   │       ├── gvk_test.go
    │   │   │   │       ├── suite_test.go
    │   │   │   │       ├── summarize.go
    │   │   │   │       ├── summarized.go
    │   │   │   │       ├── summarizers.go
    │   │   │   │       └── summarizers_test.go
    │   │   │   ├── register/
    │   │   │   │   ├── register.go
    │   │   │   │   └── register_test.go
    │   │   │   └── trigger/
    │   │   │       └── watcher.go
    │   │   ├── cli/
    │   │   │   ├── apply.go
    │   │   │   ├── apply_test.go
    │   │   │   ├── cleanup.go
    │   │   │   ├── deploy.go
    │   │   │   ├── root.go
    │   │   │   ├── target.go
    │   │   │   ├── test.go
    │   │   │   ├── apply/
    │   │   │   │   ├── apply.go
    │   │   │   │   ├── apply_concurrency_test.go
    │   │   │   │   └── apply_test.go
    │   │   │   ├── cleanup/
    │   │   │   │   └── cleanup.go
    │   │   │   ├── gitcloner/
    │   │   │   │   ├── cloner.go
    │   │   │   │   ├── cloner_test.go
    │   │   │   │   ├── cmd.go
    │   │   │   │   └── cmd_test.go
    │   │   │   ├── match/
    │   │   │   │   └── match.go
    │   │   │   └── writer/
    │   │   │       └── writer.go
    │   │   └── controller/
    │   │       ├── operator.go
    │   │       ├── root.go
    │   │       ├── agentmanagement/
    │   │       │   ├── root.go
    │   │       │   ├── start.go
    │   │       │   ├── agent/
    │   │       │   │   ├── agent.go
    │   │       │   │   ├── config.go
    │   │       │   │   ├── manifest.go
    │   │       │   │   └── manifest_test.go
    │   │       │   ├── connection/
    │   │       │   │   └── connection.go
    │   │       │   ├── controllers/
    │   │       │   │   ├── controllers.go
    │   │       │   │   ├── bootstrap/
    │   │       │   │   │   └── bootstrap.go
    │   │       │   │   ├── cluster/
    │   │       │   │   │   ├── controller.go
    │   │       │   │   │   ├── import.go
    │   │       │   │   │   └── import_test.go
    │   │       │   │   ├── clusterregistration/
    │   │       │   │   │   ├── controller.go
    │   │       │   │   │   ├── controller_test.go
    │   │       │   │   │   └── suite_test.go
    │   │       │   │   ├── clusterregistrationtoken/
    │   │       │   │   │   └── handler.go
    │   │       │   │   ├── config/
    │   │       │   │   │   └── controller.go
    │   │       │   │   ├── manageagent/
    │   │       │   │   │   ├── manageagent.go
    │   │       │   │   │   └── manageagent_test.go
    │   │       │   │   └── resources/
    │   │       │   │       └── data.go
    │   │       │   ├── scheduling/
    │   │       │   │   ├── scheduling.go
    │   │       │   │   └── scheduling_test.go
    │   │       │   └── secret/
    │   │       │       └── util.go
    │   │       ├── cleanup/
    │   │       │   ├── root.go
    │   │       │   ├── start.go
    │   │       │   └── controllers/
    │   │       │       ├── controllers.go
    │   │       │       └── cleanup/
    │   │       │           └── controller.go
    │   │       ├── errorutil/
    │   │       │   └── errorutil.go
    │   │       ├── finalize/
    │   │       │   └── finalize.go
    │   │       ├── gitops/
    │   │       │   ├── operator.go
    │   │       │   └── reconciler/
    │   │       │       ├── gitjob.go
    │   │       │       ├── gitjob_controller.go
    │   │       │       ├── gitjob_test.go
    │   │       │       ├── polling_job.go
    │   │       │       ├── polling_job_test.go
    │   │       │       ├── predicate.go
    │   │       │       ├── rbac.go
    │   │       │       ├── restrictions.go
    │   │       │       ├── restrictions_test.go
    │   │       │       ├── status_controller.go
    │   │       │       ├── suite_test.go
    │   │       │       └── targetsyaml.go
    │   │       ├── helmops/
    │   │       │   ├── operator.go
    │   │       │   └── reconciler/
    │   │       │       ├── helmop_controller.go
    │   │       │       ├── helmop_controller_test.go
    │   │       │       ├── helmop_status.go
    │   │       │       └── polling_job.go
    │   │       ├── imagescan/
    │   │       │   ├── gitcommit_job.go
    │   │       │   ├── tagscan_job.go
    │   │       │   ├── tagscan_job_test.go
    │   │       │   └── update/
    │   │       │       ├── README.md
    │   │       │       ├── filereader.go
    │   │       │       ├── filter.go
    │   │       │       ├── result.go
    │   │       │       └── setters.go
    │   │       ├── namespace/
    │   │       │   └── util.go
    │   │       ├── options/
    │   │       │   └── calculate.go
    │   │       ├── quartz/
    │   │       │   ├── trigger.go
    │   │       │   └── trigger_test.go
    │   │       ├── reconciler/
    │   │       │   ├── bundle_controller.go
    │   │       │   ├── bundle_controller_test.go
    │   │       │   ├── bundle_status.go
    │   │       │   ├── bundledeployment_controller.go
    │   │       │   ├── bundledeployment_controller_test.go
    │   │       │   ├── cluster_controller.go
    │   │       │   ├── cluster_controller_test.go
    │   │       │   ├── clustergroup_controller.go
    │   │       │   ├── config_controller.go
    │   │       │   ├── cronduration_job.go
    │   │       │   ├── cronduration_job_test.go
    │   │       │   ├── error_handling.go
    │   │       │   ├── imagescan_controller.go
    │   │       │   ├── predicate.go
    │   │       │   ├── schedule_controller.go
    │   │       │   ├── schedule_controller_test.go
    │   │       │   └── suite_test.go
    │   │       ├── status/
    │   │       │   └── status.go
    │   │       └── summary/
    │   │           ├── summary.go
    │   │           └── summary_test.go
    │   ├── config/
    │   │   ├── config.go
    │   │   ├── config_test.go
    │   │   ├── overrides.go
    │   │   └── suite_test.go
    │   ├── content/
    │   │   └── helpers.go
    │   ├── experimental/
    │   │   └── experimental.go
    │   ├── fleetyaml/
    │   │   ├── fleetyaml.go
    │   │   └── fleetyaml_test.go
    │   ├── github/
    │   │   ├── app.go
    │   │   ├── app_test.go
    │   │   ├── auth.go
    │   │   ├── auth_test.go
    │   │   ├── helpers.go
    │   │   └── helpers_test.go
    │   ├── helmdeployer/
    │   │   ├── capabilities.go
    │   │   ├── delete.go
    │   │   ├── deployer.go
    │   │   ├── history.go
    │   │   ├── impersonate.go
    │   │   ├── install.go
    │   │   ├── install_test.go
    │   │   ├── list.go
    │   │   ├── list_test.go
    │   │   ├── lookup.go
    │   │   ├── lookup_test.go
    │   │   ├── postrender.go
    │   │   ├── postrender_test.go
    │   │   ├── release_conversion.go
    │   │   ├── release_conversion_test.go
    │   │   ├── rollback.go
    │   │   ├── template.go
    │   │   ├── helmcache/
    │   │   │   ├── secret.go
    │   │   │   └── secret_test.go
    │   │   ├── kustomize/
    │   │   │   └── kustomize.go
    │   │   ├── rawyaml/
    │   │   │   └── resources.go
    │   │   └── render/
    │   │       ├── helm.go
    │   │       └── patch/
    │   │           └── patch.go
    │   ├── helmupdater/
    │   │   ├── helmupdater.go
    │   │   └── helmupdater_test.go
    │   ├── helmvalues/
    │   │   ├── extract.go
    │   │   ├── extract_test.go
    │   │   ├── hash.go
    │   │   ├── hash_test.go
    │   │   └── set.go
    │   ├── manifest/
    │   │   ├── lookup.go
    │   │   ├── manifest.go
    │   │   ├── output.go
    │   │   ├── store.go
    │   │   └── store_test.go
    │   ├── metrics/
    │   │   ├── bundle_metrics.go
    │   │   ├── bundledeployment_metrics.go
    │   │   ├── cluster_metrics.go
    │   │   ├── clustergroup_metrics.go
    │   │   ├── gitrepo_metrics.go
    │   │   ├── helm_metrics.go
    │   │   └── metrics.go
    │   ├── mocks/
    │   │   ├── client_mock.go
    │   │   ├── eventrecorder_mock.go
    │   │   ├── helm_deployer_mock.go
    │   │   ├── manifest_store_mock.go
    │   │   ├── oci_client_mock.go
    │   │   ├── orastarget_mock.go
    │   │   ├── reader_mock.go
    │   │   ├── scheduler_mock.go
    │   │   ├── status_writer_mock.go
    │   │   └── target_builder_mock.go
    │   ├── names/
    │   │   ├── keyhash.go
    │   │   ├── name.go
    │   │   ├── name_test.go
    │   │   ├── safeconcat.go
    │   │   ├── safeconcat_test.go
    │   │   └── suite_test.go
    │   ├── namespaces/
    │   │   └── namespaces.go
    │   ├── ocistorage/
    │   │   ├── ociwrapper.go
    │   │   ├── ociwrapper_test.go
    │   │   ├── secret.go
    │   │   ├── secret_test.go
    │   │   └── suite_test.go
    │   ├── registration/
    │   │   └── secret.go
    │   ├── resourcestatus/
    │   │   ├── resourcekey.go
    │   │   └── resourcekey_test.go
    │   └── ssh/
    │       ├── knownhosts.go
    │       ├── knownhosts_test.go
    │       ├── url.go
    │       └── url_test.go
    └── package/
        ├── Dockerfile
        ├── Dockerfile.agent
        └── log.sh

================================================
FILE: README.md
================================================
# Introduction
[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Francher%2Ffleet.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Francher%2Ffleet?ref=badge_shield)


[![Unit](https://github.com/rancher/fleet/actions/workflows/ci.yml/badge.svg)](https://github.com/rancher/fleet/actions/workflows/ci.yml)
[![E2E Examples](https://github.com/rancher/fleet/actions/workflows/e2e-ci.yml/badge.svg?event=schedule)](https://github.com/rancher/fleet/actions/workflows/e2e-ci.yml)
[![E2E Multi-Cluster Examples](https://github.com/rancher/fleet/actions/workflows/e2e-multicluster-ci.yml/badge.svg?event=schedule)](https://github.com/rancher/fleet/actions/workflows/e2e-multicluster-ci.yml)
[![golangci-lint](https://github.com/rancher/fleet/actions/workflows/golangci-lint.yml/badge.svg?event=schedule)](https://github.com/rancher/fleet/actions/workflows/golangci-lint.yml)

![](./docs/arch.png)

Fleet is GitOps and HelmOps at scale. Fleet is designed to manage multiple clusters. It's also lightweight
enough that it works great for a single cluster too, but it really shines
when you get to a large scale. By large scale we mean either a lot of clusters, a lot of deployments, or a lot of
teams in a single organization.

Fleet can manage deployments from git of raw Kubernetes YAML, Helm charts, or Kustomize or any combination of the three.
Regardless of the source all resources are dynamically turned into Helm charts and Helm is used as the engine to
deploy everything in the cluster. This gives a high degree of control, consistency, and auditability. Fleet focuses not only on
the ability to scale, but to give one a high degree of control and visibility to exactly what is installed on the cluster.

# Quick Start

For more information, have a look at Fleet's [documentation](https://fleet.rancher.io/).

## Install

Get `helm` if you don't have it.  Helm 3 is just a CLI and won't do bad insecure
things to your cluster.

For instance, using Homebrew:
```
brew install helm
```

Install the Fleet Helm charts (there's two because we separate out CRDs for ultimate flexibility.)

```shell
helm -n cattle-fleet-system install --create-namespace --wait \
    fleet-crd https://github.com/rancher/fleet/releases/download/v0.14.0/fleet-crd-0.14.0.tgz
helm -n cattle-fleet-system install --create-namespace --wait \
    fleet https://github.com/rancher/fleet/releases/download/v0.14.0/fleet-0.14.0.tgz
```

## Add a Git Repo to watch

Change `spec.repo` to your git repo of choice.  Kubernetes manifest files that should
be deployed should be in `/manifests` in your repo.

```bash
cat > example.yaml << "EOF"
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: sample
  # This namespace is special and auto-wired to deploy to the local cluster
  namespace: fleet-local
spec:
  # Everything from this repo will be run in this cluster. You trust me right?
  repo: "https://github.com/rancher/fleet-examples"
  paths:
  - simple
EOF

kubectl apply -f example.yaml
```

## Get Status

Get status of what Fleet is doing:

```shell
kubectl -n fleet-local get fleet
```

You should see something like this get created in your cluster.

```
kubectl get deploy frontend
```
```
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
frontend   3/3     3            3           116m
```

Enjoy and read the [docs](https://fleet.rancher.io/).

## License
[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Francher%2Ffleet.svg?type=large)](https://app.fossa.com/projects/git%2Bgithub.com%2Francher%2Ffleet?ref=badge_large)

For developer and maintainer documentation, see [DEVELOPING.md](./DEVELOPING.md).




================================================
FILE: _typos.toml
================================================
[files]
extend-exclude = [
  "go.mod",
  "go.sum",
  "ssh_host_ecdsa_key",
  "cloner_test.go",
  "netutils_test.go",
  "pkg/apis/internal/scheme.go",
]

[default.extend-identifiers]
"passin" = "passin"
"ANDed" = "ANDed"
"HoTWPQxTJ5dIY31" = "HoTWPQxTJ5dIY31"
"wre" = "wre"                         # warning regular expression, not 'were'

[default]
extend-ignore-re = [
    "-{5}BEGIN RSA PRIVATE KEY-{5}(?:$|[^-]{63,}-{5}END)",
    "-{5}BEGIN PRIVATE KEY-{5}(?:$|[^-]{63,}-{5}END)",
]



================================================
FILE: CODEOWNERS
================================================
# Order is important. The last matching pattern has the most precedence.

* @rancher/fleet



================================================
FILE: DEVELOPING.md
================================================
# Developing

This document contains tips, workflows, and more for local development within this repository.

More documentation for maintainers and developers of Fleet can be found in [docs/](docs/).

## Recommendations on writing tests

Fleet's test suites should be as reliable as possible, as they are a pillar of Fleet's quality.

Flaky tests cause delays and confusion; here are a few recommendations which can help detect and prevent them.

With many of Fleet's tests relying on [ginkgo](https://onsi.github.io/ginkgo/), suites can be run repeatedly to identify
flaky tests locally, by using `ginkgo --until-it-fails $path`. Flag `-v` adds verbosity to test runs, outputting precious
clues on what may be failing.

Making tests as lightweight as possible is a good first step towards making them less likely to be flaky, as it reduces
complexity and makes them easier to set up and tear down. This includes using unit tests where possible, especially when
many different combinations of similar logic must be tested. If unit tests cannot cover a given case, we recommend
integration tests instead, which are still fairly easy to debug and flexible with mocking.
End-to-end tests should be the last resort, as they spin up full-blown clusters, sometimes with additional test
infrastructure.

When using end-to-end tests, we recommend:

- Deploying the smallest possible amount of resources to speed up deployments and reduce the load on resources. For
instance, using [test charts](https://github.com/rancher/fleet-test-data/tree/master/simple-chart) containing only
config maps, since much of Fleet's end-to-end testing is independent of the workload itself, but rather focuses on
status changes, readiness, etc.

- Randomising namespaces whenever possible, to reduce the risk of conflicts between multiple runs of the same test
suite. The same applies to randomising release names and deployed resource names.

- Cleaning up created resources after execution, leaving a clean slate for further testing. This makes conflicts less
likely and reduces the load on the machine running tests. Pre-execution cleanup could also be a good option, ensuring
that testing only begins if cleanup is successful. [Gomega](http://github.com/onsi/gomega)'s `BeforeEach` and
`AfterEach` are our good friends there.

- When waiting for a (set of) condition(s) to be met, not using `time.Sleep(<duration>)` but rather Gomega's
[Eventually](https://onsi.github.io/gomega/#making-asynchronous-assertions). As functions can be passed to `Eventually`,
we prefer using functions which take a `Gomega` object as a parameter, which allows assertions to be made _inside_ that
function, leading to more detailed output in case of failures. Using a function that returns a boolean and expecting it
to succeed will simply lead to output about `false` expected to be `true`, which is harder to troubleshoot.

- Avoiding dependencies on ephemeral information, such as logs or events (and especially their absence). If that is
really necessary, deployments should be scaled down and back up before test runs.

## Dev Scripts & Running E2E Tests

Development scripts are provided under `/dev` to make it easier setting up a local development Fleet standalone
environment and running the E2E tests against it. These scripts are intended only for local Fleet development, not for
production nor any other real world scenario.

Setting up the local development environment and running the E2E tests is described in the [/dev/README.md](/dev/README.md).



================================================
FILE: generate.go
================================================
//go:generate go run ./cmd/codegen/cleanup/main.go
//go:generate go run ./cmd/codegen/main.go
//go:generate bash ./cmd/codegen/hack/generate_and_sort_crds.sh ./charts/fleet-crd/templates/crds.yaml

package main



================================================
FILE: charts/fleet/README.md
================================================
# Fleet Helm Chart

Fleet is GitOps and HelmOps at scale. Fleet is designed to manage multiple clusters.

## What is Fleet?

* Cluster engine: Fleet is a container management and deployment engine designed to offer users more control on the local cluster and constant monitoring through GitOps. Fleet focuses not only on the ability to scale, but it also gives users a high degree of control and visibility to monitor exactly what is installed on the cluster.

* Deployment management: Fleet can manage deployments from git of raw Kubernetes YAML, Helm charts, Kustomize, or any combination of the three. Regardless of the source, all resources are dynamically turned into Helm charts, and Helm is used as the engine to deploy all resources in the cluster. As a result, users can enjoy a high degree of control, consistency, and auditability of their clusters.

## Introduction

This chart deploys Fleet on a Kubernetes cluster. It also deploys some of its dependencies as subcharts.

The documentation is centralized in the [doc website](https://fleet.rancher.io/).

## Prerequisites

Get helm if you don't have it. Helm 3 is just a CLI.


## Install Fleet

Install the Fleet Helm charts (there are two because we separate out CRDs for ultimate flexibility.):

```
$ helm repo add fleet https://rancher.github.io/fleet-helm-charts/
$ helm -n cattle-fleet-system install --create-namespace --wait fleet-crd fleet/fleet-crd
$ helm -n cattle-fleet-system install --create-namespace --wait fleet fleet/fleet
```


================================================
FILE: charts/fleet/Chart.yaml
================================================
annotations:
  catalog.cattle.io/auto-install: fleet-crd=match
  catalog.cattle.io/certified: rancher
  catalog.cattle.io/experimental: "true"
  catalog.cattle.io/hidden: "true"
  catalog.cattle.io/kube-version: '>= 1.33.0-0 < 1.36.0-0'
  catalog.cattle.io/namespace: cattle-fleet-system
  catalog.cattle.io/os: linux
  catalog.cattle.io/permits-os: linux,windows
  catalog.cattle.io/provides-gvr: clusters.fleet.cattle.io/v1alpha1
  catalog.cattle.io/rancher-version: '>= 2.14.0-0 < 2.15.0-0'
  catalog.cattle.io/release-name: fleet
apiVersion: v2
appVersion: 0.0.0
description: Fleet Controller - GitOps at Scale
icon: https://charts.rancher.io/assets/logos/fleet.svg
name: fleet
version: 0.0.0



================================================
FILE: charts/fleet/values.yaml
================================================
image:
  repository: rancher/fleet
  tag: dev
  imagePullPolicy: IfNotPresent

agentImage:
  repository: rancher/fleet-agent
  tag: dev
  imagePullPolicy: IfNotPresent

# For cluster registration the public URL of the Kubernetes API server must be set here
# Example: https://example.com:6443
apiServerURL: ""

# For cluster registration the pem encoded value of the CA of the Kubernetes API server must be set here
# If left empty it is assumed this Kubernetes API TLS is signed by a well known CA.
apiServerCA: ""

# Determines whether the agent should trust CA bundles from the operating system's trust store when connecting to a
# management cluster. True in `system-store` mode, false in `strict` mode.
agentTLSMode: "system-store"

# A duration string for how often agents should report a heartbeat
agentCheckinInterval: "15m"

# The amount of time that agents will wait before they clean up old Helm releases.
# A non-existent value or 0 will result in an interval of 15 minutes.
garbageCollectionInterval: "15m"

# Whether you want to allow cluster upon registration to specify their labels.
ignoreClusterRegistrationLabels: false

# Counts from gitrepo are out of sync with bundleDeployment state.
# Just retry in a number of seconds as there is no great way to trigger an event that doesn't cause a loop.
# If not set default is 15 seconds.
# clusterEnqueueDelay: 120s

# http[s] proxy server
# proxy: http://<username>@<password>:<url>:<port>

# comma separated list of domains or ip addresses that will not use the proxy
noProxy: 127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local

# The amount of time to wait for a response from the server before canceling the
# request.  Used to retrieve the latest commit of configured git repositories.
# A non-existent value or 0 will result in a timeout of 30 seconds.
gitClientTimeout: 30s

bootstrap:
  enabled: true
  # The namespace that will be autocreated and the local cluster will be registered in
  namespace: fleet-local
  # The namespace where the fleet agent for the local cluster will be ran, if empty
  # this will default to cattle-fleet-system
  agentNamespace: ""
  # A repo to add at install time that will deploy to the local cluster. This allows
  # one to fully bootstrap fleet, its configuration and all its downstream clusters
  # in one shot.
  repo: ""
  secret: ""
  branch: master
  paths: ""

global:
  cattle:
    systemDefaultRegistry: ""

## Node labels for pod assignment
## Ref: https://kubernetes.io/docs/user-guide/node-selection/
##
nodeSelector: {}

## List of node taints to tolerate (requires Kubernetes >= 1.6)
tolerations: []

## Pod affinity for the controllers.
affinity: {}

# Pod resource limits and requests for the controllers
resources: {}
  # limits:
  #   cpu: 8000m
  #   memory: 8Gi
  # requests:
  #   cpu: 250m
  #   memory: 768Mi

## PriorityClassName assigned to deployment.
priorityClassName: ""

## Determines whether SSH operations (eg. cloning git repos, downloading Helm charts) could succeed if
## host verification fails. Insecure when set to true. Default: false.
insecureSkipHostKeyChecks: false

gitops:
  enabled: true
  # syncPeriod is used to pick up polling for lost gitrepo events.
  # It should be larger than the largest gitrepo pollinginterval.
  syncPeriod: 2h

metrics:
  enabled: true

debug: false
debugLevel: 0
propagateDebugSettingsToAgents: true
disableSecurityContext: false

migrations:
  clusterRegistrationCleanup: true
  gitrepoJobsCleanup: true

## Leader election configuration
leaderElection:
  leaseDuration: 30s
  retryPeriod: 10s
  renewDeadline: 25s

## Fleet controller configuration
controller:
  replicas: 1
  reconciler:
    # The number of workers that are allowed to each type of reconciler
    workers:
      gitrepo: "50"
      bundle: "50"
      bundledeployment: "50"
      cluster: "50"
      clustergroup: "50"
      imagescan: "50"
      schedule: "50"

gitjob:
  replicas: 1

helmops:
  enabled: true
  replicas: 1

## Fleet agent configuration
agent:
  replicas: 1
  reconciler:
    # The number of workers that are allowed for each type of reconciler
    workers:
      bundledeployment: "50"
      drift: "50"
  ## Leader election configuration
  leaderElection:
    leaseDuration: 30s
    retryPeriod: 10s
    renewDeadline: 25s


# Extra environment variables passed to the fleet pods.
# extraEnv:
# - name: OCI_STORAGE
#   value: "false"
# - name: EXPERIMENTAL_COPY_RESOURCES_DOWNSTREAM
#   value: "false"

# shards:
#   - id: shard0
#     nodeSelector:
#       kubernetes.io/hostname: k3d-upstream-server-0
#   - id: shard1
#     nodeSelector:
#       kubernetes.io/hostname: k3d-upstream-server-1
#   - id: shard2
#     nodeSelector:
#       kubernetes.io/hostname: k3d-upstream-server-2

# Extra labels passed to the fleet pods.
# extraLabels:
#   fleetController:
#     new-label: "new-label-value"
#     new-label-2: "new-label-value-2"
#   gitjob:
#     new-label: "new-label-value"
#     new-label-2: "new-label-value-2"
#   helmops:
#     new-label: "new-label-value"
#     new-label-2: "new-label-value-2"

# Extra annotations passed to the fleet pods.
# extraAnnotations:
#   fleetController:
#     new-annotation: "new-annotation-value"
#     new-annotation-2: "new-annotation-value-2"
#   gitjob:
#     new-annotation: "new-annotation-value"
#     new-annotation-2: "new-annotation-value-2"
#   helmops:
#     new-annotation: "new-annotation-value"
#     new-annotation-2: "new-annotation-value-2"



================================================
FILE: charts/fleet/ci/debug-values.yaml
================================================
bootstrap:
  enabled: true

global:
  cattle:
    systemDefaultRegistry: "ghcr.io"

nodeSelector:
  kubernetes.io/os: os2

tolerations:
  - key: node.cloudprovider.kubernetes.io/uninitialized
    operator: "Equal"
    value: "true"
    effect: NoSchedule

priorityClassName: "prio1"

gitops:
  enabled: true

helmops:
  enabled: true

metrics:
  enabled: true

debug: true
debugLevel: 4
propagateDebugSettingsToAgents: true
disableSecurityContext: true

cpuPprof:
  period: "60s"
  volumeConfiguration:
    hostPath:
      path: /tmp/pprof
      type: DirectoryOrCreate

migrations:
  clusterRegistrationCleanup: true

leaderElection:
  leaseDuration: 30s
  retryPeriod: 10s
  renewDeadline: 25s

controller:
  reconciler:
    workers:
      gitrepo: "1"
      bundle: "1"
      bundledeployment: "1"
      schedule: "1"

shards:
  - id: shard0
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-0
  - id: shard1
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-1
  - id: shard2
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-2

extraEnv:
  - name: EXPERIMENTAL_COPY_RESOURCES_DOWNSTREAM
    value: "true"
  - name: EXPERIMENTAL_SCHEDULES
    value: "true"



================================================
FILE: charts/fleet/ci/default-values.yaml
================================================



================================================
FILE: charts/fleet/ci/nobootstrap-values.yaml
================================================
bootstrap:
  enabled: false

global:
  cattle:
    systemDefaultRegistry: "ghcr.io"

nodeSelector:
  kubernetes.io/os: mac

tolerations:
  - key: node.cloudprovider.kubernetes.io/uninitialized
    operator: "Equal"
    value: "true"
    effect: NoSchedule

priorityClassName: "prio1"

gitops:
  enabled: true

helmops:
  enabled: true

metrics:
  enabled: true

debug: false
debugLevel: 4
propagateDebugSettingsToAgents: true

cpuPprof:
  period: "60s"
  volumeConfiguration:
    hostPath:
      path: /tmp/pprof
      type: DirectoryOrCreate

migrations:
  clusterRegistrationCleanup: true

leaderElection:
  leaseDuration: 30s
  retryPeriod: 10s
  renewDeadline: 25s

controller:
  reconciler:
    workers:
      gitrepo: "1"
      bundle: "1"
      bundledeployment: "1"
      schedule: "1"

shards:
  - id: shard0
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-0
  - id: shard1
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-1
  - id: shard2
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-2

extraEnv:
  - name: EXPERIMENTAL_COPY_RESOURCES_DOWNSTREAM
    value: "true"
  - name: EXPERIMENTAL_SCHEDULES
    value: "true"



================================================
FILE: charts/fleet/ci/nodebug-values.yaml
================================================
bootstrap:
  enabled: true

global:
  cattle:
    systemDefaultRegistry: "ghcr.io"

nodeSelector:
  kubernetes.io/os: plan9

tolerations:
  - key: node.cloudprovider.kubernetes.io/uninitialized
    operator: "Equal"
    value: "true"
    effect: NoSchedule

priorityClassName: "prio1"

gitops:
  enabled: true

helmops:
  enabled: true

metrics:
  enabled: true

debug: false
debugLevel: 4
propagateDebugSettingsToAgents: true

cpuPprof:
  period: "60s"
  volumeConfiguration:
    hostPath:
      path: /tmp/pprof
      type: DirectoryOrCreate

migrations:
  clusterRegistrationCleanup: true

leaderElection:
  leaseDuration: 30s
  retryPeriod: 10s
  renewDeadline: 25s

controller:
  reconciler:
    workers:
      gitrepo: "1"
      bundle: "1"
      bundledeployment: "1"
      schedule: "1"

shards:
  - id: shard0
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-0
  - id: shard1
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-1
  - id: shard2
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-2

extraEnv:
  - name: EXPERIMENTAL_COPY_RESOURCES_DOWNSTREAM
    value: "true"
  - name: EXPERIMENTAL_SCHEDULES
    value: "true"



================================================
FILE: charts/fleet/ci/nogitops-values.yaml
================================================
bootstrap:
  enabled: true

global:
  cattle:
    systemDefaultRegistry: "ghcr.io"

nodeSelector:
  kubernetes.io/os: winxp

tolerations:
  - key: node.cloudprovider.kubernetes.io/uninitialized
    operator: "Equal"
    value: "true"
    effect: NoSchedule

priorityClassName: "prio1"

gitops:
  enabled: false

helmops:
  enabled: true

metrics:
  enabled: true

debug: false
debugLevel: 4
propagateDebugSettingsToAgents: true

cpuPprof:
  period: "60s"
  volumeConfiguration:
    hostPath:
      path: /tmp/pprof
      type: DirectoryOrCreate

migrations:
  clusterRegistrationCleanup: true

leaderElection:
  leaseDuration: 30s
  retryPeriod: 10s
  renewDeadline: 25s

controller:
  reconciler:
    workers:
      gitrepo: "1"
      bundle: "1"
      bundledeployment: "1"
      schedule: "1"

shards:
  - id: shard0
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-0
  - id: shard1
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-1
  - id: shard2
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-2

extraEnv:
  - name: EXPERIMENTAL_COPY_RESOURCES_DOWNSTREAM
    value: "true"
  - name: EXPERIMENTAL_SCHEDULES
    value: "true"



================================================
FILE: charts/fleet/ci/nohelmops-values.yaml
================================================
bootstrap:
  enabled: true

global:
  cattle:
    systemDefaultRegistry: "ghcr.io"

nodeSelector:
  kubernetes.io/os: winxp

tolerations:
  - key: node.cloudprovider.kubernetes.io/uninitialized
    operator: "Equal"
    value: "true"
    effect: NoSchedule

priorityClassName: "prio1"

gitops:
  enabled: true

helmops:
  enabled: false

metrics:
  enabled: true

debug: false
debugLevel: 4
propagateDebugSettingsToAgents: true

cpuPprof:
  period: "60s"
  volumeConfiguration:
    hostPath:
      path: /tmp/pprof
      type: DirectoryOrCreate

migrations:
  clusterRegistrationCleanup: true

leaderElection:
  leaseDuration: 30s
  retryPeriod: 10s
  renewDeadline: 25s

controller:
  reconciler:
    workers:
      gitrepo: "1"
      bundle: "1"
      bundledeployment: "1"
      schedule: "1"

shards:
  - id: shard0
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-0
  - id: shard1
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-1
  - id: shard2
    nodeSelector:
      kubernetes.io/hostname: k3d-upstream-server-2



================================================
FILE: charts/fleet/templates/_helpers.tpl
================================================
{{- define "system_default_registry" -}}
{{- if .Values.global.cattle.systemDefaultRegistry -}}
{{- printf "%s/" .Values.global.cattle.systemDefaultRegistry -}}
{{- else -}}
{{- "" -}}
{{- end -}}
{{- end -}}

{{/*
Windows cluster will add default taint for linux nodes,
add below linux tolerations to workloads could be scheduled to those linux nodes
*/}}
{{- define "linux-node-tolerations" -}}
- key: "cattle.io/os"
  value: "linux"
  effect: "NoSchedule"
  operator: "Equal"
{{- end -}}

{{- define "linux-node-selector" -}}
kubernetes.io/os: linux
{{- end -}}


================================================
FILE: charts/fleet/templates/configmap.yaml
================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: fleet-controller
data:
  config: |
    {
      "systemDefaultRegistry": "{{ template "system_default_registry" . }}",
      "agentImage": "{{ template "system_default_registry" . }}{{.Values.agentImage.repository}}:{{.Values.agentImage.tag}}",
      "agentImagePullPolicy": "{{ .Values.agentImage.imagePullPolicy }}",
      "apiServerURL": "{{.Values.apiServerURL}}",
      "apiServerCA": "{{b64enc .Values.apiServerCA}}",
      "agentCheckinInterval": "{{.Values.agentCheckinInterval}}",
      "agentTLSMode": "{{.Values.agentTLSMode}}",
      "agentWorkers": {
            "bundledeployment": "{{.Values.agent.reconciler.workers.bundledeployment}}",
            "drift": "{{.Values.agent.reconciler.workers.drift}}"
      },
      {{ if .Values.garbageCollectionInterval }}
      "garbageCollectionInterval": "{{.Values.garbageCollectionInterval}}",
      {{ end }}
      "ignoreClusterRegistrationLabels": {{.Values.ignoreClusterRegistrationLabels}},
      "bootstrap": {
        "paths": "{{.Values.bootstrap.paths}}",
        "repo": "{{.Values.bootstrap.repo}}",
        "secret": "{{.Values.bootstrap.secret}}",
        "branch":  "{{.Values.bootstrap.branch}}",
        "namespace": "{{.Values.bootstrap.namespace}}",
        "agentNamespace": "{{.Values.bootstrap.agentNamespace}}"
      },
      "webhookReceiverURL": "{{.Values.webhookReceiverURL}}",
      "githubURLPrefix": "{{.Values.githubURLPrefix}}",
      "gitClientTimeout": "{{.Values.gitClientTimeout}}"
    }



================================================
FILE: charts/fleet/templates/configmap_known_hosts.yaml
================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: known-hosts
data:
  known_hosts: |
    bitbucket.org ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBPIQmuzMBuKdWeF4+a2sjSSpBK0iqitSQ+5BM9KhpexuGt20JpTVM7u5BDZngncgrqDMbWdxMWWOGtZ9UgbqgZE=
    bitbucket.org ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIazEu89wgQZ4bqs3d63QSMzYVa0MuJ2e2gKTKqu+UUO
    bitbucket.org ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDQeJzhupRu0u0cdegZIa8e86EG2qOCsIsD1Xw0xSeiPDlCr7kq97NLmMbpKTX6Esc30NuoqEEHCuc7yWtwp8dI76EEEB1VqY9QJq6vk+aySyboD5QF61I/1WeTwu+deCbgKMGbUijeXhtfbxSxm6JwGrXrhBdofTsbKRUsrN1WoNgUa8uqN1Vx6WAJw1JHPhglEGGHea6QICwJOAr/6mrui/oB7pkaWKHj3z7d1IC4KWLtY47elvjbaTlkN04Kc/5LFEirorGYVbt15kAUlqGM65pk6ZBxtaO3+30LVlORZkxOh+LKL/BvbZ/iRNhItLqNyieoQj/uh/7Iv4uyH/cV/0b4WDSd3DptigWq84lJubb9t/DnZlrJazxyDCulTmKdOR7vs9gMTo+uoIrPSb8ScTtvw65+odKAlBj59dhnVp9zd7QUojOpXlL62Aw56U4oO+FALuevvMjiWeavKhJqlR7i5n9srYcrNV7ttmDw7kf/97P5zauIhxcjX+xHv4M=
    github.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=
    github.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl
    github.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=
    gitlab.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBFSMqzJeV9rUzU4kWitGjeR4PWSa29SPqJ1fVkhtj3Hw9xjLVXVYrU9QlYWrOLXBpQ6KWjbjTDTdDkoohFzgbEY=
    gitlab.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAfuCHKVTjquxvt6CM6tdG4SLp1Btn/nOeHHE5UOzRdf
    gitlab.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsj2bNKTBSpIYDEGk9KxsGh3mySTRgMtXL583qmBpzeQ+jqCMRgBqB98u3z++J1sKlXHWfM9dyhSevkMwSbhoR8XIq/U0tCNyokEi/ueaBMCvbcTHhO7FcwzY92WK4Yt0aGROY5qX2UKSeOvuP4D6TPqKF1onrSzH9bx9XUf2lEdWT/ia1NEKjunUqu1xOB/StKDHMoX4/OKyIzuS0q/T1zOATthvasJFoPrAjkohTyaDUz2LN5JoH839hViyEG82yB+MjcFV5MU3N1l1QL3cVUCh93xSaua1N85qivl+siMkPGbO5xR/En4iEY6K2XPASUEMaieWVNTRCtJ4S8H+9
    ssh.dev.azure.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H
    vs-ssh.visualstudio.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H



================================================
FILE: charts/fleet/templates/deployment.yaml
================================================
{{- $shards := list (dict "id" "" "nodeSelector" dict) -}}
{{- $uniqueShards := list -}}
{{- if .Values.shards -}}
  {{- range .Values.shards -}}
    {{- if not (has .id $uniqueShards) -}}
      {{- $shards = append $shards . -}}
      {{- $uniqueShards = append $uniqueShards .id -}}
    {{- end -}}
  {{- end -}}
{{- end -}}

{{ range $shard := $shards }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "fleet-controller{{if $shard.id }}-shard-{{ $shard.id }}{{end}}"
spec:
  replicas: {{ $.Values.controller.replicas }}
  selector:
    matchLabels:
      app: fleet-controller
  template:
    metadata:
      labels:
        app: fleet-controller
        fleet.cattle.io/shard-id: "{{ $shard.id }}"
        {{- if empty $shard.id }}
        fleet.cattle.io/shard-default: "true"
        {{- end }}
{{- if and $.Values.extraLabels $.Values.extraLabels.fleetController }}
{{ toYaml $.Values.extraLabels.fleetController | indent 8 }}
{{- end }}
{{- if and $.Values.extraAnnotations $.Values.extraAnnotations.fleetController }}
      annotations:
{{ toYaml $.Values.extraAnnotations.fleetController | indent 8 }}
{{- end }}
    spec:
      containers:
      - env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        {{- if $.Values.clusterEnqueueDelay }}
        - name: FLEET_CLUSTER_ENQUEUE_DELAY
          value: {{ $.Values.clusterEnqueueDelay }}
        {{- end }}
        {{- if $.Values.proxy }}
        - name: HTTP_PROXY
          value: {{ $.Values.proxy }}
        - name: HTTPS_PROXY
          value: {{ $.Values.proxy }}
        - name: NO_PROXY
          value: {{ $.Values.noProxy }}
        {{- end }}
        {{- if $.Values.leaderElection.leaseDuration }}
        - name: CATTLE_ELECTION_LEASE_DURATION
          value: {{$.Values.leaderElection.leaseDuration}}
        {{- end }}
        {{- if $.Values.leaderElection.retryPeriod }}
        - name: CATTLE_ELECTION_RETRY_PERIOD
          value: {{$.Values.leaderElection.retryPeriod}}
        {{- end }}
        {{- if $.Values.leaderElection.renewDeadline }}
        - name: CATTLE_ELECTION_RENEW_DEADLINE
          value: {{$.Values.leaderElection.renewDeadline}}
        {{- end }}
        {{- if $.Values.debug }}
        - name: CATTLE_DEV_MODE
          value: "true"
        {{- end }}
        {{- if $.Values.controller.reconciler.workers.bundle }}
        - name: BUNDLE_RECONCILER_WORKERS
          value: {{ quote $.Values.controller.reconciler.workers.bundle }}
        {{- end }}
        {{- if $.Values.controller.reconciler.workers.bundledeployment }}
        - name: BUNDLEDEPLOYMENT_RECONCILER_WORKERS
          value: {{ quote $.Values.controller.reconciler.workers.bundledeployment }}
        {{- end }}
        {{- if $.Values.controller.reconciler.workers.cluster }}
        - name: CLUSTER_RECONCILER_WORKERS
          value: {{ quote $.Values.controller.reconciler.workers.cluster }}
        {{- end }}
        {{- if $.Values.controller.reconciler.workers.clustergroup }}
        - name: CLUSTERGROUP_RECONCILER_WORKERS
          value: {{ quote $.Values.controller.reconciler.workers.clustergroup }}
        {{- end }}
        {{- if $.Values.controller.reconciler.workers.imagescan }}
        - name: IMAGESCAN_RECONCILER_WORKERS
          value: {{ quote $.Values.controller.reconciler.workers.imagescan }}
        {{- end }}
        {{- if $.Values.controller.reconciler.workers.schedule }}
        - name: SCHEDULE_RECONCILER_WORKERS
          value: {{ quote $.Values.controller.reconciler.workers.schedule }}
        {{- end }}
{{- if $.Values.extraEnv }}
{{ toYaml $.Values.extraEnv | indent 8}}
{{- end }}
        image: '{{ template "system_default_registry" $ }}{{ $.Values.image.repository }}:{{ $.Values.image.tag }}'
        name: fleet-controller
        imagePullPolicy: "{{ $.Values.image.imagePullPolicy }}"
        {{- if $.Values.metrics.enabled }}
        ports:
        - containerPort: 8080
          name: metrics
        {{- end }}
        command:
        - fleetcontroller
        {{- if $shard.id }}
        - --shard-id
        - {{ quote $shard.id }}
        {{- end }}
        {{- if not $.Values.metrics.enabled }}
        - --disable-metrics
        {{- end }}
        {{- if $.Values.debug }}
        - --debug
        - --debug-level
        - {{ quote $.Values.debugLevel }}
        {{- end }}
        {{- if not $.Values.disableSecurityContext }}
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          privileged: false
          capabilities:
            drop:
            - ALL
        {{- end }}
        volumeMounts:
          - mountPath: /tmp
            name: tmp
      {{- if not $shard.id }} # Only deploy cleanup and agent management through sharding-less deployment
      - env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        {{- if $.Values.debug }}
        - name: CATTLE_DEV_MODE
          value: "true"
        {{- end }}
        {{- if $.Values.leaderElection.leaseDuration }}
        - name: CATTLE_ELECTION_LEASE_DURATION
          value: {{$.Values.leaderElection.leaseDuration}}
        {{- end }}
        {{- if $.Values.leaderElection.retryPeriod }}
        - name: CATTLE_ELECTION_RETRY_PERIOD
          value: {{$.Values.leaderElection.retryPeriod}}
        {{- end }}
        {{- if $.Values.leaderElection.renewDeadline }}
        - name: CATTLE_ELECTION_RENEW_DEADLINE
          value: {{$.Values.leaderElection.renewDeadline}}
        {{- end }}
        image: '{{ template "system_default_registry" $ }}{{ $.Values.image.repository }}:{{ $.Values.image.tag }}'
        name: fleet-cleanup
        imagePullPolicy: "{{ $.Values.image.imagePullPolicy }}"
        command:
        - fleetcontroller
        - cleanup
        {{- if $.Values.debug }}
        - --debug
        - --debug-level
        - {{ quote $.Values.debugLevel }}
        {{- end }}
        {{- if not $.Values.disableSecurityContext }}
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          privileged: false
          capabilities:
            drop:
            - ALL
        {{- end }}
      - env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: FLEET_PROPAGATE_DEBUG_SETTINGS_TO_AGENTS
          value: {{ quote $.Values.propagateDebugSettingsToAgents }}
        - name: FLEET_DEBUG_DISABLE_SECURITY_CONTEXT
          value: {{ quote $.Values.disableSecurityContext }}
        {{- if $.Values.debug }}
        - name: CATTLE_DEV_MODE
          value: "true"
        {{- end }}
        {{- if $.Values.leaderElection.leaseDuration }}
        - name: CATTLE_ELECTION_LEASE_DURATION
          value: {{$.Values.leaderElection.leaseDuration}}
        {{- end }}
        {{- if $.Values.leaderElection.retryPeriod }}
        - name: CATTLE_ELECTION_RETRY_PERIOD
          value: {{$.Values.leaderElection.retryPeriod}}
        {{- end }}
        {{- if $.Values.leaderElection.renewDeadline }}
        - name: CATTLE_ELECTION_RENEW_DEADLINE
          value: {{$.Values.leaderElection.renewDeadline}}
        {{- end }}
        {{- if $.Values.agent.replicas }}
        - name: FLEET_AGENT_REPLICA_COUNT
          value: "{{ $.Values.agent.replicas }}"
        {{- end }}
        {{- if $.Values.agent.leaderElection.leaseDuration }}
        - name: FLEET_AGENT_ELECTION_LEASE_DURATION
          value: {{$.Values.agent.leaderElection.leaseDuration}}
        {{- end }}
        {{- if $.Values.agent.leaderElection.retryPeriod }}
        - name: FLEET_AGENT_ELECTION_RETRY_PERIOD
          value: {{$.Values.agent.leaderElection.retryPeriod}}
        {{- end }}
        {{- if $.Values.agent.leaderElection.renewDeadline }}
        - name: FLEET_AGENT_ELECTION_RENEW_DEADLINE
          value: {{$.Values.agent.leaderElection.renewDeadline}}
        {{- end }}
{{- if $.Values.extraEnv }}
{{ toYaml $.Values.extraEnv | indent 8}}
{{- end }}
        image: '{{ template "system_default_registry" $ }}{{ $.Values.image.repository }}:{{ $.Values.image.tag }}'
        name: fleet-agentmanagement
        imagePullPolicy: "{{ $.Values.image.imagePullPolicy }}"
        command:
        - fleetcontroller
        - agentmanagement
        {{- if not $.Values.bootstrap.enabled }}
        - --disable-bootstrap
        {{- end }}
        {{- if $.Values.debug }}
        - --debug
        - --debug-level
        - {{ quote $.Values.debugLevel }}
        {{- end }}
        {{- if not $.Values.disableSecurityContext }}
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          privileged: false
          capabilities:
            drop:
            - ALL
        {{- end }}
      {{- end }}
      volumes:
        - name: tmp
          emptyDir: {}

      serviceAccountName: fleet-controller
      nodeSelector: {{ include "linux-node-selector" $shard.id | nindent 8 }}
{{- if $.Values.nodeSelector }}
{{ toYaml $.Values.nodeSelector | indent 8 }}
{{- end }}
{{- if $shard.nodeSelector -}}
{{- range $key, $value := $shard.nodeSelector }}
{{ $key | indent 8}}: {{ $value }}
{{- end }}
{{- end }}
      tolerations: {{ include "linux-node-tolerations" $shard.id | nindent 8 }}
{{- if $.Values.tolerations }}
{{ toYaml $.Values.tolerations | indent 8 }}
{{- end }}
      {{- with $.Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with $.Values.resources }}
      resources:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- if $.Values.priorityClassName }}
      priorityClassName: "{{$.Values.priorityClassName}}"
      {{- end }}

{{- if not $.Values.disableSecurityContext }}
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
{{- end }}
---
{{- end }}



================================================
FILE: charts/fleet/templates/deployment_gitjob.yaml
================================================
{{- $shards := list (dict "id" "" "nodeSelector" dict) -}}
{{- $uniqueShards := list -}}
{{- if .Values.shards -}}
  {{- range .Values.shards -}}
    {{- if not (has .id $uniqueShards) -}}
      {{- $shards = append $shards . -}}
      {{- $uniqueShards = append $uniqueShards .id -}}
    {{- end -}}
  {{- end -}}
{{- end -}}

{{ range $shard := $shards }}
{{- if $.Values.gitops.enabled }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "gitjob{{if $shard.id }}-shard-{{ $shard.id }}{{end}}"
spec:
  replicas: {{ $.Values.gitjob.replicas }}
  selector:
    matchLabels:
      app: "gitjob"
  template:
    metadata:
      labels:
        app: "gitjob"
        fleet.cattle.io/shard-id: "{{ $shard.id }}"
        {{- if empty $shard.id }}
        fleet.cattle.io/shard-default: "true"
        {{- end }}
{{- if and $.Values.extraLabels $.Values.extraLabels.gitjob }}
{{ toYaml $.Values.extraLabels.gitjob | indent 8 }}
{{- end }}
{{- if and $.Values.extraAnnotations $.Values.extraAnnotations.gitjob }}
      annotations:
{{ toYaml $.Values.extraAnnotations.gitjob | indent 8 }}
{{- end }}
    spec:
      serviceAccountName: gitjob
      containers:
        - image: "{{ template "system_default_registry" $ }}{{ $.Values.image.repository }}:{{ $.Values.image.tag }}"
          name: gitjob
          {{- if $.Values.metrics.enabled }}
          ports:
          - containerPort: 8081
            name: metrics
          {{- end }}
          args:
          - fleetcontroller
          - gitjob
          - --gitjob-image
          - "{{ template "system_default_registry" $ }}{{ $.Values.image.repository }}:{{ $.Values.image.tag }}"
          {{- if $.Values.debug }}
          - --debug
          - --debug-level
          - {{ quote $.Values.debugLevel }}
          {{- end }}
          {{- if $shard.id }}
          - --shard-id
          - {{ quote $shard.id }}
          {{- end }}
          {{- if $shard.nodeSelector }}
          - --shard-node-selector
          - {{ toJson $shard.nodeSelector | squote }}
          {{- end }}
          {{- if not $.Values.metrics.enabled }}
          - --disable-metrics
          {{- end }}
          {{- if $.Values.insecureSkipHostKeyChecks }}
          - --insecure-skip-host-key-checks
          {{- end }}
          env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          {{- if $.Values.leaderElection.leaseDuration }}
            - name: CATTLE_ELECTION_LEASE_DURATION
              value: {{$.Values.leaderElection.leaseDuration}}
          {{- end }}
          {{- if $.Values.leaderElection.retryPeriod }}
            - name: CATTLE_ELECTION_RETRY_PERIOD
              value: {{$.Values.leaderElection.retryPeriod}}
          {{- end }}
          {{- if $.Values.leaderElection.renewDeadline }}
            - name: CATTLE_ELECTION_RENEW_DEADLINE
              value: {{$.Values.leaderElection.renewDeadline}}
          {{- end }}
          {{- if $.Values.proxy }}
            - name: HTTP_PROXY
              value: {{ $.Values.proxy }}
            - name: HTTPS_PROXY
              value: {{ $.Values.proxy }}
            - name: NO_PROXY
              value: {{ $.Values.noProxy }}
          {{- end }}
          {{- if $.Values.gitops.syncPeriod }}
            - name: GITREPO_SYNC_PERIOD
              value: {{ quote $.Values.gitops.syncPeriod }}
          {{- end }}
          {{- if $.Values.controller.reconciler.workers.gitrepo }}
            - name: GITREPO_RECONCILER_WORKERS
              value: {{ quote $.Values.controller.reconciler.workers.gitrepo }}
          {{- end }}
{{- if $.Values.extraEnv }}
{{ toYaml $.Values.extraEnv | indent 12}}
{{- end }}
          {{- if $.Values.debug }}
            - name: CATTLE_DEV_MODE
              value: "true"
          {{- end }}
          {{- if not $.Values.disableSecurityContext }}
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            privileged: false
            capabilities:
                drop:
                - ALL
          {{- end }}
          volumeMounts:
            - mountPath: /tmp
              name: tmp
      nodeSelector: {{ include "linux-node-selector" $shard.id | nindent 8 }}
{{- if $.Values.nodeSelector }}
{{ toYaml $.Values.nodeSelector | indent 8 }}
{{- end }}
{{- if $shard.nodeSelector -}}
{{- range $key, $value := $shard.nodeSelector }}
{{ $key | indent 8}}: {{ $value }}
{{- end }}
{{- end }}
      tolerations: {{ include "linux-node-tolerations" $shard.id | nindent 8 }}
{{- if $.Values.tolerations }}
{{ toYaml $.Values.tolerations | indent 8 }}
{{- end }}
      {{- with $.Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with $.Values.resources }}
      resources:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- if $.Values.priorityClassName }}
      priorityClassName: "{{$.Values.priorityClassName}}"
      {{- end }}

{{- if not $.Values.disableSecurityContext }}
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
{{- end }}
      volumes:
        - name: tmp
          emptyDir: {}
{{- end }}
---
{{- end }}



================================================
FILE: charts/fleet/templates/deployment_helmops.yaml
================================================
{{- if $.Values.helmops.enabled }}
{{- $shards := list (dict "id" "" "nodeSelector" dict) -}}
{{- $uniqueShards := list -}}
{{- if .Values.shards -}}
  {{- range .Values.shards -}}
    {{- if not (has .id $uniqueShards) -}}
      {{- $shards = append $shards . -}}
      {{- $uniqueShards = append $uniqueShards .id -}}
    {{- end -}}
  {{- end -}}
{{- end -}}

{{ range $shard := $shards }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "helmops{{if $shard.id }}-shard-{{ $shard.id }}{{end}}"
spec:
  replicas: {{ $.Values.helmops.replicas }}
  selector:
    matchLabels:
      app: "helmops"
  template:
    metadata:
      labels:
        app: "helmops"
        fleet.cattle.io/shard-id: "{{ $shard.id }}"
        {{- if empty $shard.id }}
        fleet.cattle.io/shard-default: "true"
        {{- end }}
{{- if and $.Values.extraLabels $.Values.extraLabels.helmops }}
{{ toYaml $.Values.extraLabels.helmops | indent 8 }}
{{- end }}
{{- if and $.Values.extraAnnotations $.Values.extraAnnotations.helmops }}
      annotations:
{{ toYaml $.Values.extraAnnotations.helmops | indent 8 }}
{{- end }}
    spec:
      serviceAccountName: helmops
      containers:
        - image: "{{ template "system_default_registry" $ }}{{ $.Values.image.repository }}:{{ $.Values.image.tag }}"
          name: helmops
          {{- if $.Values.metrics.enabled }}
          ports:
          - containerPort: 8081
            name: metrics
          {{- end }}
          args:
          - fleetcontroller
          - helmops
          {{- if $.Values.debug }}
          - --debug
          - --debug-level
          - {{ quote $.Values.debugLevel }}
          {{- end }}
          {{- if $shard.id }}
          - --shard-id
          - {{ quote $shard.id }}
          {{- end }}
          {{- if not $.Values.metrics.enabled }}
          - --disable-metrics
          {{- end }}
          env:
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          {{- if $.Values.leaderElection.leaseDuration }}
            - name: CATTLE_ELECTION_LEASE_DURATION
              value: {{$.Values.leaderElection.leaseDuration}}
          {{- end }}
          {{- if $.Values.leaderElection.retryPeriod }}
            - name: CATTLE_ELECTION_RETRY_PERIOD
              value: {{$.Values.leaderElection.retryPeriod}}
          {{- end }}
          {{- if $.Values.leaderElection.renewDeadline }}
            - name: CATTLE_ELECTION_RENEW_DEADLINE
              value: {{$.Values.leaderElection.renewDeadline}}
          {{- end }}
          {{- if $.Values.proxy }}
            - name: HTTP_PROXY
              value: {{ $.Values.proxy }}
            - name: HTTPS_PROXY
              value: {{ $.Values.proxy }}
            - name: NO_PROXY
              value: {{ $.Values.noProxy }}
          {{- end }}
          {{- if $.Values.controller.reconciler.workers.gitrepo }}
            - name: HELMOPS_RECONCILER_WORKERS
              value: {{ quote $.Values.controller.reconciler.workers.gitrepo }}
          {{- end }}
{{- if $.Values.extraEnv }}
{{ toYaml $.Values.extraEnv | indent 12}}
{{- end }}
          {{- if $.Values.debug }}
            - name: CATTLE_DEV_MODE
              value: "true"
          {{- end }}
          {{- if not $.Values.disableSecurityContext }}
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            privileged: false
            capabilities:
                drop:
                - ALL
          {{- end }}
          volumeMounts:
            - mountPath: /tmp
              name: tmp
      nodeSelector: {{ include "linux-node-selector" $shard.id | nindent 8 }}
{{- if $.Values.nodeSelector }}
{{ toYaml $.Values.nodeSelector | indent 8 }}
{{- end }}
{{- if $shard.nodeSelector -}}
{{- range $key, $value := $shard.nodeSelector }}
{{ $key | indent 8}}: {{ $value }}
{{- end }}
{{- end }}
      tolerations: {{ include "linux-node-tolerations" $shard.id | nindent 8 }}
{{- if $.Values.tolerations }}
{{ toYaml $.Values.tolerations | indent 8 }}
{{- end }}
      {{- with $.Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with $.Values.resources }}
      resources:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- if $.Values.priorityClassName }}
      priorityClassName: "{{$.Values.priorityClassName}}"
      {{- end }}

{{- if not $.Values.disableSecurityContext }}
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
{{- end }}
      volumes:
        - name: tmp
          emptyDir: {}
---
{{- end }}
{{- end }}



================================================
FILE: charts/fleet/templates/job_cleanup_clusterregistrations.yaml
================================================
{{- if .Values.migrations.clusterRegistrationCleanup }}
---
apiVersion: batch/v1
kind: Job
metadata:
  name: fleet-cleanup-clusterregistrations
  annotations:
    "helm.sh/hook": post-install, post-upgrade
    "helm.sh/hook-delete-policy": hook-succeeded, before-hook-creation
spec:
  template:
    metadata:
      labels:
        app: fleet-job
    spec:
      serviceAccountName: fleet-controller
      restartPolicy: Never
      securityContext:
        runAsNonRoot: true
        runAsGroup: 1000
        runAsUser: 1000
      containers:
      - name: cleanup
        image: "{{ template "system_default_registry" . }}{{.Values.image.repository}}:{{.Values.image.tag}}"
        imagePullPolicy: {{ .Values.global.imagePullPolicy }}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: false
          privileged: false
        command:
        - fleet
        args:
        - cleanup
        - clusterregistration
      nodeSelector: {{ include "linux-node-selector" . | nindent 8 }}
      tolerations: {{ include "linux-node-tolerations" . | nindent 8 }}
{{- if $.Values.tolerations }}
{{ toYaml $.Values.tolerations | indent 8 }}
{{- end }}
  backoffLimit: 1
{{- end }}



================================================
FILE: charts/fleet/templates/job_cleanup_gitrepojobs.yaml
================================================
{{- if and .Values.migrations.gitrepoJobsCleanup .Values.gitops.enabled }}
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: fleet-cleanup-gitrepo-jobs
spec:
  schedule: "@daily"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: fleet-job
        spec:
          serviceAccountName: gitjob
          restartPolicy: Never
          securityContext:
            runAsNonRoot: true
            runAsGroup: 1000
            runAsUser: 1000
          containers:
          - name: cleanup
            image: "{{ template "system_default_registry" . }}{{.Values.image.repository}}:{{.Values.image.tag}}"
            imagePullPolicy: {{ .Values.global.imagePullPolicy }}
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                - ALL
              readOnlyRootFilesystem: false
              privileged: false
            command:
            - fleet
            args:
            - cleanup
            - gitjob
          nodeSelector: {{ include "linux-node-selector" . | nindent 12 }}
          tolerations: {{ include "linux-node-tolerations" . | nindent 12 }}
{{- if $.Values.tolerations }}
{{ toYaml $.Values.tolerations | indent 12 }}
{{- end }}
      backoffLimit: 1
{{- end }}



================================================
FILE: charts/fleet/templates/rbac.yaml
================================================
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fleet-controller
rules:
- apiGroups:
  - fleet.cattle.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - namespaces
  - serviceaccounts
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - secrets
  - configmaps
  verbs:
  - '*'
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - clusterroles
  - clusterrolebindings
  - roles
  - rolebindings
  verbs:
  - '*'
- apiGroups:
    - ""
  resources:
    - 'events'
  verbs:
    - '*'
- apiGroups:
    - "apps"
  resources:
    - 'deployments'
  verbs:
    - 'watch'
    - 'list'
    - 'get'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fleet-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fleet-controller
subjects:
- kind: ServiceAccount
  name: fleet-controller
  namespace: {{.Release.Namespace}}

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: fleet-controller
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - '*'
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - '*'

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: fleet-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: fleet-controller
subjects:
- kind: ServiceAccount
  name: fleet-controller

{{- if .Values.bootstrap.enabled }}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fleet-controller-bootstrap
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fleet-controller-bootstrap
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fleet-controller-bootstrap
subjects:
- kind: ServiceAccount
  name: fleet-controller-bootstrap
  namespace: {{.Release.Namespace}}
{{- end }}



================================================
FILE: charts/fleet/templates/rbac_gitjob.yaml
================================================
{{- if .Values.gitops.enabled }}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gitjob
rules:
  - apiGroups:
      - "batch"
    resources:
      - 'jobs'
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - 'pods'
    verbs:
      - 'list'
      - 'get'
      - 'watch'
  - apiGroups:
      - ""
    resources:
      - 'secrets'
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - 'configmaps'
    verbs:
      - '*'
  - apiGroups:
      - "fleet.cattle.io"
    resources:
      - "gitrepos"
      - "gitrepos/status"
    verbs:
      - "*"
  - apiGroups:
      - "fleet.cattle.io"
    resources:
      - "gitreporestrictions"
    verbs:
      - list
      - get
      - watch
  - apiGroups:
      - "fleet.cattle.io"
    resources:
      - "bundles"
      - "bundledeployments"
      - "imagescans"
      - "contents"
    verbs:
      - list
      - delete
      - get
      - watch
      - update
  - apiGroups:
      - ""
    resources:
      - 'events'
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
    verbs:
      - "create"
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - "create"
      - "delete"
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - roles
    verbs:
      - escalate
      - create
      - bind
      - get
      - list
      - watch
      - update
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - rolebindings
    verbs:
      - create
      - get
      - list
      - watch
      - update
  - apiGroups:
      - "apps"
    resources:
      - 'deployments'
    verbs:
      - 'list'
      - 'get'
      - 'watch'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: gitjob-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: gitjob
subjects:
  - kind: ServiceAccount
    name: gitjob
    namespace: {{ .Release.Namespace }}

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: gitjob
rules:
  - apiGroups:
      - "coordination.k8s.io"
    resources:
      - "leases"
    verbs:
      - "*"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: gitjob
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: gitjob
subjects:
  - kind: ServiceAccount
    name: gitjob
{{- end }}



================================================
FILE: charts/fleet/templates/rbac_helmops.yaml
================================================
{{- if $.Values.helmops.enabled }}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: helmops
rules:
  - apiGroups:
      - ""
    resources:
      - 'secrets'
    verbs:
      - "create"
      - "list"
      - "watch"
  - apiGroups:
      - ""
    resources:
      - 'configmaps'
    verbs:
      - '*'
  - apiGroups:
      - "fleet.cattle.io"
    resources:
      - "helmops"
      - "helmops/status"
    verbs:
      - "*"
  - apiGroups:
      - "fleet.cattle.io"
    resources:
      - "bundles"
      - "bundledeployments"
    verbs:
      - list
      - delete
      - get
      - watch
      - update
      - patch
      - create
  - apiGroups:
      - ""
    resources:
      - 'events'
    verbs:
      - "create"
      - "patch"
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
    verbs:
      - "create"
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - "create"
      - "delete"

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: helmops-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: helmops
subjects:
  - kind: ServiceAccount
    name: helmops
    namespace: {{ .Release.Namespace }}

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: helmops
rules:
  - apiGroups:
      - "coordination.k8s.io"
    resources:
      - "leases"
    verbs:
      - "*"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: helmops
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: helmops
subjects:
  - kind: ServiceAccount
    name: helmops
{{- end }}



================================================
FILE: charts/fleet/templates/service.yaml
================================================
{{- if .Values.metrics.enabled }}
{{- $shards := list (dict "id" "" "nodeSelector" dict) -}}
{{- $uniqueShards := list -}}
{{- if .Values.shards -}}
  {{- range .Values.shards -}}
    {{- if not (has .id $uniqueShards) -}}
      {{- $shards = append $shards . -}}
      {{- $uniqueShards = append $uniqueShards .id -}}
    {{- end -}}
  {{- end -}}
{{- end -}}

{{ range $shard := $shards }}
apiVersion: v1
kind: Service
metadata:
  name: "monitoring-fleet-controller{{if  $shard.id  }}-shard-{{  $shard.id   }}{{end}}"
  labels:
    app: fleet-controller
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
    protocol: TCP
    name: metrics
  selector:
    app: fleet-controller
    {{- if empty  $shard.id   }}
    fleet.cattle.io/shard-default: "true"
    {{- else }}
    fleet.cattle.io/shard-id: "{{  $shard.id   }}"
    {{- end }}
---
{{- end }}
{{- end }}



================================================
FILE: charts/fleet/templates/service_gitjob.yaml
================================================
{{- if .Values.gitops.enabled }}
apiVersion: v1
kind: Service
metadata:
  name: gitjob
spec:
  ports:
    - name: http-80
      port: 80
      protocol: TCP
      targetPort: 8080
  selector:
    app: "gitjob"
---
{{- if .Values.metrics.enabled }}
{{- $shards := list (dict "id" "" "nodeSelector" dict) -}}
{{- $uniqueShards := list -}}
{{- if .Values.shards -}}
  {{- range .Values.shards -}}
    {{- if not (has .id $uniqueShards) -}}
      {{- $shards = append $shards . -}}
      {{- $uniqueShards = append $uniqueShards .id -}}
    {{- end -}}
  {{- end -}}
{{- end -}}

{{ range $shard := $shards }}
apiVersion: v1
kind: Service
metadata:
  name: "monitoring-gitjob{{if $shard.id }}-shard-{{ $shard.id }}{{end}}"
  labels:
    app: gitjob
spec:
  type: ClusterIP
  ports:
  - port: 8081
    targetPort: 8081
    protocol: TCP
    name: metrics
  selector:
    app: gitjob
    {{- if empty $shard.id }}
    fleet.cattle.io/shard-default: "true"
    {{- else }}
    fleet.cattle.io/shard-id: "{{ $shard.id }}"
    {{- end }}
---
{{- end }}
{{- end }}
{{- end }}



================================================
FILE: charts/fleet/templates/serviceaccount.yaml
================================================
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fleet-controller

{{- if .Values.bootstrap.enabled }}
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fleet-controller-bootstrap
{{- end }}



================================================
FILE: charts/fleet/templates/serviceaccount_gitjob.yaml
================================================
{{- if .Values.gitops.enabled }}
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gitjob
{{- end }}



================================================
FILE: charts/fleet/templates/serviceaccount_helmops.yaml
================================================
{{- if $.Values.helmops.enabled }}
apiVersion: v1
kind: ServiceAccount
metadata:
  name: helmops
{{- end }}



================================================
FILE: charts/fleet/tests/affinity_test.yaml
================================================
suite: test affinity rules
templates:
  - deployment.yaml
  - deployment_gitjob.yaml
  - deployment_helmops.yaml
tests:
  - it: should set affinity rules
    set:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
                - key: disktype
                  operator: In
                  values:
                    - ssd
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: security
                    operator: In
                    values:
                      - S1
              topologyKey: "kubernetes.io/hostname"
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - fleet
              topologyKey: "kubernetes.io/hostname"
    asserts:
      - equal:
          path: spec.template.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].key
          value: disktype
      - equal:
          path: spec.template.spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution[0].labelSelector.matchExpressions[0].key
          value: security
      - equal:
          path: spec.template.spec.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution[0].labelSelector.matchExpressions[0].key
          value: app

  - it: should work without affinity rules
    set:
      affinity: {}
    asserts:
      - notExists:
          path: spec.template.spec.affinity



================================================
FILE: charts/fleet/tests/agent-leader-election.yaml
================================================
suite: leader election values test for fleet agent
templates:
  - deployment.yaml
tests:
  - it: should set the environment variable to the duration
    set:
      agent:
        leaderElection:
          leaseDuration: 60s
          retryPeriod: 5s
          renewDeadline: 10s
    asserts:
      - isKind:
          of: Deployment
      - equal:
          path: spec.template.spec.containers[?(@.name == 'fleet-agentmanagement')].env[?(@.name == 'FLEET_AGENT_ELECTION_LEASE_DURATION')].value
          value: "60s"
      - equal:
          path: spec.template.spec.containers[?(@.name == 'fleet-agentmanagement')].env[?(@.name == 'FLEET_AGENT_ELECTION_RETRY_PERIOD')].value
          value: "5s"
      - equal:
          path: spec.template.spec.containers[?(@.name == 'fleet-agentmanagement')].env[?(@.name == 'FLEET_AGENT_ELECTION_RENEW_DEADLINE')].value
          value: "10s"





================================================
FILE: charts/fleet/tests/agent_replica_count_test.yaml
================================================
suite: replica values test for fleet agent
templates:
  - deployment.yaml
tests:
  - it: should set the environment variable to the number of replicas
    set:
      agent.replicas: 3
    asserts:
      - isKind:
          of: Deployment
      - equal:
          path: spec.template.spec.containers[?(@.name == 'fleet-agentmanagement')].env[?(@.name == 'FLEET_AGENT_REPLICA_COUNT')].value
          value: "3"



================================================
FILE: charts/fleet/tests/extraAnnotations_test.yaml
================================================
suite: extraAnnotations tests
templates:
  - deployment.yaml
  - deployment_gitjob.yaml
  - deployment_helmops.yaml
tests:
  - it: should set extraAnnotations variables in fleet-controller deployment
    set:
      extraAnnotations.fleetController:
        test-annotation-1: testvalue1
        test-annotation-2: testvalue2
    template: deployment.yaml
    asserts:
      - hasDocuments:
          count: 1
      - isKind:
          of: Deployment
      - equal:
          path: spec.template.metadata.annotations.test-annotation-1
          value: testvalue1
      - equal:
          path: spec.template.metadata.annotations.test-annotation-2
          value: testvalue2

  - it: should not set extraAnnotations variables in gitjob
    set:
      extraAnnotations.fleetController:
        test-annotation-1: testvalue1
        test-annotation-2: testvalue2
    template: deployment_gitjob.yaml
    asserts:
      - hasDocuments:
          count: 1
      - isKind:
          of: Deployment
      - notEqual:
          path: spec.template.metadata.annotations.test-annotation-1
          value: testvalue1
      - notEqual:
          path: spec.template.metadata.annotations.test-annotation-2
          value: testvalue2

  - it: should not set extraAnnotation variables in helmops
    set:
      extraAnnotations.fleetController:
        test-annotation-1: testvalue1
        test-annotation-2: testvalue2
    template: deployment_helmops.yaml
    asserts:
      - hasDocuments:
          count: 1
      - isKind:
          of: Deployment
      - notEqual:
          path: spec.template.metadata.annotations.test-annotation-1
          value: testvalue1
      - notEqual:
          path: spec.template.metadata.annotations.test-annotation-2
          value: testvalue2

  - it: should set extraAnnotations variables in gitjob deployment
    set:
      extraAnnotations.gitjob:
        test-annotation-1: testvalue1
        test-annotation-2: testvalue2
      gitops:
        enabled: true
    template: deployment_gitjob.yaml
    asserts:
      - hasDocuments:
          count: 1
      - isKind:
          of: Deployment
      - equal:
          path: spec.template.metadata.annotations.test-annotation-1
          value: testvalue1
      - equal:
          path: spec.template.metadata.annotations.test-annotation-2
          value: testvalue2

  - it: should set extraAnnotations variables in helmops deployment
    set:
      extraAnnotations.helmops:
        test-annotation-1: testvalue1
        test-annotation-2: testvalue2
    template: deployment_helmops.yaml
    asserts:
      - hasDocuments:
          count: 1
      - isKind:
          of: Deployment
      - equal:
          path: spec.template.metadata.annotations.test-annotation-1
          value: testvalue1
      - equal:
          path: spec.template.metadata.annotations.test-annotation-2
          value: testvalue2

  - it: should not set extraAnnotations variables when empty
    set:
      extraAnnotations: {}
    asserts:
      - hasDocuments:
          count: 1
      - isKind:
          of: Deployment
      - notExists:
          path: spec.template.metadata.annotations



================================================
FILE: charts/fleet/tests/extraLabels_test.yaml
================================================
suite: extraLabels tests
tests:
  - it: should set extraLabels variables in fleet-controller deployment
    set:
      extraLabels.fleetController:
        test-label-1: testvalue1
        test-label-2: testvalue2
    template: deployment.yaml
    asserts:
      - hasDocuments:
          count: 1
      - isKind:
          of: Deployment
      - equal:
          path: spec.template.metadata.labels
          value:
            app: fleet-controller
            fleet.cattle.io/shard-default: "true"
            fleet.cattle.io/shard-id: ""
            test-label-1: testvalue1
            test-label-2: testvalue2
  - it: should set extraLabels variables in gitjob deployment
    set:
      extraLabels.gitjob:
        test-label-1: testvalue1
        test-label-2: testvalue2
      gitops:
        enabled: true
    template: deployment_gitjob.yaml
    asserts:
      - hasDocuments:
          count: 1
      - isKind:
          of: Deployment
      - equal:
          path: spec.template.metadata.labels
          value:
            app: gitjob
            fleet.cattle.io/shard-default: "true"
            fleet.cattle.io/shard-id: ""
            test-label-1: testvalue1
            test-label-2: testvalue2
  - it: should set extraLabels variables in helmops deployment
    set:
      extraLabels.helmops:
        test-label-1: testvalue1
        test-label-2: testvalue2
    template: deployment_helmops.yaml
    asserts:
      - hasDocuments:
          count: 1
      - isKind:
          of: Deployment
      - equal:
          path: spec.template.metadata.labels
          value:
            app: helmops
            fleet.cattle.io/shard-default: "true"
            fleet.cattle.io/shard-id: ""
            test-label-1: testvalue1
            test-label-2: testvalue2
  - it: should not set more labels in fleet-controller deployment when extraLabels is empty
    set:
      extraLabels: {}
    template: deployment.yaml
    asserts:
      - hasDocuments:
          count: 1
      - isKind:
          of: Deployment
      - equal:
          path: spec.template.metadata.labels
          value:
            app: fleet-controller
            fleet.cattle.io/shard-default: "true"
            fleet.cattle.io/shard-id: ""

  - it: should not set more labels in gitjob deployment when extraLabels is empty
    set:
      extraLabels: {}
    template: deployment_gitjob.yaml
    asserts:
      - hasDocuments:
          count: 1
      - isKind:
          of: Deployment
      - equal:
          path: spec.template.metadata.labels
          value:
            app: gitjob
            fleet.cattle.io/shard-default: "true"
            fleet.cattle.io/shard-id: ""

  - it: should not set more labels in helmops deployment when extraLabels is empty
    set:
      extraLabels: {}
    template: deployment_helmops.yaml
    asserts:
      - hasDocuments:
          count: 1
      - isKind:
          of: Deployment
      - equal:
          path: spec.template.metadata.labels
          value:
            app: helmops
            fleet.cattle.io/shard-default: "true"
            fleet.cattle.io/shard-id: ""



================================================
FILE: charts/fleet/tests/fleet_controller_replica_count_test.yaml
================================================
suite: replica values test for fleet controller
templates:
  - deployment.yaml
tests:
  - it: should set spec.replicas to the number of replicas specified in the controller's replicas field
    set:
      controller.replicas: 3
    asserts:
      - isKind:
          of: Deployment
      - equal:
          path: spec.replicas
          value: 3



================================================
FILE: charts/fleet/tests/gitjob_controller_replica_count_test.yaml
================================================
suite: replica values test for fleet controller
templates:
  - deployment_gitjob.yaml
tests:
  - it: should set spec.replicas to the number of replicas specified in the controller's replicas field
    set:
      gitjob.replicas: 3
    asserts:
      - isKind:
          of: Deployment
      - equal:
          path: spec.replicas
          value: 3



================================================
FILE: charts/fleet/tests/helmops_controller_replica_count_test.yaml
================================================
suite: replica values test for fleet controller
templates:
  - deployment_helmops.yaml
tests:
  - it: should set spec.replicas to the number of replicas specified in the controller's replicas field
    set:
      helmops.replicas: 3
      helmops.enabled: true
    asserts:
      - isKind:
          of: Deployment
      - equal:
          path: spec.replicas
          value: 3

  - it: should not render a document at all when helmops.enabled is false
    set:
      helmops.replicas: 3
      helmops.enabled: false
    asserts:
      - hasDocuments:
          count: 0



================================================
FILE: charts/fleet/tests/resources_test.yaml
================================================
suite: test resources
templates:
  - deployment.yaml
  - deployment_gitjob.yaml
  - deployment_helmops.yaml
tests:
  - it: should set resources
    set:
      resources:
        limits:
          cpu: 8000m
          memory: 8Gi
        requests:
          cpu: 250m
          memory: 768Mi
    asserts:
      - equal:
          path: spec.template.spec.resources.limits.cpu
          value: 8000m
      - equal:
          path: spec.template.spec.resources.limits.memory
          value: 8Gi
      - equal:
          path: spec.template.spec.resources.requests.cpu
          value: 250m
      - equal:
          path: spec.template.spec.resources.requests.memory
          value: 768Mi
  - it: should work without resources
    set:
      resources: {}
    asserts:
      - notExists:
          path: spec.template.spec.resources



================================================
FILE: charts/fleet-agent/README.md
================================================
## Fleet Agent Helm Chart

Every Fleet-managed downstream cluster will run an agent that communicates back to the Fleet controller. This agent is just another set of Kubernetes controllers running in the downstream cluster.

Standalone Fleet users use this chart for agent-initiated registration. For more details see [agent-initiated registration](https://fleet.rancher.io/cluster-registration#agent-initiated).
Fleet in Rancher does not use this chart, but creates the agent deployments programmatically.

The Fleet documentation is centralized in the [doc website](https://fleet.rancher.io/).


================================================
FILE: charts/fleet-agent/Chart.yaml
================================================
annotations:
  catalog.cattle.io/certified: rancher
  catalog.cattle.io/hidden: "true"
  catalog.cattle.io/kube-version: '>= 1.33.0-0 < 1.36.0-0'
  catalog.cattle.io/namespace: cattle-fleet-system
  catalog.cattle.io/os: linux
  catalog.cattle.io/permits-os: linux,windows
  catalog.cattle.io/rancher-version: '>= 2.14.0-0 < 2.15.0-0'
  catalog.cattle.io/release-name: fleet-agent
apiVersion: v2
appVersion: 0.0.0
description: Fleet Agent - GitOps at Scale
icon: https://charts.rancher.io/assets/logos/fleet.svg
name: fleet-agent
version: 0.0.0



================================================
FILE: charts/fleet-agent/values.yaml
================================================
image:
  os: "windows,linux"
  repository: rancher/fleet-agent
  tag: dev

# The public URL of the Kubernetes API server running the Fleet Controller must be set here
# Example: https://example.com:6443
apiServerURL: ""

# The pem encoded value of the CA of the Kubernetes API server running the Fleet Controller.
# If left empty it is assumed this Kubernetes API TLS is signed by a well known CA.
apiServerCA: ""

# Determines whether the agent should trust CA bundles from the operating system's trust store when connecting to a
# management cluster. True in `system-store` mode, false in `strict` mode.
agentTLSMode: "system-store"

# The amount of time that agents will wait before they clean up old Helm releases.
# A non-existent value or 0 will result in an interval of 15 minutes.
garbageCollectionInterval: "15m"

# The cluster registration value
token: ""

# Labels to add to the cluster upon registration only. They are not added after the fact.
# labels:
#   foo: bar

# The client ID of the cluster to associate with
clientID: ""

# The namespace of the cluster we are register with
clusterNamespace: ""

# The namespace containing the clusters registration secrets
systemRegistrationNamespace: cattle-fleet-clusters-system

# Please do not change the below setting unless you really know what you are doing
internal:
  systemNamespace: cattle-fleet-system
  managedReleaseName: fleet-agent

# The nodeSelector and tolerations for the agent deployment
fleetAgent:
  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## List of node taints to tolerate (requires Kubernetes >= 1.6)
  tolerations: []
  ## HostNetwork setting for the agent deployment.
  ## When set allows for provisioning of network related bundles (CNI configuration) in a cluster without CNI.
  hostNetwork: false
kubectl:
  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## List of node taints to tolerate (requires Kubernetes >= 1.6)
  tolerations:
  - key: node.cloudprovider.kubernetes.io/uninitialized
    operator: "Equal"
    value: "true"
    effect: NoSchedule

global:
  cattle:
    systemDefaultRegistry: ""

debug: false
debugLevel: 0
disableSecurityContext: false

## Fleet agent configuration
agent:
  replicas: 1
  reconciler:
    # The number of workers that are allowed for each type of reconciler
    workers:
      bundledeployment: "50"
      drift: "50"

## Leader election configuration
leaderElection:
  leaseDuration: 30s
  retryPeriod: 10s
  renewDeadline: 25s



================================================
FILE: charts/fleet-agent/ci/default-values.yaml
================================================
apiServerURL: "https://localhost"
apiServerCA: "abc"



================================================
FILE: charts/fleet-agent/templates/_helpers.tpl
================================================
{{- define "system_default_registry" -}}
{{- if .Values.global.cattle.systemDefaultRegistry -}}
{{- printf "%s/" .Values.global.cattle.systemDefaultRegistry -}}
{{- else -}}
{{- "" -}}
{{- end -}}
{{- end -}}

{{/*
Windows cluster will add default taint for linux nodes,
add below linux tolerations to workloads could be scheduled to those linux nodes
*/}}
{{- define "linux-node-tolerations" -}}
- key: "cattle.io/os"
  value: "linux"
  effect: "NoSchedule"
  operator: "Equal"
{{- end -}}

{{- define "linux-node-selector" -}}
kubernetes.io/os: linux
{{- end -}}


================================================
FILE: charts/fleet-agent/templates/configmap.yaml
================================================
kind: ConfigMap
apiVersion: v1
metadata:
  name: fleet-agent
data:
  config: |-
    {
      {{ if .Values.labels }}
      "labels":{{toJson .Values.labels}},
      {{ end }}
      "clientID":"{{.Values.clientID}}",
      {{ if .Values.garbageCollectionInterval }}
      "garbageCollectionInterval": "{{.Values.garbageCollectionInterval}}",
      {{ end }}
      "agentTLSMode": "{{.Values.agentTLSMode}}"
    }



================================================
FILE: charts/fleet-agent/templates/deployment.yaml
================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fleet-agent
spec:
  replicas: {{ .Values.agent.replicas }}
  selector:
    matchLabels:
      app: fleet-agent
  template:
    metadata:
      labels:
        app: fleet-agent
    spec:
      containers:
      - env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        {{- if $.Values.agent.reconciler.workers.bundledeployment }}
        - name: BUNDLEDEPLOYMENT_RECONCILER_WORKERS
          value: {{ quote $.Values.agent.reconciler.workers.bundledeployment }}
        {{- end }}
        {{- if $.Values.agent.reconciler.workers.drift }}
        - name: DRIFT_RECONCILER_WORKERS
          value: {{ quote $.Values.agent.reconciler.workers.drift }}
        {{- end }}
        {{- if $.Values.leaderElection.leaseDuration }}
        - name: CATTLE_ELECTION_LEASE_DURATION
          value: {{$.Values.leaderElection.leaseDuration}}
        {{- end }}
        {{- if $.Values.leaderElection.retryPeriod }}
        - name: CATTLE_ELECTION_RETRY_PERIOD
          value: {{$.Values.leaderElection.retryPeriod}}
        {{- end }}
        {{- if $.Values.leaderElection.renewDeadline }}
        - name: CATTLE_ELECTION_RENEW_DEADLINE
          value: {{$.Values.leaderElection.renewDeadline}}
        {{- end }}
        image: '{{ template "system_default_registry" . }}{{.Values.image.repository}}:{{.Values.image.tag}}'
        name: fleet-agent
        command:
        - fleetagent
        {{- if .Values.debug }}
        - --debug
        - --debug-level
        - {{ quote .Values.debugLevel }}
        {{- end }}
        {{- if not .Values.disableSecurityContext }}
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          privileged: false
          capabilities:
            drop:
            - ALL
        {{- end }}
        volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /.kube
            name: kube
      volumes:
        - name: tmp
          emptyDir: {}
        - name: kube
          emptyDir: {}
      serviceAccountName: fleet-agent
      {{- if .Values.fleetAgent.hostNetwork }}
      hostNetwork: true
      {{- end }}
      nodeSelector: {{ include "linux-node-selector" . | nindent 8 }}
{{- if .Values.fleetAgent.nodeSelector }}
{{ toYaml .Values.fleetAgent.nodeSelector | indent 8 }}
{{- end }}
      tolerations: {{ include "linux-node-tolerations" . | nindent 8 }}
{{- if .Values.fleetAgent.tolerations }}
{{ toYaml .Values.fleetAgent.tolerations | indent 8 }}
{{- end }}
{{- if not .Values.disableSecurityContext }}
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
{{- end }}



================================================
FILE: charts/fleet-agent/templates/network_policy_allow_all.yaml
================================================
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-allow-all
  namespace: {{ .Values.internal.systemNamespace }}
spec:
  podSelector: {}
  ingress:
  - {}
  egress:
  - {}
  policyTypes:
  - Ingress
  - Egress



================================================
FILE: charts/fleet-agent/templates/rbac.yaml
================================================
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fleet-agent-system-fleet-agent-role
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
- nonResourceURLs:
  - "*"
  verbs:
  - "*"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fleet-agent-system-fleet-agent-role-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fleet-agent-system-fleet-agent-role
subjects:
- kind: ServiceAccount
  name: fleet-agent
  namespace: {{.Release.Namespace}}



================================================
FILE: charts/fleet-agent/templates/secret.yaml
================================================
apiVersion: v1
data:
  systemRegistrationNamespace: "{{b64enc .Values.systemRegistrationNamespace}}"
  clusterNamespace: "{{b64enc .Values.clusterNamespace}}"
  token: "{{b64enc .Values.token}}"
  apiServerURL: "{{b64enc .Values.apiServerURL}}"
  apiServerCA: "{{b64enc .Values.apiServerCA}}"
kind: Secret
metadata:
  name: fleet-agent-bootstrap



================================================
FILE: charts/fleet-agent/templates/serviceaccount.yaml
================================================
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fleet-agent



================================================
FILE: charts/fleet-agent/templates/validate.yaml
================================================
{{if ne .Release.Namespace .Values.internal.systemNamespace }}
{{ fail (printf "This chart must be installed in the %s namespace" .Values.internal.systemNamespace) }}
{{end}}

{{if ne .Release.Name .Values.internal.managedReleaseName }}
{{ fail (printf "This chart must be installed with release name %s" .Values.internal.managedReleaseName) }}
{{end}}

{{if not .Values.apiServerURL }}
{{ fail "apiServerURL is required to be set, and most likely also apiServerCA" }}
{{end}}



================================================
FILE: charts/fleet-crd/README.md
================================================
# Fleet CRD Helm Chart

Fleet CustomResourceDefinitions Helm chart is a requirement for the Fleet Helm Chart.

The Fleet documentation is centralized in the [doc website](https://fleet.rancher.io/).



================================================
FILE: charts/fleet-crd/Chart.yaml
================================================
annotations:
  catalog.cattle.io/certified: rancher
  catalog.cattle.io/hidden: "true"
  catalog.cattle.io/namespace: cattle-fleet-system
  catalog.cattle.io/os: linux
  catalog.cattle.io/permits-os: linux,windows
  catalog.cattle.io/release-name: fleet-crd
apiVersion: v2
appVersion: 0.0.0
description: Fleet CustomResourceDefinitions
icon: https://charts.rancher.io/assets/logos/fleet.svg
name: fleet-crd
version: 0.0.0



================================================
FILE: charts/fleet-crd/values.yaml
================================================
# This file is intentionally empty



================================================
FILE: charts/fleet-crd/templates/crds.yaml
================================================
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.19.0
  name: bundledeployments.fleet.cattle.io
spec:
  group: fleet.cattle.io
  names:
    kind: BundleDeployment
    listKind: BundleDeploymentList
    plural: bundledeployments
    singular: bundledeployment
  scope: Namespaced
  versions:
    - additionalPrinterColumns:
        - jsonPath: .status.display.deployed
          name: Deployed
          type: string
        - jsonPath: .status.display.monitored
          name: Monitored
          type: string
        - jsonPath: .status.conditions[?(@.type=="Ready")].message
          name: Status
          type: string
      name: v1alpha1
      schema:
        openAPIV3Schema:
          description: 'BundleDeployment is used internally by Fleet and should not
            be used directly.

            When a Bundle is deployed to a cluster an instance of a Bundle is called
            a

            BundleDeployment. A BundleDeployment represents the state of that Bundle
            on

            a specific cluster with its cluster-specific customizations. The Fleet
            agent

            is only aware of BundleDeployment resources that are created for the cluster

            the agent is managing.'
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
                of an object.

                Servers should convert recognized schemas to the latest internal value,
                and

                may reject unrecognized values.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource
                this object represents.

                Servers may infer this from the endpoint the client submits requests
                to.

                Cannot be updated.

                In CamelCase.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
            spec:
              properties:
                correctDrift:
                  description: CorrectDrift specifies how drift correction should
                    work.
                  properties:
                    enabled:
                      description: Enabled correct drift if true.
                      type: boolean
                    force:
                      description: Force helm rollback with --force option will be
                        used if true. This will try to recreate all resources in the
                        release.
                      type: boolean
                    keepFailHistory:
                      description: KeepFailHistory keeps track of failed rollbacks
                        in the helm history.
                      type: boolean
                  type: object
                dependsOn:
                  description: DependsOn refers to the bundles which must be ready
                    before this bundle can be deployed.
                  items:
                    properties:
                      name:
                        description: Name of the bundle.
                        nullable: true
                        type: string
                      selector:
                        description: Selector matching bundle's labels.
                        nullable: true
                        properties:
                          matchExpressions:
                            description: matchExpressions is a list of label selector
                              requirements. The requirements are ANDed.
                            items:
                              description: 'A label selector requirement is a selector
                                that contains values, a key, and an operator that

                                relates the key and values.'
                              properties:
                                key:
                                  description: key is the label key that the selector
                                    applies to.
                                  type: string
                                operator:
                                  description: 'operator represents a key''s relationship
                                    to a set of values.

                                    Valid operators are In, NotIn, Exists and DoesNotExist.'
                                  type: string
                                values:
                                  description: 'values is an array of string values.
                                    If the operator is In or NotIn,

                                    the values array must be non-empty. If the operator
                                    is Exists or DoesNotExist,

                                    the values array must be empty. This array is
                                    replaced during a strategic

                                    merge patch.'
                                  items:
                                    type: string
                                  type: array
                                  x-kubernetes-list-type: atomic
                              required:
                                - key
                                - operator
                              type: object
                            type: array
                            x-kubernetes-list-type: atomic
                          matchLabels:
                            additionalProperties:
                              type: string
                            description: 'matchLabels is a map of {key,value} pairs.
                              A single {key,value} in the matchLabels

                              map is equivalent to an element of matchExpressions,
                              whose key field is "key", the

                              operator is "In", and the values array contains only
                              "value". The requirements are ANDed.'
                            type: object
                        type: object
                        x-kubernetes-map-type: atomic
                    type: object
                  nullable: true
                  type: array
                deploymentID:
                  description: DeploymentID is the ID of the currently applied deployment.
                  nullable: true
                  type: string
                helmChartOptions:
                  description: 'HelmChartOptions is not nil and has the helm chart
                    config details when contents

                    should be downloaded from a helm chart'
                  properties:
                    helmOpInsecureSkipTLSVerify:
                      description: InsecureSkipTLSverify will use insecure HTTPS to
                        clone the helm app resource.
                      type: boolean
                    helmOpSecretName:
                      description: 'SecretName stores the secret name for storing
                        credentials when accessing

                        a remote helm repository defined in a HelmOp resource'
                      type: string
                  type: object
                ociContents:
                  description: OCIContents is true when this deployment's contents
                    is stored in an oci registry
                  type: boolean
                offSchedule:
                  description: 'OffSchedule specifies if the BundleDeployment can
                    be updated.

                    If set to true, will stop any BundleDeployments from being

                    updated.

                    If true, BundleDeployments will be marked as out of sync

                    when changes are detected.'
                  type: boolean
                options:
                  description: Options are the deployment options, that are currently
                    applied.
                  properties:
                    correctDrift:
                      description: CorrectDrift specifies how drift correction should
                        work.
                      properties:
                        enabled:
                          description: Enabled correct drift if true.
                          type: boolean
                        force:
                          description: Force helm rollback with --force option will
                            be used if true. This will try to recreate all resources
                            in the release.
                          type: boolean
                        keepFailHistory:
                          description: KeepFailHistory keeps track of failed rollbacks
                            in the helm history.
                          type: boolean
                      type: object
                    defaultNamespace:
                      description: 'DefaultNamespace is the namespace to use for resources
                        that do not

                        specify a namespace. This field is not used to enforce or
                        lock down

                        the deployment to a specific namespace.'
                      nullable: true
                      type: string
                    deleteCRDResources:
                      description: DeleteCRDResources deletes CRDs. Warning! this
                        will also delete all your Custom Resources.
                      type: boolean
                    deleteNamespace:
                      description: DeleteNamespace can be used to delete the deployed
                        namespace when removing the bundle
                      type: boolean
                    diff:
                      description: Diff can be used to ignore the modified state of
                        objects which are amended at runtime.
                      nullable: true
                      properties:
                        comparePatches:
                          description: ComparePatches match a resource and remove
                            fields, or the resource itself from the check for modifications.
                          items:
                            description: ComparePatch matches a resource and removes
                              fields from the check for modifications.
                            properties:
                              apiVersion:
                                description: APIVersion is the apiVersion of the resource
                                  to match.
                                nullable: true
                                type: string
                              jsonPointers:
                                description: JSONPointers ignore diffs at a certain
                                  JSON path.
                                items:
                                  type: string
                                nullable: true
                                type: array
                              kind:
                                description: Kind is the kind of the resource to match.
                                nullable: true
                                type: string
                              name:
                                description: Name is the name of the resource to match.
                                nullable: true
                                type: string
                              namespace:
                                description: Namespace is the namespace of the resource
                                  to match.
                                nullable: true
                                type: string
                              operations:
                                description: Operations remove a JSON path from the
                                  resource.
                                items:
                                  description: 'Operation of a ComparePatch, usually:

                                    * "remove" to remove a specific path in a resource

                                    * "ignore" to remove the entire resource from
                                    checks for modifications.'
                                  properties:
                                    op:
                                      description: Op is usually "remove" or "ignore"
                                      nullable: true
                                      type: string
                                    path:
                                      description: Path is the JSON path to remove.
                                        Not needed if Op is "ignore".
                                      nullable: true
                                      type: string
                                    value:
                                      description: Value is usually empty.
                                      nullable: true
                                      type: string
                                  type: object
                                nullable: true
                                type: array
                            type: object
                          nullable: true
                          type: array
                      type: object
                    downstreamResources:
                      description: 'DownstreamResources points to resources to be
                        copied into downstream clusters, from the bundle''s

                        namespace.'
                      items:
                        description: 'DownstreamResource contains identifiers for
                          a resource to be copied from the parent bundle''s namespace
                          to each

                          downstream cluster.'
                        properties:
                          kind:
                            type: string
                          name:
                            type: string
                        type: object
                      type: array
                    forceSyncGeneration:
                      description: ForceSyncGeneration is used to force a redeployment
                      format: int64
                      type: integer
                    helm:
                      description: Helm options for the deployment, like the chart
                        name, repo and values.
                      properties:
                        atomic:
                          description: Atomic sets the --atomic flag when Helm is
                            performing an upgrade
                          type: boolean
                        chart:
                          description: 'Chart can refer to any go-getter URL or OCI
                            registry based helm

                            chart URL. The chart will be downloaded.'
                          nullable: true
                          type: string
                        disableDNS:
                          description: DisableDNS can be used to customize Helm's
                            EnableDNS option, which Fleet sets to `true` by default.
                          type: boolean
                        disableDependencyUpdate:
                          description: DisableDependencyUpdate allows skipping chart
                            dependencies update
                          type: boolean
                        disablePreProcess:
                          description: DisablePreProcess disables template processing
                            in values
                          type: boolean
                        force:
                          description: Force allows to override immutable resources.
                            This could be dangerous.
                          type: boolean
                        maxHistory:
                          description: MaxHistory limits the maximum number of revisions
                            saved per release by Helm.
                          type: integer
                        releaseName:
                          description: 'ReleaseName sets a custom release name to
                            deploy the chart as. If

                            not specified a release name will be generated by combining
                            the

                            invoking GitRepo.name + GitRepo.path.'
                          maxLength: 53
                          nullable: true
                          pattern: ^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$
                          type: string
                        repo:
                          description: Repo is the name of the HTTPS helm repo to
                            download the chart from.
                          nullable: true
                          type: string
                        skipSchemaValidation:
                          description: SkipSchemaValidation allows skipping schema
                            validation against the chart values
                          type: boolean
                        takeOwnership:
                          description: TakeOwnership makes helm skip the check for
                            its own annotations
                          type: boolean
                        templateValues:
                          additionalProperties:
                            type: string
                          description: 'Template Values passed to Helm. It is possible
                            to specify the keys and values

                            as go template strings. Unlike .values, content of each
                            key will be templated

                            first, before serializing to yaml. This allows to template
                            complex values,

                            like ranges and maps.

                            templateValues keys have precedence over values keys in
                            case of conflict.'
                          nullable: true
                          type: object
                        timeoutSeconds:
                          description: TimeoutSeconds is the time to wait for Helm
                            operations.
                          type: integer
                        values:
                          description: 'Values passed to Helm. It is possible to specify
                            the keys and values

                            as go template strings.'
                          nullable: true
                          type: object
                          x-kubernetes-preserve-unknown-fields: true
                        valuesFiles:
                          description: ValuesFiles is a list of files to load values
                            from.
                          items:
                            type: string
                          nullable: true
                          type: array
                        valuesFrom:
                          description: ValuesFrom loads the values from configmaps
                            and secrets.
                          items:
                            description: 'Define helm values that can come from configmap,
                              secret or external. Credit: https://github.com/fluxcd/helm-operator/blob/0cfea875b5d44bea995abe7324819432070dfbdc/pkg/apis/helm.fluxcd.io/v1/types_helmrelease.go#L439'
                            properties:
                              configMapKeyRef:
                                description: The reference to a config map with release
                                  values.
                                nullable: true
                                properties:
                                  key:
                                    nullable: true
                                    type: string
                                  name:
                                    description: Name of a resource in the same namespace
                                      as the referent.
                                    nullable: true
                                    type: string
                                  namespace:
                                    nullable: true
                                    type: string
                                type: object
                              secretKeyRef:
                                description: The reference to a secret with release
                                  values.
                                nullable: true
                                properties:
                                  key:
                                    nullable: true
                                    type: string
                                  name:
                                    description: Name of a resource in the same namespace
                                      as the referent.
                                    nullable: true
                                    type: string
                                  namespace:
                                    nullable: true
                                    type: string
                                type: object
                            type: object
                          nullable: true
                          type: array
                        version:
                          description: Version of the chart to download
                          nullable: true
                          type: string
                        waitForJobs:
                          description: 'WaitForJobs if set and timeoutSeconds provided,
                            will wait until all

                            Jobs have been completed before marking the GitRepo as
                            ready. It

                            will wait for as long as timeoutSeconds'
                          type: boolean
                      type: object
                    ignore:
                      description: IgnoreOptions can be used to ignore fields when
                        monitoring the bundle.
                      nullable: true
                      properties:
                        conditions:
                          description: Conditions is a list of conditions to be ignored
                            when monitoring the Bundle.
                          items:
                            additionalProperties:
                              type: string
                            type: object
                          nullable: true
                          type: array
                      type: object
                    keepResources:
                      description: KeepResources can be used to keep the deployed
                        resources when removing the bundle
                      type: boolean
                    kustomize:
                      description: 'Kustomize options for the deployment, like the
                        dir containing the

                        kustomization.yaml file.'
                      nullable: true
                      properties:
                        dir:
                          description: 'Dir points to a custom folder for kustomize
                            resources. This folder must contain

                            a kustomization.yaml file.'
                          nullable: true
                          type: string
                      type: object
                    namespace:
                      description: 'TargetNamespace if present will assign all resource
                        to this

                        namespace and if any cluster scoped resource exists the deployment

                        will fail.'
                      nullable: true
                      type: string
                    namespaceAnnotations:
                      additionalProperties:
                        type: string
                      description: NamespaceAnnotations are annotations that will
                        be appended to the namespace created by Fleet.
                      nullable: true
                      type: object
                    namespaceLabels:
                      additionalProperties:
                        type: string
                      description: NamespaceLabels are labels that will be appended
                        to the namespace created by Fleet.
                      nullable: true
                      type: object
                    overwrites:
                      description: 'Overwrites indicates which resources, if any,
                        come from this bundle and overwrite another existing bundle.

                        This flag is set internally by Fleet, and should not be altered
                        by users.'
                      items:
                        properties:
                          kind:
                            type: string
                          name:
                            type: string
                          namespace:
                            type: string
                        type: object
                      type: array
                    serviceAccount:
                      description: ServiceAccount which will be used to perform this
                        deployment.
                      nullable: true
                      type: string
                    yaml:
                      description: 'YAML options, if using raw YAML these are names
                        that map to

                        overlays/{name} files that will be used to replace or patch
                        a resource.'
                      nullable: true
                      properties:
                        overlays:
                          description: 'Overlays is a list of names that maps to folders
                            in "overlays/".

                            If you wish to customize the file ./subdir/resource.yaml
                            then a file

                            ./overlays/myoverlay/subdir/resource.yaml will replace
                            the base

                            file.

                            A file named ./overlays/myoverlay/subdir/resource_patch.yaml
                            will patch the base file.'
                          items:
                            type: string
                          nullable: true
                          type: array
                      type: object
                  type: object
                paused:
                  description: 'Paused if set to true, will stop any BundleDeployments
                    from being

                    updated. If true, BundleDeployments will be marked as out of sync

                    when changes are detected.'
                  type: boolean
                stagedDeploymentID:
                  description: StagedDeploymentID is the ID of the staged deployment.
                  nullable: true
                  type: string
                stagedOptions:
                  description: 'StagedOptions are the deployment options, that are
                    staged for

                    the next deployment.'
                  properties:
                    correctDrift:
                      description: CorrectDrift specifies how drift correction should
                        work.
                      properties:
                        enabled:
                          description: Enabled correct drift if true.
                          type: boolean
                        force:
                          description: Force helm rollback with --force option will
                            be used if true. This will try to recreate all resources
                            in the release.
                          type: boolean
                        keepFailHistory:
                          description: KeepFailHistory keeps track of failed rollbacks
                            in the helm history.
                          type: boolean
                      type: object
                    defaultNamespace:
                      description: 'DefaultNamespace is the namespace to use for resources
                        that do not

                        specify a namespace. This field is not used to enforce or
                        lock down

                        the deployment to a specific namespace.'
                      nullable: true
                      type: string
                    deleteCRDResources:
                      description: DeleteCRDResources deletes CRDs. Warning! this
                        will also delete all your Custom Resources.
                      type: boolean
                    deleteNamespace:
                      description: DeleteNamespace can be used to delete the deployed
                        namespace when removing the bundle
                      type: boolean
                    diff:
                      description: Diff can be used to ignore the modified state of
                        objects which are amended at runtime.
                      nullable: true
                      properties:
                        comparePatches:
                          description: ComparePatches match a resource and remove
                            fields, or the resource itself from the check for modifications.
                          items:
                            description: ComparePatch matches a resource and removes
                              fields from the check for modifications.
                            properties:
                              apiVersion:
                                description: APIVersion is the apiVersion of the resource
                                  to match.
                                nullable: true
                                type: string
                              jsonPointers:
                                description: JSONPointers ignore diffs at a certain
                                  JSON path.
                                items:
                                  type: string
                                nullable: true
                                type: array
                              kind:
                                description: Kind is the kind of the resource to match.
                                nullable: true
                                type: string
                              name:
                                description: Name is the name of the resource to match.
                                nullable: true
                                type: string
                              namespace:
                                description: Namespace is the namespace of the resource
                                  to match.
                                nullable: true
                                type: string
                              operations:
                                description: Operations remove a JSON path from the
                                  resource.
                                items:
                                  description: 'Operation of a ComparePatch, usually:

                                    * "remove" to remove a specific path in a resource

                                    * "ignore" to remove the entire resource from
                                    checks for modifications.'
                                  properties:
                                    op:
                                      description: Op is usually "remove" or "ignore"
                                      nullable: true
                                      type: string
                                    path:
                                      description: Path is the JSON path to remove.
                                        Not needed if Op is "ignore".
                                      nullable: true
                                      type: string
                                    value:
                                      description: Value is usually empty.
                                      nullable: true
                                      type: string
                                  type: object
                                nullable: true
                                type: array
                            type: object
                          nullable: true
                          type: array
                      type: object
                    downstreamResources:
                      description: 'DownstreamResources points to resources to be
                        copied into downstream clusters, from the bundle''s

                        namespace.'
                      items:
                        description: 'DownstreamResource contains identifiers for
                          a resource to be copied from the parent bundle''s namespace
                          to each

                          downstream cluster.'
                        properties:
                          kind:
                            type: string
                          name:
                            type: string
                        type: object
                      type: array
                    forceSyncGeneration:
                      description: ForceSyncGeneration is used to force a redeployment
                      format: int64
                      type: integer
                    helm:
                      description: Helm options for the deployment, like the chart
                        name, repo and values.
                      properties:
                        atomic:
                          description: Atomic sets the --atomic flag when Helm is
                            performing an upgrade
                          type: boolean
                        chart:
                          description: 'Chart can refer to any go-getter URL or OCI
                            registry based helm

                            chart URL. The chart will be downloaded.'
                          nullable: true
                          type: string
                        disableDNS:
                          description: DisableDNS can be used to customize Helm's
                            EnableDNS option, which Fleet sets to `true` by default.
                          type: boolean
                        disableDependencyUpdate:
                          description: DisableDependencyUpdate allows skipping chart
                            dependencies update
                          type: boolean
                        disablePreProcess:
                          description: DisablePreProcess disables template processing
                            in values
                          type: boolean
                        force:
                          description: Force allows to override immutable resources.
                            This could be dangerous.
                          type: boolean
                        maxHistory:
                          description: MaxHistory limits the maximum number of revisions
                            saved per release by Helm.
                          type: integer
                        releaseName:
                          description: 'ReleaseName sets a custom release name to
                            deploy the chart as. If

                            not specified a release name will be generated by combining
                            the

                            invoking GitRepo.name + GitRepo.path.'
                          maxLength: 53
                          nullable: true
                          pattern: ^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$
                          type: string
                        repo:
                          description: Repo is the name of the HTTPS helm repo to
                            download the chart from.
                          nullable: true
                          type: string
                        skipSchemaValidation:
                          description: SkipSchemaValidation allows skipping schema
                            validation against the chart values
                          type: boolean
                        takeOwnership:
                          description: TakeOwnership makes helm skip the check for
                            its own annotations
                          type: boolean
                        templateValues:
                          additionalProperties:
                            type: string
                          description: 'Template Values passed to Helm. It is possible
                            to specify the keys and values

                            as go template strings. Unlike .values, content of each
                            key will be templated

                            first, before serializing to yaml. This allows to template
                            complex values,

                            like ranges and maps.

                            templateValues keys have precedence over values keys in
                            case of conflict.'
                          nullable: true
                          type: object
                        timeoutSeconds:
                          description: TimeoutSeconds is the time to wait for Helm
                            operations.
                          type: integer
                        values:
                          description: 'Values passed to Helm. It is possible to specify
                            the keys and values

                            as go template strings.'
                          nullable: true
                          type: object
                          x-kubernetes-preserve-unknown-fields: true
                        valuesFiles:
                          description: ValuesFiles is a list of files to load values
                            from.
                          items:
                            type: string
                          nullable: true
                          type: array
                        valuesFrom:
                          description: ValuesFrom loads the values from configmaps
                            and secrets.
                          items:
                            description: 'Define helm values that can come from configmap,
                              secret or external. Credit: https://github.com/fluxcd/helm-operator/blob/0cfea875b5d44bea995abe7324819432070dfbdc/pkg/apis/helm.fluxcd.io/v1/types_helmrelease.go#L439'
                            properties:
                              configMapKeyRef:
                                description: The reference to a config map with release
                                  values.
                                nullable: true
                                properties:
                                  key:
                                    nullable: true
                                    type: string
                                  name:
                                    description: Name of a resource in the same namespace
                                      as the referent.
                                    nullable: true
                                    type: string
                                  namespace:
                                    nullable: true
                                    type: string
                                type: object
                              secretKeyRef:
                                description: The reference to a secret with release
                                  values.
                                nullable: true
                                properties:
                                  key:
                                    nullable: true
                                    type: string
                                  name:
                                    description: Name of a resource in the same namespace
                                      as the referent.
                                    nullable: true
                                    type: string
                                  namespace:
                                    nullable: true
                                    type: string
                                type: object
                            type: object
                          nullable: true
                          type: array
                        version:
                          description: Version of the chart to download
                          nullable: true
                          type: string
                        waitForJobs:
                          description: 'WaitForJobs if set and timeoutSeconds provided,
                            will wait until all

                            Jobs have been completed before marking the GitRepo as
                            ready. It

                            will wait for as long as timeoutSeconds'
                          type: boolean
                      type: object
                    ignore:
                      description: IgnoreOptions can be used to ignore fields when
                        monitoring the bundle.
                      nullable: true
                      properties:
                        conditions:
                          description: Conditions is a list of conditions to be ignored
                            when monitoring the Bundle.
                          items:
                            additionalProperties:
                              type: string
                            type: object
                          nullable: true
                          type: array
                      type: object
                    keepResources:
                      description: KeepResources can be used to keep the deployed
                        resources when removing the bundle
                      type: boolean
                    kustomize:
                      description: 'Kustomize options for the deployment, like the
                        dir containing the

                        kustomization.yaml file.'
                      nullable: true
                      properties:
                        dir:
                          description: 'Dir points to a custom folder for kustomize
                            resources. This folder must contain

                            a kustomization.yaml file.'
                          nullable: true
                          type: string
                      type: object
                    namespace:
                      description: 'TargetNamespace if present will assign all resource
                        to this

                        namespace and if any cluster scoped resource exists the deployment

                        will fail.'
                      nullable: true
                      type: string
                    namespaceAnnotations:
                      additionalProperties:
                        type: string
                      description: NamespaceAnnotations are annotations that will
                        be appended to the namespace created by Fleet.
                      nullable: true
                      type: object
                    namespaceLabels:
                      additionalProperties:
                        type: string
                      description: NamespaceLabels are labels that will be appended
                        to the namespace created by Fleet.
                      nullable: true
                      type: object
                    overwrites:
                      description: 'Overwrites indicates which resources, if any,
                        come from this bundle and overwrite another existing bundle.

                        This flag is set internally by Fleet, and should not be altered
                        by users.'
                      items:
                        properties:
                          kind:
                            type: string
                          name:
                            type: string
                          namespace:
                            type: string
                        type: object
                      type: array
                    serviceAccount:
                      description: ServiceAccount which will be used to perform this
                        deployment.
                      nullable: true
                      type: string
                    yaml:
                      description: 'YAML options, if using raw YAML these are names
                        that map to

                        overlays/{name} files that will be used to replace or patch
                        a resource.'
                      nullable: true
                      properties:
                        overlays:
                          description: 'Overlays is a list of names that maps to folders
                            in "overlays/".

                            If you wish to customize the file ./subdir/resource.yaml
                            then a file

                            ./overlays/myoverlay/subdir/resource.yaml will replace
                            the base

                            file.

                            A file named ./overlays/myoverlay/subdir/resource_patch.yaml
                            will patch the base file.'
                          items:
                            type: string
                          nullable: true
                          type: array
                      type: object
                  type: object
                valuesHash:
                  description: ValuesHash is the hash of the values used to deploy
                    the bundle.
                  nullable: true
                  type: string
              type: object
            status:
              properties:
                appliedDeploymentID:
                  nullable: true
                  type: string
                conditions:
                  items:
                    properties:
                      lastTransitionTime:
                        description: Last time the condition transitioned from one
                          status to another.
                        type: string
                      lastUpdateTime:
                        description: The last time this condition was updated.
                        type: string
                      message:
                        description: Human-readable message indicating details about
                          last transition
                        type: string
                      reason:
                        description: The reason for the condition's last transition.
                        type: string
                      status:
                        description: Status of the condition, one of True, False,
                          Unknown.
                        type: string
                      type:
                        description: Type of cluster condition.
                        type: string
                    required:
                      - status
                      - type
                    type: object
                  nullable: true
                  type: array
                display:
                  nullable: true
                  properties:
                    deployed:
                      nullable: true
                      type: string
                    monitored:
                      nullable: true
                      type: string
                    state:
                      nullable: true
                      type: string
                  type: object
                incompleteState:
                  description: IncompleteState is true if there are more than 10 non-ready
                    or modified resources, meaning that the lists in those fields
                    have been truncated.
                  type: boolean
                modifiedStatus:
                  items:
                    description: 'ModifiedStatus is used to report the status of a
                      resource that is modified.

                      It indicates if the modification was a create, a delete or a
                      patch.'
                    properties:
                      apiVersion:
                        nullable: true
                        type: string
                      delete:
                        type: boolean
                      exist:
                        description: Exist is true if the resource exists but is not
                          owned by us. This can happen if a resource was adopted by
                          another bundle whereas the first bundle still exists and
                          due to that reports that it does not own it.
                        type: boolean
                      kind:
                        nullable: true
                        type: string
                      missing:
                        type: boolean
                      name:
                        nullable: true
                        type: string
                      namespace:
                        nullable: true
                        type: string
                      patch:
                        nullable: true
                        type: string
                    type: object
                  nullable: true
                  type: array
                nonModified:
                  type: boolean
                nonReadyStatus:
                  items:
                    description: NonReadyStatus is used to report the status of a
                      resource that is not ready. It includes a summary.
                    properties:
                      apiVersion:
                        nullable: true
                        type: string
                      kind:
                        nullable: true
                        type: string
                      name:
                        nullable: true
                        type: string
                      namespace:
                        nullable: true
                        type: string
                      summary:
                        properties:
                          error:
                            type: boolean
                          message:
                            items:
                              type: string
                            type: array
                          state:
                            type: string
                          transitioning:
                            type: boolean
                        type: object
                      uid:
                        description: 'UID is a type that holds unique ID values, including
                          UUIDs.  Because we

                          don''t ONLY use UUIDs, this is an alias to string.  Being
                          a type captures

                          intent and helps make sure that UIDs and names do not get
                          conflated.'
                        nullable: true
                        type: string
                    type: object
                  nullable: true
                  type: array
                ready:
                  type: boolean
                release:
                  description: Release is the Helm release ID
                  nullable: true
                  type: string
                resourceCounts:
                  description: ResourceCounts contains the number of resources in
                    each state.
                  properties:
                    desiredReady:
                      description: DesiredReady is the number of resources that should
                        be ready.
                      type: integer
                    missing:
                      description: Missing is the number of missing resources.
                      type: integer
                    modified:
                      description: Modified is the number of resources that have been
                        modified.
                      type: integer
                    notReady:
                      description: 'NotReady is the number of not ready resources.
                        Resources are not

                        ready if they do not match any other state.'
                      type: integer
                    orphaned:
                      description: Orphaned is the number of orphaned resources.
                      type: integer
                    ready:
                      description: Ready is the number of ready resources.
                      type: integer
                    unknown:
                      description: Unknown is the number of resources in an unknown
                        state.
                      type: integer
                    waitApplied:
                      description: WaitApplied is the number of resources that are
                        waiting to be applied.
                      type: integer
                  type: object
                resources:
                  description: 'Resources lists the metadata of resources that were
                    deployed

                    according to the helm release history.'
                  items:
                    description: BundleDeploymentResource contains the metadata of
                      a deployed resource.
                    properties:
                      apiVersion:
                        nullable: true
                        type: string
                      createdAt:
                        format: date-time
                        nullable: true
                        type: string
                      kind:
                        nullable: true
                        type: string
                      name:
                        nullable: true
                        type: string
                      namespace:
                        nullable: true
                        type: string
                    type: object
                  nullable: true
                  type: array
                syncGeneration:
                  format: int64
                  nullable: true
                  type: integer
              type: object
          type: object
      served: true
      storage: true
      subresources:
        status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.19.0
  name: bundlenamespacemappings.fleet.cattle.io
spec:
  group: fleet.cattle.io
  names:
    kind: BundleNamespaceMapping
    listKind: BundleNamespaceMappingList
    plural: bundlenamespacemappings
    singular: bundlenamespacemapping
  scope: Namespaced
  versions:
    - name: v1alpha1
      schema:
        openAPIV3Schema:
          description: BundleNamespaceMapping maps bundles to clusters in other namespaces.
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
                of an object.

                Servers should convert recognized schemas to the latest internal value,
                and

                may reject unrecognized values.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            bundleSelector:
              description: 'A label selector is a label query over a set of resources.
                The result of matchLabels and

                matchExpressions are ANDed. An empty label selector matches all objects.
                A null

                label selector matches no objects.'
              nullable: true
              properties:
                matchExpressions:
                  description: matchExpressions is a list of label selector requirements.
                    The requirements are ANDed.
                  items:
                    description: 'A label selector requirement is a selector that
                      contains values, a key, and an operator that

                      relates the key and values.'
                    properties:
                      key:
                        description: key is the label key that the selector applies
                          to.
                        type: string
                      operator:
                        description: 'operator represents a key''s relationship to
                          a set of values.

                          Valid operators are In, NotIn, Exists and DoesNotExist.'
                        type: string
                      values:
                        description: 'values is an array of string values. If the
                          operator is In or NotIn,

                          the values array must be non-empty. If the operator is Exists
                          or DoesNotExist,

                          the values array must be empty. This array is replaced during
                          a strategic

                          merge patch.'
                        items:
                          type: string
                        type: array
                        x-kubernetes-list-type: atomic
                    required:
                      - key
                      - operator
                    type: object
                  type: array
                  x-kubernetes-list-type: atomic
                matchLabels:
                  additionalProperties:
                    type: string
                  description: 'matchLabels is a map of {key,value} pairs. A single
                    {key,value} in the matchLabels

                    map is equivalent to an element of matchExpressions, whose key
                    field is "key", the

                    operator is "In", and the values array contains only "value".
                    The requirements are ANDed.'
                  type: object
              type: object
              x-kubernetes-map-type: atomic
            kind:
              description: 'Kind is a string value representing the REST resource
                this object represents.

                Servers may infer this from the endpoint the client submits requests
                to.

                Cannot be updated.

                In CamelCase.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
            namespaceSelector:
              description: 'A label selector is a label query over a set of resources.
                The result of matchLabels and

                matchExpressions are ANDed. An empty label selector matches all objects.
                A null

                label selector matches no objects.'
              nullable: true
              properties:
                matchExpressions:
                  description: matchExpressions is a list of label selector requirements.
                    The requirements are ANDed.
                  items:
                    description: 'A label selector requirement is a selector that
                      contains values, a key, and an operator that

                      relates the key and values.'
                    properties:
                      key:
                        description: key is the label key that the selector applies
                          to.
                        type: string
                      operator:
                        description: 'operator represents a key''s relationship to
                          a set of values.

                          Valid operators are In, NotIn, Exists and DoesNotExist.'
                        type: string
                      values:
                        description: 'values is an array of string values. If the
                          operator is In or NotIn,

                          the values array must be non-empty. If the operator is Exists
                          or DoesNotExist,

                          the values array must be empty. This array is replaced during
                          a strategic

                          merge patch.'
                        items:
                          type: string
                        type: array
                        x-kubernetes-list-type: atomic
                    required:
                      - key
                      - operator
                    type: object
                  type: array
                  x-kubernetes-list-type: atomic
                matchLabels:
                  additionalProperties:
                    type: string
                  description: 'matchLabels is a map of {key,value} pairs. A single
                    {key,value} in the matchLabels

                    map is equivalent to an element of matchExpressions, whose key
                    field is "key", the

                    operator is "In", and the values array contains only "value".
                    The requirements are ANDed.'
                  type: object
              type: object
              x-kubernetes-map-type: atomic
          type: object
      served: true
      storage: true
      subresources:
        status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.19.0
  name: bundles.fleet.cattle.io
spec:
  group: fleet.cattle.io
  names:
    kind: Bundle
    listKind: BundleList
    plural: bundles
    singular: bundle
  scope: Namespaced
  versions:
    - additionalPrinterColumns:
        - jsonPath: .status.display.readyClusters
          name: BundleDeployments-Ready
          type: string
        - jsonPath: .status.conditions[?(@.type=="Ready")].message
          name: Status
          type: string
      name: v1alpha1
      schema:
        openAPIV3Schema:
          description: 'Bundle contains the resources of an application and its deployment
            options.

            It will be deployed as a Helm chart to target clusters.


            When a GitRepo is scanned it will produce one or more bundles. Bundles
            are

            a collection of resources that get deployed to one or more cluster(s).
            Bundle is the

            fundamental deployment unit used in Fleet. The contents of a Bundle may
            be

            Kubernetes manifests, Kustomize configuration, or Helm charts. Regardless

            of the source the contents are dynamically rendered into a Helm chart
            by

            the agent and installed into the downstream cluster as a Helm release.'
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
                of an object.

                Servers should convert recognized schemas to the latest internal value,
                and

                may reject unrecognized values.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource
                this object represents.

                Servers may infer this from the endpoint the client submits requests
                to.

                Cannot be updated.

                In CamelCase.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
            spec:
              properties:
                contentsId:
                  description: ContentsID stores the contents id when deploying contents
                    using an OCI registry.
                  nullable: true
                  type: string
                correctDrift:
                  description: CorrectDrift specifies how drift correction should
                    work.
                  properties:
                    enabled:
                      description: Enabled correct drift if true.
                      type: boolean
                    force:
                      description: Force helm rollback with --force option will be
                        used if true. This will try to recreate all resources in the
                        release.
                      type: boolean
                    keepFailHistory:
                      description: KeepFailHistory keeps track of failed rollbacks
                        in the helm history.
                      type: boolean
                  type: object
                defaultNamespace:
                  description: 'DefaultNamespace is the namespace to use for resources
                    that do not

                    specify a namespace. This field is not used to enforce or lock
                    down

                    the deployment to a specific namespace.'
                  nullable: true
                  type: string
                deleteCRDResources:
                  description: DeleteCRDResources deletes CRDs. Warning! this will
                    also delete all your Custom Resources.
                  type: boolean
                deleteNamespace:
                  description: DeleteNamespace can be used to delete the deployed
                    namespace when removing the bundle
                  type: boolean
                dependsOn:
                  description: DependsOn refers to the bundles which must be ready
                    before this bundle can be deployed.
                  items:
                    properties:
                      name:
                        description: Name of the bundle.
                        nullable: true
                        type: string
                      selector:
                        description: Selector matching bundle's labels.
                        nullable: true
                        properties:
                          matchExpressions:
                            description: matchExpressions is a list of label selector
                              requirements. The requirements are ANDed.
                            items:
                              description: 'A label selector requirement is a selector
                                that contains values, a key, and an operator that

                                relates the key and values.'
                              properties:
                                key:
                                  description: key is the label key that the selector
                                    applies to.
                                  type: string
                                operator:
                                  description: 'operator represents a key''s relationship
                                    to a set of values.

                                    Valid operators are In, NotIn, Exists and DoesNotExist.'
                                  type: string
                                values:
                                  description: 'values is an array of string values.
                                    If the operator is In or NotIn,

                                    the values array must be non-empty. If the operator
                                    is Exists or DoesNotExist,

                                    the values array must be empty. This array is
                                    replaced during a strategic

                                    merge patch.'
                                  items:
                                    type: string
                                  type: array
                                  x-kubernetes-list-type: atomic
                              required:
                                - key
                                - operator
                              type: object
                            type: array
                            x-kubernetes-list-type: atomic
                          matchLabels:
                            additionalProperties:
                              type: string
                            description: 'matchLabels is a map of {key,value} pairs.
                              A single {key,value} in the matchLabels

                              map is equivalent to an element of matchExpressions,
                              whose key field is "key", the

                              operator is "In", and the values array contains only
                              "value". The requirements are ANDed.'
                            type: object
                        type: object
                        x-kubernetes-map-type: atomic
                    type: object
                  nullable: true
                  type: array
                diff:
                  description: Diff can be used to ignore the modified state of objects
                    which are amended at runtime.
                  nullable: true
                  properties:
                    comparePatches:
                      description: ComparePatches match a resource and remove fields,
                        or the resource itself from the check for modifications.
                      items:
                        description: ComparePatch matches a resource and removes fields
                          from the check for modifications.
                        properties:
                          apiVersion:
                            description: APIVersion is the apiVersion of the resource
                              to match.
                            nullable: true
                            type: string
                          jsonPointers:
                            description: JSONPointers ignore diffs at a certain JSON
                              path.
                            items:
                              type: string
                            nullable: true
                            type: array
                          kind:
                            description: Kind is the kind of the resource to match.
                            nullable: true
                            type: string
                          name:
                            description: Name is the name of the resource to match.
                            nullable: true
                            type: string
                          namespace:
                            description: Namespace is the namespace of the resource
                              to match.
                            nullable: true
                            type: string
                          operations:
                            description: Operations remove a JSON path from the resource.
                            items:
                              description: 'Operation of a ComparePatch, usually:

                                * "remove" to remove a specific path in a resource

                                * "ignore" to remove the entire resource from checks
                                for modifications.'
                              properties:
                                op:
                                  description: Op is usually "remove" or "ignore"
                                  nullable: true
                                  type: string
                                path:
                                  description: Path is the JSON path to remove. Not
                                    needed if Op is "ignore".
                                  nullable: true
                                  type: string
                                value:
                                  description: Value is usually empty.
                                  nullable: true
                                  type: string
                              type: object
                            nullable: true
                            type: array
                        type: object
                      nullable: true
                      type: array
                  type: object
                downstreamResources:
                  description: 'DownstreamResources points to resources to be copied
                    into downstream clusters, from the bundle''s

                    namespace.'
                  items:
                    description: 'DownstreamResource contains identifiers for a resource
                      to be copied from the parent bundle''s namespace to each

                      downstream cluster.'
                    properties:
                      kind:
                        type: string
                      name:
                        type: string
                    type: object
                  type: array
                forceSyncGeneration:
                  description: ForceSyncGeneration is used to force a redeployment
                  format: int64
                  type: integer
                helm:
                  description: Helm options for the deployment, like the chart name,
                    repo and values.
                  properties:
                    atomic:
                      description: Atomic sets the --atomic flag when Helm is performing
                        an upgrade
                      type: boolean
                    chart:
                      description: 'Chart can refer to any go-getter URL or OCI registry
                        based helm

                        chart URL. The chart will be downloaded.'
                      nullable: true
                      type: string
                    disableDNS:
                      description: DisableDNS can be used to customize Helm's EnableDNS
                        option, which Fleet sets to `true` by default.
                      type: boolean
                    disableDependencyUpdate:
                      description: DisableDependencyUpdate allows skipping chart dependencies
                        update
                      type: boolean
                    disablePreProcess:
                      description: DisablePreProcess disables template processing
                        in values
                      type: boolean
                    force:
                      description: Force allows to override immutable resources. This
                        could be dangerous.
                      type: boolean
                    maxHistory:
                      description: MaxHistory limits the maximum number of revisions
                        saved per release by Helm.
                      type: integer
                    releaseName:
                      description: 'ReleaseName sets a custom release name to deploy
                        the chart as. If

                        not specified a release name will be generated by combining
                        the

                        invoking GitRepo.name + GitRepo.path.'
                      maxLength: 53
                      nullable: true
                      pattern: ^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$
                      type: string
                    repo:
                      description: Repo is the name of the HTTPS helm repo to download
                        the chart from.
                      nullable: true
                      type: string
                    skipSchemaValidation:
                      description: SkipSchemaValidation allows skipping schema validation
                        against the chart values
                      type: boolean
                    takeOwnership:
                      description: TakeOwnership makes helm skip the check for its
                        own annotations
                      type: boolean
                    templateValues:
                      additionalProperties:
                        type: string
                      description: 'Template Values passed to Helm. It is possible
                        to specify the keys and values

                        as go template strings. Unlike .values, content of each key
                        will be templated

                        first, before serializing to yaml. This allows to template
                        complex values,

                        like ranges and maps.

                        templateValues keys have precedence over values keys in case
                        of conflict.'
                      nullable: true
                      type: object
                    timeoutSeconds:
                      description: TimeoutSeconds is the time to wait for Helm operations.
                      type: integer
                    values:
                      description: 'Values passed to Helm. It is possible to specify
                        the keys and values

                        as go template strings.'
                      nullable: true
                      type: object
                      x-kubernetes-preserve-unknown-fields: true
                    valuesFiles:
                      description: ValuesFiles is a list of files to load values from.
                      items:
                        type: string
                      nullable: true
                      type: array
                    valuesFrom:
                      description: ValuesFrom loads the values from configmaps and
                        secrets.
                      items:
                        description: 'Define helm values that can come from configmap,
                          secret or external. Credit: https://github.com/fluxcd/helm-operator/blob/0cfea875b5d44bea995abe7324819432070dfbdc/pkg/apis/helm.fluxcd.io/v1/types_helmrelease.go#L439'
                        properties:
                          configMapKeyRef:
                            description: The reference to a config map with release
                              values.
                            nullable: true
                            properties:
                              key:
                                nullable: true
                                type: string
                              name:
                                description: Name of a resource in the same namespace
                                  as the referent.
                                nullable: true
                                type: string
                              namespace:
                                nullable: true
                                type: string
                            type: object
                          secretKeyRef:
                            description: The reference to a secret with release values.
                            nullable: true
                            properties:
                              key:
                                nullable: true
                                type: string
                              name:
                                description: Name of a resource in the same namespace
                                  as the referent.
                                nullable: true
                                type: string
                              namespace:
                                nullable: true
                                type: string
                            type: object
                        type: object
                      nullable: true
                      type: array
                    version:
                      description: Version of the chart to download
                      nullable: true
                      type: string
                    waitForJobs:
                      description: 'WaitForJobs if set and timeoutSeconds provided,
                        will wait until all

                        Jobs have been completed before marking the GitRepo as ready.
                        It

                        will wait for as long as timeoutSeconds'
                      type: boolean
                  type: object
                helmOpOptions:
                  description: 'HelmOpOptions stores the options relative to HelmOp
                    resources

                    Non-nil HelmOpOptions indicate that the source of resources is
                    a Helm chart,

                    not a git repository.'
                  nullable: true
                  properties:
                    helmOpInsecureSkipTLSVerify:
                      description: InsecureSkipTLSverify will use insecure HTTPS to
                        clone the helm app resource.
                      type: boolean
                    helmOpSecretName:
                      description: 'SecretName stores the secret name for storing
                        credentials when accessing

                        a remote helm repository defined in a HelmOp resource'
                      type: string
                  type: object
                ignore:
                  description: IgnoreOptions can be used to ignore fields when monitoring
                    the bundle.
                  nullable: true
                  properties:
                    conditions:
                      description: Conditions is a list of conditions to be ignored
                        when monitoring the Bundle.
                      items:
                        additionalProperties:
                          type: string
                        type: object
                      nullable: true
                      type: array
                  type: object
                keepResources:
                  description: KeepResources can be used to keep the deployed resources
                    when removing the bundle
                  type: boolean
                kustomize:
                  description: 'Kustomize options for the deployment, like the dir
                    containing the

                    kustomization.yaml file.'
                  nullable: true
                  properties:
                    dir:
                      description: 'Dir points to a custom folder for kustomize resources.
                        This folder must contain

                        a kustomization.yaml file.'
                      nullable: true
                      type: string
                  type: object
                namespace:
                  description: 'TargetNamespace if present will assign all resource
                    to this

                    namespace and if any cluster scoped resource exists the deployment

                    will fail.'
                  nullable: true
                  type: string
                namespaceAnnotations:
                  additionalProperties:
                    type: string
                  description: NamespaceAnnotations are annotations that will be appended
                    to the namespace created by Fleet.
                  nullable: true
                  type: object
                namespaceLabels:
                  additionalProperties:
                    type: string
                  description: NamespaceLabels are labels that will be appended to
                    the namespace created by Fleet.
                  nullable: true
                  type: object
                overwrites:
                  description: 'Overwrites indicates which resources, if any, come
                    from this bundle and overwrite another existing bundle.

                    This flag is set internally by Fleet, and should not be altered
                    by users.'
                  items:
                    properties:
                      kind:
                        type: string
                      name:
                        type: string
                      namespace:
                        type: string
                    type: object
                  type: array
                paused:
                  description: Paused if set to true, will stop any BundleDeployments
                    from being updated. It will be marked as out of sync.
                  type: boolean
                resources:
                  description: 'Resources contains the resources that were read from
                    the bundle''s

                    path. This includes the content of downloaded helm charts.'
                  items:
                    description: BundleResource represents the content of a single
                      resource from the bundle, like a YAML manifest.
                    properties:
                      content:
                        description: The content of the resource, can be compressed.
                        nullable: true
                        type: string
                      encoding:
                        description: Encoding is either empty or "base64+gz".
                        nullable: true
                        type: string
                      name:
                        description: Name of the resource, can include the bundle's
                          internal path.
                        nullable: true
                        type: string
                    type: object
                  nullable: true
                  type: array
                rolloutStrategy:
                  description: 'RolloutStrategy controls the rollout of bundles, by
                    defining

                    partitions, canaries and percentages for cluster availability.'
                  nullable: true
                  properties:
                    autoPartitionSize:
                      anyOf:
                        - type: integer
                        - type: string
                      description: 'A number or percentage of how to automatically
                        partition clusters if no

                        specific partitioning strategy is configured.

                        default: 25%'
                      nullable: true
                      x-kubernetes-int-or-string: true
                    maxUnavailable:
                      anyOf:
                        - type: integer
                        - type: string
                      description: 'A number or percentage of clusters that can be
                        unavailable during an update

                        of a bundle. This follows the same basic approach as a deployment
                        rollout

                        strategy. Once the number of clusters meets unavailable state
                        update will be

                        paused. Default value is 100% which doesn''t take effect on
                        update.

                        default: 100%'
                      nullable: true
                      x-kubernetes-int-or-string: true
                    maxUnavailablePartitions:
                      anyOf:
                        - type: integer
                        - type: string
                      description: 'A number or percentage of cluster partitions that
                        can be unavailable during

                        an update of a bundle.

                        default: 0'
                      nullable: true
                      x-kubernetes-int-or-string: true
                    partitions:
                      description: 'A list of definitions of partitions.  If any target
                        clusters do not match

                        the configuration they are added to partitions at the end
                        following the

                        autoPartitionSize.'
                      items:
                        description: Partition defines a separate rollout strategy
                          for a set of clusters.
                        properties:
                          clusterGroup:
                            description: A cluster group name to include in this partition
                            type: string
                          clusterGroupSelector:
                            description: Selector matching cluster group labels to
                              include in this partition
                            nullable: true
                            properties:
                              matchExpressions:
                                description: matchExpressions is a list of label selector
                                  requirements. The requirements are ANDed.
                                items:
                                  description: 'A label selector requirement is a
                                    selector that contains values, a key, and an operator
                                    that

                                    relates the key and values.'
                                  properties:
                                    key:
                                      description: key is the label key that the selector
                                        applies to.
                                      type: string
                                    operator:
                                      description: 'operator represents a key''s relationship
                                        to a set of values.

                                        Valid operators are In, NotIn, Exists and
                                        DoesNotExist.'
                                      type: string
                                    values:
                                      description: 'values is an array of string values.
                                        If the operator is In or NotIn,

                                        the values array must be non-empty. If the
                                        operator is Exists or DoesNotExist,

                                        the values array must be empty. This array
                                        is replaced during a strategic

                                        merge patch.'
                                      items:
                                        type: string
                                      type: array
                                      x-kubernetes-list-type: atomic
                                  required:
                                    - key
                                    - operator
                                  type: object
                                type: array
                                x-kubernetes-list-type: atomic
                              matchLabels:
                                additionalProperties:
                                  type: string
                                description: 'matchLabels is a map of {key,value}
                                  pairs. A single {key,value} in the matchLabels

                                  map is equivalent to an element of matchExpressions,
                                  whose key field is "key", the

                                  operator is "In", and the values array contains
                                  only "value". The requirements are ANDed.'
                                type: object
                            type: object
                            x-kubernetes-map-type: atomic
                          clusterName:
                            description: ClusterName is the name of a cluster to include
                              in this partition
                            type: string
                          clusterSelector:
                            description: Selector matching cluster labels to include
                              in this partition
                            properties:
                              matchExpressions:
                                description: matchExpressions is a list of label selector
                                  requirements. The requirements are ANDed.
                                items:
                                  description: 'A label selector requirement is a
                                    selector that contains values, a key, and an operator
                                    that

                                    relates the key and values.'
                                  properties:
                                    key:
                                      description: key is the label key that the selector
                                        applies to.
                                      type: string
                                    operator:
                                      description: 'operator represents a key''s relationship
                                        to a set of values.

                                        Valid operators are In, NotIn, Exists and
                                        DoesNotExist.'
                                      type: string
                                    values:
                                      description: 'values is an array of string values.
                                        If the operator is In or NotIn,

                                        the values array must be non-empty. If the
                                        operator is Exists or DoesNotExist,

                                        the values array must be empty. This array
                                        is replaced during a strategic

                                        merge patch.'
                                      items:
                                        type: string
                                      type: array
                                      x-kubernetes-list-type: atomic
                                  required:
                                    - key
                                    - operator
                                  type: object
                                type: array
                                x-kubernetes-list-type: atomic
                              matchLabels:
                                additionalProperties:
                                  type: string
                                description: 'matchLabels is a map of {key,value}
                                  pairs. A single {key,value} in the matchLabels

                                  map is equivalent to an element of matchExpressions,
                                  whose key field is "key", the

                                  operator is "In", and the values array contains
                                  only "value". The requirements are ANDed.'
                                type: object
                            type: object
                            x-kubernetes-map-type: atomic
                          maxUnavailable:
                            anyOf:
                              - type: integer
                              - type: string
                            description: 'A number or percentage of clusters that
                              can be unavailable in this

                              partition before this partition is treated as done.

                              default: 10%'
                            x-kubernetes-int-or-string: true
                          name:
                            description: A user-friendly name given to the partition
                              used for Display (optional).
                            nullable: true
                            type: string
                        type: object
                      nullable: true
                      type: array
                  type: object
                serviceAccount:
                  description: ServiceAccount which will be used to perform this deployment.
                  nullable: true
                  type: string
                targetRestrictions:
                  description: TargetRestrictions is an allow list, which controls
                    if a bundledeployment is created for a target.
                  items:
                    description: 'BundleTargetRestriction is used internally by Fleet
                      and should not be modified.

                      It acts as an allow list, to prevent the creation of BundleDeployments
                      from

                      Targets created by TargetCustomizations in fleet.yaml.'
                    properties:
                      clusterGroup:
                        nullable: true
                        type: string
                      clusterGroupSelector:
                        description: 'A label selector is a label query over a set
                          of resources. The result of matchLabels and

                          matchExpressions are ANDed. An empty label selector matches
                          all objects. A null

                          label selector matches no objects.'
                        nullable: true
                        properties:
                          matchExpressions:
                            description: matchExpressions is a list of label selector
                              requirements. The requirements are ANDed.
                            items:
                              description: 'A label selector requirement is a selector
                                that contains values, a key, and an operator that

                                relates the key and values.'
                              properties:
                                key:
                                  description: key is the label key that the selector
                                    applies to.
                                  type: string
                                operator:
                                  description: 'operator represents a key''s relationship
                                    to a set of values.

                                    Valid operators are In, NotIn, Exists and DoesNotExist.'
                                  type: string
                                values:
                                  description: 'values is an array of string values.
                                    If the operator is In or NotIn,

                                    the values array must be non-empty. If the operator
                                    is Exists or DoesNotExist,

                                    the values array must be empty. This array is
                                    replaced during a strategic

                                    merge patch.'
                                  items:
                                    type: string
                                  type: array
                                  x-kubernetes-list-type: atomic
                              required:
                                - key
                                - operator
                              type: object
                            type: array
                            x-kubernetes-list-type: atomic
                          matchLabels:
                            additionalProperties:
                              type: string
                            description: 'matchLabels is a map of {key,value} pairs.
                              A single {key,value} in the matchLabels

                              map is equivalent to an element of matchExpressions,
                              whose key field is "key", the

                              operator is "In", and the values array contains only
                              "value". The requirements are ANDed.'
                            type: object
                        type: object
                        x-kubernetes-map-type: atomic
                      clusterName:
                        nullable: true
                        type: string
                      clusterSelector:
                        description: 'A label selector is a label query over a set
                          of resources. The result of matchLabels and

                          matchExpressions are ANDed. An empty label selector matches
                          all objects. A null

                          label selector matches no objects.'
                        nullable: true
                        properties:
                          matchExpressions:
                            description: matchExpressions is a list of label selector
                              requirements. The requirements are ANDed.
                            items:
                              description: 'A label selector requirement is a selector
                                that contains values, a key, and an operator that

                                relates the key and values.'
                              properties:
                                key:
                                  description: key is the label key that the selector
                                    applies to.
                                  type: string
                                operator:
                                  description: 'operator represents a key''s relationship
                                    to a set of values.

                                    Valid operators are In, NotIn, Exists and DoesNotExist.'
                                  type: string
                                values:
                                  description: 'values is an array of string values.
                                    If the operator is In or NotIn,

                                    the values array must be non-empty. If the operator
                                    is Exists or DoesNotExist,

                                    the values array must be empty. This array is
                                    replaced during a strategic

                                    merge patch.'
                                  items:
                                    type: string
                                  type: array
                                  x-kubernetes-list-type: atomic
                              required:
                                - key
                                - operator
                              type: object
                            type: array
                            x-kubernetes-list-type: atomic
                          matchLabels:
                            additionalProperties:
                              type: string
                            description: 'matchLabels is a map of {key,value} pairs.
                              A single {key,value} in the matchLabels

                              map is equivalent to an element of matchExpressions,
                              whose key field is "key", the

                              operator is "In", and the values array contains only
                              "value". The requirements are ANDed.'
                            type: object
                        type: object
                        x-kubernetes-map-type: atomic
                      name:
                        nullable: true
                        type: string
                    type: object
                  type: array
                targets:
                  description: 'Targets refer to the clusters which will be deployed
                    to.

                    Targets are evaluated in order and the first one to match is used.'
                  items:
                    description: 'BundleTarget declares clusters to deploy to. Fleet
                      will merge the

                      BundleDeploymentOptions from customizations into this struct.'
                    properties:
                      clusterGroup:
                        description: ClusterGroup to match a specific cluster group
                          by name.
                        nullable: true
                        type: string
                      clusterGroupSelector:
                        description: ClusterGroupSelector is a selector to match cluster
                          groups.
                        nullable: true
                        properties:
                          matchExpressions:
                            description: matchExpressions is a list of label selector
                              requirements. The requirements are ANDed.
                            items:
                              description: 'A label selector requirement is a selector
                                that contains values, a key, and an operator that

                                relates the key and values.'
                              properties:
                                key:
                                  description: key is the label key that the selector
                                    applies to.
                                  type: string
                                operator:
                                  description: 'operator represents a key''s relationship
                                    to a set of values.

                                    Valid operators are In, NotIn, Exists and DoesNotExist.'
                                  type: string
                                values:
                                  description: 'values is an array of string values.
                                    If the operator is In or NotIn,

                                    the values array must be non-empty. If the operator
                                    is Exists or DoesNotExist,

                                    the values array must be empty. This array is
                                    replaced during a strategic

                                    merge patch.'
                                  items:
                                    type: string
                                  type: array
                                  x-kubernetes-list-type: atomic
                              required:
                                - key
                                - operator
                              type: object
                            type: array
                            x-kubernetes-list-type: atomic
                          matchLabels:
                            additionalProperties:
                              type: string
                            description: 'matchLabels is a map of {key,value} pairs.
                              A single {key,value} in the matchLabels

                              map is equivalent to an element of matchExpressions,
                              whose key field is "key", the

                              operator is "In", and the values array contains only
                              "value". The requirements are ANDed.'
                            type: object
                        type: object
                        x-kubernetes-map-type: atomic
                      clusterName:
                        description: 'ClusterName to match a specific cluster by name
                          that will be

                          selected'
                        nullable: true
                        type: string
                      clusterSelector:
                        description: 'ClusterSelector is a selector to match clusters.
                          The structure is

                          the standard metav1.LabelSelector format. If clusterGroupSelector
                          or

                          clusterGroup is specified, clusterSelector will be used
                          only to

                          further refine the selection after clusterGroupSelector
                          and

                          clusterGroup is evaluated.'
                        nullable: true
                        properties:
                          matchExpressions:
                            description: matchExpressions is a list of label selector
                              requirements. The requirements are ANDed.
                            items:
                              description: 'A label selector requirement is a selector
                                that contains values, a key, and an operator that

                                relates the key and values.'
                              properties:
                                key:
                                  description: key is the label key that the selector
                                    applies to.
                                  type: string
                                operator:
                                  description: 'operator represents a key''s relationship
                                    to a set of values.

                                    Valid operators are In, NotIn, Exists and DoesNotExist.'
                                  type: string
                                values:
                                  description: 'values is an array of string values.
                                    If the operator is In or NotIn,

                                    the values array must be non-empty. If the operator
                                    is Exists or DoesNotExist,

                                    the values array must be empty. This array is
                                    replaced during a strategic

                                    merge patch.'
                                  items:
                                    type: string
                                  type: array
                                  x-kubernetes-list-type: atomic
                              required:
                                - key
                                - operator
                              type: object
                            type: array
                            x-kubernetes-list-type: atomic
                          matchLabels:
                            additionalProperties:
                              type: string
                            description: 'matchLabels is a map of {key,value} pairs.
                              A single {key,value} in the matchLabels

                              map is equivalent to an element of matchExpressions,
                              whose key field is "key", the

                              operator is "In", and the values array contains only
                              "value". The requirements are ANDed.'
                            type: object
                        type: object
                        x-kubernetes-map-type: atomic
                      correctDrift:
                        description: CorrectDrift specifies how drift correction should
                          work.
                        properties:
                          enabled:
                            description: Enabled correct drift if true.
                            type: boolean
                          force:
                            description: Force helm rollback with --force option will
                              be used if true. This will try to recreate all resources
                              in the release.
                            type: boolean
                          keepFailHistory:
                            description: KeepFailHistory keeps track of failed rollbacks
                              in the helm history.
                            type: boolean
                        type: object
                      defaultNamespace:
                        description: 'DefaultNamespace is the namespace to use for
                          resources that do not

                          specify a namespace. This field is not used to enforce or
                          lock down

                          the deployment to a specific namespace.'
                        nullable: true
                        type: string
                      deleteCRDResources:
                        description: DeleteCRDResources deletes CRDs. Warning! this
                          will also delete all your Custom Resources.
                        type: boolean
                      deleteNamespace:
                        description: DeleteNamespace can be used to delete the deployed
                          namespace when removing the bundle
                        type: boolean
                      diff:
                        description: Diff can be used to ignore the modified state
                          of objects which are amended at runtime.
                        nullable: true
                        properties:
                          comparePatches:
                            description: ComparePatches match a resource and remove
                              fields, or the resource itself from the check for modifications.
                            items:
                              description: ComparePatch matches a resource and removes
                                fields from the check for modifications.
                              properties:
                                apiVersion:
                                  description: APIVersion is the apiVersion of the
                                    resource to match.
                                  nullable: true
                                  type: string
                                jsonPointers:
                                  description: JSONPointers ignore diffs at a certain
                                    JSON path.
                                  items:
                                    type: string
                                  nullable: true
                                  type: array
                                kind:
                                  description: Kind is the kind of the resource to
                                    match.
                                  nullable: true
                                  type: string
                                name:
                                  description: Name is the name of the resource to
                                    match.
                                  nullable: true
                                  type: string
                                namespace:
                                  description: Namespace is the namespace of the resource
                                    to match.
                                  nullable: true
                                  type: string
                                operations:
                                  description: Operations remove a JSON path from
                                    the resource.
                                  items:
                                    description: 'Operation of a ComparePatch, usually:

                                      * "remove" to remove a specific path in a resource

                                      * "ignore" to remove the entire resource from
                                      checks for modifications.'
                                    properties:
                                      op:
                                        description: Op is usually "remove" or "ignore"
                                        nullable: true
                                        type: string
                                      path:
                                        description: Path is the JSON path to remove.
                                          Not needed if Op is "ignore".
                                        nullable: true
                                        type: string
                                      value:
                                        description: Value is usually empty.
                                        nullable: true
                                        type: string
                                    type: object
                                  nullable: true
                                  type: array
                              type: object
                            nullable: true
                            type: array
                        type: object
                      doNotDeploy:
                        description: DoNotDeploy if set to true, will not deploy to
                          this target.
                        type: boolean
                      downstreamResources:
                        description: 'DownstreamResources points to resources to be
                          copied into downstream clusters, from the bundle''s

                          namespace.'
                        items:
                          description: 'DownstreamResource contains identifiers for
                            a resource to be copied from the parent bundle''s namespace
                            to each

                            downstream cluster.'
                          properties:
                            kind:
                              type: string
                            name:
                              type: string
                          type: object
                        type: array
                      forceSyncGeneration:
                        description: ForceSyncGeneration is used to force a redeployment
                        format: int64
                        type: integer
                      helm:
                        description: Helm options for the deployment, like the chart
                          name, repo and values.
                        properties:
                          atomic:
                            description: Atomic sets the --atomic flag when Helm is
                              performing an upgrade
                            type: boolean
                          chart:
                            description: 'Chart can refer to any go-getter URL or
                              OCI registry based helm

                              chart URL. The chart will be downloaded.'
                            nullable: true
                            type: string
                          disableDNS:
                            description: DisableDNS can be used to customize Helm's
                              EnableDNS option, which Fleet sets to `true` by default.
                            type: boolean
                          disableDependencyUpdate:
                            description: DisableDependencyUpdate allows skipping chart
                              dependencies update
                            type: boolean
                          disablePreProcess:
                            description: DisablePreProcess disables template processing
                              in values
                            type: boolean
                          force:
                            description: Force allows to override immutable resources.
                              This could be dangerous.
                            type: boolean
                          maxHistory:
                            description: MaxHistory limits the maximum number of revisions
                              saved per release by Helm.
                            type: integer
                          releaseName:
                            description: 'ReleaseName sets a custom release name to
                              deploy the chart as. If

                              not specified a release name will be generated by combining
                              the

                              invoking GitRepo.name + GitRepo.path.'
                            maxLength: 53
                            nullable: true
                            pattern: ^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$
                            type: string
                          repo:
                            description: Repo is the name of the HTTPS helm repo to
                              download the chart from.
                            nullable: true
                            type: string
                          skipSchemaValidation:
                            description: SkipSchemaValidation allows skipping schema
                              validation against the chart values
                            type: boolean
                          takeOwnership:
                            description: TakeOwnership makes helm skip the check for
                              its own annotations
                            type: boolean
                          templateValues:
                            additionalProperties:
                              type: string
                            description: 'Template Values passed to Helm. It is possible
                              to specify the keys and values

                              as go template strings. Unlike .values, content of each
                              key will be templated

                              first, before serializing to yaml. This allows to template
                              complex values,

                              like ranges and maps.

                              templateValues keys have precedence over values keys
                              in case of conflict.'
                            nullable: true
                            type: object
                          timeoutSeconds:
                            description: TimeoutSeconds is the time to wait for Helm
                              operations.
                            type: integer
                          values:
                            description: 'Values passed to Helm. It is possible to
                              specify the keys and values

                              as go template strings.'
                            nullable: true
                            type: object
                            x-kubernetes-preserve-unknown-fields: true
                          valuesFiles:
                            description: ValuesFiles is a list of files to load values
                              from.
                            items:
                              type: string
                            nullable: true
                            type: array
                          valuesFrom:
                            description: ValuesFrom loads the values from configmaps
                              and secrets.
                            items:
                              description: 'Define helm values that can come from
                                configmap, secret or external. Credit: https://github.com/fluxcd/helm-operator/blob/0cfea875b5d44bea995abe7324819432070dfbdc/pkg/apis/helm.fluxcd.io/v1/types_helmrelease.go#L439'
                              properties:
                                configMapKeyRef:
                                  description: The reference to a config map with
                                    release values.
                                  nullable: true
                                  properties:
                                    key:
                                      nullable: true
                                      type: string
                                    name:
                                      description: Name of a resource in the same
                                        namespace as the referent.
                                      nullable: true
                                      type: string
                                    namespace:
                                      nullable: true
                                      type: string
                                  type: object
                                secretKeyRef:
                                  description: The reference to a secret with release
                                    values.
                                  nullable: true
                                  properties:
                                    key:
                                      nullable: true
                                      type: string
                                    name:
                                      description: Name of a resource in the same
                                        namespace as the referent.
                                      nullable: true
                                      type: string
                                    namespace:
                                      nullable: true
                                      type: string
                                  type: object
                              type: object
                            nullable: true
                            type: array
                          version:
                            description: Version of the chart to download
                            nullable: true
                            type: string
                          waitForJobs:
                            description: 'WaitForJobs if set and timeoutSeconds provided,
                              will wait until all

                              Jobs have been completed before marking the GitRepo
                              as ready. It

                              will wait for as long as timeoutSeconds'
                            type: boolean
                        type: object
                      ignore:
                        description: IgnoreOptions can be used to ignore fields when
                          monitoring the bundle.
                        nullable: true
                        properties:
                          conditions:
                            description: Conditions is a list of conditions to be
                              ignored when monitoring the Bundle.
                            items:
                              additionalProperties:
                                type: string
                              type: object
                            nullable: true
                            type: array
                        type: object
                      keepResources:
                        description: KeepResources can be used to keep the deployed
                          resources when removing the bundle
                        type: boolean
                      kustomize:
                        description: 'Kustomize options for the deployment, like the
                          dir containing the

                          kustomization.yaml file.'
                        nullable: true
                        properties:
                          dir:
                            description: 'Dir points to a custom folder for kustomize
                              resources. This folder must contain

                              a kustomization.yaml file.'
                            nullable: true
                            type: string
                        type: object
                      name:
                        description: 'Name of target. This value is largely for display
                          and logging. If

                          not specified a default name of the format "target000" will
                          be used'
                        type: string
                      namespace:
                        description: 'TargetNamespace if present will assign all resource
                          to this

                          namespace and if any cluster scoped resource exists the
                          deployment

                          will fail.'
                        nullable: true
                        type: string
                      namespaceAnnotations:
                        additionalProperties:
                          type: string
                        description: NamespaceAnnotations are annotations that will
                          be appended to the namespace created by Fleet.
                        nullable: true
                        type: object
                      namespaceLabels:
                        additionalProperties:
                          type: string
                        description: NamespaceLabels are labels that will be appended
                          to the namespace created by Fleet.
                        nullable: true
                        type: object
                      overwrites:
                        description: 'Overwrites indicates which resources, if any,
                          come from this bundle and overwrite another existing bundle.

                          This flag is set internally by Fleet, and should not be
                          altered by users.'
                        items:
                          properties:
                            kind:
                              type: string
                            name:
                              type: string
                            namespace:
                              type: string
                          type: object
                        type: array
                      serviceAccount:
                        description: ServiceAccount which will be used to perform
                          this deployment.
                        nullable: true
                        type: string
                      yaml:
                        description: 'YAML options, if using raw YAML these are names
                          that map to

                          overlays/{name} files that will be used to replace or patch
                          a resource.'
                        nullable: true
                        properties:
                          overlays:
                            description: 'Overlays is a list of names that maps to
                              folders in "overlays/".

                              If you wish to customize the file ./subdir/resource.yaml
                              then a file

                              ./overlays/myoverlay/subdir/resource.yaml will replace
                              the base

                              file.

                              A file named ./overlays/myoverlay/subdir/resource_patch.yaml
                              will patch the base file.'
                            items:
                              type: string
                            nullable: true
                            type: array
                        type: object
                    type: object
                  type: array
                valuesHash:
                  description: 'ValuesHash is the hash of the values used to render
                    the Helm chart.

                    It changes when any values from fleet.yaml, values from ValuesFiles
                    or values from target

                    customization changes.'
                  type: string
                yaml:
                  description: 'YAML options, if using raw YAML these are names that
                    map to

                    overlays/{name} files that will be used to replace or patch a
                    resource.'
                  nullable: true
                  properties:
                    overlays:
                      description: 'Overlays is a list of names that maps to folders
                        in "overlays/".

                        If you wish to customize the file ./subdir/resource.yaml then
                        a file

                        ./overlays/myoverlay/subdir/resource.yaml will replace the
                        base

                        file.

                        A file named ./overlays/myoverlay/subdir/resource_patch.yaml
                        will patch the base file.'
                      items:
                        type: string
                      nullable: true
                      type: array
                  type: object
              type: object
            status:
              properties:
                conditions:
                  description: 'Conditions is a list of Wrangler conditions that describe
                    the state

                    of the bundle.'
                  items:
                    properties:
                      lastTransitionTime:
                        description: Last time the condition transitioned from one
                          status to another.
                        type: string
                      lastUpdateTime:
                        description: The last time this condition was updated.
                        type: string
                      message:
                        description: Human-readable message indicating details about
                          last transition
                        type: string
                      reason:
                        description: The reason for the condition's last transition.
                        type: string
                      status:
                        description: Status of the condition, one of True, False,
                          Unknown.
                        type: string
                      type:
                        description: Type of cluster condition.
                        type: string
                    required:
                      - status
                      - type
                    type: object
                  type: array
                display:
                  description: 'Display contains the number of ready, desiredready
                    clusters and a

                    summary state for the bundle''s resources.'
                  properties:
                    readyClusters:
                      description: 'ReadyClusters is a string in the form "%d/%d",
                        that describes the

                        number of clusters that are ready vs. the number of clusters
                        desired

                        to be ready.'
                      nullable: true
                      type: string
                    state:
                      description: State is a summary state for the bundle, calculated
                        over the non-ready resources.
                      nullable: true
                      type: string
                  type: object
                maxNew:
                  description: 'MaxNew is always 50. A bundle change can only stage
                    50

                    bundledeployments at a time.'
                  type: integer
                maxUnavailable:
                  description: 'MaxUnavailable is the maximum number of unavailable
                    deployments. See

                    rollout configuration.'
                  type: integer
                maxUnavailablePartitions:
                  description: 'MaxUnavailablePartitions is the maximum number of
                    unavailable

                    partitions. The rollout configuration defines a maximum number
                    or

                    percentage of unavailable partitions.'
                  type: integer
                newlyCreated:
                  description: 'NewlyCreated is the number of bundle deployments that
                    have been created,

                    not updated.'
                  type: integer
                observedGeneration:
                  description: ObservedGeneration is the current generation of the
                    bundle.
                  format: int64
                  type: integer
                ociReference:
                  description: 'OCIReference is the OCI reference used to store contents,
                    this is

                    only for informational purposes.'
                  type: string
                partitions:
                  description: PartitionStatus lists the status of each partition.
                  items:
                    description: PartitionStatus is the status of a single rollout
                      partition.
                    properties:
                      count:
                        description: Count is the number of clusters in the partition.
                        type: integer
                      maxUnavailable:
                        description: MaxUnavailable is the maximum number of unavailable
                          clusters in the partition.
                        type: integer
                      name:
                        description: Name is the name of the partition.
                        nullable: true
                        type: string
                      summary:
                        description: Summary is a summary state for the partition,
                          calculated over its non-ready resources.
                        properties:
                          desiredReady:
                            description: 'DesiredReady is the number of bundle deployments
                              that should be

                              ready.'
                            type: integer
                          errApplied:
                            description: 'ErrApplied is the number of bundle deployments
                              that have been synced

                              from the Fleet controller and the downstream cluster,
                              but with some

                              errors when deploying the bundle.'
                            type: integer
                          modified:
                            description: 'Modified is the number of bundle deployments
                              that have been deployed

                              and for which all resources are ready, but where some
                              changes from the

                              Git repository have not yet been synced.'
                            type: integer
                          nonReadyResources:
                            description: 'NonReadyClusters is a list of states, which
                              is filled for a bundle

                              that is not ready.'
                            items:
                              description: 'NonReadyResource contains information
                                about a bundle that is not ready for a

                                given state like "ErrApplied". It contains a list
                                of non-ready or modified

                                resources and their states.'
                              properties:
                                bundleState:
                                  description: State is the state of the resource,
                                    like e.g. "NotReady" or "ErrApplied".
                                  nullable: true
                                  type: string
                                message:
                                  description: Message contains information why the
                                    bundle is not ready.
                                  nullable: true
                                  type: string
                                modifiedStatus:
                                  description: ModifiedStatus lists the state for
                                    each modified resource.
                                  items:
                                    description: 'ModifiedStatus is used to report
                                      the status of a resource that is modified.

                                      It indicates if the modification was a create,
                                      a delete or a patch.'
                                    properties:
                                      apiVersion:
                                        nullable: true
                                        type: string
                                      delete:
                                        type: boolean
                                      exist:
                                        description: Exist is true if the resource
                                          exists but is not owned by us. This can
                                          happen if a resource was adopted by another
                                          bundle whereas the first bundle still exists
                                          and due to that reports that it does not
                                          own it.
                                        type: boolean
                                      kind:
                                        nullable: true
                                        type: string
                                      missing:
                                        type: boolean
                                      name:
                                        nullable: true
                                        type: string
                                      namespace:
                                        nullable: true
                                        type: string
                                      patch:
                                        nullable: true
                                        type: string
                                    type: object
                                  nullable: true
                                  type: array
                                name:
                                  description: Name is the name of the resource.
                                  nullable: true
                                  type: string
                                nonReadyStatus:
                                  description: NonReadyStatus lists the state for
                                    each non-ready resource.
                                  items:
                                    description: NonReadyStatus is used to report
                                      the status of a resource that is not ready.
                                      It includes a summary.
                                    properties:
                                      apiVersion:
                                        nullable: true
                                        type: string
                                      kind:
                                        nullable: true
                                        type: string
                                      name:
                                        nullable: true
                                        type: string
                                      namespace:
                                        nullable: true
                                        type: string
                                      summary:
                                        properties:
                                          error:
                                            type: boolean
                                          message:
                                            items:
                                              type: string
                                            type: array
                                          state:
                                            type: string
                                          transitioning:
                                            type: boolean
                                        type: object
                                      uid:
                                        description: 'UID is a type that holds unique
                                          ID values, including UUIDs.  Because we

                                          don''t ONLY use UUIDs, this is an alias
                                          to string.  Being a type captures

                                          intent and helps make sure that UIDs and
                                          names do not get conflated.'
                                        nullable: true
                                        type: string
                                    type: object
                                  nullable: true
                                  type: array
                              type: object
                            nullable: true
                            type: array
                          notReady:
                            description: 'NotReady is the number of bundle deployments
                              that have been deployed

                              where some resources are not ready.'
                            type: integer
                          outOfSync:
                            description: 'OutOfSync is the number of bundle deployments
                              that have been synced

                              from Fleet controller, but not yet by the downstream
                              agent.'
                            type: integer
                          pending:
                            description: 'Pending is the number of bundle deployments
                              that are being processed

                              by Fleet controller.'
                            type: integer
                          ready:
                            description: 'Ready is the number of bundle deployments
                              that have been deployed

                              where all resources are ready.'
                            type: integer
                          waitApplied:
                            description: 'WaitApplied is the number of bundle deployments
                              that have been

                              synced from Fleet controller and downstream cluster,
                              but are waiting

                              to be deployed.'
                            type: integer
                        type: object
                      unavailable:
                        description: Unavailable is the number of unavailable clusters
                          in the partition.
                        type: integer
                    type: object
                  type: array
                resourceKey:
                  description: 'ResourceKey lists resources, which will likely be
                    deployed. The

                    actual list of resources on a cluster might differ, depending
                    on the

                    helm chart, value templating, etc.. (deprecated to reduce bundle
                    size)'
                  items:
                    description: ResourceKey lists resources, which will likely be
                      deployed.
                    properties:
                      apiVersion:
                        description: APIVersion is the k8s api version of the resource.
                        nullable: true
                        type: string
                      kind:
                        description: Kind is the k8s api kind of the resource.
                        nullable: true
                        type: string
                      name:
                        description: Name is the name of the resource.
                        nullable: true
                        type: string
                      namespace:
                        description: Namespace is the namespace of the resource.
                        nullable: true
                        type: string
                    type: object
                  nullable: true
                  type: array
                resourcesSha256Sum:
                  description: ResourcesSHA256Sum corresponds to the JSON serialization
                    of the .Spec.Resources field
                  type: string
                summary:
                  description: 'Summary contains the number of bundle deployments
                    in each state and

                    a list of non-ready resources.'
                  properties:
                    desiredReady:
                      description: 'DesiredReady is the number of bundle deployments
                        that should be

                        ready.'
                      type: integer
                    errApplied:
                      description: 'ErrApplied is the number of bundle deployments
                        that have been synced

                        from the Fleet controller and the downstream cluster, but
                        with some

                        errors when deploying the bundle.'
                      type: integer
                    modified:
                      description: 'Modified is the number of bundle deployments that
                        have been deployed

                        and for which all resources are ready, but where some changes
                        from the

                        Git repository have not yet been synced.'
                      type: integer
                    nonReadyResources:
                      description: 'NonReadyClusters is a list of states, which is
                        filled for a bundle

                        that is not ready.'
                      items:
                        description: 'NonReadyResource contains information about
                          a bundle that is not ready for a

                          given state like "ErrApplied". It contains a list of non-ready
                          or modified

                          resources and their states.'
                        properties:
                          bundleState:
                            description: State is the state of the resource, like
                              e.g. "NotReady" or "ErrApplied".
                            nullable: true
                            type: string
                          message:
                            description: Message contains information why the bundle
                              is not ready.
                            nullable: true
                            type: string
                          modifiedStatus:
                            description: ModifiedStatus lists the state for each modified
                              resource.
                            items:
                              description: 'ModifiedStatus is used to report the status
                                of a resource that is modified.

                                It indicates if the modification was a create, a delete
                                or a patch.'
                              properties:
                                apiVersion:
                                  nullable: true
                                  type: string
                                delete:
                                  type: boolean
                                exist:
                                  description: Exist is true if the resource exists
                                    but is not owned by us. This can happen if a resource
                                    was adopted by another bundle whereas the first
                                    bundle still exists and due to that reports that
                                    it does not own it.
                                  type: boolean
                                kind:
                                  nullable: true
                                  type: string
                                missing:
                                  type: boolean
                                name:
                                  nullable: true
                                  type: string
                                namespace:
                                  nullable: true
                                  type: string
                                patch:
                                  nullable: true
                                  type: string
                              type: object
                            nullable: true
                            type: array
                          name:
                            description: Name is the name of the resource.
                            nullable: true
                            type: string
                          nonReadyStatus:
                            description: NonReadyStatus lists the state for each non-ready
                              resource.
                            items:
                              description: NonReadyStatus is used to report the status
                                of a resource that is not ready. It includes a summary.
                              properties:
                                apiVersion:
                                  nullable: true
                                  type: string
                                kind:
                                  nullable: true
                                  type: string
                                name:
                                  nullable: true
                                  type: string
                                namespace:
                                  nullable: true
                                  type: string
                                summary:
                                  properties:
                                    error:
                                      type: boolean
                                    message:
                                      items:
                                        type: string
                                      type: array
                                    state:
                                      type: string
                                    transitioning:
                                      type: boolean
                                  type: object
                                uid:
                                  description: 'UID is a type that holds unique ID
                                    values, including UUIDs.  Because we

                                    don''t ONLY use UUIDs, this is an alias to string.  Being
                                    a type captures

                                    intent and helps make sure that UIDs and names
                                    do not get conflated.'
                                  nullable: true
                                  type: string
                              type: object
                            nullable: true
                            type: array
                        type: object
                      nullable: true
                      type: array
                    notReady:
                      description: 'NotReady is the number of bundle deployments that
                        have been deployed

                        where some resources are not ready.'
                      type: integer
                    outOfSync:
                      description: 'OutOfSync is the number of bundle deployments
                        that have been synced

                        from Fleet controller, but not yet by the downstream agent.'
                      type: integer
                    pending:
                      description: 'Pending is the number of bundle deployments that
                        are being processed

                        by Fleet controller.'
                      type: integer
                    ready:
                      description: 'Ready is the number of bundle deployments that
                        have been deployed

                        where all resources are ready.'
                      type: integer
                    waitApplied:
                      description: 'WaitApplied is the number of bundle deployments
                        that have been

                        synced from Fleet controller and downstream cluster, but are
                        waiting

                        to be deployed.'
                      type: integer
                  type: object
                unavailable:
                  description: 'Unavailable is the number of bundle deployments that
                    are not ready or

                    where the AppliedDeploymentID in the status does not match the

                    DeploymentID from the spec.'
                  type: integer
                unavailablePartitions:
                  description: UnavailablePartitions is the number of unavailable
                    partitions.
                  type: integer
              type: object
          type: object
      served: true
      storage: true
      subresources:
        status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.19.0
  name: clustergroups.fleet.cattle.io
spec:
  group: fleet.cattle.io
  names:
    categories:
      - fleet
    kind: ClusterGroup
    listKind: ClusterGroupList
    plural: clustergroups
    singular: clustergroup
  scope: Namespaced
  versions:
    - additionalPrinterColumns:
        - jsonPath: .status.display.readyClusters
          name: Clusters-Ready
          type: string
        - jsonPath: .status.display.readyBundles
          name: Bundles-Ready
          type: string
        - jsonPath: .status.conditions[?(@.type=="Ready")].message
          name: Status
          type: string
      name: v1alpha1
      schema:
        openAPIV3Schema:
          description: ClusterGroup is a re-usable selector to target a group of clusters.
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
                of an object.

                Servers should convert recognized schemas to the latest internal value,
                and

                may reject unrecognized values.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource
                this object represents.

                Servers may infer this from the endpoint the client submits requests
                to.

                Cannot be updated.

                In CamelCase.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
            spec:
              properties:
                selector:
                  description: Selector is a label selector, used to select clusters
                    for this group.
                  nullable: true
                  properties:
                    matchExpressions:
                      description: matchExpressions is a list of label selector requirements.
                        The requirements are ANDed.
                      items:
                        description: 'A label selector requirement is a selector that
                          contains values, a key, and an operator that

                          relates the key and values.'
                        properties:
                          key:
                            description: key is the label key that the selector applies
                              to.
                            type: string
                          operator:
                            description: 'operator represents a key''s relationship
                              to a set of values.

                              Valid operators are In, NotIn, Exists and DoesNotExist.'
                            type: string
                          values:
                            description: 'values is an array of string values. If
                              the operator is In or NotIn,

                              the values array must be non-empty. If the operator
                              is Exists or DoesNotExist,

                              the values array must be empty. This array is replaced
                              during a strategic

                              merge patch.'
                            items:
                              type: string
                            type: array
                            x-kubernetes-list-type: atomic
                        required:
                          - key
                          - operator
                        type: object
                      type: array
                      x-kubernetes-list-type: atomic
                    matchLabels:
                      additionalProperties:
                        type: string
                      description: 'matchLabels is a map of {key,value} pairs. A single
                        {key,value} in the matchLabels

                        map is equivalent to an element of matchExpressions, whose
                        key field is "key", the

                        operator is "In", and the values array contains only "value".
                        The requirements are ANDed.'
                      type: object
                  type: object
                  x-kubernetes-map-type: atomic
              type: object
            status:
              properties:
                clusterCount:
                  description: ClusterCount is the number of clusters in the cluster
                    group.
                  type: integer
                conditions:
                  description: Conditions is a list of conditions and their statuses
                    for the cluster group.
                  items:
                    properties:
                      lastTransitionTime:
                        description: Last time the condition transitioned from one
                          status to another.
                        type: string
                      lastUpdateTime:
                        description: The last time this condition was updated.
                        type: string
                      message:
                        description: Human-readable message indicating details about
                          last transition
                        type: string
                      reason:
                        description: The reason for the condition's last transition.
                        type: string
                      status:
                        description: Status of the condition, one of True, False,
                          Unknown.
                        type: string
                      type:
                        description: Type of cluster condition.
                        type: string
                    required:
                      - status
                      - type
                    type: object
                  type: array
                display:
                  description: 'Display contains the number of ready, desiredready
                    clusters and a

                    summary state for the bundle''s resources.'
                  properties:
                    readyBundles:
                      description: 'ReadyBundles is a string in the form "%d/%d",
                        that describes the

                        number of bundles that are ready vs. the number of bundles
                        desired

                        to be ready.'
                      nullable: true
                      type: string
                    readyClusters:
                      description: 'ReadyClusters is a string in the form "%d/%d",
                        that describes the

                        number of clusters that are ready vs. the number of clusters
                        desired

                        to be ready.'
                      nullable: true
                      type: string
                    state:
                      description: 'State is a summary state for the cluster group,
                        showing "NotReady" if

                        there are non-ready resources.'
                      nullable: true
                      type: string
                  type: object
                nonReadyClusterCount:
                  description: NonReadyClusterCount is the number of clusters that
                    are not ready.
                  type: integer
                nonReadyClusters:
                  description: NonReadyClusters is a list of cluster names that are
                    not ready.
                  items:
                    type: string
                  nullable: true
                  type: array
                resourceCounts:
                  description: 'ResourceCounts contains the number of resources in
                    each state over

                    all bundles in the cluster group.'
                  properties:
                    desiredReady:
                      description: DesiredReady is the number of resources that should
                        be ready.
                      type: integer
                    missing:
                      description: Missing is the number of missing resources.
                      type: integer
                    modified:
                      description: Modified is the number of resources that have been
                        modified.
                      type: integer
                    notReady:
                      description: 'NotReady is the number of not ready resources.
                        Resources are not

                        ready if they do not match any other state.'
                      type: integer
                    orphaned:
                      description: Orphaned is the number of orphaned resources.
                      type: integer
                    ready:
                      description: Ready is the number of ready resources.
                      type: integer
                    unknown:
                      description: Unknown is the number of resources in an unknown
                        state.
                      type: integer
                    waitApplied:
                      description: WaitApplied is the number of resources that are
                        waiting to be applied.
                      type: integer
                  type: object
                summary:
                  description: 'Summary is a summary of the bundle deployments and
                    their resources

                    in the cluster group.'
                  properties:
                    desiredReady:
                      description: 'DesiredReady is the number of bundle deployments
                        that should be

                        ready.'
                      type: integer
                    errApplied:
                      description: 'ErrApplied is the number of bundle deployments
                        that have been synced

                        from the Fleet controller and the downstream cluster, but
                        with some

                        errors when deploying the bundle.'
                      type: integer
                    modified:
                      description: 'Modified is the number of bundle deployments that
                        have been deployed

                        and for which all resources are ready, but where some changes
                        from the

                        Git repository have not yet been synced.'
                      type: integer
                    nonReadyResources:
                      description: 'NonReadyClusters is a list of states, which is
                        filled for a bundle

                        that is not ready.'
                      items:
                        description: 'NonReadyResource contains information about
                          a bundle that is not ready for a

                          given state like "ErrApplied". It contains a list of non-ready
                          or modified

                          resources and their states.'
                        properties:
                          bundleState:
                            description: State is the state of the resource, like
                              e.g. "NotReady" or "ErrApplied".
                            nullable: true
                            type: string
                          message:
                            description: Message contains information why the bundle
                              is not ready.
                            nullable: true
                            type: string
                          modifiedStatus:
                            description: ModifiedStatus lists the state for each modified
                              resource.
                            items:
                              description: 'ModifiedStatus is used to report the status
                                of a resource that is modified.

                                It indicates if the modification was a create, a delete
                                or a patch.'
                              properties:
                                apiVersion:
                                  nullable: true
                                  type: string
                                delete:
                                  type: boolean
                                exist:
                                  description: Exist is true if the resource exists
                                    but is not owned by us. This can happen if a resource
                                    was adopted by another bundle whereas the first
                                    bundle still exists and due to that reports that
                                    it does not own it.
                                  type: boolean
                                kind:
                                  nullable: true
                                  type: string
                                missing:
                                  type: boolean
                                name:
                                  nullable: true
                                  type: string
                                namespace:
                                  nullable: true
                                  type: string
                                patch:
                                  nullable: true
                                  type: string
                              type: object
                            nullable: true
                            type: array
                          name:
                            description: Name is the name of the resource.
                            nullable: true
                            type: string
                          nonReadyStatus:
                            description: NonReadyStatus lists the state for each non-ready
                              resource.
                            items:
                              description: NonReadyStatus is used to report the status
                                of a resource that is not ready. It includes a summary.
                              properties:
                                apiVersion:
                                  nullable: true
                                  type: string
                                kind:
                                  nullable: true
                                  type: string
                                name:
                                  nullable: true
                                  type: string
                                namespace:
                                  nullable: true
                                  type: string
                                summary:
                                  properties:
                                    error:
                                      type: boolean
                                    message:
                                      items:
                                        type: string
                                      type: array
                                    state:
                                      type: string
                                    transitioning:
                                      type: boolean
                                  type: object
                                uid:
                                  description: 'UID is a type that holds unique ID
                                    values, including UUIDs.  Because we

                                    don''t ONLY use UUIDs, this is an alias to string.  Being
                                    a type captures

                                    intent and helps make sure that UIDs and names
                                    do not get conflated.'
                                  nullable: true
                                  type: string
                              type: object
                            nullable: true
                            type: array
                        type: object
                      nullable: true
                      type: array
                    notReady:
                      description: 'NotReady is the number of bundle deployments that
                        have been deployed

                        where some resources are not ready.'
                      type: integer
                    outOfSync:
                      description: 'OutOfSync is the number of bundle deployments
                        that have been synced

                        from Fleet controller, but not yet by the downstream agent.'
                      type: integer
                    pending:
                      description: 'Pending is the number of bundle deployments that
                        are being processed

                        by Fleet controller.'
                      type: integer
                    ready:
                      description: 'Ready is the number of bundle deployments that
                        have been deployed

                        where all resources are ready.'
                      type: integer
                    waitApplied:
                      description: 'WaitApplied is the number of bundle deployments
                        that have been

                        synced from Fleet controller and downstream cluster, but are
                        waiting

                        to be deployed.'
                      type: integer
                  type: object
              type: object
          type: object
      served: true
      storage: true
      subresources:
        status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.19.0
  name: clusterregistrations.fleet.cattle.io
spec:
  group: fleet.cattle.io
  names:
    kind: ClusterRegistration
    listKind: ClusterRegistrationList
    plural: clusterregistrations
    singular: clusterregistration
  scope: Namespaced
  versions:
    - additionalPrinterColumns:
        - jsonPath: .status.clusterName
          name: Cluster-Name
          type: string
        - jsonPath: .spec.clusterLabels
          name: Labels
          type: string
      name: v1alpha1
      schema:
        openAPIV3Schema:
          description: ClusterRegistration is used internally by Fleet and should
            not be used directly.
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
                of an object.

                Servers should convert recognized schemas to the latest internal value,
                and

                may reject unrecognized values.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource
                this object represents.

                Servers may infer this from the endpoint the client submits requests
                to.

                Cannot be updated.

                In CamelCase.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
            spec:
              properties:
                clientID:
                  description: 'ClientID is a unique string that will identify the
                    cluster. The

                    agent either uses the configured ID or the kubeSystem.UID.'
                  nullable: true
                  type: string
                clientRandom:
                  description: 'ClientRandom is a random string that the agent generates.
                    When

                    fleet-controller grants a registration, it creates a registration

                    secret with this string in the name.'
                  nullable: true
                  type: string
                clusterLabels:
                  additionalProperties:
                    type: string
                  description: ClusterLabels are copied to the cluster resource during
                    the registration.
                  nullable: true
                  type: object
              type: object
            status:
              properties:
                clusterName:
                  description: 'ClusterName is only set after the registration is
                    being processed by

                    fleet-controller.'
                  nullable: true
                  type: string
                granted:
                  description: 'Granted is set to true, if the request service account
                    is present

                    and its token secret exists. This happens directly before creating

                    the registration secret, roles and rolebindings.'
                  type: boolean
              type: object
          type: object
      served: true
      storage: true
      subresources:
        status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.19.0
  name: clusterregistrationtokens.fleet.cattle.io
spec:
  group: fleet.cattle.io
  names:
    kind: ClusterRegistrationToken
    listKind: ClusterRegistrationTokenList
    plural: clusterregistrationtokens
    singular: clusterregistrationtoken
  scope: Namespaced
  versions:
    - additionalPrinterColumns:
        - jsonPath: .status.secretName
          name: Secret-Name
          type: string
      name: v1alpha1
      schema:
        openAPIV3Schema:
          description: ClusterRegistrationToken is used by agents to register a new
            cluster.
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
                of an object.

                Servers should convert recognized schemas to the latest internal value,
                and

                may reject unrecognized values.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource
                this object represents.

                Servers may infer this from the endpoint the client submits requests
                to.

                Cannot be updated.

                In CamelCase.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
            spec:
              properties:
                ttl:
                  description: 'TTL is the time to live for the token. It is used
                    to calculate the

                    expiration time. If the token expires, it will be deleted.'
                  nullable: true
                  type: string
              type: object
            status:
              properties:
                expires:
                  description: Expires is the time when the token expires.
                  format: date-time
                  type: string
                secretName:
                  description: SecretName is the name of the secret containing the
                    token.
                  nullable: true
                  type: string
              type: object
          type: object
      served: true
      storage: true
      subresources:
        status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.19.0
  name: clusters.fleet.cattle.io
spec:
  group: fleet.cattle.io
  names:
    kind: Cluster
    listKind: ClusterList
    plural: clusters
    singular: cluster
  scope: Namespaced
  versions:
    - additionalPrinterColumns:
        - jsonPath: .status.display.readyBundles
          name: Bundles-Ready
          type: string
        - jsonPath: .status.agent.lastSeen
          name: Last-Seen
          type: string
        - jsonPath: .status.conditions[?(@.type=="Ready")].message
          name: Status
          type: string
      name: v1alpha1
      schema:
        openAPIV3Schema:
          description: 'Cluster corresponds to a Kubernetes cluster. Fleet deploys
            bundles to targeted clusters.

            Clusters to which Fleet deploys manifests are referred to as downstream

            clusters. In the single cluster use case, the Fleet Kubernetes

            cluster is both the manager and downstream cluster at the same time.'
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
                of an object.

                Servers should convert recognized schemas to the latest internal value,
                and

                may reject unrecognized values.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource
                this object represents.

                Servers may infer this from the endpoint the client submits requests
                to.

                Cannot be updated.

                In CamelCase.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
            spec:
              properties:
                agentAffinity:
                  description: 'AgentAffinity overrides the default affinity for the
                    cluster''s agent

                    deployment. If this value is nil the default affinity is used.'
                  nullable: true
                  properties:
                    nodeAffinity:
                      description: Describes node affinity scheduling rules for the
                        pod.
                      properties:
                        preferredDuringSchedulingIgnoredDuringExecution:
                          description: 'The scheduler will prefer to schedule pods
                            to nodes that satisfy

                            the affinity expressions specified by this field, but
                            it may choose

                            a node that violates one or more of the expressions. The
                            node that is

                            most preferred is the one with the greatest sum of weights,
                            i.e.

                            for each node that meets all of the scheduling requirements
                            (resource

                            request, requiredDuringScheduling affinity expressions,
                            etc.),

                            compute a sum by iterating through the elements of this
                            field and adding

                            "weight" to the sum if the node matches the corresponding
                            matchExpressions; the

                            node(s) with the highest sum are the most preferred.'
                          items:
                            description: 'An empty preferred scheduling term matches
                              all objects with implicit weight 0

                              (i.e. it''s a no-op). A null preferred scheduling term
                              matches no objects (i.e. is also a no-op).'
                            properties:
                              preference:
                                description: A node selector term, associated with
                                  the corresponding weight.
                                properties:
                                  matchExpressions:
                                    description: A list of node selector requirements
                                      by node's labels.
                                    items:
                                      description: 'A node selector requirement is
                                        a selector that contains values, a key, and
                                        an operator

                                        that relates the key and values.'
                                      properties:
                                        key:
                                          description: The label key that the selector
                                            applies to.
                                          type: string
                                        operator:
                                          description: 'Represents a key''s relationship
                                            to a set of values.

                                            Valid operators are In, NotIn, Exists,
                                            DoesNotExist. Gt, and Lt.'
                                          type: string
                                        values:
                                          description: 'An array of string values.
                                            If the operator is In or NotIn,

                                            the values array must be non-empty. If
                                            the operator is Exists or DoesNotExist,

                                            the values array must be empty. If the
                                            operator is Gt or Lt, the values

                                            array must have a single element, which
                                            will be interpreted as an integer.

                                            This array is replaced during a strategic
                                            merge patch.'
                                          items:
                                            type: string
                                          type: array
                                          x-kubernetes-list-type: atomic
                                      required:
                                        - key
                                        - operator
                                      type: object
                                    type: array
                                    x-kubernetes-list-type: atomic
                                  matchFields:
                                    description: A list of node selector requirements
                                      by node's fields.
                                    items:
                                      description: 'A node selector requirement is
                                        a selector that contains values, a key, and
                                        an operator

                                        that relates the key and values.'
                                      properties:
                                        key:
                                          description: The label key that the selector
                                            applies to.
                                          type: string
                                        operator:
                                          description: 'Represents a key''s relationship
                                            to a set of values.

                                            Valid operators are In, NotIn, Exists,
                                            DoesNotExist. Gt, and Lt.'
                                          type: string
                                        values:
                                          description: 'An array of string values.
                                            If the operator is In or NotIn,

                                            the values array must be non-empty. If
                                            the operator is Exists or DoesNotExist,

                                            the values array must be empty. If the
                                            operator is Gt or Lt, the values

                                            array must have a single element, which
                                            will be interpreted as an integer.

                                            This array is replaced during a strategic
                                            merge patch.'
                                          items:
                                            type: string
                                          type: array
                                          x-kubernetes-list-type: atomic
                                      required:
                                        - key
                                        - operator
                                      type: object
                                    type: array
                                    x-kubernetes-list-type: atomic
                                type: object
                                x-kubernetes-map-type: atomic
                              weight:
                                description: Weight associated with matching the corresponding
                                  nodeSelectorTerm, in the range 1-100.
                                format: int32
                                type: integer
                            required:
                              - preference
                              - weight
                            type: object
                          type: array
                          x-kubernetes-list-type: atomic
                        requiredDuringSchedulingIgnoredDuringExecution:
                          description: 'If the affinity requirements specified by
                            this field are not met at

                            scheduling time, the pod will not be scheduled onto the
                            node.

                            If the affinity requirements specified by this field cease
                            to be met

                            at some point during pod execution (e.g. due to an update),
                            the system

                            may or may not try to eventually evict the pod from its
                            node.'
                          properties:
                            nodeSelectorTerms:
                              description: Required. A list of node selector terms.
                                The terms are ORed.
                              items:
                                description: 'A null or empty node selector term matches
                                  no objects. The requirements of

                                  them are ANDed.

                                  The TopologySelectorTerm type implements a subset
                                  of the NodeSelectorTerm.'
                                properties:
                                  matchExpressions:
                                    description: A list of node selector requirements
                                      by node's labels.
                                    items:
                                      description: 'A node selector requirement is
                                        a selector that contains values, a key, and
                                        an operator

                                        that relates the key and values.'
                                      properties:
                                        key:
                                          description: The label key that the selector
                                            applies to.
                                          type: string
                                        operator:
                                          description: 'Represents a key''s relationship
                                            to a set of values.

                                            Valid operators are In, NotIn, Exists,
                                            DoesNotExist. Gt, and Lt.'
                                          type: string
                                        values:
                                          description: 'An array of string values.
                                            If the operator is In or NotIn,

                                            the values array must be non-empty. If
                                            the operator is Exists or DoesNotExist,

                                            the values array must be empty. If the
                                            operator is Gt or Lt, the values

                                            array must have a single element, which
                                            will be interpreted as an integer.

                                            This array is replaced during a strategic
                                            merge patch.'
                                          items:
                                            type: string
                                          type: array
                                          x-kubernetes-list-type: atomic
                                      required:
                                        - key
                                        - operator
                                      type: object
                                    type: array
                                    x-kubernetes-list-type: atomic
                                  matchFields:
                                    description: A list of node selector requirements
                                      by node's fields.
                                    items:
                                      description: 'A node selector requirement is
                                        a selector that contains values, a key, and
                                        an operator

                                        that relates the key and values.'
                                      properties:
                                        key:
                                          description: The label key that the selector
                                            applies to.
                                          type: string
                                        operator:
                                          description: 'Represents a key''s relationship
                                            to a set of values.

                                            Valid operators are In, NotIn, Exists,
                                            DoesNotExist. Gt, and Lt.'
                                          type: string
                                        values:
                                          description: 'An array of string values.
                                            If the operator is In or NotIn,

                                            the values array must be non-empty. If
                                            the operator is Exists or DoesNotExist,

                                            the values array must be empty. If the
                                            operator is Gt or Lt, the values

                                            array must have a single element, which
                                            will be interpreted as an integer.

                                            This array is replaced during a strategic
                                            merge patch.'
                                          items:
                                            type: string
                                          type: array
                                          x-kubernetes-list-type: atomic
                                      required:
                                        - key
                                        - operator
                                      type: object
                                    type: array
                                    x-kubernetes-list-type: atomic
                                type: object
                                x-kubernetes-map-type: atomic
                              type: array
                              x-kubernetes-list-type: atomic
                          required:
                            - nodeSelectorTerms
                          type: object
                          x-kubernetes-map-type: atomic
                      type: object
                    podAffinity:
                      description: Describes pod affinity scheduling rules (e.g. co-locate
                        this pod in the same node, zone, etc. as some other pod(s)).
                      properties:
                        preferredDuringSchedulingIgnoredDuringExecution:
                          description: 'The scheduler will prefer to schedule pods
                            to nodes that satisfy

                            the affinity expressions specified by this field, but
                            it may choose

                            a node that violates one or more of the expressions. The
                            node that is

                            most preferred is the one with the greatest sum of weights,
                            i.e.

                            for each node that meets all of the scheduling requirements
                            (resource

                            request, requiredDuringScheduling affinity expressions,
                            etc.),

                            compute a sum by iterating through the elements of this
                            field and adding

                            "weight" to the sum if the node has pods which matches
                            the corresponding podAffinityTerm; the

                            node(s) with the highest sum are the most preferred.'
                          items:
                            description: The weights of all of the matched WeightedPodAffinityTerm
                              fields are added per-node to find the most preferred
                              node(s)
                            properties:
                              podAffinityTerm:
                                description: Required. A pod affinity term, associated
                                  with the corresponding weight.
                                properties:
                                  labelSelector:
                                    description: 'A label query over a set of resources,
                                      in this case pods.

                                      If it''s null, this PodAffinityTerm matches
                                      with no Pods.'
                                    properties:
                                      matchExpressions:
                                        description: matchExpressions is a list of
                                          label selector requirements. The requirements
                                          are ANDed.
                                        items:
                                          description: 'A label selector requirement
                                            is a selector that contains values, a
                                            key, and an operator that

                                            relates the key and values.'
                                          properties:
                                            key:
                                              description: key is the label key that
                                                the selector applies to.
                                              type: string
                                            operator:
                                              description: 'operator represents a
                                                key''s relationship to a set of values.

                                                Valid operators are In, NotIn, Exists
                                                and DoesNotExist.'
                                              type: string
                                            values:
                                              description: 'values is an array of
                                                string values. If the operator is
                                                In or NotIn,

                                                the values array must be non-empty.
                                                If the operator is Exists or DoesNotExist,

                                                the values array must be empty. This
                                                array is replaced during a strategic

                                                merge patch.'
                                              items:
                                                type: string
                                              type: array
                                              x-kubernetes-list-type: atomic
                                          required:
                                            - key
                                            - operator
                                          type: object
                                        type: array
                                        x-kubernetes-list-type: atomic
                                      matchLabels:
                                        additionalProperties:
                                          type: string
                                        description: 'matchLabels is a map of {key,value}
                                          pairs. A single {key,value} in the matchLabels

                                          map is equivalent to an element of matchExpressions,
                                          whose key field is "key", the

                                          operator is "In", and the values array contains
                                          only "value". The requirements are ANDed.'
                                        type: object
                                    type: object
                                    x-kubernetes-map-type: atomic
                                  matchLabelKeys:
                                    description: 'MatchLabelKeys is a set of pod label
                                      keys to select which pods will

                                      be taken into consideration. The keys are used
                                      to lookup values from the

                                      incoming pod labels, those key-value labels
                                      are merged with `labelSelector` as `key in (value)`

                                      to select the group of existing pods which pods
                                      will be taken into consideration

                                      for the incoming pod''s pod (anti) affinity.
                                      Keys that don''t exist in the incoming

                                      pod labels will be ignored. The default value
                                      is empty.

                                      The same key is forbidden to exist in both matchLabelKeys
                                      and labelSelector.

                                      Also, matchLabelKeys cannot be set when labelSelector
                                      isn''t set.'
                                    items:
                                      type: string
                                    type: array
                                    x-kubernetes-list-type: atomic
                                  mismatchLabelKeys:
                                    description: 'MismatchLabelKeys is a set of pod
                                      label keys to select which pods will

                                      be taken into consideration. The keys are used
                                      to lookup values from the

                                      incoming pod labels, those key-value labels
                                      are merged with `labelSelector` as `key notin
                                      (value)`

                                      to select the group of existing pods which pods
                                      will be taken into consideration

                                      for the incoming pod''s pod (anti) affinity.
                                      Keys that don''t exist in the incoming

                                      pod labels will be ignored. The default value
                                      is empty.

                                      The same key is forbidden to exist in both mismatchLabelKeys
                                      and labelSelector.

                                      Also, mismatchLabelKeys cannot be set when labelSelector
                                      isn''t set.'
                                    items:
                                      type: string
                                    type: array
                                    x-kubernetes-list-type: atomic
                                  namespaceSelector:
                                    description: 'A label query over the set of namespaces
                                      that the term applies to.

                                      The term is applied to the union of the namespaces
                                      selected by this field

                                      and the ones listed in the namespaces field.

                                      null selector and null or empty namespaces list
                                      means "this pod''s namespace".

                                      An empty selector ({}) matches all namespaces.'
                                    properties:
                                      matchExpressions:
                                        description: matchExpressions is a list of
                                          label selector requirements. The requirements
                                          are ANDed.
                                        items:
                                          description: 'A label selector requirement
                                            is a selector that contains values, a
                                            key, and an operator that

                                            relates the key and values.'
                                          properties:
                                            key:
                                              description: key is the label key that
                                                the selector applies to.
                                              type: string
                                            operator:
                                              description: 'operator represents a
                                                key''s relationship to a set of values.

                                                Valid operators are In, NotIn, Exists
                                                and DoesNotExist.'
                                              type: string
                                            values:
                                              description: 'values is an array of
                                                string values. If the operator is
                                                In or NotIn,

                                                the values array must be non-empty.
                                                If the operator is Exists or DoesNotExist,

                                                the values array must be empty. This
                                                array is replaced during a strategic

                                                merge patch.'
                                              items:
                                                type: string
                                              type: array
                                              x-kubernetes-list-type: atomic
                                          required:
                                            - key
                                            - operator
                                          type: object
                                        type: array
                                        x-kubernetes-list-type: atomic
                                      matchLabels:
                                        additionalProperties:
                                          type: string
                                        description: 'matchLabels is a map of {key,value}
                                          pairs. A single {key,value} in the matchLabels

                                          map is equivalent to an element of matchExpressions,
                                          whose key field is "key", the

                                          operator is "In", and the values array contains
                                          only "value". The requirements are ANDed.'
                                        type: object
                                    type: object
                                    x-kubernetes-map-type: atomic
                                  namespaces:
                                    description: 'namespaces specifies a static list
                                      of namespace names that the term applies to.

                                      The term is applied to the union of the namespaces
                                      listed in this field

                                      and the ones selected by namespaceSelector.

                                      null or empty namespaces list and null namespaceSelector
                                      means "this pod''s namespace".'
                                    items:
                                      type: string
                                    type: array
                                    x-kubernetes-list-type: atomic
                                  topologyKey:
                                    description: 'This pod should be co-located (affinity)
                                      or not co-located (anti-affinity) with the pods
                                      matching

                                      the labelSelector in the specified namespaces,
                                      where co-located is defined as running on a
                                      node

                                      whose value of the label with key topologyKey
                                      matches that of any node on which any of the

                                      selected pods is running.

                                      Empty topologyKey is not allowed.'
                                    type: string
                                required:
                                  - topologyKey
                                type: object
                              weight:
                                description: 'weight associated with matching the
                                  corresponding podAffinityTerm,

                                  in the range 1-100.'
                                format: int32
                                type: integer
                            required:
                              - podAffinityTerm
                              - weight
                            type: object
                          type: array
                          x-kubernetes-list-type: atomic
                        requiredDuringSchedulingIgnoredDuringExecution:
                          description: 'If the affinity requirements specified by
                            this field are not met at

                            scheduling time, the pod will not be scheduled onto the
                            node.

                            If the affinity requirements specified by this field cease
                            to be met

                            at some point during pod execution (e.g. due to a pod
                            label update), the

                            system may or may not try to eventually evict the pod
                            from its node.

                            When there are multiple elements, the lists of nodes corresponding
                            to each

                            podAffinityTerm are intersected, i.e. all terms must be
                            satisfied.'
                          items:
                            description: 'Defines a set of pods (namely those matching
                              the labelSelector

                              relative to the given namespace(s)) that this pod should
                              be

                              co-located (affinity) or not co-located (anti-affinity)
                              with,

                              where co-located is defined as running on a node whose
                              value of

                              the label with key <topologyKey> matches that of any
                              node on which

                              a pod of the set of pods is running'
                            properties:
                              labelSelector:
                                description: 'A label query over a set of resources,
                                  in this case pods.

                                  If it''s null, this PodAffinityTerm matches with
                                  no Pods.'
                                properties:
                                  matchExpressions:
                                    description: matchExpressions is a list of label
                                      selector requirements. The requirements are
                                      ANDed.
                                    items:
                                      description: 'A label selector requirement is
                                        a selector that contains values, a key, and
                                        an operator that

                                        relates the key and values.'
                                      properties:
                                        key:
                                          description: key is the label key that the
                                            selector applies to.
                                          type: string
                                        operator:
                                          description: 'operator represents a key''s
                                            relationship to a set of values.

                                            Valid operators are In, NotIn, Exists
                                            and DoesNotExist.'
                                          type: string
                                        values:
                                          description: 'values is an array of string
                                            values. If the operator is In or NotIn,

                                            the values array must be non-empty. If
                                            the operator is Exists or DoesNotExist,

                                            the values array must be empty. This array
                                            is replaced during a strategic

                                            merge patch.'
                                          items:
                                            type: string
                                          type: array
                                          x-kubernetes-list-type: atomic
                                      required:
                                        - key
                                        - operator
                                      type: object
                                    type: array
                                    x-kubernetes-list-type: atomic
                                  matchLabels:
                                    additionalProperties:
                                      type: string
                                    description: 'matchLabels is a map of {key,value}
                                      pairs. A single {key,value} in the matchLabels

                                      map is equivalent to an element of matchExpressions,
                                      whose key field is "key", the

                                      operator is "In", and the values array contains
                                      only "value". The requirements are ANDed.'
                                    type: object
                                type: object
                                x-kubernetes-map-type: atomic
                              matchLabelKeys:
                                description: 'MatchLabelKeys is a set of pod label
                                  keys to select which pods will

                                  be taken into consideration. The keys are used to
                                  lookup values from the

                                  incoming pod labels, those key-value labels are
                                  merged with `labelSelector` as `key in (value)`

                                  to select the group of existing pods which pods
                                  will be taken into consideration

                                  for the incoming pod''s pod (anti) affinity. Keys
                                  that don''t exist in the incoming

                                  pod labels will be ignored. The default value is
                                  empty.

                                  The same key is forbidden to exist in both matchLabelKeys
                                  and labelSelector.

                                  Also, matchLabelKeys cannot be set when labelSelector
                                  isn''t set.'
                                items:
                                  type: string
                                type: array
                                x-kubernetes-list-type: atomic
                              mismatchLabelKeys:
                                description: 'MismatchLabelKeys is a set of pod label
                                  keys to select which pods will

                                  be taken into consideration. The keys are used to
                                  lookup values from the

                                  incoming pod labels, those key-value labels are
                                  merged with `labelSelector` as `key notin (value)`

                                  to select the group of existing pods which pods
                                  will be taken into consideration

                                  for the incoming pod''s pod (anti) affinity. Keys
                                  that don''t exist in the incoming

                                  pod labels will be ignored. The default value is
                                  empty.

                                  The same key is forbidden to exist in both mismatchLabelKeys
                                  and labelSelector.

                                  Also, mismatchLabelKeys cannot be set when labelSelector
                                  isn''t set.'
                                items:
                                  type: string
                                type: array
                                x-kubernetes-list-type: atomic
                              namespaceSelector:
                                description: 'A label query over the set of namespaces
                                  that the term applies to.

                                  The term is applied to the union of the namespaces
                                  selected by this field

                                  and the ones listed in the namespaces field.

                                  null selector and null or empty namespaces list
                                  means "this pod''s namespace".

                                  An empty selector ({}) matches all namespaces.'
                                properties:
                                  matchExpressions:
                                    description: matchExpressions is a list of label
                                      selector requirements. The requirements are
                                      ANDed.
                                    items:
                                      description: 'A label selector requirement is
                                        a selector that contains values, a key, and
                                        an operator that

                                        relates the key and values.'
                                      properties:
                                        key:
                                          description: key is the label key that the
                                            selector applies to.
                                          type: string
                                        operator:
                                          description: 'operator represents a key''s
                                            relationship to a set of values.

                                            Valid operators are In, NotIn, Exists
                                            and DoesNotExist.'
                                          type: string
                                        values:
                                          description: 'values is an array of string
                                            values. If the operator is In or NotIn,

                                            the values array must be non-empty. If
                                            the operator is Exists or DoesNotExist,

                                            the values array must be empty. This array
                                            is replaced during a strategic

                                            merge patch.'
                                          items:
                                            type: string
                                          type: array
                                          x-kubernetes-list-type: atomic
                                      required:
                                        - key
                                        - operator
                                      type: object
                                    type: array
                                    x-kubernetes-list-type: atomic
                                  matchLabels:
                                    additionalProperties:
                                      type: string
                                    description: 'matchLabels is a map of {key,value}
                                      pairs. A single {key,value} in the matchLabels

                                      map is equivalent to an element of matchExpressions,
                                      whose key field is "key", the

                                      operator is "In", and the values array contains
                                      only "value". The requirements are ANDed.'
                                    type: object
                                type: object
                                x-kubernetes-map-type: atomic
                              namespaces:
                                description: 'namespaces specifies a static list of
                                  namespace names that the term applies to.

                                  The term is applied to the union of the namespaces
                                  listed in this field

                                  and the ones selected by namespaceSelector.

                                  null or empty namespaces list and null namespaceSelector
                                  means "this pod''s namespace".'
                                items:
                                  type: string
                                type: array
                                x-kubernetes-list-type: atomic
                              topologyKey:
                                description: 'This pod should be co-located (affinity)
                                  or not co-located (anti-affinity) with the pods
                                  matching

                                  the labelSelector in the specified namespaces, where
                                  co-located is defined as running on a node

                                  whose value of the label with key topologyKey matches
                                  that of any node on which any of the

                                  selected pods is running.

                                  Empty topologyKey is not allowed.'
                                type: string
                            required:
                              - topologyKey
                            type: object
                          type: array
                          x-kubernetes-list-type: atomic
                      type: object
                    podAntiAffinity:
                      description: Describes pod anti-affinity scheduling rules (e.g.
                        avoid putting this pod in the same node, zone, etc. as some
                        other pod(s)).
                      properties:
                        preferredDuringSchedulingIgnoredDuringExecution:
                          description: 'The scheduler will prefer to schedule pods
                            to nodes that satisfy

                            the anti-affinity expressions specified by this field,
                            but it may choose

                            a node that violates one or more of the expressions. The
                            node that is

                            most preferred is the one with the greatest sum of weights,
                            i.e.

                            for each node that meets all of the scheduling requirements
                            (resource

                            request, requiredDuringScheduling anti-affinity expressions,
                            etc.),

                            compute a sum by iterating through the elements of this
                            field and subtracting

                            "weight" from the sum if the node has pods which matches
                            the corresponding podAffinityTerm; the

                            node(s) with the highest sum are the most preferred.'
                          items:
                            description: The weights of all of the matched WeightedPodAffinityTerm
                              fields are added per-node to find the most preferred
                              node(s)
                            properties:
                              podAffinityTerm:
                                description: Required. A pod affinity term, associated
                                  with the corresponding weight.
                                properties:
                                  labelSelector:
                                    description: 'A label query over a set of resources,
                                      in this case pods.

                                      If it''s null, this PodAffinityTerm matches
                                      with no Pods.'
                                    properties:
                                      matchExpressions:
                                        description: matchExpressions is a list of
                                          label selector requirements. The requirements
                                          are ANDed.
                                        items:
                                          description: 'A label selector requirement
                                            is a selector that contains values, a
                                            key, and an operator that

                                            relates the key and values.'
                                          properties:
                                            key:
                                              description: key is the label key that
                                                the selector applies to.
                                              type: string
                                            operator:
                                              description: 'operator represents a
                                                key''s relationship to a set of values.

                                                Valid operators are In, NotIn, Exists
                                                and DoesNotExist.'
                                              type: string
                                            values:
                                              description: 'values is an array of
                                                string values. If the operator is
                                                In or NotIn,

                                                the values array must be non-empty.
                                                If the operator is Exists or DoesNotExist,

                                                the values array must be empty. This
                                                array is replaced during a strategic

                                                merge patch.'
                                              items:
                                                type: string
                                              type: array
                                              x-kubernetes-list-type: atomic
                                          required:
                                            - key
                                            - operator
                                          type: object
                                        type: array
                                        x-kubernetes-list-type: atomic
                                      matchLabels:
                                        additionalProperties:
                                          type: string
                                        description: 'matchLabels is a map of {key,value}
                                          pairs. A single {key,value} in the matchLabels

                                          map is equivalent to an element of matchExpressions,
                                          whose key field is "key", the

                                          operator is "In", and the values array contains
                                          only "value". The requirements are ANDed.'
                                        type: object
                                    type: object
                                    x-kubernetes-map-type: atomic
                                  matchLabelKeys:
                                    description: 'MatchLabelKeys is a set of pod label
                                      keys to select which pods will

                                      be taken into consideration. The keys are used
                                      to lookup values from the

                                      incoming pod labels, those key-value labels
                                      are merged with `labelSelector` as `key in (value)`

                                      to select the group of existing pods which pods
                                      will be taken into consideration

                                      for the incoming pod''s pod (anti) affinity.
                                      Keys that don''t exist in the incoming

                                      pod labels will be ignored. The default value
                                      is empty.

                                      The same key is forbidden to exist in both matchLabelKeys
                                      and labelSelector.

                                      Also, matchLabelKeys cannot be set when labelSelector
                                      isn''t set.'
                                    items:
                                      type: string
                                    type: array
                                    x-kubernetes-list-type: atomic
                                  mismatchLabelKeys:
                                    description: 'MismatchLabelKeys is a set of pod
                                      label keys to select which pods will

                                      be taken into consideration. The keys are used
                                      to lookup values from the

                                      incoming pod labels, those key-value labels
                                      are merged with `labelSelector` as `key notin
                                      (value)`

                                      to select the group of existing pods which pods
                                      will be taken into consideration

                                      for the incoming pod''s pod (anti) affinity.
                                      Keys that don''t exist in the incoming

                                      pod labels will be ignored. The default value
                                      is empty.

                                      The same key is forbidden to exist in both mismatchLabelKeys
                                      and labelSelector.

                                      Also, mismatchLabelKeys cannot be set when labelSelector
                                      isn''t set.'
                                    items:
                                      type: string
                                    type: array
                                    x-kubernetes-list-type: atomic
                                  namespaceSelector:
                                    description: 'A label query over the set of namespaces
                                      that the term applies to.

                                      The term is applied to the union of the namespaces
                                      selected by this field

                                      and the ones listed in the namespaces field.

                                      null selector and null or empty namespaces list
                                      means "this pod''s namespace".

                                      An empty selector ({}) matches all namespaces.'
                                    properties:
                                      matchExpressions:
                                        description: matchExpressions is a list of
                                          label selector requirements. The requirements
                                          are ANDed.
                                        items:
                                          description: 'A label selector requirement
                                            is a selector that contains values, a
                                            key, and an operator that

                                            relates the key and values.'
                                          properties:
                                            key:
                                              description: key is the label key that
                                                the selector applies to.
                                              type: string
                                            operator:
                                              description: 'operator represents a
                                                key''s relationship to a set of values.

                                                Valid operators are In, NotIn, Exists
                                                and DoesNotExist.'
                                              type: string
                                            values:
                                              description: 'values is an array of
                                                string values. If the operator is
                                                In or NotIn,

                                                the values array must be non-empty.
                                                If the operator is Exists or DoesNotExist,

                                                the values array must be empty. This
                                                array is replaced during a strategic

                                                merge patch.'
                                              items:
                                                type: string
                                              type: array
                                              x-kubernetes-list-type: atomic
                                          required:
                                            - key
                                            - operator
                                          type: object
                                        type: array
                                        x-kubernetes-list-type: atomic
                                      matchLabels:
                                        additionalProperties:
                                          type: string
                                        description: 'matchLabels is a map of {key,value}
                                          pairs. A single {key,value} in the matchLabels

                                          map is equivalent to an element of matchExpressions,
                                          whose key field is "key", the

                                          operator is "In", and the values array contains
                                          only "value". The requirements are ANDed.'
                                        type: object
                                    type: object
                                    x-kubernetes-map-type: atomic
                                  namespaces:
                                    description: 'namespaces specifies a static list
                                      of namespace names that the term applies to.

                                      The term is applied to the union of the namespaces
                                      listed in this field

                                      and the ones selected by namespaceSelector.

                                      null or empty namespaces list and null namespaceSelector
                                      means "this pod''s namespace".'
                                    items:
                                      type: string
                                    type: array
                                    x-kubernetes-list-type: atomic
                                  topologyKey:
                                    description: 'This pod should be co-located (affinity)
                                      or not co-located (anti-affinity) with the pods
                                      matching

                                      the labelSelector in the specified namespaces,
                                      where co-located is defined as running on a
                                      node

                                      whose value of the label with key topologyKey
                                      matches that of any node on which any of the

                                      selected pods is running.

                                      Empty topologyKey is not allowed.'
                                    type: string
                                required:
                                  - topologyKey
                                type: object
                              weight:
                                description: 'weight associated with matching the
                                  corresponding podAffinityTerm,

                                  in the range 1-100.'
                                format: int32
                                type: integer
                            required:
                              - podAffinityTerm
                              - weight
                            type: object
                          type: array
                          x-kubernetes-list-type: atomic
                        requiredDuringSchedulingIgnoredDuringExecution:
                          description: 'If the anti-affinity requirements specified
                            by this field are not met at

                            scheduling time, the pod will not be scheduled onto the
                            node.

                            If the anti-affinity requirements specified by this field
                            cease to be met

                            at some point during pod execution (e.g. due to a pod
                            label update), the

                            system may or may not try to eventually evict the pod
                            from its node.

                            When there are multiple elements, the lists of nodes corresponding
                            to each

                            podAffinityTerm are intersected, i.e. all terms must be
                            satisfied.'
                          items:
                            description: 'Defines a set of pods (namely those matching
                              the labelSelector

                              relative to the given namespace(s)) that this pod should
                              be

                              co-located (affinity) or not co-located (anti-affinity)
                              with,

                              where co-located is defined as running on a node whose
                              value of

                              the label with key <topologyKey> matches that of any
                              node on which

                              a pod of the set of pods is running'
                            properties:
                              labelSelector:
                                description: 'A label query over a set of resources,
                                  in this case pods.

                                  If it''s null, this PodAffinityTerm matches with
                                  no Pods.'
                                properties:
                                  matchExpressions:
                                    description: matchExpressions is a list of label
                                      selector requirements. The requirements are
                                      ANDed.
                                    items:
                                      description: 'A label selector requirement is
                                        a selector that contains values, a key, and
                                        an operator that

                                        relates the key and values.'
                                      properties:
                                        key:
                                          description: key is the label key that the
                                            selector applies to.
                                          type: string
                                        operator:
                                          description: 'operator represents a key''s
                                            relationship to a set of values.

                                            Valid operators are In, NotIn, Exists
                                            and DoesNotExist.'
                                          type: string
                                        values:
                                          description: 'values is an array of string
                                            values. If the operator is In or NotIn,

                                            the values array must be non-empty. If
                                            the operator is Exists or DoesNotExist,

                                            the values array must be empty. This array
                                            is replaced during a strategic

                                            merge patch.'
                                          items:
                                            type: string
                                          type: array
                                          x-kubernetes-list-type: atomic
                                      required:
                                        - key
                                        - operator
                                      type: object
                                    type: array
                                    x-kubernetes-list-type: atomic
                                  matchLabels:
                                    additionalProperties:
                                      type: string
                                    description: 'matchLabels is a map of {key,value}
                                      pairs. A single {key,value} in the matchLabels

                                      map is equivalent to an element of matchExpressions,
                                      whose key field is "key", the

                                      operator is "In", and the values array contains
                                      only "value". The requirements are ANDed.'
                                    type: object
                                type: object
                                x-kubernetes-map-type: atomic
                              matchLabelKeys:
                                description: 'MatchLabelKeys is a set of pod label
                                  keys to select which pods will

                                  be taken into consideration. The keys are used to
                                  lookup values from the

                                  incoming pod labels, those key-value labels are
                                  merged with `labelSelector` as `key in (value)`

                                  to select the group of existing pods which pods
                                  will be taken into consideration

                                  for the incoming pod''s pod (anti) affinity. Keys
                                  that don''t exist in the incoming

                                  pod labels will be ignored. The default value is
                                  empty.

                                  The same key is forbidden to exist in both matchLabelKeys
                                  and labelSelector.

                                  Also, matchLabelKeys cannot be set when labelSelector
                                  isn''t set.'
                                items:
                                  type: string
                                type: array
                                x-kubernetes-list-type: atomic
                              mismatchLabelKeys:
                                description: 'MismatchLabelKeys is a set of pod label
                                  keys to select which pods will

                                  be taken into consideration. The keys are used to
                                  lookup values from the

                                  incoming pod labels, those key-value labels are
                                  merged with `labelSelector` as `key notin (value)`

                                  to select the group of existing pods which pods
                                  will be taken into consideration

                                  for the incoming pod''s pod (anti) affinity. Keys
                                  that don''t exist in the incoming

                                  pod labels will be ignored. The default value is
                                  empty.

                                  The same key is forbidden to exist in both mismatchLabelKeys
                                  and labelSelector.

                                  Also, mismatchLabelKeys cannot be set when labelSelector
                                  isn''t set.'
                                items:
                                  type: string
                                type: array
                                x-kubernetes-list-type: atomic
                              namespaceSelector:
                                description: 'A label query over the set of namespaces
                                  that the term applies to.

                                  The term is applied to the union of the namespaces
                                  selected by this field

                                  and the ones listed in the namespaces field.

                                  null selector and null or empty namespaces list
                                  means "this pod''s namespace".

                                  An empty selector ({}) matches all namespaces.'
                                properties:
                                  matchExpressions:
                                    description: matchExpressions is a list of label
                                      selector requirements. The requirements are
                                      ANDed.
                                    items:
                                      description: 'A label selector requirement is
                                        a selector that contains values, a key, and
                                        an operator that

                                        relates the key and values.'
                                      properties:
                                        key:
                                          description: key is the label key that the
                                            selector applies to.
                                          type: string
                                        operator:
                                          description: 'operator represents a key''s
                                            relationship to a set of values.

                                            Valid operators are In, NotIn, Exists
                                            and DoesNotExist.'
                                          type: string
                                        values:
                                          description: 'values is an array of string
                                            values. If the operator is In or NotIn,

                                            the values array must be non-empty. If
                                            the operator is Exists or DoesNotExist,

                                            the values array must be empty. This array
                                            is replaced during a strategic

                                            merge patch.'
                                          items:
                                            type: string
                                          type: array
                                          x-kubernetes-list-type: atomic
                                      required:
                                        - key
                                        - operator
                                      type: object
                                    type: array
                                    x-kubernetes-list-type: atomic
                                  matchLabels:
                                    additionalProperties:
                                      type: string
                                    description: 'matchLabels is a map of {key,value}
                                      pairs. A single {key,value} in the matchLabels

                                      map is equivalent to an element of matchExpressions,
                                      whose key field is "key", the

                                      operator is "In", and the values array contains
                                      only "value". The requirements are ANDed.'
                                    type: object
                                type: object
                                x-kubernetes-map-type: atomic
                              namespaces:
                                description: 'namespaces specifies a static list of
                                  namespace names that the term applies to.

                                  The term is applied to the union of the namespaces
                                  listed in this field

                                  and the ones selected by namespaceSelector.

                                  null or empty namespaces list and null namespaceSelector
                                  means "this pod''s namespace".'
                                items:
                                  type: string
                                type: array
                                x-kubernetes-list-type: atomic
                              topologyKey:
                                description: 'This pod should be co-located (affinity)
                                  or not co-located (anti-affinity) with the pods
                                  matching

                                  the labelSelector in the specified namespaces, where
                                  co-located is defined as running on a node

                                  whose value of the label with key topologyKey matches
                                  that of any node on which any of the

                                  selected pods is running.

                                  Empty topologyKey is not allowed.'
                                type: string
                            required:
                              - topologyKey
                            type: object
                          type: array
                          x-kubernetes-list-type: atomic
                      type: object
                  type: object
                agentEnvVars:
                  description: AgentEnvVars are extra environment variables to be
                    added to the agent deployment.
                  items:
                    description: EnvVar represents an environment variable present
                      in a Container.
                    properties:
                      name:
                        description: 'Name of the environment variable.

                          May consist of any printable ASCII characters except ''=''.'
                        type: string
                      value:
                        description: 'Variable references $(VAR_NAME) are expanded

                          using the previously defined environment variables in the
                          container and

                          any service environment variables. If a variable cannot
                          be resolved,

                          the reference in the input string will be unchanged. Double
                          $$ are reduced

                          to a single $, which allows for escaping the $(VAR_NAME)
                          syntax: i.e.

                          "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".

                          Escaped references will never be expanded, regardless of
                          whether the variable

                          exists or not.

                          Defaults to "".'
                        type: string
                      valueFrom:
                        description: Source for the environment variable's value.
                          Cannot be used if value is not empty.
                        properties:
                          configMapKeyRef:
                            description: Selects a key of a ConfigMap.
                            properties:
                              key:
                                description: The key to select.
                                type: string
                              name:
                                default: ''
                                description: 'Name of the referent.

                                  This field is effectively required, but due to backwards
                                  compatibility is

                                  allowed to be empty. Instances of this type with
                                  an empty value here are

                                  almost certainly wrong.

                                  More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names'
                                type: string
                              optional:
                                description: Specify whether the ConfigMap or its
                                  key must be defined
                                type: boolean
                            required:
                              - key
                            type: object
                            x-kubernetes-map-type: atomic
                          fieldRef:
                            description: 'Selects a field of the pod: supports metadata.name,
                              metadata.namespace, `metadata.labels[''<KEY>'']`, `metadata.annotations[''<KEY>'']`,

                              spec.nodeName, spec.serviceAccountName, status.hostIP,
                              status.podIP, status.podIPs.'
                            properties:
                              apiVersion:
                                description: Version of the schema the FieldPath is
                                  written in terms of, defaults to "v1".
                                type: string
                              fieldPath:
                                description: Path of the field to select in the specified
                                  API version.
                                type: string
                            required:
                              - fieldPath
                            type: object
                            x-kubernetes-map-type: atomic
                          fileKeyRef:
                            description: 'FileKeyRef selects a key of the env file.

                              Requires the EnvFiles feature gate to be enabled.'
                            properties:
                              key:
                                description: 'The key within the env file. An invalid
                                  key will prevent the pod from starting.

                                  The keys defined within a source may consist of
                                  any printable ASCII characters except ''=''.

                                  During Alpha stage of the EnvFiles feature gate,
                                  the key size is limited to 128 characters.'
                                type: string
                              optional:
                                default: false
                                description: 'Specify whether the file or its key
                                  must be defined. If the file or key

                                  does not exist, then the env var is not published.

                                  If optional is set to true and the specified key
                                  does not exist,

                                  the environment variable will not be set in the
                                  Pod''s containers.


                                  If optional is set to false and the specified key
                                  does not exist,

                                  an error will be returned during Pod creation.'
                                type: boolean
                              path:
                                description: 'The path within the volume from which
                                  to select the file.

                                  Must be relative and may not contain the ''..''
                                  path or start with ''..''.'
                                type: string
                              volumeName:
                                description: The name of the volume mount containing
                                  the env file.
                                type: string
                            required:
                              - key
                              - path
                              - volumeName
                            type: object
                            x-kubernetes-map-type: atomic
                          resourceFieldRef:
                            description: 'Selects a resource of the container: only
                              resources limits and requests

                              (limits.cpu, limits.memory, limits.ephemeral-storage,
                              requests.cpu, requests.memory and requests.ephemeral-storage)
                              are currently supported.'
                            properties:
                              containerName:
                                description: 'Container name: required for volumes,
                                  optional for env vars'
                                type: string
                              divisor:
                                anyOf:
                                  - type: integer
                                  - type: string
                                description: Specifies the output format of the exposed
                                  resources, defaults to "1"
                                pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
                                x-kubernetes-int-or-string: true
                              resource:
                                description: 'Required: resource to select'
                                type: string
                            required:
                              - resource
                            type: object
                            x-kubernetes-map-type: atomic
                          secretKeyRef:
                            description: Selects a key of a secret in the pod's namespace
                            properties:
                              key:
                                description: The key of the secret to select from.  Must
                                  be a valid secret key.
                                type: string
                              name:
                                default: ''
                                description: 'Name of the referent.

                                  This field is effectively required, but due to backwards
                                  compatibility is

                                  allowed to be empty. Instances of this type with
                                  an empty value here are

                                  almost certainly wrong.

                                  More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names'
                                type: string
                              optional:
                                description: Specify whether the Secret or its key
                                  must be defined
                                type: boolean
                            required:
                              - key
                            type: object
                            x-kubernetes-map-type: atomic
                        type: object
                    required:
                      - name
                    type: object
                  nullable: true
                  type: array
                agentNamespace:
                  description: AgentNamespace defaults to the system namespace, e.g.
                    cattle-fleet-system.
                  nullable: true
                  type: string
                agentResources:
                  description: AgentResources sets the resources for the cluster's
                    agent deployment.
                  nullable: true
                  properties:
                    claims:
                      description: 'Claims lists the names of resources, defined in
                        spec.resourceClaims,

                        that are used by this container.


                        This field depends on the

                        DynamicResourceAllocation feature gate.


                        This field is immutable. It can only be set for containers.'
                      items:
                        description: ResourceClaim references one entry in PodSpec.ResourceClaims.
                        properties:
                          name:
                            description: 'Name must match the name of one entry in
                              pod.spec.resourceClaims of

                              the Pod where this field is used. It makes that resource
                              available

                              inside a container.'
                            type: string
                          request:
                            description: 'Request is the name chosen for a request
                              in the referenced claim.

                              If empty, everything from the claim is made available,
                              otherwise

                              only the result of this request.'
                            type: string
                        required:
                          - name
                        type: object
                      type: array
                      x-kubernetes-list-map-keys:
                        - name
                      x-kubernetes-list-type: map
                    limits:
                      additionalProperties:
                        anyOf:
                          - type: integer
                          - type: string
                        pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
                        x-kubernetes-int-or-string: true
                      description: 'Limits describes the maximum amount of compute
                        resources allowed.

                        More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/'
                      type: object
                    requests:
                      additionalProperties:
                        anyOf:
                          - type: integer
                          - type: string
                        pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
                        x-kubernetes-int-or-string: true
                      description: 'Requests describes the minimum amount of compute
                        resources required.

                        If Requests is omitted for a container, it defaults to Limits
                        if that is explicitly specified,

                        otherwise to an implementation-defined value. Requests cannot
                        exceed Limits.

                        More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/'
                      type: object
                  type: object
                agentSchedulingCustomization:
                  properties:
                    podDisruptionBudget:
                      properties:
                        maxUnavailable:
                          type: string
                        minAvailable:
                          type: string
                      type: object
                    priorityClass:
                      properties:
                        preemptionPolicy:
                          description: PreemptionPolicy describes a policy for if/when
                            to preempt a pod.
                          type: string
                        value:
                          type: integer
                      type: object
                  type: object
                agentTolerations:
                  description: AgentTolerations defines an extra set of Tolerations
                    to be added to the Agent deployment.
                  items:
                    description: 'The pod this Toleration is attached to tolerates
                      any taint that matches

                      the triple <key,value,effect> using the matching operator <operator>.'
                    properties:
                      effect:
                        description: 'Effect indicates the taint effect to match.
                          Empty means match all taint effects.

                          When specified, allowed values are NoSchedule, PreferNoSchedule
                          and NoExecute.'
                        type: string
                      key:
                        description: 'Key is the taint key that the toleration applies
                          to. Empty means match all taint keys.

                          If the key is empty, operator must be Exists; this combination
                          means to match all values and all keys.'
                        type: string
                      operator:
                        description: 'Operator represents a key''s relationship to
                          the value.

                          Valid operators are Exists and Equal. Defaults to Equal.

                          Exists is equivalent to wildcard for value, so that a pod
                          can

                          tolerate all taints of a particular category.'
                        type: string
                      tolerationSeconds:
                        description: 'TolerationSeconds represents the period of time
                          the toleration (which must be

                          of effect NoExecute, otherwise this field is ignored) tolerates
                          the taint. By default,

                          it is not set, which means tolerate the taint forever (do
                          not evict). Zero and

                          negative values will be treated as 0 (evict immediately)
                          by the system.'
                        format: int64
                        type: integer
                      value:
                        description: 'Value is the taint value the toleration matches
                          to.

                          If the operator is Exists, the value should be empty, otherwise
                          just a regular string.'
                        type: string
                    type: object
                  nullable: true
                  type: array
                clientID:
                  description: 'ClientID is a unique string that will identify the
                    cluster. It can

                    either be predefined, or generated when importing the cluster.'
                  nullable: true
                  type: string
                hostNetwork:
                  description: 'HostNetwork sets the agent Deployment to use hostNetwork:
                    true setting.

                    Allows for provisioning of network related bundles (CNI configuration).'
                  nullable: true
                  type: boolean
                kubeConfigSecret:
                  description: 'KubeConfigSecret is the name of the secret containing
                    the kubeconfig for the downstream cluster.

                    It can optionally contain a APIServerURL and CA to override the

                    values in the fleet-controller''s configmap.'
                  nullable: true
                  type: string
                kubeConfigSecretNamespace:
                  description: 'KubeConfigSecretNamespace is the namespace of the
                    secret containing the kubeconfig for the downstream cluster.

                    If unset, it will be assumed the secret can be found in the namespace
                    that the Cluster object resides within.'
                  nullable: true
                  type: string
                paused:
                  description: Paused if set to true, will stop any BundleDeployments
                    from being updated.
                  type: boolean
                privateRepoURL:
                  description: PrivateRepoURL prefixes the image name and overrides
                    a global repo URL from the agents config.
                  nullable: true
                  type: string
                redeployAgentGeneration:
                  description: RedeployAgentGeneration can be used to force redeploying
                    the agent.
                  format: int64
                  type: integer
                templateValues:
                  description: TemplateValues defines a cluster specific mapping of
                    values to be sent to fleet.yaml values templating.
                  nullable: true
                  type: object
                  x-kubernetes-preserve-unknown-fields: true
              type: object
            status:
              properties:
                activeSchedule:
                  description: 'ActiveSchedule specifies if the cluster is in schedule,
                    which means BundleDeployments can

                    be updated and deployed. If ActiveSchedule is set to false and
                    Scheduled is set to true

                    BundleDeployments are not updated nor deployed.'
                  type: boolean
                agent:
                  description: AgentStatus contains information about the agent.
                  properties:
                    lastSeen:
                      description: 'LastSeen is the last time the agent checked in
                        to update the status

                        of the cluster resource.'
                      format: date-time
                      nullable: true
                      type: string
                    namespace:
                      description: Namespace is the namespace of the agent deployment,
                        e.g. "cattle-fleet-system".
                      nullable: true
                      type: string
                  type: object
                agentAffinityHash:
                  description: 'AgentAffinityHash is a hash of the agent''s affinity
                    configuration,

                    used to detect changes.'
                  type: string
                agentConfigChanged:
                  description: 'AgentConfigChanged is set to true if any of the agent
                    configuration

                    changed, like the API server URL or CA. Setting it to true will

                    trigger a re-import of the cluster.'
                  type: boolean
                agentDeployedGeneration:
                  description: AgentDeployedGeneration is the generation of the agent
                    that is currently deployed.
                  format: int64
                  nullable: true
                  type: integer
                agentEnvVarsHash:
                  description: AgentEnvVarsHash is a hash of the agent's env vars,
                    used to detect changes.
                  nullable: true
                  type: string
                agentHostNetwork:
                  description: AgentHostNetwork defines observed state of spec.hostNetwork
                    setting that is currently used.
                  nullable: true
                  type: boolean
                agentMigrated:
                  description: 'AgentMigrated is always set to true after importing
                    a cluster. If

                    false, it will trigger a migration. Old agents don''t have

                    this in their status.'
                  type: boolean
                agentNamespaceMigrated:
                  description: 'AgentNamespaceMigrated is always set to true after
                    importing a

                    cluster. If false, it will trigger a migration. Old Fleet agents

                    don''t have this in their status.'
                  type: boolean
                agentPrivateRepoURL:
                  description: AgentPrivateRepoURL is the private repo URL for the
                    agent that is currently used.
                  nullable: true
                  type: string
                agentResourcesHash:
                  description: 'AgentResourcesHash is a hash of the agent''s resources
                    configuration,

                    used to detect changes.'
                  nullable: true
                  type: string
                agentSchedulingCustomizationHash:
                  type: string
                agentTLSMode:
                  description: 'AgentTLSMode supports two values: `system-store` and
                    `strict`. If set to

                    `system-store`, instructs the agent to trust CA bundles from the
                    operating

                    system''s store. If set to `strict`, then the agent shall only
                    connect to a

                    server which uses the exact CA configured when creating/updating
                    the agent.'
                  nullable: true
                  type: string
                agentTolerationsHash:
                  description: 'AgentTolerationsHash is a hash of the agent''s tolerations

                    configuration, used to detect changes.'
                  nullable: true
                  type: string
                apiServerCAHash:
                  description: APIServerCAHash is a hash of the upstream API server
                    CA, used to detect changes.
                  nullable: true
                  type: string
                apiServerURL:
                  description: 'APIServerURL is the currently used URL of the API
                    server that the

                    cluster uses to connect to upstream.'
                  nullable: true
                  type: string
                cattleNamespaceMigrated:
                  description: 'CattleNamespaceMigrated is always set to true after
                    importing a

                    cluster. If false, it will trigger a migration. Old Fleet agents,

                    don''t have this in their status.'
                  type: boolean
                conditions:
                  items:
                    properties:
                      lastTransitionTime:
                        description: Last time the condition transitioned from one
                          status to another.
                        type: string
                      lastUpdateTime:
                        description: The last time this condition was updated.
                        type: string
                      message:
                        description: Human-readable message indicating details about
                          last transition
                        type: string
                      reason:
                        description: The reason for the condition's last transition.
                        type: string
                      status:
                        description: Status of the condition, one of True, False,
                          Unknown.
                        type: string
                      type:
                        description: Type of cluster condition.
                        type: string
                    required:
                      - status
                      - type
                    type: object
                  type: array
                desiredReadyGitRepos:
                  description: 'DesiredReadyGitRepos is the number of gitrepos for
                    this cluster that

                    are desired to be ready.'
                  type: integer
                desiredReadyHelmOps:
                  description: 'DesiredReadyHelmOps is the number of helmop resources
                    for this cluster that

                    are desired to be ready.'
                  type: integer
                display:
                  description: Display contains the number of ready bundles, nodes
                    and a summary state.
                  properties:
                    readyBundles:
                      description: 'ReadyBundles is a string in the form "%d/%d",
                        that describes the

                        number of bundles that are ready vs. the number of bundles
                        desired

                        to be ready.'
                      type: string
                    state:
                      description: State of the cluster, either one of the bundle
                        states, or "WaitCheckIn".
                      nullable: true
                      type: string
                  type: object
                garbageCollectionInterval:
                  description: GarbageCollectionInterval determines how often agents
                    clean up obsolete Helm releases.
                  type: string
                namespace:
                  description: 'Namespace is the cluster namespace, it contains the
                    clusters service

                    account as well as any bundledeployments. Example:

                    "cluster-fleet-local-cluster-294db1acfa77-d9ccf852678f"'
                  type: string
                readyGitRepos:
                  description: ReadyGitRepos is the number of gitrepos for this cluster
                    that are ready.
                  type: integer
                readyHelmOps:
                  description: ReadyHelmOps is the number of helmop resources for
                    this cluster that are ready.
                  type: integer
                resourceCounts:
                  description: ResourceCounts is an aggregate over the ResourceCounts.
                  properties:
                    desiredReady:
                      description: DesiredReady is the number of resources that should
                        be ready.
                      type: integer
                    missing:
                      description: Missing is the number of missing resources.
                      type: integer
                    modified:
                      description: Modified is the number of resources that have been
                        modified.
                      type: integer
                    notReady:
                      description: 'NotReady is the number of not ready resources.
                        Resources are not

                        ready if they do not match any other state.'
                      type: integer
                    orphaned:
                      description: Orphaned is the number of orphaned resources.
                      type: integer
                    ready:
                      description: Ready is the number of ready resources.
                      type: integer
                    unknown:
                      description: Unknown is the number of resources in an unknown
                        state.
                      type: integer
                    waitApplied:
                      description: WaitApplied is the number of resources that are
                        waiting to be applied.
                      type: integer
                  type: object
                scheduled:
                  description: 'Scheduled specifies if the cluster has been added
                    to any Schedule.

                    When set to true ActiveSchedule is taken into account to check
                    if the deployment

                    can be deployed.'
                  type: boolean
                summary:
                  description: Summary is a summary of the bundledeployments.
                  properties:
                    desiredReady:
                      description: 'DesiredReady is the number of bundle deployments
                        that should be

                        ready.'
                      type: integer
                    errApplied:
                      description: 'ErrApplied is the number of bundle deployments
                        that have been synced

                        from the Fleet controller and the downstream cluster, but
                        with some

                        errors when deploying the bundle.'
                      type: integer
                    modified:
                      description: 'Modified is the number of bundle deployments that
                        have been deployed

                        and for which all resources are ready, but where some changes
                        from the

                        Git repository have not yet been synced.'
                      type: integer
                    nonReadyResources:
                      description: 'NonReadyClusters is a list of states, which is
                        filled for a bundle

                        that is not ready.'
                      items:
                        description: 'NonReadyResource contains information about
                          a bundle that is not ready for a

                          given state like "ErrApplied". It contains a list of non-ready
                          or modified

                          resources and their states.'
                        properties:
                          bundleState:
                            description: State is the state of the resource, like
                              e.g. "NotReady" or "ErrApplied".
                            nullable: true
                            type: string
                          message:
                            description: Message contains information why the bundle
                              is not ready.
                            nullable: true
                            type: string
                          modifiedStatus:
                            description: ModifiedStatus lists the state for each modified
                              resource.
                            items:
                              description: 'ModifiedStatus is used to report the status
                                of a resource that is modified.

                                It indicates if the modification was a create, a delete
                                or a patch.'
                              properties:
                                apiVersion:
                                  nullable: true
                                  type: string
                                delete:
                                  type: boolean
                                exist:
                                  description: Exist is true if the resource exists
                                    but is not owned by us. This can happen if a resource
                                    was adopted by another bundle whereas the first
                                    bundle still exists and due to that reports that
                                    it does not own it.
                                  type: boolean
                                kind:
                                  nullable: true
                                  type: string
                                missing:
                                  type: boolean
                                name:
                                  nullable: true
                                  type: string
                                namespace:
                                  nullable: true
                                  type: string
                                patch:
                                  nullable: true
                                  type: string
                              type: object
                            nullable: true
                            type: array
                          name:
                            description: Name is the name of the resource.
                            nullable: true
                            type: string
                          nonReadyStatus:
                            description: NonReadyStatus lists the state for each non-ready
                              resource.
                            items:
                              description: NonReadyStatus is used to report the status
                                of a resource that is not ready. It includes a summary.
                              properties:
                                apiVersion:
                                  nullable: true
                                  type: string
                                kind:
                                  nullable: true
                                  type: string
                                name:
                                  nullable: true
                                  type: string
                                namespace:
                                  nullable: true
                                  type: string
                                summary:
                                  properties:
                                    error:
                                      type: boolean
                                    message:
                                      items:
                                        type: string
                                      type: array
                                    state:
                                      type: string
                                    transitioning:
                                      type: boolean
                                  type: object
                                uid:
                                  description: 'UID is a type that holds unique ID
                                    values, including UUIDs.  Because we

                                    don''t ONLY use UUIDs, this is an alias to string.  Being
                                    a type captures

                                    intent and helps make sure that UIDs and names
                                    do not get conflated.'
                                  nullable: true
                                  type: string
                              type: object
                            nullable: true
                            type: array
                        type: object
                      nullable: true
                      type: array
                    notReady:
                      description: 'NotReady is the number of bundle deployments that
                        have been deployed

                        where some resources are not ready.'
                      type: integer
                    outOfSync:
                      description: 'OutOfSync is the number of bundle deployments
                        that have been synced

                        from Fleet controller, but not yet by the downstream agent.'
                      type: integer
                    pending:
                      description: 'Pending is the number of bundle deployments that
                        are being processed

                        by Fleet controller.'
                      type: integer
                    ready:
                      description: 'Ready is the number of bundle deployments that
                        have been deployed

                        where all resources are ready.'
                      type: integer
                    waitApplied:
                      description: 'WaitApplied is the number of bundle deployments
                        that have been

                        synced from Fleet controller and downstream cluster, but are
                        waiting

                        to be deployed.'
                      type: integer
                  type: object
              type: object
          type: object
      served: true
      storage: true
      subresources:
        status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.19.0
  name: contents.fleet.cattle.io
spec:
  group: fleet.cattle.io
  names:
    kind: Content
    listKind: ContentList
    plural: contents
    singular: content
  scope: Cluster
  versions:
    - name: v1alpha1
      schema:
        openAPIV3Schema:
          description: 'Content is used internally by Fleet and should not be used
            directly. It

            contains the resources from a bundle for a specific target cluster.'
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
                of an object.

                Servers should convert recognized schemas to the latest internal value,
                and

                may reject unrecognized values.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            content:
              description: 'Content is a byte array, which contains the manifests
                of a bundle.

                The bundle resources are copied into the bundledeployment''s content

                resource, so the downstream agent can deploy them.'
              format: byte
              nullable: true
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource
                this object represents.

                Servers may infer this from the endpoint the client submits requests
                to.

                Cannot be updated.

                In CamelCase.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
            sha256sum:
              description: SHA256Sum of the Content field
              type: string
          type: object
      served: true
      storage: true
      subresources:
        status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.19.0
  name: gitreporestrictions.fleet.cattle.io
spec:
  group: fleet.cattle.io
  names:
    kind: GitRepoRestriction
    listKind: GitRepoRestrictionList
    plural: gitreporestrictions
    singular: gitreporestriction
  scope: Namespaced
  versions:
    - additionalPrinterColumns:
        - jsonPath: .defaultServiceAccount
          name: Default-ServiceAccount
          type: string
        - jsonPath: .allowedServiceAccounts
          name: Allowed-ServiceAccounts
          type: string
      name: v1alpha1
      schema:
        openAPIV3Schema:
          description: 'GitRepoRestriction is a resource that can optionally be used
            to restrict

            the options of GitRepos in the same namespace.'
          properties:
            allowedClientSecretNames:
              description: AllowedClientSecretNames is a list of client secret names
                that GitRepos are allowed to use.
              items:
                type: string
              nullable: true
              type: array
            allowedRepoPatterns:
              description: 'AllowedRepoPatterns is a list of regex patterns that restrict
                the

                valid values of the Repo field of a GitRepo.'
              items:
                type: string
              nullable: true
              type: array
            allowedServiceAccounts:
              description: AllowedServiceAccounts is a list of service accounts that
                GitRepos are allowed to use.
              items:
                type: string
              nullable: true
              type: array
            allowedTargetNamespaces:
              description: 'AllowedTargetNamespaces restricts TargetNamespace to the
                given

                namespaces. If AllowedTargetNamespaces is set, TargetNamespace must

                be set.'
              items:
                type: string
              nullable: true
              type: array
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
                of an object.

                Servers should convert recognized schemas to the latest internal value,
                and

                may reject unrecognized values.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            defaultClientSecretName:
              description: DefaultClientSecretName overrides the GitRepo's default
                client secret.
              nullable: true
              type: string
            defaultServiceAccount:
              description: DefaultServiceAccount overrides the GitRepo's default service
                account.
              nullable: true
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource
                this object represents.

                Servers may infer this from the endpoint the client submits requests
                to.

                Cannot be updated.

                In CamelCase.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
          type: object
      served: true
      storage: true
      subresources:
        status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.19.0
  name: gitrepos.fleet.cattle.io
spec:
  group: fleet.cattle.io
  names:
    categories:
      - fleet
    kind: GitRepo
    listKind: GitRepoList
    plural: gitrepos
    singular: gitrepo
  scope: Namespaced
  versions:
    - additionalPrinterColumns:
        - jsonPath: .spec.repo
          name: Repo
          type: string
        - jsonPath: .status.commit
          name: Commit
          type: string
        - jsonPath: .status.display.readyBundleDeployments
          name: BundleDeployments-Ready
          type: string
        - jsonPath: .status.conditions[?(@.type=="Ready")].message
          name: Status
          type: string
      name: v1alpha1
      schema:
        openAPIV3Schema:
          description: 'GitRepo describes a git repository that is watched by Fleet.

            The resource contains the necessary information to deploy the repo, or
            parts

            of it, to target clusters.'
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
                of an object.

                Servers should convert recognized schemas to the latest internal value,
                and

                may reject unrecognized values.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource
                this object represents.

                Servers may infer this from the endpoint the client submits requests
                to.

                Cannot be updated.

                In CamelCase.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
            spec:
              properties:
                branch:
                  description: Branch The git branch to follow.
                  nullable: true
                  type: string
                bundles:
                  description: 'Bundles defines the paths of bundles to be read.

                    This drives the fleet resource scanner that simply loads the specified
                    folders'
                  items:
                    properties:
                      base:
                        description: Base is the base path for the bundle resources
                        type: string
                      options:
                        description: Options is the path (relative to path above)
                          that defines a fleet.yaml file to configure the bundle
                        nullable: true
                        type: string
                    type: object
                  type: array
                caBundle:
                  description: CABundle is a PEM encoded CA bundle which will be used
                    to validate the repo's certificate.
                  format: byte
                  nullable: true
                  type: string
                clientSecretName:
                  description: 'ClientSecretName is the name of the client secret
                    to be used to connect to the repo

                    It is expected the secret be of type "kubernetes.io/basic-auth"
                    or "kubernetes.io/ssh-auth".'
                  nullable: true
                  type: string
                correctDrift:
                  description: CorrectDrift specifies how drift correction should
                    work.
                  properties:
                    enabled:
                      description: Enabled correct drift if true.
                      type: boolean
                    force:
                      description: Force helm rollback with --force option will be
                        used if true. This will try to recreate all resources in the
                        release.
                      type: boolean
                    keepFailHistory:
                      description: KeepFailHistory keeps track of failed rollbacks
                        in the helm history.
                      type: boolean
                  type: object
                deleteNamespace:
                  description: DeleteNamespace specifies if the namespace created
                    must be deleted after deleting the GitRepo.
                  type: boolean
                disablePolling:
                  description: Disables git polling. When enabled only webhooks will
                    be used.
                  type: boolean
                forceSyncGeneration:
                  description: Increment this number to force a redeployment of contents
                    from Git.
                  format: int64
                  type: integer
                helmRepoURLRegex:
                  description: 'HelmRepoURLRegex Helm credentials will be used if
                    the helm repo matches this regex

                    Credentials will always be used if this is empty or not provided.'
                  nullable: true
                  type: string
                helmSecretName:
                  description: HelmSecretName contains the auth secret for a private
                    Helm repository.
                  nullable: true
                  type: string
                helmSecretNameForPaths:
                  description: HelmSecretNameForPaths contains the auth secret for
                    private Helm repository for each path.
                  nullable: true
                  type: string
                imageScanCommit:
                  description: Commit specifies how to commit to the git repo when
                    a new image is scanned and written back to git repo.
                  properties:
                    authorEmail:
                      description: AuthorEmail gives the email to provide when making
                        a commit
                      type: string
                    authorName:
                      description: AuthorName gives the name to provide when making
                        a commit
                      type: string
                    messageTemplate:
                      description: 'MessageTemplate provides a template for the commit
                        message,

                        into which will be interpolated the details of the change
                        made.'
                      type: string
                  type: object
                imageScanInterval:
                  description: ImageScanInterval is the interval of syncing scanned
                    images and writing back to git repo.
                  type: string
                insecureSkipTLSVerify:
                  description: InsecureSkipTLSverify will use insecure HTTPS to clone
                    the repo.
                  type: boolean
                keepResources:
                  description: KeepResources specifies if the resources created must
                    be kept after deleting the GitRepo.
                  type: boolean
                ociRegistrySecret:
                  description: OCIRegistrySecret contains the name of the secret to
                    be used for retrieving the OCI registry connection details.
                  type: string
                paths:
                  description: 'Paths is the directories relative to the git repo
                    root that contain resources to be applied.

                    Path globbing is supported, for example ["charts/*"] will match
                    all folders as a subdirectory of charts/

                    If empty, "/" is the default.'
                  items:
                    type: string
                  nullable: true
                  type: array
                paused:
                  description: 'Paused, when true, causes changes in Git not to be
                    propagated down to the clusters but instead to mark

                    resources as OutOfSync.'
                  type: boolean
                pollingInterval:
                  description: PollingInterval is how often to check git for new updates.
                  nullable: true
                  type: string
                repo:
                  description: Repo is a URL to a git repo to clone and index.
                  minLength: 1
                  type: string
                revision:
                  description: Revision A specific commit or tag to operate on.
                  nullable: true
                  type: string
                serviceAccount:
                  description: ServiceAccount used in the downstream cluster for deployment.
                  nullable: true
                  type: string
                targetNamespace:
                  description: 'Ensure that all resources are created in this namespace

                    Any cluster scoped resource will be rejected if this is set

                    Additionally this namespace will be created on demand.'
                  nullable: true
                  type: string
                targets:
                  description: Targets is a list of targets this repo will deploy
                    to.
                  items:
                    description: GitTarget is a cluster or cluster group to deploy
                      to.
                    properties:
                      clusterGroup:
                        description: ClusterGroup is the name of a cluster group in
                          the same namespace as the clusters.
                        nullable: true
                        type: string
                      clusterGroupSelector:
                        description: ClusterGroupSelector is a label selector to select
                          cluster groups.
                        nullable: true
                        properties:
                          matchExpressions:
                            description: matchExpressions is a list of label selector
                              requirements. The requirements are ANDed.
                            items:
                              description: 'A label selector requirement is a selector
                                that contains values, a key, and an operator that

                                relates the key and values.'
                              properties:
                                key:
                                  description: key is the label key that the selector
                                    applies to.
                                  type: string
                                operator:
                                  description: 'operator represents a key''s relationship
                                    to a set of values.

                                    Valid operators are In, NotIn, Exists and DoesNotExist.'
                                  type: string
                                values:
                                  description: 'values is an array of string values.
                                    If the operator is In or NotIn,

                                    the values array must be non-empty. If the operator
                                    is Exists or DoesNotExist,

                                    the values array must be empty. This array is
                                    replaced during a strategic

                                    merge patch.'
                                  items:
                                    type: string
                                  type: array
                                  x-kubernetes-list-type: atomic
                              required:
                                - key
                                - operator
                              type: object
                            type: array
                            x-kubernetes-list-type: atomic
                          matchLabels:
                            additionalProperties:
                              type: string
                            description: 'matchLabels is a map of {key,value} pairs.
                              A single {key,value} in the matchLabels

                              map is equivalent to an element of matchExpressions,
                              whose key field is "key", the

                              operator is "In", and the values array contains only
                              "value". The requirements are ANDed.'
                            type: object
                        type: object
                        x-kubernetes-map-type: atomic
                      clusterName:
                        description: ClusterName is the name of a cluster.
                        nullable: true
                        type: string
                      clusterSelector:
                        description: ClusterSelector is a label selector to select
                          clusters.
                        nullable: true
                        properties:
                          matchExpressions:
                            description: matchExpressions is a list of label selector
                              requirements. The requirements are ANDed.
                            items:
                              description: 'A label selector requirement is a selector
                                that contains values, a key, and an operator that

                                relates the key and values.'
                              properties:
                                key:
                                  description: key is the label key that the selector
                                    applies to.
                                  type: string
                                operator:
                                  description: 'operator represents a key''s relationship
                                    to a set of values.

                                    Valid operators are In, NotIn, Exists and DoesNotExist.'
                                  type: string
                                values:
                                  description: 'values is an array of string values.
                                    If the operator is In or NotIn,

                                    the values array must be non-empty. If the operator
                                    is Exists or DoesNotExist,

                                    the values array must be empty. This array is
                                    replaced during a strategic

                                    merge patch.'
                                  items:
                                    type: string
                                  type: array
                                  x-kubernetes-list-type: atomic
                              required:
                                - key
                                - operator
                              type: object
                            type: array
                            x-kubernetes-list-type: atomic
                          matchLabels:
                            additionalProperties:
                              type: string
                            description: 'matchLabels is a map of {key,value} pairs.
                              A single {key,value} in the matchLabels

                              map is equivalent to an element of matchExpressions,
                              whose key field is "key", the

                              operator is "In", and the values array contains only
                              "value". The requirements are ANDed.'
                            type: object
                        type: object
                        x-kubernetes-map-type: atomic
                      name:
                        description: Name is the name of this target.
                        nullable: true
                        type: string
                    type: object
                  type: array
                webhookSecret:
                  description: WebhookSecret contains the name of the secret to use
                    for webhook parsing
                  type: string
              required:
                - repo
              type: object
            status:
              properties:
                commit:
                  description: Commit is the Git commit hash from the last git job
                    run.
                  type: string
                conditions:
                  description: 'Conditions is a list of Wrangler conditions that describe
                    the state

                    of the resource.'
                  items:
                    properties:
                      lastTransitionTime:
                        description: Last time the condition transitioned from one
                          status to another.
                        type: string
                      lastUpdateTime:
                        description: The last time this condition was updated.
                        type: string
                      message:
                        description: Human-readable message indicating details about
                          last transition
                        type: string
                      reason:
                        description: The reason for the condition's last transition.
                        type: string
                      status:
                        description: Status of the condition, one of True, False,
                          Unknown.
                        type: string
                      type:
                        description: Type of cluster condition.
                        type: string
                    required:
                      - status
                      - type
                    type: object
                  type: array
                desiredReadyClusters:
                  description: "DesiredReadyClusters\tis the number of clusters that\
                    \ should be ready for bundles of this resource."
                  type: integer
                display:
                  description: Display contains a human readable summary of the status.
                  properties:
                    error:
                      description: Error is true if a message is present.
                      type: boolean
                    message:
                      description: Message contains the relevant message from the
                        deployment conditions.
                      type: string
                    readyBundleDeployments:
                      description: 'ReadyBundleDeployments is a string in the form
                        "%d/%d", that describes the

                        number of ready bundledeployments over the total number of
                        bundledeployments.'
                      type: string
                    state:
                      description: 'State is the state of the resource, e.g. "GitUpdating"
                        or the maximal

                        BundleState according to StateRank.'
                      type: string
                  type: object
                gitJobStatus:
                  description: GitJobStatus is the status of the last Git job run,
                    e.g. "Current" if there was no error.
                  type: string
                lastPollingTriggered:
                  description: LastPollingTime is the last time the polling check
                    was triggered
                  format: date-time
                  type: string
                lastSyncedImageScanTime:
                  description: LastSyncedImageScanTime is the time of the last image
                    scan.
                  format: date-time
                  type: string
                observedGeneration:
                  description: 'ObservedGeneration is the current generation of the
                    resource in the cluster. It is copied from k8s

                    metadata.Generation. The value is incremented for all changes,
                    except for changes to .metadata or .status.'
                  format: int64
                  type: integer
                perClusterResourceCounts:
                  additionalProperties:
                    description: ResourceCounts contains the number of resources in
                      each state.
                    properties:
                      desiredReady:
                        description: DesiredReady is the number of resources that
                          should be ready.
                        type: integer
                      missing:
                        description: Missing is the number of missing resources.
                        type: integer
                      modified:
                        description: Modified is the number of resources that have
                          been modified.
                        type: integer
                      notReady:
                        description: 'NotReady is the number of not ready resources.
                          Resources are not

                          ready if they do not match any other state.'
                        type: integer
                      orphaned:
                        description: Orphaned is the number of orphaned resources.
                        type: integer
                      ready:
                        description: Ready is the number of ready resources.
                        type: integer
                      unknown:
                        description: Unknown is the number of resources in an unknown
                          state.
                        type: integer
                      waitApplied:
                        description: WaitApplied is the number of resources that are
                          waiting to be applied.
                        type: integer
                    type: object
                  description: PerClusterResourceCounts contains the number of resources
                    in each state over all bundles, per cluster.
                  type: object
                pollingCommit:
                  description: PollingCommit is the latest Git commit hash received
                    from polling
                  type: string
                readyClusters:
                  description: 'ReadyClusters is the lowest number of clusters that
                    are ready over

                    all the bundles of this resource.'
                  type: integer
                resourceCounts:
                  description: ResourceCounts contains the number of resources in
                    each state over all bundles.
                  properties:
                    desiredReady:
                      description: DesiredReady is the number of resources that should
                        be ready.
                      type: integer
                    missing:
                      description: Missing is the number of missing resources.
                      type: integer
                    modified:
                      description: Modified is the number of resources that have been
                        modified.
                      type: integer
                    notReady:
                      description: 'NotReady is the number of not ready resources.
                        Resources are not

                        ready if they do not match any other state.'
                      type: integer
                    orphaned:
                      description: Orphaned is the number of orphaned resources.
                      type: integer
                    ready:
                      description: Ready is the number of ready resources.
                      type: integer
                    unknown:
                      description: Unknown is the number of resources in an unknown
                        state.
                      type: integer
                    waitApplied:
                      description: WaitApplied is the number of resources that are
                        waiting to be applied.
                      type: integer
                  type: object
                resources:
                  description: Resources contains metadata about the resources of
                    each bundle.
                  items:
                    description: Resource contains metadata about the resources of
                      a bundle.
                    properties:
                      apiVersion:
                        description: APIVersion is the API version of the resource.
                        nullable: true
                        type: string
                      error:
                        description: Error is true if any Error in the PerClusterState
                          is true.
                        type: boolean
                      id:
                        description: ID is the name of the resource, e.g. "namespace1/my-config"
                          or "backingimagemanagers.storage.io".
                        nullable: true
                        type: string
                      incompleteState:
                        description: 'IncompleteState is true if a bundle summary
                          has 10 or more non-ready

                          resources or a non-ready resource has more 10 or more non-ready
                          or

                          modified states.'
                        type: boolean
                      kind:
                        description: Kind is the k8s kind of the resource.
                        nullable: true
                        type: string
                      message:
                        description: Message is the first message from the PerClusterStates.
                        nullable: true
                        type: string
                      name:
                        description: Name of the resource.
                        nullable: true
                        type: string
                      namespace:
                        description: Namespace of the resource.
                        nullable: true
                        type: string
                      perClusterState:
                        description: PerClusterState contains lists of cluster IDs
                          for every State for this resource
                        nullable: true
                        properties:
                          missing:
                            description: Missing is a list of cluster IDs for which
                              this a resource is in Missing state
                            items:
                              type: string
                            type: array
                          modified:
                            description: Modified is a list of cluster IDs for which
                              this a resource is in Modified state
                            items:
                              type: string
                            type: array
                          notReady:
                            description: NotReady is a list of cluster IDs for which
                              this a resource is in NotReady state
                            items:
                              type: string
                            type: array
                          orphaned:
                            description: Orphaned is a list of cluster IDs for which
                              this a resource is in Orphaned state
                            items:
                              type: string
                            type: array
                          pending:
                            description: Pending is a list of cluster IDs for which
                              this a resource is in Pending state
                            items:
                              type: string
                            type: array
                          ready:
                            description: Ready is a list of cluster IDs for which
                              this a resource is in Ready state
                            items:
                              type: string
                            type: array
                          unknown:
                            description: Unknown is a list of cluster IDs for which
                              this a resource is in Unknown state
                            items:
                              type: string
                            type: array
                          waitApplied:
                            description: WaitApplied is a list of cluster IDs for
                              which this a resource is in WaitApplied state
                            items:
                              type: string
                            type: array
                        type: object
                      state:
                        description: State is the state of the resource, e.g. "Unknown",
                          "WaitApplied", "ErrApplied" or "Ready".
                        type: string
                      transitioning:
                        description: Transitioning is true if any Transitioning in
                          the PerClusterState is true.
                        type: boolean
                      type:
                        description: Type is the type of the resource, e.g. "apiextensions.k8s.io.customresourcedefinition"
                          or "configmap".
                        type: string
                    required:
                      - perClusterState
                    type: object
                  type: array
                summary:
                  description: Summary contains the number of bundle deployments in
                    each state and a list of non-ready resources.
                  properties:
                    desiredReady:
                      description: 'DesiredReady is the number of bundle deployments
                        that should be

                        ready.'
                      type: integer
                    errApplied:
                      description: 'ErrApplied is the number of bundle deployments
                        that have been synced

                        from the Fleet controller and the downstream cluster, but
                        with some

                        errors when deploying the bundle.'
                      type: integer
                    modified:
                      description: 'Modified is the number of bundle deployments that
                        have been deployed

                        and for which all resources are ready, but where some changes
                        from the

                        Git repository have not yet been synced.'
                      type: integer
                    nonReadyResources:
                      description: 'NonReadyClusters is a list of states, which is
                        filled for a bundle

                        that is not ready.'
                      items:
                        description: 'NonReadyResource contains information about
                          a bundle that is not ready for a

                          given state like "ErrApplied". It contains a list of non-ready
                          or modified

                          resources and their states.'
                        properties:
                          bundleState:
                            description: State is the state of the resource, like
                              e.g. "NotReady" or "ErrApplied".
                            nullable: true
                            type: string
                          message:
                            description: Message contains information why the bundle
                              is not ready.
                            nullable: true
                            type: string
                          modifiedStatus:
                            description: ModifiedStatus lists the state for each modified
                              resource.
                            items:
                              description: 'ModifiedStatus is used to report the status
                                of a resource that is modified.

                                It indicates if the modification was a create, a delete
                                or a patch.'
                              properties:
                                apiVersion:
                                  nullable: true
                                  type: string
                                delete:
                                  type: boolean
                                exist:
                                  description: Exist is true if the resource exists
                                    but is not owned by us. This can happen if a resource
                                    was adopted by another bundle whereas the first
                                    bundle still exists and due to that reports that
                                    it does not own it.
                                  type: boolean
                                kind:
                                  nullable: true
                                  type: string
                                missing:
                                  type: boolean
                                name:
                                  nullable: true
                                  type: string
                                namespace:
                                  nullable: true
                                  type: string
                                patch:
                                  nullable: true
                                  type: string
                              type: object
                            nullable: true
                            type: array
                          name:
                            description: Name is the name of the resource.
                            nullable: true
                            type: string
                          nonReadyStatus:
                            description: NonReadyStatus lists the state for each non-ready
                              resource.
                            items:
                              description: NonReadyStatus is used to report the status
                                of a resource that is not ready. It includes a summary.
                              properties:
                                apiVersion:
                                  nullable: true
                                  type: string
                                kind:
                                  nullable: true
                                  type: string
                                name:
                                  nullable: true
                                  type: string
                                namespace:
                                  nullable: true
                                  type: string
                                summary:
                                  properties:
                                    error:
                                      type: boolean
                                    message:
                                      items:
                                        type: string
                                      type: array
                                    state:
                                      type: string
                                    transitioning:
                                      type: boolean
                                  type: object
                                uid:
                                  description: 'UID is a type that holds unique ID
                                    values, including UUIDs.  Because we

                                    don''t ONLY use UUIDs, this is an alias to string.  Being
                                    a type captures

                                    intent and helps make sure that UIDs and names
                                    do not get conflated.'
                                  nullable: true
                                  type: string
                              type: object
                            nullable: true
                            type: array
                        type: object
                      nullable: true
                      type: array
                    notReady:
                      description: 'NotReady is the number of bundle deployments that
                        have been deployed

                        where some resources are not ready.'
                      type: integer
                    outOfSync:
                      description: 'OutOfSync is the number of bundle deployments
                        that have been synced

                        from Fleet controller, but not yet by the downstream agent.'
                      type: integer
                    pending:
                      description: 'Pending is the number of bundle deployments that
                        are being processed

                        by Fleet controller.'
                      type: integer
                    ready:
                      description: 'Ready is the number of bundle deployments that
                        have been deployed

                        where all resources are ready.'
                      type: integer
                    waitApplied:
                      description: 'WaitApplied is the number of bundle deployments
                        that have been

                        synced from Fleet controller and downstream cluster, but are
                        waiting

                        to be deployed.'
                      type: integer
                  type: object
                updateGeneration:
                  description: Update generation is the force update generation if
                    spec.forceSyncGeneration is set
                  format: int64
                  type: integer
                webhookCommit:
                  description: WebhookCommit is the latest Git commit hash received
                    from a webhook
                  type: string
              type: object
          type: object
      served: true
      storage: true
      subresources:
        status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.19.0
  name: helmops.fleet.cattle.io
spec:
  group: fleet.cattle.io
  names:
    categories:
      - fleet
    kind: HelmOp
    listKind: HelmOpList
    plural: helmops
    singular: helmop
  scope: Namespaced
  versions:
    - additionalPrinterColumns:
        - jsonPath: .spec.helm.repo
          name: Repo
          type: string
        - jsonPath: .spec.helm.chart
          name: Chart
          type: string
        - jsonPath: .status.version
          name: Version
          type: string
        - jsonPath: .status.display.readyBundleDeployments
          name: BundleDeployments-Ready
          type: string
        - jsonPath: .status.conditions[?(@.type=="Ready")].message
          name: Status
          type: string
      name: v1alpha1
      schema:
        openAPIV3Schema:
          description: 'HelmOp describes a helm chart information.

            The resource contains the necessary information to deploy the chart to
            target clusters.'
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
                of an object.

                Servers should convert recognized schemas to the latest internal value,
                and

                may reject unrecognized values.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource
                this object represents.

                Servers may infer this from the endpoint the client submits requests
                to.

                Cannot be updated.

                In CamelCase.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
            spec:
              properties:
                contentsId:
                  description: ContentsID stores the contents id when deploying contents
                    using an OCI registry.
                  nullable: true
                  type: string
                correctDrift:
                  description: CorrectDrift specifies how drift correction should
                    work.
                  properties:
                    enabled:
                      description: Enabled correct drift if true.
                      type: boolean
                    force:
                      description: Force helm rollback with --force option will be
                        used if true. This will try to recreate all resources in the
                        release.
                      type: boolean
                    keepFailHistory:
                      description: KeepFailHistory keeps track of failed rollbacks
                        in the helm history.
                      type: boolean
                  type: object
                defaultNamespace:
                  description: 'DefaultNamespace is the namespace to use for resources
                    that do not

                    specify a namespace. This field is not used to enforce or lock
                    down

                    the deployment to a specific namespace.'
                  nullable: true
                  type: string
                deleteCRDResources:
                  description: DeleteCRDResources deletes CRDs. Warning! this will
                    also delete all your Custom Resources.
                  type: boolean
                deleteNamespace:
                  description: DeleteNamespace can be used to delete the deployed
                    namespace when removing the bundle
                  type: boolean
                dependsOn:
                  description: DependsOn refers to the bundles which must be ready
                    before this bundle can be deployed.
                  items:
                    properties:
                      name:
                        description: Name of the bundle.
                        nullable: true
                        type: string
                      selector:
                        description: Selector matching bundle's labels.
                        nullable: true
                        properties:
                          matchExpressions:
                            description: matchExpressions is a list of label selector
                              requirements. The requirements are ANDed.
                            items:
                              description: 'A label selector requirement is a selector
                                that contains values, a key, and an operator that

                                relates the key and values.'
                              properties:
                                key:
                                  description: key is the label key that the selector
                                    applies to.
                                  type: string
                                operator:
                                  description: 'operator represents a key''s relationship
                                    to a set of values.

                                    Valid operators are In, NotIn, Exists and DoesNotExist.'
                                  type: string
                                values:
                                  description: 'values is an array of string values.
                                    If the operator is In or NotIn,

                                    the values array must be non-empty. If the operator
                                    is Exists or DoesNotExist,

                                    the values array must be empty. This array is
                                    replaced during a strategic

                                    merge patch.'
                                  items:
                                    type: string
                                  type: array
                                  x-kubernetes-list-type: atomic
                              required:
                                - key
                                - operator
                              type: object
                            type: array
                            x-kubernetes-list-type: atomic
                          matchLabels:
                            additionalProperties:
                              type: string
                            description: 'matchLabels is a map of {key,value} pairs.
                              A single {key,value} in the matchLabels

                              map is equivalent to an element of matchExpressions,
                              whose key field is "key", the

                              operator is "In", and the values array contains only
                              "value". The requirements are ANDed.'
                            type: object
                        type: object
                        x-kubernetes-map-type: atomic
                    type: object
                  nullable: true
                  type: array
                diff:
                  description: Diff can be used to ignore the modified state of objects
                    which are amended at runtime.
                  nullable: true
                  properties:
                    comparePatches:
                      description: ComparePatches match a resource and remove fields,
                        or the resource itself from the check for modifications.
                      items:
                        description: ComparePatch matches a resource and removes fields
                          from the check for modifications.
                        properties:
                          apiVersion:
                            description: APIVersion is the apiVersion of the resource
                              to match.
                            nullable: true
                            type: string
                          jsonPointers:
                            description: JSONPointers ignore diffs at a certain JSON
                              path.
                            items:
                              type: string
                            nullable: true
                            type: array
                          kind:
                            description: Kind is the kind of the resource to match.
                            nullable: true
                            type: string
                          name:
                            description: Name is the name of the resource to match.
                            nullable: true
                            type: string
                          namespace:
                            description: Namespace is the namespace of the resource
                              to match.
                            nullable: true
                            type: string
                          operations:
                            description: Operations remove a JSON path from the resource.
                            items:
                              description: 'Operation of a ComparePatch, usually:

                                * "remove" to remove a specific path in a resource

                                * "ignore" to remove the entire resource from checks
                                for modifications.'
                              properties:
                                op:
                                  description: Op is usually "remove" or "ignore"
                                  nullable: true
                                  type: string
                                path:
                                  description: Path is the JSON path to remove. Not
                                    needed if Op is "ignore".
                                  nullable: true
                                  type: string
                                value:
                                  description: Value is usually empty.
                                  nullable: true
                                  type: string
                              type: object
                            nullable: true
                            type: array
                        type: object
                      nullable: true
                      type: array
                  type: object
                downstreamResources:
                  description: 'DownstreamResources points to resources to be copied
                    into downstream clusters, from the bundle''s

                    namespace.'
                  items:
                    description: 'DownstreamResource contains identifiers for a resource
                      to be copied from the parent bundle''s namespace to each

                      downstream cluster.'
                    properties:
                      kind:
                        type: string
                      name:
                        type: string
                    type: object
                  type: array
                forceSyncGeneration:
                  description: ForceSyncGeneration is used to force a redeployment
                  format: int64
                  type: integer
                helm:
                  description: Helm options for the deployment, like the chart name,
                    repo and values.
                  properties:
                    atomic:
                      description: Atomic sets the --atomic flag when Helm is performing
                        an upgrade
                      type: boolean
                    chart:
                      description: 'Chart can refer to any go-getter URL or OCI registry
                        based helm

                        chart URL. The chart will be downloaded.'
                      nullable: true
                      type: string
                    disableDNS:
                      description: DisableDNS can be used to customize Helm's EnableDNS
                        option, which Fleet sets to `true` by default.
                      type: boolean
                    disableDependencyUpdate:
                      description: DisableDependencyUpdate allows skipping chart dependencies
                        update
                      type: boolean
                    disablePreProcess:
                      description: DisablePreProcess disables template processing
                        in values
                      type: boolean
                    force:
                      description: Force allows to override immutable resources. This
                        could be dangerous.
                      type: boolean
                    maxHistory:
                      description: MaxHistory limits the maximum number of revisions
                        saved per release by Helm.
                      type: integer
                    releaseName:
                      description: 'ReleaseName sets a custom release name to deploy
                        the chart as. If

                        not specified a release name will be generated by combining
                        the

                        invoking GitRepo.name + GitRepo.path.'
                      maxLength: 53
                      nullable: true
                      pattern: ^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$
                      type: string
                    repo:
                      description: Repo is the name of the HTTPS helm repo to download
                        the chart from.
                      nullable: true
                      type: string
                    skipSchemaValidation:
                      description: SkipSchemaValidation allows skipping schema validation
                        against the chart values
                      type: boolean
                    takeOwnership:
                      description: TakeOwnership makes helm skip the check for its
                        own annotations
                      type: boolean
                    templateValues:
                      additionalProperties:
                        type: string
                      description: 'Template Values passed to Helm. It is possible
                        to specify the keys and values

                        as go template strings. Unlike .values, content of each key
                        will be templated

                        first, before serializing to yaml. This allows to template
                        complex values,

                        like ranges and maps.

                        templateValues keys have precedence over values keys in case
                        of conflict.'
                      nullable: true
                      type: object
                    timeoutSeconds:
                      description: TimeoutSeconds is the time to wait for Helm operations.
                      type: integer
                    values:
                      description: 'Values passed to Helm. It is possible to specify
                        the keys and values

                        as go template strings.'
                      nullable: true
                      type: object
                      x-kubernetes-preserve-unknown-fields: true
                    valuesFiles:
                      description: ValuesFiles is a list of files to load values from.
                      items:
                        type: string
                      nullable: true
                      type: array
                    valuesFrom:
                      description: ValuesFrom loads the values from configmaps and
                        secrets.
                      items:
                        description: 'Define helm values that can come from configmap,
                          secret or external. Credit: https://github.com/fluxcd/helm-operator/blob/0cfea875b5d44bea995abe7324819432070dfbdc/pkg/apis/helm.fluxcd.io/v1/types_helmrelease.go#L439'
                        properties:
                          configMapKeyRef:
                            description: The reference to a config map with release
                              values.
                            nullable: true
                            properties:
                              key:
                                nullable: true
                                type: string
                              name:
                                description: Name of a resource in the same namespace
                                  as the referent.
                                nullable: true
                                type: string
                              namespace:
                                nullable: true
                                type: string
                            type: object
                          secretKeyRef:
                            description: The reference to a secret with release values.
                            nullable: true
                            properties:
                              key:
                                nullable: true
                                type: string
                              name:
                                description: Name of a resource in the same namespace
                                  as the referent.
                                nullable: true
                                type: string
                              namespace:
                                nullable: true
                                type: string
                            type: object
                        type: object
                      nullable: true
                      type: array
                    version:
                      description: Version of the chart to download
                      nullable: true
                      type: string
                    waitForJobs:
                      description: 'WaitForJobs if set and timeoutSeconds provided,
                        will wait until all

                        Jobs have been completed before marking the GitRepo as ready.
                        It

                        will wait for as long as timeoutSeconds'
                      type: boolean
                  type: object
                helmOpOptions:
                  description: 'HelmOpOptions stores the options relative to HelmOp
                    resources

                    Non-nil HelmOpOptions indicate that the source of resources is
                    a Helm chart,

                    not a git repository.'
                  nullable: true
                  properties:
                    helmOpInsecureSkipTLSVerify:
                      description: InsecureSkipTLSverify will use insecure HTTPS to
                        clone the helm app resource.
                      type: boolean
                    helmOpSecretName:
                      description: 'SecretName stores the secret name for storing
                        credentials when accessing

                        a remote helm repository defined in a HelmOp resource'
                      type: string
                  type: object
                helmSecretName:
                  description: 'HelmSecretName contains the auth secret with the credentials
                    to access

                    a private Helm repository.'
                  nullable: true
                  type: string
                ignore:
                  description: IgnoreOptions can be used to ignore fields when monitoring
                    the bundle.
                  nullable: true
                  properties:
                    conditions:
                      description: Conditions is a list of conditions to be ignored
                        when monitoring the Bundle.
                      items:
                        additionalProperties:
                          type: string
                        type: object
                      nullable: true
                      type: array
                  type: object
                insecureSkipTLSVerify:
                  description: InsecureSkipTLSverify will use insecure HTTPS to clone
                    the helm app resource.
                  type: boolean
                keepResources:
                  description: KeepResources can be used to keep the deployed resources
                    when removing the bundle
                  type: boolean
                kustomize:
                  description: 'Kustomize options for the deployment, like the dir
                    containing the

                    kustomization.yaml file.'
                  nullable: true
                  properties:
                    dir:
                      description: 'Dir points to a custom folder for kustomize resources.
                        This folder must contain

                        a kustomization.yaml file.'
                      nullable: true
                      type: string
                  type: object
                labels:
                  additionalProperties:
                    type: string
                  description: 'Labels are copied to the bundle and can be used in
                    a

                    dependsOn.selector.'
                  type: object
                namespace:
                  description: 'TargetNamespace if present will assign all resource
                    to this

                    namespace and if any cluster scoped resource exists the deployment

                    will fail.'
                  nullable: true
                  type: string
                namespaceAnnotations:
                  additionalProperties:
                    type: string
                  description: NamespaceAnnotations are annotations that will be appended
                    to the namespace created by Fleet.
                  nullable: true
                  type: object
                namespaceLabels:
                  additionalProperties:
                    type: string
                  description: NamespaceLabels are labels that will be appended to
                    the namespace created by Fleet.
                  nullable: true
                  type: object
                overwrites:
                  description: 'Overwrites indicates which resources, if any, come
                    from this bundle and overwrite another existing bundle.

                    This flag is set internally by Fleet, and should not be altered
                    by users.'
                  items:
                    properties:
                      kind:
                        type: string
                      name:
                        type: string
                      namespace:
                        type: string
                    type: object
                  type: array
                paused:
                  description: Paused if set to true, will stop any BundleDeployments
                    from being updated. It will be marked as out of sync.
                  type: boolean
                pollingInterval:
                  description: PollingInterval is how often to check the Helm repository
                    for new updates.
                  nullable: true
                  type: string
                resources:
                  description: 'Resources contains the resources that were read from
                    the bundle''s

                    path. This includes the content of downloaded helm charts.'
                  items:
                    description: BundleResource represents the content of a single
                      resource from the bundle, like a YAML manifest.
                    properties:
                      content:
                        description: The content of the resource, can be compressed.
                        nullable: true
                        type: string
                      encoding:
                        description: Encoding is either empty or "base64+gz".
                        nullable: true
                        type: string
                      name:
                        description: Name of the resource, can include the bundle's
                          internal path.
                        nullable: true
                        type: string
                    type: object
                  nullable: true
                  type: array
                rolloutStrategy:
                  description: 'RolloutStrategy controls the rollout of bundles, by
                    defining

                    partitions, canaries and percentages for cluster availability.'
                  nullable: true
                  properties:
                    autoPartitionSize:
                      anyOf:
                        - type: integer
                        - type: string
                      description: 'A number or percentage of how to automatically
                        partition clusters if no

                        specific partitioning strategy is configured.

                        default: 25%'
                      nullable: true
                      x-kubernetes-int-or-string: true
                    maxUnavailable:
                      anyOf:
                        - type: integer
                        - type: string
                      description: 'A number or percentage of clusters that can be
                        unavailable during an update

                        of a bundle. This follows the same basic approach as a deployment
                        rollout

                        strategy. Once the number of clusters meets unavailable state
                        update will be

                        paused. Default value is 100% which doesn''t take effect on
                        update.

                        default: 100%'
                      nullable: true
                      x-kubernetes-int-or-string: true
                    maxUnavailablePartitions:
                      anyOf:
                        - type: integer
                        - type: string
                      description: 'A number or percentage of cluster partitions that
                        can be unavailable during

                        an update of a bundle.

                        default: 0'
                      nullable: true
                      x-kubernetes-int-or-string: true
                    partitions:
                      description: 'A list of definitions of partitions.  If any target
                        clusters do not match

                        the configuration they are added to partitions at the end
                        following the

                        autoPartitionSize.'
                      items:
                        description: Partition defines a separate rollout strategy
                          for a set of clusters.
                        properties:
                          clusterGroup:
                            description: A cluster group name to include in this partition
                            type: string
                          clusterGroupSelector:
                            description: Selector matching cluster group labels to
                              include in this partition
                            nullable: true
                            properties:
                              matchExpressions:
                                description: matchExpressions is a list of label selector
                                  requirements. The requirements are ANDed.
                                items:
                                  description: 'A label selector requirement is a
                                    selector that contains values, a key, and an operator
                                    that

                                    relates the key and values.'
                                  properties:
                                    key:
                                      description: key is the label key that the selector
                                        applies to.
                                      type: string
                                    operator:
                                      description: 'operator represents a key''s relationship
                                        to a set of values.

                                        Valid operators are In, NotIn, Exists and
                                        DoesNotExist.'
                                      type: string
                                    values:
                                      description: 'values is an array of string values.
                                        If the operator is In or NotIn,

                                        the values array must be non-empty. If the
                                        operator is Exists or DoesNotExist,

                                        the values array must be empty. This array
                                        is replaced during a strategic

                                        merge patch.'
                                      items:
                                        type: string
                                      type: array
                                      x-kubernetes-list-type: atomic
                                  required:
                                    - key
                                    - operator
                                  type: object
                                type: array
                                x-kubernetes-list-type: atomic
                              matchLabels:
                                additionalProperties:
                                  type: string
                                description: 'matchLabels is a map of {key,value}
                                  pairs. A single {key,value} in the matchLabels

                                  map is equivalent to an element of matchExpressions,
                                  whose key field is "key", the

                                  operator is "In", and the values array contains
                                  only "value". The requirements are ANDed.'
                                type: object
                            type: object
                            x-kubernetes-map-type: atomic
                          clusterName:
                            description: ClusterName is the name of a cluster to include
                              in this partition
                            type: string
                          clusterSelector:
                            description: Selector matching cluster labels to include
                              in this partition
                            properties:
                              matchExpressions:
                                description: matchExpressions is a list of label selector
                                  requirements. The requirements are ANDed.
                                items:
                                  description: 'A label selector requirement is a
                                    selector that contains values, a key, and an operator
                                    that

                                    relates the key and values.'
                                  properties:
                                    key:
                                      description: key is the label key that the selector
                                        applies to.
                                      type: string
                                    operator:
                                      description: 'operator represents a key''s relationship
                                        to a set of values.

                                        Valid operators are In, NotIn, Exists and
                                        DoesNotExist.'
                                      type: string
                                    values:
                                      description: 'values is an array of string values.
                                        If the operator is In or NotIn,

                                        the values array must be non-empty. If the
                                        operator is Exists or DoesNotExist,

                                        the values array must be empty. This array
                                        is replaced during a strategic

                                        merge patch.'
                                      items:
                                        type: string
                                      type: array
                                      x-kubernetes-list-type: atomic
                                  required:
                                    - key
                                    - operator
                                  type: object
                                type: array
                                x-kubernetes-list-type: atomic
                              matchLabels:
                                additionalProperties:
                                  type: string
                                description: 'matchLabels is a map of {key,value}
                                  pairs. A single {key,value} in the matchLabels

                                  map is equivalent to an element of matchExpressions,
                                  whose key field is "key", the

                                  operator is "In", and the values array contains
                                  only "value". The requirements are ANDed.'
                                type: object
                            type: object
                            x-kubernetes-map-type: atomic
                          maxUnavailable:
                            anyOf:
                              - type: integer
                              - type: string
                            description: 'A number or percentage of clusters that
                              can be unavailable in this

                              partition before this partition is treated as done.

                              default: 10%'
                            x-kubernetes-int-or-string: true
                          name:
                            description: A user-friendly name given to the partition
                              used for Display (optional).
                            nullable: true
                            type: string
                        type: object
                      nullable: true
                      type: array
                  type: object
                serviceAccount:
                  description: ServiceAccount which will be used to perform this deployment.
                  nullable: true
                  type: string
                targetRestrictions:
                  description: TargetRestrictions is an allow list, which controls
                    if a bundledeployment is created for a target.
                  items:
                    description: 'BundleTargetRestriction is used internally by Fleet
                      and should not be modified.

                      It acts as an allow list, to prevent the creation of BundleDeployments
                      from

                      Targets created by TargetCustomizations in fleet.yaml.'
                    properties:
                      clusterGroup:
                        nullable: true
                        type: string
                      clusterGroupSelector:
                        description: 'A label selector is a label query over a set
                          of resources. The result of matchLabels and

                          matchExpressions are ANDed. An empty label selector matches
                          all objects. A null

                          label selector matches no objects.'
                        nullable: true
                        properties:
                          matchExpressions:
                            description: matchExpressions is a list of label selector
                              requirements. The requirements are ANDed.
                            items:
                              description: 'A label selector requirement is a selector
                                that contains values, a key, and an operator that

                                relates the key and values.'
                              properties:
                                key:
                                  description: key is the label key that the selector
                                    applies to.
                                  type: string
                                operator:
                                  description: 'operator represents a key''s relationship
                                    to a set of values.

                                    Valid operators are In, NotIn, Exists and DoesNotExist.'
                                  type: string
                                values:
                                  description: 'values is an array of string values.
                                    If the operator is In or NotIn,

                                    the values array must be non-empty. If the operator
                                    is Exists or DoesNotExist,

                                    the values array must be empty. This array is
                                    replaced during a strategic

                                    merge patch.'
                                  items:
                                    type: string
                                  type: array
                                  x-kubernetes-list-type: atomic
                              required:
                                - key
                                - operator
                              type: object
                            type: array
                            x-kubernetes-list-type: atomic
                          matchLabels:
                            additionalProperties:
                              type: string
                            description: 'matchLabels is a map of {key,value} pairs.
                              A single {key,value} in the matchLabels

                              map is equivalent to an element of matchExpressions,
                              whose key field is "key", the

                              operator is "In", and the values array contains only
                              "value". The requirements are ANDed.'
                            type: object
                        type: object
                        x-kubernetes-map-type: atomic
                      clusterName:
                        nullable: true
                        type: string
                      clusterSelector:
                        description: 'A label selector is a label query over a set
                          of resources. The result of matchLabels and

                          matchExpressions are ANDed. An empty label selector matches
                          all objects. A null

                          label selector matches no objects.'
                        nullable: true
                        properties:
                          matchExpressions:
                            description: matchExpressions is a list of label selector
                              requirements. The requirements are ANDed.
                            items:
                              description: 'A label selector requirement is a selector
                                that contains values, a key, and an operator that

                                relates the key and values.'
                              properties:
                                key:
                                  description: key is the label key that the selector
                                    applies to.
                                  type: string
                                operator:
                                  description: 'operator represents a key''s relationship
                                    to a set of values.

                                    Valid operators are In, NotIn, Exists and DoesNotExist.'
                                  type: string
                                values:
                                  description: 'values is an array of string values.
                                    If the operator is In or NotIn,

                                    the values array must be non-empty. If the operator
                                    is Exists or DoesNotExist,

                                    the values array must be empty. This array is
                                    replaced during a strategic

                                    merge patch.'
                                  items:
                                    type: string
                                  type: array
                                  x-kubernetes-list-type: atomic
                              required:
                                - key
                                - operator
                              type: object
                            type: array
                            x-kubernetes-list-type: atomic
                          matchLabels:
                            additionalProperties:
                              type: string
                            description: 'matchLabels is a map of {key,value} pairs.
                              A single {key,value} in the matchLabels

                              map is equivalent to an element of matchExpressions,
                              whose key field is "key", the

                              operator is "In", and the values array contains only
                              "value". The requirements are ANDed.'
                            type: object
                        type: object
                        x-kubernetes-map-type: atomic
                      name:
                        nullable: true
                        type: string
                    type: object
                  type: array
                targets:
                  description: 'Targets refer to the clusters which will be deployed
                    to.

                    Targets are evaluated in order and the first one to match is used.'
                  items:
                    description: 'BundleTarget declares clusters to deploy to. Fleet
                      will merge the

                      BundleDeploymentOptions from customizations into this struct.'
                    properties:
                      clusterGroup:
                        description: ClusterGroup to match a specific cluster group
                          by name.
                        nullable: true
                        type: string
                      clusterGroupSelector:
                        description: ClusterGroupSelector is a selector to match cluster
                          groups.
                        nullable: true
                        properties:
                          matchExpressions:
                            description: matchExpressions is a list of label selector
                              requirements. The requirements are ANDed.
                            items:
                              description: 'A label selector requirement is a selector
                                that contains values, a key, and an operator that

                                relates the key and values.'
                              properties:
                                key:
                                  description: key is the label key that the selector
                                    applies to.
                                  type: string
                                operator:
                                  description: 'operator represents a key''s relationship
                                    to a set of values.

                                    Valid operators are In, NotIn, Exists and DoesNotExist.'
                                  type: string
                                values:
                                  description: 'values is an array of string values.
                                    If the operator is In or NotIn,

                                    the values array must be non-empty. If the operator
                                    is Exists or DoesNotExist,

                                    the values array must be empty. This array is
                                    replaced during a strategic

                                    merge patch.'
                                  items:
                                    type: string
                                  type: array
                                  x-kubernetes-list-type: atomic
                              required:
                                - key
                                - operator
                              type: object
                            type: array
                            x-kubernetes-list-type: atomic
                          matchLabels:
                            additionalProperties:
                              type: string
                            description: 'matchLabels is a map of {key,value} pairs.
                              A single {key,value} in the matchLabels

                              map is equivalent to an element of matchExpressions,
                              whose key field is "key", the

                              operator is "In", and the values array contains only
                              "value". The requirements are ANDed.'
                            type: object
                        type: object
                        x-kubernetes-map-type: atomic
                      clusterName:
                        description: 'ClusterName to match a specific cluster by name
                          that will be

                          selected'
                        nullable: true
                        type: string
                      clusterSelector:
                        description: 'ClusterSelector is a selector to match clusters.
                          The structure is

                          the standard metav1.LabelSelector format. If clusterGroupSelector
                          or

                          clusterGroup is specified, clusterSelector will be used
                          only to

                          further refine the selection after clusterGroupSelector
                          and

                          clusterGroup is evaluated.'
                        nullable: true
                        properties:
                          matchExpressions:
                            description: matchExpressions is a list of label selector
                              requirements. The requirements are ANDed.
                            items:
                              description: 'A label selector requirement is a selector
                                that contains values, a key, and an operator that

                                relates the key and values.'
                              properties:
                                key:
                                  description: key is the label key that the selector
                                    applies to.
                                  type: string
                                operator:
                                  description: 'operator represents a key''s relationship
                                    to a set of values.

                                    Valid operators are In, NotIn, Exists and DoesNotExist.'
                                  type: string
                                values:
                                  description: 'values is an array of string values.
                                    If the operator is In or NotIn,

                                    the values array must be non-empty. If the operator
                                    is Exists or DoesNotExist,

                                    the values array must be empty. This array is
                                    replaced during a strategic

                                    merge patch.'
                                  items:
                                    type: string
                                  type: array
                                  x-kubernetes-list-type: atomic
                              required:
                                - key
                                - operator
                              type: object
                            type: array
                            x-kubernetes-list-type: atomic
                          matchLabels:
                            additionalProperties:
                              type: string
                            description: 'matchLabels is a map of {key,value} pairs.
                              A single {key,value} in the matchLabels

                              map is equivalent to an element of matchExpressions,
                              whose key field is "key", the

                              operator is "In", and the values array contains only
                              "value". The requirements are ANDed.'
                            type: object
                        type: object
                        x-kubernetes-map-type: atomic
                      correctDrift:
                        description: CorrectDrift specifies how drift correction should
                          work.
                        properties:
                          enabled:
                            description: Enabled correct drift if true.
                            type: boolean
                          force:
                            description: Force helm rollback with --force option will
                              be used if true. This will try to recreate all resources
                              in the release.
                            type: boolean
                          keepFailHistory:
                            description: KeepFailHistory keeps track of failed rollbacks
                              in the helm history.
                            type: boolean
                        type: object
                      defaultNamespace:
                        description: 'DefaultNamespace is the namespace to use for
                          resources that do not

                          specify a namespace. This field is not used to enforce or
                          lock down

                          the deployment to a specific namespace.'
                        nullable: true
                        type: string
                      deleteCRDResources:
                        description: DeleteCRDResources deletes CRDs. Warning! this
                          will also delete all your Custom Resources.
                        type: boolean
                      deleteNamespace:
                        description: DeleteNamespace can be used to delete the deployed
                          namespace when removing the bundle
                        type: boolean
                      diff:
                        description: Diff can be used to ignore the modified state
                          of objects which are amended at runtime.
                        nullable: true
                        properties:
                          comparePatches:
                            description: ComparePatches match a resource and remove
                              fields, or the resource itself from the check for modifications.
                            items:
                              description: ComparePatch matches a resource and removes
                                fields from the check for modifications.
                              properties:
                                apiVersion:
                                  description: APIVersion is the apiVersion of the
                                    resource to match.
                                  nullable: true
                                  type: string
                                jsonPointers:
                                  description: JSONPointers ignore diffs at a certain
                                    JSON path.
                                  items:
                                    type: string
                                  nullable: true
                                  type: array
                                kind:
                                  description: Kind is the kind of the resource to
                                    match.
                                  nullable: true
                                  type: string
                                name:
                                  description: Name is the name of the resource to
                                    match.
                                  nullable: true
                                  type: string
                                namespace:
                                  description: Namespace is the namespace of the resource
                                    to match.
                                  nullable: true
                                  type: string
                                operations:
                                  description: Operations remove a JSON path from
                                    the resource.
                                  items:
                                    description: 'Operation of a ComparePatch, usually:

                                      * "remove" to remove a specific path in a resource

                                      * "ignore" to remove the entire resource from
                                      checks for modifications.'
                                    properties:
                                      op:
                                        description: Op is usually "remove" or "ignore"
                                        nullable: true
                                        type: string
                                      path:
                                        description: Path is the JSON path to remove.
                                          Not needed if Op is "ignore".
                                        nullable: true
                                        type: string
                                      value:
                                        description: Value is usually empty.
                                        nullable: true
                                        type: string
                                    type: object
                                  nullable: true
                                  type: array
                              type: object
                            nullable: true
                            type: array
                        type: object
                      doNotDeploy:
                        description: DoNotDeploy if set to true, will not deploy to
                          this target.
                        type: boolean
                      downstreamResources:
                        description: 'DownstreamResources points to resources to be
                          copied into downstream clusters, from the bundle''s

                          namespace.'
                        items:
                          description: 'DownstreamResource contains identifiers for
                            a resource to be copied from the parent bundle''s namespace
                            to each

                            downstream cluster.'
                          properties:
                            kind:
                              type: string
                            name:
                              type: string
                          type: object
                        type: array
                      forceSyncGeneration:
                        description: ForceSyncGeneration is used to force a redeployment
                        format: int64
                        type: integer
                      helm:
                        description: Helm options for the deployment, like the chart
                          name, repo and values.
                        properties:
                          atomic:
                            description: Atomic sets the --atomic flag when Helm is
                              performing an upgrade
                            type: boolean
                          chart:
                            description: 'Chart can refer to any go-getter URL or
                              OCI registry based helm

                              chart URL. The chart will be downloaded.'
                            nullable: true
                            type: string
                          disableDNS:
                            description: DisableDNS can be used to customize Helm's
                              EnableDNS option, which Fleet sets to `true` by default.
                            type: boolean
                          disableDependencyUpdate:
                            description: DisableDependencyUpdate allows skipping chart
                              dependencies update
                            type: boolean
                          disablePreProcess:
                            description: DisablePreProcess disables template processing
                              in values
                            type: boolean
                          force:
                            description: Force allows to override immutable resources.
                              This could be dangerous.
                            type: boolean
                          maxHistory:
                            description: MaxHistory limits the maximum number of revisions
                              saved per release by Helm.
                            type: integer
                          releaseName:
                            description: 'ReleaseName sets a custom release name to
                              deploy the chart as. If

                              not specified a release name will be generated by combining
                              the

                              invoking GitRepo.name + GitRepo.path.'
                            maxLength: 53
                            nullable: true
                            pattern: ^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$
                            type: string
                          repo:
                            description: Repo is the name of the HTTPS helm repo to
                              download the chart from.
                            nullable: true
                            type: string
                          skipSchemaValidation:
                            description: SkipSchemaValidation allows skipping schema
                              validation against the chart values
                            type: boolean
                          takeOwnership:
                            description: TakeOwnership makes helm skip the check for
                              its own annotations
                            type: boolean
                          templateValues:
                            additionalProperties:
                              type: string
                            description: 'Template Values passed to Helm. It is possible
                              to specify the keys and values

                              as go template strings. Unlike .values, content of each
                              key will be templated

                              first, before serializing to yaml. This allows to template
                              complex values,

                              like ranges and maps.

                              templateValues keys have precedence over values keys
                              in case of conflict.'
                            nullable: true
                            type: object
                          timeoutSeconds:
                            description: TimeoutSeconds is the time to wait for Helm
                              operations.
                            type: integer
                          values:
                            description: 'Values passed to Helm. It is possible to
                              specify the keys and values

                              as go template strings.'
                            nullable: true
                            type: object
                            x-kubernetes-preserve-unknown-fields: true
                          valuesFiles:
                            description: ValuesFiles is a list of files to load values
                              from.
                            items:
                              type: string
                            nullable: true
                            type: array
                          valuesFrom:
                            description: ValuesFrom loads the values from configmaps
                              and secrets.
                            items:
                              description: 'Define helm values that can come from
                                configmap, secret or external. Credit: https://github.com/fluxcd/helm-operator/blob/0cfea875b5d44bea995abe7324819432070dfbdc/pkg/apis/helm.fluxcd.io/v1/types_helmrelease.go#L439'
                              properties:
                                configMapKeyRef:
                                  description: The reference to a config map with
                                    release values.
                                  nullable: true
                                  properties:
                                    key:
                                      nullable: true
                                      type: string
                                    name:
                                      description: Name of a resource in the same
                                        namespace as the referent.
                                      nullable: true
                                      type: string
                                    namespace:
                                      nullable: true
                                      type: string
                                  type: object
                                secretKeyRef:
                                  description: The reference to a secret with release
                                    values.
                                  nullable: true
                                  properties:
                                    key:
                                      nullable: true
                                      type: string
                                    name:
                                      description: Name of a resource in the same
                                        namespace as the referent.
                                      nullable: true
                                      type: string
                                    namespace:
                                      nullable: true
                                      type: string
                                  type: object
                              type: object
                            nullable: true
                            type: array
                          version:
                            description: Version of the chart to download
                            nullable: true
                            type: string
                          waitForJobs:
                            description: 'WaitForJobs if set and timeoutSeconds provided,
                              will wait until all

                              Jobs have been completed before marking the GitRepo
                              as ready. It

                              will wait for as long as timeoutSeconds'
                            type: boolean
                        type: object
                      ignore:
                        description: IgnoreOptions can be used to ignore fields when
                          monitoring the bundle.
                        nullable: true
                        properties:
                          conditions:
                            description: Conditions is a list of conditions to be
                              ignored when monitoring the Bundle.
                            items:
                              additionalProperties:
                                type: string
                              type: object
                            nullable: true
                            type: array
                        type: object
                      keepResources:
                        description: KeepResources can be used to keep the deployed
                          resources when removing the bundle
                        type: boolean
                      kustomize:
                        description: 'Kustomize options for the deployment, like the
                          dir containing the

                          kustomization.yaml file.'
                        nullable: true
                        properties:
                          dir:
                            description: 'Dir points to a custom folder for kustomize
                              resources. This folder must contain

                              a kustomization.yaml file.'
                            nullable: true
                            type: string
                        type: object
                      name:
                        description: 'Name of target. This value is largely for display
                          and logging. If

                          not specified a default name of the format "target000" will
                          be used'
                        type: string
                      namespace:
                        description: 'TargetNamespace if present will assign all resource
                          to this

                          namespace and if any cluster scoped resource exists the
                          deployment

                          will fail.'
                        nullable: true
                        type: string
                      namespaceAnnotations:
                        additionalProperties:
                          type: string
                        description: NamespaceAnnotations are annotations that will
                          be appended to the namespace created by Fleet.
                        nullable: true
                        type: object
                      namespaceLabels:
                        additionalProperties:
                          type: string
                        description: NamespaceLabels are labels that will be appended
                          to the namespace created by Fleet.
                        nullable: true
                        type: object
                      overwrites:
                        description: 'Overwrites indicates which resources, if any,
                          come from this bundle and overwrite another existing bundle.

                          This flag is set internally by Fleet, and should not be
                          altered by users.'
                        items:
                          properties:
                            kind:
                              type: string
                            name:
                              type: string
                            namespace:
                              type: string
                          type: object
                        type: array
                      serviceAccount:
                        description: ServiceAccount which will be used to perform
                          this deployment.
                        nullable: true
                        type: string
                      yaml:
                        description: 'YAML options, if using raw YAML these are names
                          that map to

                          overlays/{name} files that will be used to replace or patch
                          a resource.'
                        nullable: true
                        properties:
                          overlays:
                            description: 'Overlays is a list of names that maps to
                              folders in "overlays/".

                              If you wish to customize the file ./subdir/resource.yaml
                              then a file

                              ./overlays/myoverlay/subdir/resource.yaml will replace
                              the base

                              file.

                              A file named ./overlays/myoverlay/subdir/resource_patch.yaml
                              will patch the base file.'
                            items:
                              type: string
                            nullable: true
                            type: array
                        type: object
                    type: object
                  type: array
                valuesHash:
                  description: 'ValuesHash is the hash of the values used to render
                    the Helm chart.

                    It changes when any values from fleet.yaml, values from ValuesFiles
                    or values from target

                    customization changes.'
                  type: string
                yaml:
                  description: 'YAML options, if using raw YAML these are names that
                    map to

                    overlays/{name} files that will be used to replace or patch a
                    resource.'
                  nullable: true
                  properties:
                    overlays:
                      description: 'Overlays is a list of names that maps to folders
                        in "overlays/".

                        If you wish to customize the file ./subdir/resource.yaml then
                        a file

                        ./overlays/myoverlay/subdir/resource.yaml will replace the
                        base

                        file.

                        A file named ./overlays/myoverlay/subdir/resource_patch.yaml
                        will patch the base file.'
                      items:
                        type: string
                      nullable: true
                      type: array
                  type: object
              type: object
            status:
              properties:
                conditions:
                  description: 'Conditions is a list of Wrangler conditions that describe
                    the state

                    of the resource.'
                  items:
                    properties:
                      lastTransitionTime:
                        description: Last time the condition transitioned from one
                          status to another.
                        type: string
                      lastUpdateTime:
                        description: The last time this condition was updated.
                        type: string
                      message:
                        description: Human-readable message indicating details about
                          last transition
                        type: string
                      reason:
                        description: The reason for the condition's last transition.
                        type: string
                      status:
                        description: Status of the condition, one of True, False,
                          Unknown.
                        type: string
                      type:
                        description: Type of cluster condition.
                        type: string
                    required:
                      - status
                      - type
                    type: object
                  type: array
                desiredReadyClusters:
                  description: "DesiredReadyClusters\tis the number of clusters that\
                    \ should be ready for bundles of this resource."
                  type: integer
                display:
                  description: Display contains a human readable summary of the status.
                  properties:
                    error:
                      description: Error is true if a message is present.
                      type: boolean
                    message:
                      description: Message contains the relevant message from the
                        deployment conditions.
                      type: string
                    readyBundleDeployments:
                      description: 'ReadyBundleDeployments is a string in the form
                        "%d/%d", that describes the

                        number of ready bundledeployments over the total number of
                        bundledeployments.'
                      type: string
                    state:
                      description: 'State is the state of the resource, e.g. "GitUpdating"
                        or the maximal

                        BundleState according to StateRank.'
                      type: string
                  type: object
                lastPollingTriggered:
                  description: LastPollingTime is the last time the polling check
                    was triggered
                  format: date-time
                  type: string
                perClusterResourceCounts:
                  additionalProperties:
                    description: ResourceCounts contains the number of resources in
                      each state.
                    properties:
                      desiredReady:
                        description: DesiredReady is the number of resources that
                          should be ready.
                        type: integer
                      missing:
                        description: Missing is the number of missing resources.
                        type: integer
                      modified:
                        description: Modified is the number of resources that have
                          been modified.
                        type: integer
                      notReady:
                        description: 'NotReady is the number of not ready resources.
                          Resources are not

                          ready if they do not match any other state.'
                        type: integer
                      orphaned:
                        description: Orphaned is the number of orphaned resources.
                        type: integer
                      ready:
                        description: Ready is the number of ready resources.
                        type: integer
                      unknown:
                        description: Unknown is the number of resources in an unknown
                          state.
                        type: integer
                      waitApplied:
                        description: WaitApplied is the number of resources that are
                          waiting to be applied.
                        type: integer
                    type: object
                  description: PerClusterResourceCounts contains the number of resources
                    in each state over all bundles, per cluster.
                  type: object
                readyClusters:
                  description: 'ReadyClusters is the lowest number of clusters that
                    are ready over

                    all the bundles of this resource.'
                  type: integer
                resourceCounts:
                  description: ResourceCounts contains the number of resources in
                    each state over all bundles.
                  properties:
                    desiredReady:
                      description: DesiredReady is the number of resources that should
                        be ready.
                      type: integer
                    missing:
                      description: Missing is the number of missing resources.
                      type: integer
                    modified:
                      description: Modified is the number of resources that have been
                        modified.
                      type: integer
                    notReady:
                      description: 'NotReady is the number of not ready resources.
                        Resources are not

                        ready if they do not match any other state.'
                      type: integer
                    orphaned:
                      description: Orphaned is the number of orphaned resources.
                      type: integer
                    ready:
                      description: Ready is the number of ready resources.
                      type: integer
                    unknown:
                      description: Unknown is the number of resources in an unknown
                        state.
                      type: integer
                    waitApplied:
                      description: WaitApplied is the number of resources that are
                        waiting to be applied.
                      type: integer
                  type: object
                resources:
                  description: Resources contains metadata about the resources of
                    each bundle.
                  items:
                    description: Resource contains metadata about the resources of
                      a bundle.
                    properties:
                      apiVersion:
                        description: APIVersion is the API version of the resource.
                        nullable: true
                        type: string
                      error:
                        description: Error is true if any Error in the PerClusterState
                          is true.
                        type: boolean
                      id:
                        description: ID is the name of the resource, e.g. "namespace1/my-config"
                          or "backingimagemanagers.storage.io".
                        nullable: true
                        type: string
                      incompleteState:
                        description: 'IncompleteState is true if a bundle summary
                          has 10 or more non-ready

                          resources or a non-ready resource has more 10 or more non-ready
                          or

                          modified states.'
                        type: boolean
                      kind:
                        description: Kind is the k8s kind of the resource.
                        nullable: true
                        type: string
                      message:
                        description: Message is the first message from the PerClusterStates.
                        nullable: true
                        type: string
                      name:
                        description: Name of the resource.
                        nullable: true
                        type: string
                      namespace:
                        description: Namespace of the resource.
                        nullable: true
                        type: string
                      perClusterState:
                        description: PerClusterState contains lists of cluster IDs
                          for every State for this resource
                        nullable: true
                        properties:
                          missing:
                            description: Missing is a list of cluster IDs for which
                              this a resource is in Missing state
                            items:
                              type: string
                            type: array
                          modified:
                            description: Modified is a list of cluster IDs for which
                              this a resource is in Modified state
                            items:
                              type: string
                            type: array
                          notReady:
                            description: NotReady is a list of cluster IDs for which
                              this a resource is in NotReady state
                            items:
                              type: string
                            type: array
                          orphaned:
                            description: Orphaned is a list of cluster IDs for which
                              this a resource is in Orphaned state
                            items:
                              type: string
                            type: array
                          pending:
                            description: Pending is a list of cluster IDs for which
                              this a resource is in Pending state
                            items:
                              type: string
                            type: array
                          ready:
                            description: Ready is a list of cluster IDs for which
                              this a resource is in Ready state
                            items:
                              type: string
                            type: array
                          unknown:
                            description: Unknown is a list of cluster IDs for which
                              this a resource is in Unknown state
                            items:
                              type: string
                            type: array
                          waitApplied:
                            description: WaitApplied is a list of cluster IDs for
                              which this a resource is in WaitApplied state
                            items:
                              type: string
                            type: array
                        type: object
                      state:
                        description: State is the state of the resource, e.g. "Unknown",
                          "WaitApplied", "ErrApplied" or "Ready".
                        type: string
                      transitioning:
                        description: Transitioning is true if any Transitioning in
                          the PerClusterState is true.
                        type: boolean
                      type:
                        description: Type is the type of the resource, e.g. "apiextensions.k8s.io.customresourcedefinition"
                          or "configmap".
                        type: string
                    required:
                      - perClusterState
                    type: object
                  type: array
                summary:
                  description: Summary contains the number of bundle deployments in
                    each state and a list of non-ready resources.
                  properties:
                    desiredReady:
                      description: 'DesiredReady is the number of bundle deployments
                        that should be

                        ready.'
                      type: integer
                    errApplied:
                      description: 'ErrApplied is the number of bundle deployments
                        that have been synced

                        from the Fleet controller and the downstream cluster, but
                        with some

                        errors when deploying the bundle.'
                      type: integer
                    modified:
                      description: 'Modified is the number of bundle deployments that
                        have been deployed

                        and for which all resources are ready, but where some changes
                        from the

                        Git repository have not yet been synced.'
                      type: integer
                    nonReadyResources:
                      description: 'NonReadyClusters is a list of states, which is
                        filled for a bundle

                        that is not ready.'
                      items:
                        description: 'NonReadyResource contains information about
                          a bundle that is not ready for a

                          given state like "ErrApplied". It contains a list of non-ready
                          or modified

                          resources and their states.'
                        properties:
                          bundleState:
                            description: State is the state of the resource, like
                              e.g. "NotReady" or "ErrApplied".
                            nullable: true
                            type: string
                          message:
                            description: Message contains information why the bundle
                              is not ready.
                            nullable: true
                            type: string
                          modifiedStatus:
                            description: ModifiedStatus lists the state for each modified
                              resource.
                            items:
                              description: 'ModifiedStatus is used to report the status
                                of a resource that is modified.

                                It indicates if the modification was a create, a delete
                                or a patch.'
                              properties:
                                apiVersion:
                                  nullable: true
                                  type: string
                                delete:
                                  type: boolean
                                exist:
                                  description: Exist is true if the resource exists
                                    but is not owned by us. This can happen if a resource
                                    was adopted by another bundle whereas the first
                                    bundle still exists and due to that reports that
                                    it does not own it.
                                  type: boolean
                                kind:
                                  nullable: true
                                  type: string
                                missing:
                                  type: boolean
                                name:
                                  nullable: true
                                  type: string
                                namespace:
                                  nullable: true
                                  type: string
                                patch:
                                  nullable: true
                                  type: string
                              type: object
                            nullable: true
                            type: array
                          name:
                            description: Name is the name of the resource.
                            nullable: true
                            type: string
                          nonReadyStatus:
                            description: NonReadyStatus lists the state for each non-ready
                              resource.
                            items:
                              description: NonReadyStatus is used to report the status
                                of a resource that is not ready. It includes a summary.
                              properties:
                                apiVersion:
                                  nullable: true
                                  type: string
                                kind:
                                  nullable: true
                                  type: string
                                name:
                                  nullable: true
                                  type: string
                                namespace:
                                  nullable: true
                                  type: string
                                summary:
                                  properties:
                                    error:
                                      type: boolean
                                    message:
                                      items:
                                        type: string
                                      type: array
                                    state:
                                      type: string
                                    transitioning:
                                      type: boolean
                                  type: object
                                uid:
                                  description: 'UID is a type that holds unique ID
                                    values, including UUIDs.  Because we

                                    don''t ONLY use UUIDs, this is an alias to string.  Being
                                    a type captures

                                    intent and helps make sure that UIDs and names
                                    do not get conflated.'
                                  nullable: true
                                  type: string
                              type: object
                            nullable: true
                            type: array
                        type: object
                      nullable: true
                      type: array
                    notReady:
                      description: 'NotReady is the number of bundle deployments that
                        have been deployed

                        where some resources are not ready.'
                      type: integer
                    outOfSync:
                      description: 'OutOfSync is the number of bundle deployments
                        that have been synced

                        from Fleet controller, but not yet by the downstream agent.'
                      type: integer
                    pending:
                      description: 'Pending is the number of bundle deployments that
                        are being processed

                        by Fleet controller.'
                      type: integer
                    ready:
                      description: 'Ready is the number of bundle deployments that
                        have been deployed

                        where all resources are ready.'
                      type: integer
                    waitApplied:
                      description: 'WaitApplied is the number of bundle deployments
                        that have been

                        synced from Fleet controller and downstream cluster, but are
                        waiting

                        to be deployed.'
                      type: integer
                  type: object
                version:
                  description: 'Version installed for the helm chart.

                    When using * or empty version in the spec we get the latest version
                    from

                    the helm repository when possible'
                  type: string
              type: object
          type: object
      served: true
      storage: true
      subresources:
        status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.19.0
  name: imagescans.fleet.cattle.io
spec:
  group: fleet.cattle.io
  names:
    kind: ImageScan
    listKind: ImageScanList
    plural: imagescans
    singular: imagescan
  scope: Namespaced
  versions:
    - additionalPrinterColumns:
        - jsonPath: .spec.image
          name: Repository
          type: string
        - jsonPath: .status.latestTag
          name: Latest
          type: string
      name: v1alpha1
      schema:
        openAPIV3Schema:
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
                of an object.

                Servers should convert recognized schemas to the latest internal value,
                and

                may reject unrecognized values.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource
                this object represents.

                Servers may infer this from the endpoint the client submits requests
                to.

                Cannot be updated.

                In CamelCase.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
            spec:
              description: API is taken from https://github.com/fluxcd/image-reflector-controller
              properties:
                gitrepoName:
                  description: GitRepo reference name
                  nullable: true
                  type: string
                image:
                  description: Image is the name of the image repository
                  nullable: true
                  type: string
                interval:
                  description: 'Interval is the length of time to wait between

                    scans of the image repository.'
                  nullable: true
                  type: string
                policy:
                  description: 'Policy gives the particulars of the policy to be followed
                    in

                    selecting the most recent image'
                  properties:
                    alphabetical:
                      description: Alphabetical set of rules to use for alphabetical
                        ordering of the tags.
                      nullable: true
                      properties:
                        order:
                          description: 'Order specifies the sorting order of the tags.
                            Given the letters of the

                            alphabet as tags, ascending order would select Z, and
                            descending order

                            would select A.'
                          nullable: true
                          type: string
                      type: object
                    semver:
                      description: 'SemVer gives a semantic version range to check
                        against the tags

                        available.'
                      nullable: true
                      properties:
                        range:
                          description: 'Range gives a semver range for the image tag;
                            the highest

                            version within the range that''s a tag yields the latest
                            image.'
                          nullable: true
                          type: string
                      type: object
                  type: object
                secretRef:
                  description: 'SecretRef can be given the name of a secret containing

                    credentials to use for the image registry. The secret should be

                    created with `kubectl create secret docker-registry`, or the

                    equivalent.'
                  nullable: true
                  properties:
                    name:
                      default: ''
                      description: 'Name of the referent.

                        This field is effectively required, but due to backwards compatibility
                        is

                        allowed to be empty. Instances of this type with an empty
                        value here are

                        almost certainly wrong.

                        More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names'
                      type: string
                  type: object
                  x-kubernetes-map-type: atomic
                suspend:
                  description: 'This flag tells the controller to suspend subsequent
                    image scans.

                    It does not apply to already started scans. Defaults to false.'
                  type: boolean
                tagName:
                  description: TagName is the tag ref that needs to be put in manifest
                    to replace fields
                  nullable: true
                  type: string
              required:
                - image
                - interval
              type: object
            status:
              properties:
                canonicalImageName:
                  description: 'CanonicalName is the name of the image repository
                    with all the

                    implied bits made explicit; e.g., `docker.io/library/alpine`

                    rather than `alpine`.'
                  type: string
                conditions:
                  items:
                    properties:
                      lastTransitionTime:
                        description: Last time the condition transitioned from one
                          status to another.
                        type: string
                      lastUpdateTime:
                        description: The last time this condition was updated.
                        type: string
                      message:
                        description: Human-readable message indicating details about
                          last transition
                        type: string
                      reason:
                        description: The reason for the condition's last transition.
                        type: string
                      status:
                        description: Status of the condition, one of True, False,
                          Unknown.
                        type: string
                      type:
                        description: Type of cluster condition.
                        type: string
                    required:
                      - status
                      - type
                    type: object
                  type: array
                lastScanTime:
                  description: LastScanTime is the last time image was scanned
                  format: date-time
                  type: string
                latestDigest:
                  description: LatestDigest is the digest of latest tag
                  type: string
                latestImage:
                  description: 'LatestImage gives the first in the list of images
                    scanned by

                    the image repository, when filtered and ordered according to

                    the policy.'
                  type: string
                latestTag:
                  description: Latest tag is the latest tag filtered by the policy
                  type: string
                observedGeneration:
                  format: int64
                  type: integer
              type: object
          type: object
      served: true
      storage: true
      subresources:
        status: {}
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.19.0
  name: schedules.fleet.cattle.io
spec:
  group: fleet.cattle.io
  names:
    kind: Schedule
    listKind: ScheduleList
    plural: schedules
    singular: schedule
  scope: Namespaced
  versions:
    - additionalPrinterColumns:
        - jsonPath: .spec.schedule
          name: Schedule
          type: string
        - jsonPath: .spec.duration
          name: Duration
          type: string
        - jsonPath: .status.active
          name: Active
          type: string
        - jsonPath: .status.nextStartTime
          name: NextStart
          type: string
      name: v1alpha1
      schema:
        openAPIV3Schema:
          description: Schedule represents a schedule in which deployments are allowed
            or not, depending on its definition.
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
                of an object.

                Servers should convert recognized schemas to the latest internal value,
                and

                may reject unrecognized values.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource
                this object represents.

                Servers may infer this from the endpoint the client submits requests
                to.

                Cannot be updated.

                In CamelCase.

                More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
            spec:
              properties:
                duration:
                  type: string
                location:
                  type: string
                schedule:
                  type: string
                targets:
                  description: Targets is a list of resources affected by this schedule
                  properties:
                    clusters:
                      items:
                        description: ScheduleTarget represents a resource (or group
                          of resources) affected by a Schedule
                        properties:
                          clusterGroup:
                            description: ClusterGroup is the name of a cluster group
                              in the same namespace as the clusters.
                            nullable: true
                            type: string
                          clusterGroupSelector:
                            description: ClusterGroupSelector is a label selector
                              to select cluster groups.
                            nullable: true
                            properties:
                              matchExpressions:
                                description: matchExpressions is a list of label selector
                                  requirements. The requirements are ANDed.
                                items:
                                  description: 'A label selector requirement is a
                                    selector that contains values, a key, and an operator
                                    that

                                    relates the key and values.'
                                  properties:
                                    key:
                                      description: key is the label key that the selector
                                        applies to.
                                      type: string
                                    operator:
                                      description: 'operator represents a key''s relationship
                                        to a set of values.

                                        Valid operators are In, NotIn, Exists and
                                        DoesNotExist.'
                                      type: string
                                    values:
                                      description: 'values is an array of string values.
                                        If the operator is In or NotIn,

                                        the values array must be non-empty. If the
                                        operator is Exists or DoesNotExist,

                                        the values array must be empty. This array
                                        is replaced during a strategic

                                        merge patch.'
                                      items:
                                        type: string
                                      type: array
                                      x-kubernetes-list-type: atomic
                                  required:
                                    - key
                                    - operator
                                  type: object
                                type: array
                                x-kubernetes-list-type: atomic
                              matchLabels:
                                additionalProperties:
                                  type: string
                                description: 'matchLabels is a map of {key,value}
                                  pairs. A single {key,value} in the matchLabels

                                  map is equivalent to an element of matchExpressions,
                                  whose key field is "key", the

                                  operator is "In", and the values array contains
                                  only "value". The requirements are ANDed.'
                                type: object
                            type: object
                            x-kubernetes-map-type: atomic
                          clusterName:
                            description: ClusterName is the name of a cluster.
                            nullable: true
                            type: string
                          clusterSelector:
                            description: ClusterSelector is a label selector to select
                              clusters.
                            nullable: true
                            properties:
                              matchExpressions:
                                description: matchExpressions is a list of label selector
                                  requirements. The requirements are ANDed.
                                items:
                                  description: 'A label selector requirement is a
                                    selector that contains values, a key, and an operator
                                    that

                                    relates the key and values.'
                                  properties:
                                    key:
                                      description: key is the label key that the selector
                                        applies to.
                                      type: string
                                    operator:
                                      description: 'operator represents a key''s relationship
                                        to a set of values.

                                        Valid operators are In, NotIn, Exists and
                                        DoesNotExist.'
                                      type: string
                                    values:
                                      description: 'values is an array of string values.
                                        If the operator is In or NotIn,

                                        the values array must be non-empty. If the
                                        operator is Exists or DoesNotExist,

                                        the values array must be empty. This array
                                        is replaced during a strategic

                                        merge patch.'
                                      items:
                                        type: string
                                      type: array
                                      x-kubernetes-list-type: atomic
                                  required:
                                    - key
                                    - operator
                                  type: object
                                type: array
                                x-kubernetes-list-type: atomic
                              matchLabels:
                                additionalProperties:
                                  type: string
                                description: 'matchLabels is a map of {key,value}
                                  pairs. A single {key,value} in the matchLabels

                                  map is equivalent to an element of matchExpressions,
                                  whose key field is "key", the

                                  operator is "In", and the values array contains
                                  only "value". The requirements are ANDed.'
                                type: object
                            type: object
                            x-kubernetes-map-type: atomic
                          name:
                            description: Name is the name of this target.
                            nullable: true
                            type: string
                        type: object
                      type: array
                  type: object
              type: object
            status:
              properties:
                active:
                  description: Active is set to true when the Schedule is actively
                    running
                  type: boolean
                conditions:
                  description: 'Conditions is a list of Wrangler conditions that describe
                    the state

                    of the resource.'
                  items:
                    properties:
                      lastTransitionTime:
                        description: Last time the condition transitioned from one
                          status to another.
                        type: string
                      lastUpdateTime:
                        description: The last time this condition was updated.
                        type: string
                      message:
                        description: Human-readable message indicating details about
                          last transition
                        type: string
                      reason:
                        description: The reason for the condition's last transition.
                        type: string
                      status:
                        description: Status of the condition, one of True, False,
                          Unknown.
                        type: string
                      type:
                        description: Type of cluster condition.
                        type: string
                    required:
                      - status
                      - type
                    type: object
                  type: array
                matchingClusters:
                  description: MatchingClusters is the list of clusters targeted by
                    the Schedule
                  items:
                    type: string
                  type: array
                nextStartTime:
                  description: NextStartTime stores the next time this Schedule will
                    start
                  format: date-time
                  type: string
              type: object
          type: object
      served: true
      storage: true
      subresources:
        status: {}



================================================
FILE: cmd/codegen/boilerplate.go.txt
================================================
/*
Copyright (c) 2020 - YEAR SUSE LLC

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/




================================================
FILE: cmd/codegen/main.go
================================================
package main

import (
	"os"

	controllergen "github.com/rancher/wrangler/v3/pkg/controller-gen"
	"github.com/rancher/wrangler/v3/pkg/controller-gen/args"

	// Ensure gvk gets loaded in wrangler/pkg/gvk cache
	_ "github.com/rancher/wrangler/v3/pkg/generated/controllers/apiextensions.k8s.io/v1"

	// To keep the dependency in go.mod
	_ "sigs.k8s.io/controller-tools/pkg/crd"
)

func main() {
	os.Unsetenv("GOPATH")
	controllergen.Run(args.Options{
		OutputPackage: "github.com/rancher/fleet/pkg/generated",
		Boilerplate:   "cmd/codegen/boilerplate.go.txt",
		Groups: map[string]args.Group{
			"fleet.cattle.io": {
				Types: []interface{}{
					"./pkg/apis/fleet.cattle.io/v1alpha1",
				},
			},
		},
	})
}



================================================
FILE: cmd/codegen/cleanup/main.go
================================================
package main

import (
	"os"

	"github.com/sirupsen/logrus"

	"github.com/rancher/wrangler/v3/pkg/cleanup"
)

func main() {
	if err := cleanup.Cleanup("./pkg/apis"); err != nil {
		logrus.Fatal(err)
	}
	if err := os.RemoveAll("./pkg/generated"); err != nil {
		logrus.Fatal(err)
	}
}



================================================
FILE: cmd/codegen/hack/generate_and_sort_crds.sh
================================================
#!/usr/bin/env bash

set -euo pipefail

CRDS_YAML=${1?-The path to the charts.yaml file to be patched must be specified}
shift

tmpdir=$(mktemp -d)
trap 'rm -rf ${tmpdir}' EXIT

log() {
  echo "$*" >&2
}

run_yq() {
  # Use the Python-based version of yq. The Go version does not have complete support for jq's syntax
  if ! yq --help | grep -q github.com/kislyuk/yq ; then
    image="fleet-codegen-hack:yq"
    log "yq (from https://github.com/kislyuk/yq) is missing, building a helper docker image ($image)..."

    docker build -t $image - >&2 << EOF
FROM registry.suse.com/bci/python:3.11

RUN zypper in -y jq
RUN python3 -m pip install yq
EOF
    docker run --rm -i -v ${PWD}:${PWD} -w ${PWD} ${image} yq $@
  else
    yq $@
  fi
}

# Ensure the right version of controller-gen is installed
CONTROLLERGEN=controller-gen
CONTROLLERGEN_VERSION=$(go list -m -f '{{.Version}}' sigs.k8s.io/controller-tools)
if ! $CONTROLLERGEN --version | grep -q "${CONTROLLERGEN_VERSION}" ; then
  log "Downloading controller-gen ${CONTROLLERGEN_VERSION} to a temporary directory. Run 'go install sigs.k8s.io/controller-tools/cmd/controller-gen@${CONTROLLERGEN_VERSION}' to get a persistent installation"
  GOBIN="${tmpdir}/bin" go install sigs.k8s.io/controller-tools/cmd/controller-gen@${CONTROLLERGEN_VERSION}
  CONTROLLERGEN="${tmpdir}/bin/controller-gen"
fi

# Run controller-gen
${CONTROLLERGEN} object:headerFile=cmd/codegen/boilerplate.go.txt,year="$(date +%Y)" paths="./pkg/apis/..."
${CONTROLLERGEN} crd webhook paths="./pkg/apis/..." output:stdout > $CRDS_YAML
# Sort
run_yq --slurp --sort-keys --explicit-start --yaml-output -i 'sort_by(.metadata.name)[]' $CRDS_YAML



================================================
FILE: cmd/docs/generate-cli-docs.go
================================================
package main

import (
	"fmt"
	"io"
	"os"
	"path/filepath"
	"strings"

	"github.com/pkg/errors"

	fleetagent "github.com/rancher/fleet/internal/cmd/agent"
	fleetcli "github.com/rancher/fleet/internal/cmd/cli"
	fleetcontroller "github.com/rancher/fleet/internal/cmd/controller"

	"github.com/spf13/cobra"
	"github.com/spf13/cobra/doc"
)

func main() {
	if len(os.Args) < 2 {
		usage()
		os.Exit(1)
	}

	docDir := filepath.Join("./", os.Args[1])

	// fleet cli for gitjob
	cmd := fleetcli.App()
	cmd.DisableAutoGenTag = true

	err := generateCmdDoc(cmd, filepath.Join(docDir, "fleet-cli"))
	if err != nil {
		panic(err)
	}

	// fleet agent controller
	cmd = fleetagent.App()
	cmd.DisableAutoGenTag = true

	err = generateCmdDoc(cmd, filepath.Join(docDir, "fleet-agent"))
	if err != nil {
		panic(err)
	}

	// fleet controller
	cmd = fleetcontroller.App()
	cmd.DisableAutoGenTag = true

	err = generateCmdDoc(cmd, filepath.Join(docDir, "fleet-controller"))
	if err != nil {
		panic(err)
	}

}

// generateCmdDoc will generate the documentation for the given command, in the given directory
func generateCmdDoc(cmd *cobra.Command, dir string) error {
	if cmd.Hidden {
		return nil
	}

	// create the directory if it doesn't exist
	err := os.MkdirAll(dir, 0700)
	if err != nil {
		return errors.Wrapf(err, "error creating directory [%s]", dir)
	}

	// create the documentation for the given command
	err = createMarkdownFile(cmd, dir)
	if err != nil {
		return errors.Wrapf(err, "error creating markdown file for command [%s]", cmd.Name())
	}

	// create the documentation for its subcommands
	for _, subcmd := range cmd.Commands() {
		// if the subcommand does not have other subcommands, just generate the doc and continue
		if !subcmd.HasSubCommands() {
			err = createMarkdownFile(subcmd, dir)
			if err != nil {
				return errors.Wrapf(err, "error creating markdown file for command [%s]", subcmd.Name())
			}
			continue
		}

		// if the subcommand has other subcommands, then recurse in its own directory
		subdir := filepath.Join(dir, subcmd.Name())
		err = generateCmdDoc(subcmd, subdir)
		if err != nil {
			return errors.Wrapf(err, "error generating doc for command [%s]", subcmd.Name())
		}
	}

	return nil
}

// createMarkdownFile will create the markdown file for the given command in the given directory
func createMarkdownFile(cmd *cobra.Command, dir string) error {
	// skip 'help' command
	if cmd.Name() == "help" {
		return nil
	}

	basename := strings.ReplaceAll(cmd.CommandPath(), " ", "_") + ".md"
	filename := filepath.Join(dir, basename)

	f, err := os.Create(filename)
	if err != nil {
		return errors.Wrap(err, "error creating file")
	}
	defer f.Close()

	err = writeFileHeader(f, cmd.CommandPath())
	if err != nil {
		return errors.Wrapf(err, "error writing file header for command [%s]", cmd.Name())
	}

	err = doc.GenMarkdownCustom(cmd, f, linkHandler(cmd, dir))
	return errors.Wrap(err, "error generating markdown custom")
}

// linkHandler will return a function that will handle the markdown link generation
func linkHandler(cmd *cobra.Command, _ string) func(link string) string {
	return func(link string) string {
		link = strings.TrimSuffix(link, ".md")
		cmdPathLink := strings.ReplaceAll(link, "_", " ")

		// check if the link was referring to the parent command
		// we also need to check if the current command has subcommands, because if it does not then the link needs to point to the same directory
		if cmd.HasParent() && cmd.Parent().CommandPath() == cmdPathLink && cmd.HasSubCommands() {
			return "../" + link
		}

		for _, subcmd := range cmd.Commands() {
			// if the subcommand has other subcommands then it will have its own directory
			if subcmd.CommandPath() == cmdPathLink && subcmd.HasSubCommands() {
				return fmt.Sprintf("./%s/%s", subcmd.Name(), link)
			}
		}

		return "./" + link
	}
}

func writeFileHeader(w io.Writer, sidebarLabel string) error {
	_, err := fmt.Fprintf(w, `---
title: ""
sidebar_label: "%s"
---
`, sidebarLabel)

	return errors.Wrap(err, "error writing file header")
}

func usage() {
	fmt.Fprintln(os.Stdout, "Usage: ", os.Args[0], " <directory>")
}



================================================
FILE: cmd/fleetagent/main.go
================================================
// Package main is the entrypoint for the fleet-agent binary.
package main

import (
	_ "net/http/pprof"

	"github.com/rancher/fleet/internal/cmd/agent"

	"github.com/rancher/wrangler/v3/pkg/signals"
	"github.com/sirupsen/logrus"
)

func main() {
	ctx := signals.SetupSignalContext()
	cmd := agent.App()
	if err := cmd.ExecuteContext(ctx); err != nil {
		logrus.Fatal(err)
	}
}



================================================
FILE: cmd/fleetcli/main.go
================================================
// Package main is the entry point for the fleet apply binary.
package main

import (
	"os"
	"strings"

	// Ensure GVKs are registered
	_ "github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io"
	_ "github.com/rancher/wrangler/v3/pkg/generated/controllers/apiextensions.k8s.io"
	_ "github.com/rancher/wrangler/v3/pkg/generated/controllers/apps"
	_ "github.com/rancher/wrangler/v3/pkg/generated/controllers/core"
	_ "github.com/rancher/wrangler/v3/pkg/generated/controllers/rbac"

	// Add non-default auth providers
	_ "k8s.io/client-go/plugin/pkg/client/auth"

	"github.com/rancher/wrangler/v3/pkg/signals"
	"github.com/sirupsen/logrus"

	cmds "github.com/rancher/fleet/internal/cmd/cli"
	fleetapply "github.com/rancher/fleet/internal/cmd/cli/apply"
)

func main() {
	ctx := signals.SetupSignalContext()
	cmd := cmds.App()
	if err := cmd.ExecuteContext(ctx); err != nil {
		if strings.ToLower(os.Getenv(fleetapply.JSONOutputEnvVar)) == "true" {
			log := logrus.New()
			log.SetFormatter(&logrus.JSONFormatter{})
			// use a fleet specific field name so we are sure logs from other libraries
			// are not considered.
			log.WithFields(logrus.Fields{
				"fleetErrorMessage": err.Error(),
			}).Fatal("Fleet cli failed")
		} else {
			logrus.Fatal(err)
		}
	}
}



================================================
FILE: cmd/fleetcontroller/main.go
================================================
// Package main provides the entrypoint for the fleet-controller binary.
package main

import (
	_ "net/http/pprof"

	_ "github.com/rancher/wrangler/v3/pkg/generated/controllers/apiextensions.k8s.io"
	_ "github.com/rancher/wrangler/v3/pkg/generated/controllers/networking.k8s.io"
	"github.com/rancher/wrangler/v3/pkg/signals"
	"github.com/sirupsen/logrus"

	"github.com/rancher/fleet/internal/cmd/controller"
)

func main() {
	ctx := signals.SetupSignalContext()
	cmd := controller.App()
	if err := cmd.ExecuteContext(ctx); err != nil {
		logrus.Fatal(err)
	}
}



================================================
FILE: dev/README.md
================================================
# Logging

Instructions on writing useful, well-formatted log messages can be found [here](./LOGGING.md).

# Testing: Dev Scripts

These scripts are used for running tests locally in [k3d](https://github.com/rancher/k3d).
Don't use these on production systems.

## Requirements

- docker
- git
- go
- helm
- jq
- k3d
- kubectl
- ...

### For Mac users

On Mac OS, Docker runs in a virtual machine. This means the network setup is more complicated.

> If you are unsure about which method you would like use for tunneling to localhost, we recommend [ngrok](https://ngrok.com).

More info [here](https://github.com/rancher/rancher/wiki/Setting-Up-Rancher-Development-Environment#ngrok).

## Running Tests on K3D

These commands should set up k3d and the fleet standalone images for single
cluster tests and run those.

```bash
source dev/setup-single-cluster
ginkgo e2e/single-cluster
```

Optional flags to `ginkgo` for reporting on long-running tests:
`--poll-progress-after=10s --poll-progress-interval=10s`.

For multi-cluster tests we need to configure two clusters. You also need to make
the upstream clusters API accessible to the downstream cluster. The default URL
in `dev/setup-fleet-managed-downstream` should work with most systems.

```bash
source dev/setup-multi-cluster
ginkgo e2e/multi-cluster
```

### Controlling test output verbosity

Integration tests suppress verbose output by default to keep test results clean and readable. This includes:
- Helm dependency update messages
- Log output from test servers and containers
- Debug information from test components

To enable verbose output for debugging purposes, set the `VERBOSE` environment variable:

```bash
# Run with verbose output enabled
VERBOSE=1 ./dev/run-integration-tests.sh
```

### Testing changes incrementally

To test changes incrementally, rebuild just one binary, update the image in k3d
and restart the controller. Make sure you have sourced the right configuration
for the current setup.

```bash
dev/update-agent-k3d
dev/update-controller-k3d
```

## Running tests in Rancher

Any Fleet commit can be tested against the latest Rancher release, by:

1. Running `./.github/workflows/release-against-test-charts.yml`, which will
create Fleet charts based on the chosen git ref, and store them in the specified
repo and branch, with version `999.9.9+up9.9.9` to prevent collisions with any
other existing Fleet chart versions.

2. Setting the charts branch environment variable:
```
export CHARTS_BRANCH=<branch_created_in_previous_step>
```
3. Ensuring you have access to a running cluster in which to install Rancher.
For instance, a set of k3d clusters (1 upstream + 1 downstream) can be created
on your local machine through:

```
dev/setup-k3d
dev/setup-k3ds-downstream
```

4. Commenting out the pairs of lines (name + value) setting the following
values for installing Rancher through Helm in script
`dev/setup-rancher-with-dev-fleet`, to match the location and version of the
Fleet build generated in step 1:
* `--set extraEnv[0].name=CATTLE_FLEET_VERSION \` to use a custom Fleet version,
otherwise Rancher will use the version pinned in its
[settings](https://github.com/rancher/rancher/blob/main/build.yaml#L5).
* `--set extraEnv[0].value=999.9.9+up9.9.9 \` to match the version from step 1.
* `--set extraEnv[1].name=CATTLE_CHART_DEFAULT_URL \` to set a custom chart URL
* `--set extraEnv[1].value=https://github.com/fleetrepoci/charts \`: this is the
custom charts repository used in step 1 by default.
Change it to a different URL if you have specified one in that step.
* `--set extraEnv[2].name=CATTLE_CHART_DEFAULT_BRANCH \` to set a custom chart
repo branch.
* `--set extraEnv[2].value=$branch \`: this must match the branch name created
in step 1.
* `--set rancherImageTag=$tag \`: only needed for custom Rancher builds.
If you have not built Rancher yourself, this is typically not needed and you
can leave it commented out.

5. Running script `dev/setup-rancher-with-dev-fleet`, which will install Rancher
using Helm, including test Fleet charts generated earlier. Once it is done,
single- or multi-cluster test suites can be run against an actual Rancher setup.

This also enables use of the Rancher UI, after opening the URL output by Helm
when installing the Rancher chart, then logging in using the `bootstrapPassword`
set in that script.

## Configuration

### Running scripts manually

You can set these environment variables for configuration manually, but it is
advised to put them in `.envrc` and source them before running any of the
scripts (except for `dev/setup-{single,multi}-cluster`), if the scripts are run
manually. You can rely on the environment variables being set correctly if you
source the `dev/setup-{single,multi}-cluster` scripts.

```bash
source .envrc
```

### Running setup scripts

If you use `dev/setup-single-cluster` or `dev/setup-multi-cluster` you can
simply put your custom configuration in the root of the repository as
`env.single-cluster` and `env.multi-cluster`. Those files will then be used
instead of the defaults in `dev/env.single-cluster-defaults` or
`dev/env.multi-cluster-defaults`, respectively.

If you occasionally want to specify a different file, you can set the
`FLEET_TEST_CONFIG` environment variable to point to your custom configuration
(like `.envrc`) file. This will make those scripts use the file specified in the
`FLEET_TEST_CONFIG` environment variable instead of the defaults in
`dev/env.single-cluster-defaults` and `dev/env.multi-cluster-defaults` and also
instead of the custom configuration in `env.single-cluster` and
`env.multi-cluster`.

### A list of all environment variables

```bash
# use fleet-default for fleet in Rancher, fleet-local for standalone
export FLEET_E2E_NS=fleet-local
export FLEET_E2E_NS_DOWNSTREAM=fleet-default

# running single-cluster tests in Rancher Desktop
export FLEET_E2E_CLUSTER=rancher-desktop
export FLEET_E2E_CLUSTER_DOWNSTREAM=rancher-desktop

# running single-cluster tests in k3d (setup-k3d)
export FLEET_E2E_CLUSTER=k3d-upstream
export FLEET_E2E_CLUSTER_DOWNSTREAM=k3d-upstream

# running multi-cluster tests in k3d (setup-k3d;setup-k3ds-downstream)
export FLEET_E2E_CLUSTER=k3d-upstream
export FLEET_E2E_CLUSTER_DOWNSTREAM=k3d-downstream1

# for running tests on darwin/arm64
export GOARCH=arm64

# needed for gitrepo tests, which are currently disabled but part of the
# single-cluster tests
export FORCE_GIT_SERVER_BUILD="yes" # set to an empty value to skip rebuilds
export GIT_REPO_USER="git"
export GIT_REPO_URL="git@github.com:yourprivate/repo.git"
export GIT_REPO_HOST="github.com"
export GIT_SSH_KEY="$HOME/.ssh/id_ecdsa_test"
export GIT_SSH_PUBKEY="$HOME/.ssh/id_ecdsa_test.pub"
export GIT_HTTP_USER="fleet-ci"
export GIT_HTTP_PASSWORD="foo"

# needed for OCI tests, which are part of the single-cluster tests
export CI_OCI_USERNAME="fleet-ci"
export CI_OCI_PASSWORD="foo"
export CI_OCI_READER_USERNAME="fleet-ci-reader"
export CI_OCI_READER_PASSWORD="foo-reader"
export CI_OCI_NO_DELETER_USERNAME=fleet-ci-no-deleter
export CI_OCI_NO_DELETER_PASSWORD=foo-no-deleter
export CI_OCI_CERTS_DIR="../../FleetCI-RootCA"
```

### `public_hostname`

Several scripts support the `public_hostname` configuration variable.
The variable is set to a DNS record, which points to the public interface IP of the host.
The k3d cluster is then set up with port forwardings, so that host ports are
redirected to services inside the cluster.

The default `public_hostname` is `172.18.0.1.sslip.io`, which points
to the default Docker network gateway. That gateway address might vary for
custom networks, see for example: `docker network inspect fleet -f '{{(index .IPAM.Config0).Gateway}}'`.
Careful, several internet routers provide "DNS rebind protection" and won't return an IP for `172.18.0.1.sslip.io`, unless the .sslip.io` domain is in an allow list.
Any magic wildcard DNS resolver will do, or you can create an A record in your own DNS zone.

The k3d cluster is set up with multiple port forwardings by the scripts: `-p '80:80@server:0' -p '443:443@server:0'`.
More arguments can be provided via the `k3d_args` variable.


## Different Script Folders

Our CIs and github actions use a different set of scripts. CI
does not reuse dev scripts, however dev scripts may use CI scripts. We want to
keep CI scripts short, targeted and readable. Dev scripts may change in an
incompatible way at any day.

## Run Integration Tests

```bash
./dev/run-integration-tests.sh
```

This will download and prepare setup-envtest, then it will execute all the
integration tests.

## Local Infra Setup

The local infra setup creates pods for:

- git server, using nginx with git-http-backend, port 8080/tcp
- OCI repo server, using Zot, port 8081/tcp
- Helm registry, using chartmuseum, port 8082/tcp

To build and run the infra setup command do:

```
dev/import-images-tests-k3d
# ./dev/create-zot-certs 'FleetCI-RootCA'
dev/create-secrets
go run ./e2e/testenv/infra/main.go setup
```

The resulting deployments use a loadbalancer service, which means the host must be able to reach the loadbalancer IP.
Therefore the infra setup doesn't work with the `public_hostname` config variable.
This is not a problem, unless k3d is running in a VM and not directly on the host.

It is possible to override the loadbalancer IP by setting the `external_ip` environment variable.

## Setting up a local Docker Registry Mirror

A pull-through cache for Docker Hub speeds up image pulls during repeated k3d cluster creation:

```bash
docker run -d \
  --restart=always \
  --name registry-cache \
  --network fleet \
  -p 5000:5000 \
  -e REGISTRY_PROXY_REMOTEURL=https://registry-1.docker.io \
  registry:2
```

To use it, set `export docker_mirror=http://registry-cache:5000` in your `.envrc` or before running k3d scripts.

> **__NOTE:__** The registry persists across reboots due to `--restart=always`. To remove it run:

```bash
docker rm -f registry-cache
```

## Running Github Actions locally

Sometimes, it may be beneficial to be able to run the Github Action tests using
the same configuration which is used remotely. To do this, you can use
[nektos/act](https://github.com/nektos/act).

### Requirements

- [Docker](https://www.docker.com/)
- [act](https://github.com/nektos/act#installation)

### Installation

To install `act`, please follow the [instructions on the official Github
repository](https://github.com/nektos/act#installation).

### Configuration

#### Container Image

Unlike Github Actions, `act` will use container images for running the tests
locally. The container image that will be used depends on the type of the
Github Action Runner for the specific action. You can see which images are being
used for which runner [here](https://github.com/nektos/act#runners). Most tests require `ubuntu-latest`.

The containers are available in difference sizes:

- Micro
- Medium
- Large

The default container image for Ubuntu, which is required for most actions, does
not contain all necessary tools. They are [intentionally
incomplete](https://github.com/nektos/act#default-runners-are-intentionally-incomplete).
Instead, the large container image needs to be used (read on for an
alternative).

To change the container image used for Ubuntu, you will need to create a
configuration file `$HOME/.actrc`.

```shell
-P ubuntu-latest=catthehacker/ubuntu:full-latest
```

While the medium-sized container image has a size of only 1.1GB, the large
container image is significantly larger, approximately 40GB in size. If this is
a concern, you can create your own container image using the following
`Dockerfile`. This will result in a container image of about 700MB, while still
including all the necessary tools to run both single and multi-cluster tests of
fleet.

```Dockerfile
FROM ubuntu:22.04

ARG BUILDARCH=amd64
ARG NVM_VERSION=v0.39.7
ARG NODE_VERSION=20

RUN apt update && apt upgrade -y
RUN apt install -y wget curl git jq zstd

WORKDIR /tmp

RUN curl -fsSL -o get_docker.sh \
            https://get.docker.com && \
        chmod 700 get_docker.sh && \
        ./get_docker.sh && rm get_docker.sh

RUN wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_${BUILDARCH} -O /usr/bin/yq \
        && chmod +x /usr/bin/yq

RUN curl -fsSL -o get_helm.sh \
            https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-4 && \
        chmod 700 get_helm.sh && \
        ./get_helm.sh

RUN curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/${BUILDARCH}/kubectl" && \
        chmod +x ./kubectl && \
        mv ./kubectl /usr/local/bin/kubectl

RUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/${NVM_VERSION}/install.sh | bash && \
        export NVM_DIR="$HOME/.nvm" && \
        [ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"  && \
        [ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion" && \
        nvm install ${NODE_VERSION} && \
        ln -s $(which node) /usr/bin/node && \
        ln -s $(which npm) /usr/bin/npm
```

Please also note, that the default behavior of `act` is to always pull images.
You can either use the `--pull=false` flag when running `act` or you will need
to upload this container image to a container registry. In any case, you need to
specify the container image to be used in the `$HOME/.actrc` file.

#### Github Token

Some tests may require a Github token to be set. While this seems a bit odd,
this can already be necessary in cases where `act` uses the Github API to fetch
repositories for a simple checkout action.

You can create a personal access token by following the [instructions on the
Github
website](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token).

### Running the tests

The tests are run by simply calling `act`, **but this is not recommended**, as
it starts all available tests in parallel. Usually, you would use `act -l` to
get a list of all jobs with workflows and possible events, then choose one using
`act <event-name> -j <job-name>`. But even this can start more tests in parallel
than you may like (like this could the case for `e2e-fleet-test`, for instance).
Therefore, we recommend you use the `-W` flag instead to run a specific workflow
file.

For example:

```shell
act -W .github/workflows/e2e-multicluster-ci.yml
```

Sometimes a test has a conditions, which will prevent some tests (but not
necessarily all) from running. For instance, this is the case for the acceptance
tests, which are part of the `e2e-multicluster-ci.yml` workflow. They will only
run if the `schedule` event is passed. To also run those tests, you need to pass
the `schedule` event to `act` as such:

```shell
act schedule -W .github/workflows/e2e-multicluster-ci.yml
```

### Troubleshooting

#### fatal: not a git repository

```shell
get repo root in /: output: "fatal: not a git repository (or any of the parent directories): .git\n", error: exit status 128
```

If you see an issue like this and are running the tests from a linked git
worktree, it is likely that act has copied the contents of your linked worktree
into the container but cannot access the main worktree. Running the tests from
the main worktree instead is going to resolve this issue.

You can test this by running a simple git command like `git status` inside the
working directory of the `act` container. It should be kept running in case this
issue occurred.

```shell
docker exec -it <container> bash
git status
```

#### DNS Resolution

The DNS resolution depends on the configuration of the host system. This means
that, if the host is configured to point to itself (e.g., `127.0.1.1`), DNS
resolution might not work out-of-the-box. This is due to the use of containers
to emulate the environment of a GitHub Action. The container gets the Docker
socket passed through, but the containers created from within this container may
not be able to reach the this local DNS address of the host.

If you have such a DNS server configured on your host, which points to a local
DNS server, you can configure a separate DNS server for Docker. After that, you
will need to restart the Docker daemon. You can configure the DNS for Docker in
`/etc/docker/daemon.json`, e.g.:

```json
{
  "dns": ["1.1.1.1"]
}
```

#### Changes aren't applied

If you find yourself in the situation that changes you made to the environment
do not seem to be applied, it might be that `act` did not remove its own
container image and simply re-used it. You can remove it yourself, either by

- removing it manually using `docker rm <container>`, or
- by running `act` with the `--rm` option. This may be inconvenient, because it
  will also remove the container in case of any errors, so will not be able to
  inspect the container for issues.

This container image will have `act` in his name.

## Monitoring

This sections describes how to add a monitoring stack to your development
environment. It consists of Prometheus, kube-state-metrics, kube-operator,
Node-Exporter, Alertmanager and Grafana. It does contain Grafana dashboards and
Prometheus alerts, but it does not contain any Grafana dashboards or Prometheus
alerts specific to Fleet.

### Installation

If you have a running system, run the following commands on the upstream
cluster:

```bash
helm repo add prometheus-community \
  https://prometheus-community.github.io/helm-charts
helm repo update
helm upgrade --install --create-namespace -n cattle-system-monitoring \
  monitoring prometheus-community/kube-prometheus-stack
```

That alone suffices to get a working monitoring setup for the upstream cluster.
But to connect it to the fleet-controller exported metrics, you need to add a
service monitor for each controller.

The service monitor is currently not part of the Helm chart.  However, the
necessary Kubernetes service resource is, unless you have disabled monitoring in
the Helm chart when installing fleet.

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: monitoring-fleet-controller
  namespace: cattle-fleet-system
  labels:
    release: monitoring # required to be recognized by the operator
spec:
  endpoints:
  - honorLabels: true
    path: /metrics
    scheme: http
    scrapeTimeout: 30s
    port: metrics
  jobLabel: fleet-controller
  namespaceSelector:
    matchNames:
    - cattle-fleet-system
  selector:
    matchLabels:
      app: fleet-controller
      shard-default: true
      # or:
      # shard: <shard-id>
```

This configures Prometheus to scrape the metrics from the (unsharded)
fleet-controller. Should you have sharding enabled, you will additionally need
to add one ServiceMonitor for each shard.

By accessing the Prometheus UI, you can now see the metrics from the
fleet-controller. They are all prefixed with `fleet_`, which you will need to
enter into the expression field of the Graph page to get auto-completion.
Alternatively you can use the metrics explorer, which is a button next to the
evaluation input field.

> **__NOTE:__** The Prometheus UI can be used to check the Prometheus
configuration (e.g., the scrape targets), scraped metrics, check alerts or
draft them with PromQL or to create PromQL queries to be used in Grafana
dashboards. It is not accessible by default and not meant to be shown to
casual users.

To access the Prometheus UI, you can forward the port of the Prometheus service
to your local machine and access it through that port.

```bash
kubectl port-forward -n cattle-system-monitoring \
  svc/monitoring-kube-prometheus-prometheus 9090:9090
```

Alternatively, you can forward the port of the fleet-controller to your local
machine. Then you can access the raw metrics at `http://localhost:8080/metrics`.

```bash
kubectl port-forward -n cattle-fleet-system \
  svc/monitoring-fleet-controller 8080:8080
```

### Metrics

There are metrics which will only be available when certain resources are
created. To create those resources, you can use the following file. Since a
`GitRepo` resource results in having `Bundle` and `BundleDeployment` resources,
the `Cluster` resource is already available and the `ClusterGroup` resource is
created by us, it is sufficient to create a `GitRepo` and `ClusterGroup` resource
to see all the fleet specific metrics.

```yaml
kind: GitRepo
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: simple
  namespace: fleet-local
spec:
  repo: https://github.com/rancher/fleet-examples
  paths:
  - simple
---
kind: ClusterGroup
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: local-group
  namespace: fleet-local
spec:
  selector:
    matchLabels:
      name: local
```



================================================
FILE: dev/benchmarks.sh
================================================
#!/bin/bash
set -e

date=$(date +"%F_%T")
out="b-$date.json"
FLEET_BENCH_TIMEOUT=${FLEET_BENCH_TIMEOUT-"5m"}
FLEET_BENCH_NAMESPACE=${FLEET_BENCH_NAMESPACE-"fleet-local"}

go run ./benchmarks/cmd run -d benchmarks/db -t "$FLEET_BENCH_TIMEOUT" -n "$FLEET_BENCH_NAMESPACE"



================================================
FILE: dev/build-fleet
================================================
#!/bin/bash
# Description: build fleet binary and image with debug flags

set -euxo pipefail

if [ ! -d ./cmd/fleetcontroller ]; then
  echo "please change the current directory to the fleet repo checkout"
  exit 1
fi

export GOARCH="${GOARCH:-amd64}"
export CGO_ENABLED=0

# re-generate code
if ! git diff --quiet HEAD origin/main --  pkg/apis/fleet.cattle.io/v1alpha1; then
  go generate
fi

export GOOS=linux

# The name of the container image created here is a potential source of conflict
# when fleet is set up simultaneously in multiple sets (i.e. 1 upstream + 0 to n
# downstream). build-fleet always creates a container image with the same name
# (`rancher/fleet`, rancher/fleet-agent`) and tag (`dev`). Conflicts can simply
# be avoided by making sure that the images `rancher/fleet:dev` and
# `rancher/fleet-agent:dev` are rebuilt from the right context (e.g. git clone,
# checkout or worktree) before being imported into clusters created for testing
# that context.  This can be easily achieved by ensuring that, for instance, at
# most one instance of `./dev/setup-single-cluster` or
# `./dev/setup-multi-cluster` runs at any given point in time.

# fleet
go build -gcflags='all=-N -l' -o "bin/fleet-linux-$GOARCH" ./cmd/fleetcli
go build -gcflags='all=-N -l' -o bin/fleetcontroller-linux-"$GOARCH" ./cmd/fleetcontroller
docker build -f package/Dockerfile -t rancher/fleet:dev --build-arg="ARCH=$GOARCH"  .

# fleet agent
go build -gcflags='all=-N -l' -o "bin/fleetagent-linux-$GOARCH" ./cmd/fleetagent
docker build -f package/Dockerfile.agent -t rancher/fleet-agent:dev --build-arg="ARCH=$GOARCH" .



================================================
FILE: dev/create-secrets
================================================
#!/bin/bash

./.github/scripts/create-secrets.sh "$@"



================================================
FILE: dev/create-zot-certs
================================================
#!/bin/bash

./.github/scripts/create-zot-certs.sh "$@"



================================================
FILE: dev/env.multi-cluster-defaults
================================================
export FLEET_E2E_NS=fleet-local
export FLEET_E2E_NS_DOWNSTREAM=fleet-default

export FLEET_E2E_CLUSTER=k3d-upstream
export FLEET_E2E_CLUSTER_DOWNSTREAM=k3d-downstream1

export GIT_HTTP_USER=fleet-ci
export GIT_HTTP_PASSWORD=foo

export CI_OCI_USERNAME=fleet-ci
export CI_OCI_PASSWORD=foo
export CI_OCI_READER_USERNAME=fleet-ci-reader
export CI_OCI_READER_PASSWORD=foo-reader
export CI_OCI_NO_DELETER_USERNAME=fleet-ci-no-deleter
export CI_OCI_NO_DELETER_PASSWORD=foo-no-deleter
export CI_OCI_CERTS_DIR=FleetCI-RootCA/



================================================
FILE: dev/env.single-cluster-defaults
================================================
export FLEET_E2E_NS=fleet-local

export FLEET_E2E_CLUSTER=k3d-upstream
export FLEET_E2E_CLUSTER_DOWNSTREAM=k3d-upstream

export GIT_HTTP_USER=fleet-ci
export GIT_HTTP_PASSWORD=foo

export CI_OCI_USERNAME=fleet-ci
export CI_OCI_PASSWORD=foo
export CI_OCI_READER_USERNAME=fleet-ci-reader
export CI_OCI_READER_PASSWORD=foo-reader
export CI_OCI_NO_DELETER_USERNAME=fleet-ci-no-deleter
export CI_OCI_NO_DELETER_PASSWORD=foo-no-deleter
export CI_OCI_CERTS_DIR=FleetCI-RootCA/



================================================
FILE: dev/import-images-k3d
================================================
#!/bin/bash

set -euxo pipefail

# The upstream cluster to import all the images to.
upstream_ctx="${FLEET_E2E_CLUSTER-k3d-upstream}"

# The single downstream cluster to import the agent image to.
downstream_ctx="${FLEET_E2E_CLUSTER_DOWNSTREAM-k3d-downstream1}"

k3d image import rancher/fleet:dev rancher/fleet-agent:dev -m direct -c "${upstream_ctx#k3d-}"

downstream_keyword="${downstream_ctx#k3d-}"
downstream_keyword="${downstream_keyword%[0-9]*}"
if [ "$upstream_ctx" != "$downstream_ctx" ]; then
  for cluster in $(k3d cluster list -o json | \
      jq -r ".[].name | select(. | contains(\"${downstream_keyword}\"))"); do
    k3d image import rancher/fleet-agent:dev -m direct -c "${cluster}"
  done
else
  echo "not importing agent to any downstream clusters. Set FLEET_E2E_CLUSTER_DOWNSTREAM"
fi



================================================
FILE: dev/import-images-tests-k3d
================================================
#!/bin/bash
# Build and import git server image

set -euxo pipefail

upstream_ctx="${FLEET_E2E_CLUSTER-k3d-upstream}"

gitSrvImage=$( docker image ls -q nginx-git:test )

GIT_HTTP_PASSWORD=${GIT_HTTP_PASSWORD-foo}
if [ -n "${FORCE_GIT_SERVER_BUILD-}" -o -z "$gitSrvImage" ]; then
    cd e2e/assets/gitrepo
    docker build . -f Dockerfile.gitserver --build-arg passwd=$(openssl passwd $GIT_HTTP_PASSWORD) -t nginx-git:test
else
    echo "Git test server image already present. Skipping build."
fi

k3d image import nginx-git:test -m direct -c "${upstream_ctx#k3d-}"



================================================
FILE: dev/k3d-clean
================================================
#!/usr/bin/env bash
#
# Selectively clean up k3d clusters.
#
# Clusters to be cleaned up are determined by the following environment variables:
# - FLEET_E2E_CLUSTER: The main cluster to be cleaned up.
# - FLEET_E2E_CLUSTER_DOWNSTREAM: The downstream cluster to be cleaned up.
# - FLEET_E2E_DS_CLUSTER_COUNT: The number of downstream clusters to be cleaned up.

set -eox pipefail

function k3d-cluster-delete {
    if [ -z "$1" ]; then
        return
    fi

    k3d cluster delete "${1#k3d-}"
}

if [[ -n "$FLEET_E2E_CLUSTER" ]]; then
    k3d-cluster-delete "$FLEET_E2E_CLUSTER"
fi

if [[ -n "$FLEET_E2E_CLUSTER_DOWNSTREAM" ]]; then
    # If downstream cluster is the same as the main cluster, do nothing.
    if [[ "$FLEET_E2E_CLUSTER_DOWNSTREAM" == "$FLEET_E2E_CLUSTER" ]]; then
        exit 0
    fi

    # Delete one downstream cluster.
    if [[ -z "$FLEET_E2E_DS_CLUSTER_COUNT" ]]; then
        k3d-cluster-delete "$FLEET_E2E_CLUSTER_DOWNSTREAM"
        exit 0
    fi

    # Delete all downstream clusters matching the pattern in $FLEET_E2E_CLUSTER_DOWNSTREAM.
    for ((i=1; i<FLEET_E2E_DS_CLUSTER_COUNT+1; i++)); do
        if [[ $i -eq 1 ]]; then
            k3d-cluster-delete "${FLEET_E2E_CLUSTER_DOWNSTREAM}"
        else
            k3d-cluster-delete "${FLEET_E2E_CLUSTER_DOWNSTREAM%[0-9]*}${i}"
        fi
    done
fi



================================================
FILE: dev/LOGGING.md
================================================
![Usage of Events and Log Messages](./LOGGING.png)


# Structured Logging

Since 0.10, Fleet uses [controller-runtime](https://github.com/kubernetes-sigs/controller-runtime) as its Kubernetes
controller framework. It implements _structured logging_, which basically means logging _constant_ log messages with
variable key-value pairs.

For instance:
```
// Unstructured
myloglib.Log(fmt.Sprintf("Something happened: %v", err))


// Structured
logger.V(1).Info(
    "Reconciling bundle, checking targets, calculating changes, building objects",
    "generation",
    bundle.Generation,
    "observedGeneration",
    bundle.Status.ObservedGeneration,
)
```

More info [here](https://github.com/kubernetes-sigs/controller-runtime/blob/main/TMP-LOGGING.md) on how
`controller-runtime` approaches logging.

Fleet has preexisting logging code using [logrus](https://github.com/sirupsen/logrus), which can be ported to
[zap](https://pkg.go.dev/go.uber.org/zap), for which `controller-runtime` provides helpers.

## Where to log

Logs should be produced from:

* high-level functions, which do something that the user should know about, such as reconciliation, an entity not being
found where it was expected, etc. Implementation details, which live in helper functions (eg. resetting a SHA256 sum for
a manifest), are less relevant for logging.

* functions taking a context as a parameter: these functions can time out, and knowing where the timeout happened before
  the error bubbles up can help troubleshoot it.

## What (not) to log

At the risk of stating the obvious, no sensitive information should be logged (eg. secrets).

## Levels

Fleet logging makes use of the following levels:
* `Info` for events about which users may care, `Info` logs have a verbosity. The default verbosity is 0.
* `Error` for errors

### Verbosity

Log messages can have a numerical verbosity. The default verbosity is 0, we also use 1 for debug logs and 4 for logs
that aid in tracing problems.
If log messages should not be included in the output of a binary started with the `--debug` argument, their level should
be higher than 5.

## Formatting Fields

Fields used for structured logging must follow camelCase. The name of the logger should use kebab-case.

For example:

```
logger := log.FromContext(ctx).WithName("delete-by-release").WithValues("releaseName", releaseName, "keepResources", keepResources)
```

# Consistency is key

Check how code living in the same file, or package, or higher up the call stack, does logging. Ensure that constant
log messages and field names are phrased and formatted in consistent ways.

# Useful links

* [Dave Cheney on
logging](https://web.archive.org/web/20240521184322/https://dave.cheney.net/2015/11/05/lets-talk-about-logging)



================================================
FILE: dev/logs
================================================
#!/bin/bash

app=${1-fleet-controller}

kubectl logs -n cattle-fleet-system -l "app=$app" -f



================================================
FILE: dev/remove-fleet
================================================
#!/bin/sh
# Warning: do not use this script on a production system!

upstream_ctx="${FLEET_E2E_CLUSTER-k3d-upstream}"
downstream_ctx="${FLEET_E2E_CLUSTER_DOWNSTREAM-k3d-downstream1}"

ctx=$(kubectl config current-context)

kubectl config use-context "$upstream_ctx"

# Remove controllers
helm uninstall -n cattle-fleet-system fleet

# Remove finalizers, this will prevent deletion of namespace fleet-local from hanging
for res in gitrepos.fleet.cattle.io bundledeployments.fleet.cattle.io bundles.fleet.cattle.io; do
  kubectl get "$res" -A --no-headers | while read ns name  _; do kubectl patch "$res" -n "$ns" "$name" -p '{"metadata":{"finalizers":[]}}' --type=merge; done
done

# Remove finalizers on contents too
kubectl get content -o=jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | xargs -I{} kubectl patch content {} -p '{"metadata":{"finalizers":[]}}' --type=merge

helm uninstall -n cattle-fleet-system fleet-crd

# make sure crds are removed
kubectl delete crd gitrepos.fleet.cattle.io bundledeployments.fleet.cattle.io bundles.fleet.cattle.io

kubectl delete ns cattle-fleet-system --now
kubectl delete ns cattle-fleet-clusters-system --now

kubectl delete ns fleet-local --now
kubectl delete ns -l "fleet.cattle.io/managed=true"

kubectl delete clusterrolebinding fleet-agent-get-content cattle-fleet-system-fleet-agent-role-binding
kubectl delete clusterrole cattle-fleet-system-fleet-agent-role fleet-bundle-deployment fleet-content fleet-agent-system-fleet-agent-role

kubectl config use-context "$downstream_ctx"
helm uninstall -n cattle-fleet-system fleet-agent

kubectl delete ns cattle-fleet-system --now

kubectl config use-context "$ctx"



================================================
FILE: dev/setup-cluster-config
================================================
#!/usr/bin/env bash

set -ex

if [ "$1" = "teardown" ]; then
    go run ./e2e/testenv/infra/main.go teardown
    exit 0
fi

if [ ! -f "$DEFAULT_CONFIG" ]; then
    echo >&2 "Run this from the root of the repo"
    exit 1
fi

if [ -n "$FLEET_TEST_CONFIG" ]; then
    if [ ! -f "$FLEET_TEST_CONFIG" ]; then
        echo >&2 "File not found: \$FLEET_TEST_CONFIG: $FLEET_TEST_CONFIG"
        exit 1
    fi
    echo "Using custom config file: $FLEET_TEST_CONFIG"
    # shellcheck source=/dev/null
    source "$FLEET_TEST_CONFIG"
elif [ -f "$CUSTOM_CONFIG_FILE" ]; then
    echo "Using custom config file: $CUSTOM_CONFIG_FILE"
    # shellcheck source=/dev/null
    source "$CUSTOM_CONFIG_FILE"
else
    echo "Using default config file: $DEFAULT_CONFIG"
    # shellcheck source=/dev/null
    source "$DEFAULT_CONFIG"
fi



================================================
FILE: dev/setup-fleet
================================================
#!/bin/bash
# Description: install fleet standalone into the current kubectl context

set -euxo pipefail

cluster_name=${1-upstream}
shards_json=${2:-}

if [ ! -d ./charts/fleet ]; then
  echo "please change the current directory to the fleet repo checkout"
  exit 1
fi

# single cluster
host=$( docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' "k3d-$cluster_name-server-0" )
ca=$( kubectl config view --flatten -o jsonpath='{.clusters[?(@.name == "k3d-'"$cluster_name"'")].cluster.certificate-authority-data}' | base64 -d )
server="https://$host:6443"
helm -n cattle-fleet-system upgrade --install --create-namespace --wait fleet-crd charts/fleet-crd

# Constructing the shards settings dynamically
shards_settings=""
if [ -n "$shards_json" ]; then
  index=0
  for shard in $(echo "${shards_json}" | jq -c '.[]'); do
    shard_id=$(echo "$shard" | jq -r '.id')
    shards_settings="$shards_settings --set shards[$index].id=$shard_id"
    node_selector=$(echo "$shard" | jq -r '.nodeSelector // empty')
    if [ -n "$node_selector" ]; then
      for key in $(echo "$node_selector" | jq -r 'keys[]'); do
        value=$(echo "$node_selector" | jq -r --arg key "$key" '.[$key]')
        escaped_key=$(echo "$key" | sed 's/\./\\./g')
        shards_settings="$shards_settings --set shards[$index].nodeSelector.$escaped_key=$value"
      done
    fi
    index=$((index + 1))
  done
fi

helm -n cattle-fleet-system upgrade --install --create-namespace --wait --reset-values \
  --set apiServerCA="$ca" \
  --set apiServerURL="$server" \
  $shards_settings \
  --set bootstrap.agentNamespace=cattle-fleet-local-system \
  --set agent.leaderElection.leaseDuration=10s \
  --set agent.leaderElection.retryPeriod=1s \
  --set agent.leaderElection.renewDeadline=5s \
  --set garbageCollectionInterval=1s \
  --set insecureSkipHostKeyChecks=false \
  --set-string extraEnv[0].name=EXPERIMENTAL_SCHEDULES \
  --set-string extraEnv[0].value=true \
  --set-string extraEnv[1].name=EXPERIMENTAL_COPY_RESOURCES_DOWNSTREAM \
  --set-string extraEnv[1].value=true \
  --set debug=true --set debugLevel=1 fleet charts/fleet

# wait for controller and agent rollout
kubectl -n cattle-fleet-system rollout status deployment/fleet-controller
{ grep -E -q -m 1 "fleet-agent-local.*1/1"; kill $!; } < <(kubectl get bundles -n fleet-local -w)
kubectl -n cattle-fleet-local-system rollout status deployment/fleet-agent

# label local cluster
kubectl patch clusters.fleet.cattle.io -n fleet-local local --type=json -p '[{"op": "add", "path": "/metadata/labels/management.cattle.io~1cluster-display-name", "value": "local" }]'



================================================
FILE: dev/setup-fleet-managed-downstream
================================================
#!/bin/bash
# Description: setup managed fleet agent in the downstream cluster (manager initiated registration)

set -euxo pipefail

if [ ! -d ./charts/fleet ]; then
  echo "please change the current directory to the fleet repo checkout"
  exit 1
fi

for cluster in $(k3d cluster list -o json | jq -r '.[].name | select(. | contains("downstream"))'); do
    # fetching from local kubeconfig
    host=$( docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' k3d-$cluster-server-0 )
    ca=$( kubectl config view --flatten -o jsonpath="{.clusters[?(@.name == \"k3d-$cluster\")].cluster.certificate-authority-data}" )
    client_cert=$( kubectl config view --flatten -o jsonpath="{.users[?(@.name == \"admin@k3d-$cluster\")].user.client-certificate-data}" )
    token=$( kubectl config view --flatten -o jsonpath="{.users[?(@.name == \"admin@k3d-$cluster\")].user.client-key-data}" )
    server="https://$host:6443"

    value=$(cat <<EOF
apiVersion: v1
kind: Config
current-context: default
clusters:
- cluster:
    certificate-authority-data: $ca
    server: $server
  name: cluster
contexts:
- context:
    cluster: cluster
    user: user
  name: default
preferences: {}
users:
- name: user
  user:
    client-certificate-data: $client_cert
    client-key-data: $token
EOF
    )
    kubectl create ns fleet-default || true
    kubectl delete secret -n fleet-default kbcfg-$cluster || true
    kubectl create secret generic -n fleet-default kbcfg-$cluster --from-literal=value="$value"

    kubectl apply -n fleet-default -f - <<EOF
apiVersion: "fleet.cattle.io/v1alpha1"
kind: Cluster
metadata:
  name: $cluster
  namespace: fleet-default
  labels:
    name: $cluster
spec:
  kubeConfigSecret: kbcfg-$cluster
EOF

done



================================================
FILE: dev/setup-fleet-multi-cluster
================================================
#!/bin/bash
# Description: install fleet into upstream and downstream cluster

set -euxo pipefail

if [ ! -d ./charts/fleet ]; then
  echo "please change the current directory to the fleet repo checkout"
  exit 1
fi

upstream_ctx="${FLEET_E2E_CLUSTER-k3d-upstream}"

kubectl config use-context "$upstream_ctx"

dev/setup-fleet
dev/setup-fleet-managed-downstream

kubectl config use-context "$upstream_ctx"

ns=${FLEET_E2E_NS_DOWNSTREAM-fleet-default}

# Wait for clusters to become "ready" by waiting for bundles to become ready.
num_clusters=$(k3d cluster list -o json | jq -r '.[].name | select( . | contains("downstream") )' | wc -l)
while [[ $(kubectl get clusters.fleet.cattle.io -n "$ns" | grep '1/1' -c) -ne $num_clusters ]]; do
  sleep 1
done

for cluster in $(kubectl get clusters.fleet.cattle.io -n "$ns" -o jsonpath='{.items[*].metadata.name}'); do
  kubectl patch clusters.fleet.cattle.io "$cluster" -n "$ns" --type=json -p '[{"op": "add", "path": "/metadata/labels/env", "value": "test" }]'
done



================================================
FILE: dev/setup-k3d
================================================
#!/bin/bash
# Description: Create the management cluster

set -euxo pipefail

# k3d version list k3s
# https://hub.docker.com/r/rancher/k3s/tags
# k3d_args="-i docker.io/rancher/k3s:v1.22.15-k3s1"

args=${k3d_args---network fleet}
docker_mirror=${docker_mirror-}
unique_api_port=${unique_api_port-36443}
unique_tls_port=${unique_tls_port-443}
METRICS_GITJOB_PORT=${METRICS_GITJOB_PORT-}
METRICS_CONTROLLER_PORT=${METRICS_CONTROLLER_PORT-}
METRICS_HELMOPS_PORT=${METRICS_HELMOPS_PORT-}
GITJOB_WEBHOOK_PORT=${GITJOB_WEBHOOK_PORT-}

name=${1-upstream}
offs=${2-0}

if [ -n "$METRICS_CONTROLLER_PORT" ]; then
    args="$args -p "${METRICS_CONTROLLER_PORT}:${METRICS_CONTROLLER_PORT}@server:0""
fi

if [ -n "$METRICS_GITJOB_PORT" ]; then
    args="$args -p "${METRICS_GITJOB_PORT}:${METRICS_GITJOB_PORT}@server:0""
fi

if [ -n "$METRICS_HELMOPS_PORT" ]; then
    args="$args -p "${METRICS_HELMOPS_PORT}:${METRICS_HELMOPS_PORT}@server:0""
fi

if [ -n "$GITJOB_WEBHOOK_PORT" ]; then
    args="$args -p "${GITJOB_WEBHOOK_PORT}:80@server:0""
fi

if [ -n "$docker_mirror" ]; then
  TMP_CONFIG="$(mktemp)"
  trap "rm -f $TMP_CONFIG" EXIT

  cat << EOF > "$TMP_CONFIG"
mirrors:
  "docker.io":
      endpoint:
            - $docker_mirror
EOF
  args="$args --registry-config $TMP_CONFIG"
fi

k3d cluster create "$name" \
  --servers 3 \
  --api-port "$unique_api_port" \
  -p "$(( 8080 + offs )):8080@server:0" \
  -p "$(( 8081 + offs )):8081@server:0" \
  -p "$(( 8082 + offs )):8082@server:0" \
  -p "$(( 4343 + offs )):4343@server:0" \
  -p "$unique_tls_port:443@server:0" \
  --k3s-arg '--tls-san=k3d-upstream-server-0@server:0' \
  $args



================================================
FILE: dev/setup-k3ds-downstream
================================================
#!/bin/bash
# Description: Create n downstream clusters

set -euxo pipefail

args=${k3d_args---network fleet}
docker_mirror=${docker_mirror-}
name="downstream"
FLEET_E2E_DS_CLUSTER_COUNT=${FLEET_E2E_DS_CLUSTER_COUNT-1}

if [ -n "$docker_mirror" ]; then
  TMP_CONFIG="$(mktemp)"
  trap "rm -f $TMP_CONFIG" EXIT

  cat <<EOF >"$TMP_CONFIG"
mirrors:
  "docker.io":
      endpoint:
            - $docker_mirror
EOF
  args="$args --registry-config $TMP_CONFIG"
fi

for i in $(seq 1 "$FLEET_E2E_DS_CLUSTER_COUNT"); do
  k3d cluster create "$name$i" \
    --servers 1 \
    --api-port $((36443 + i)) \
    -p "$((4080 + (1000 * i))):80@server:0" \
    -p "$((3443 + i)):443@server:0" \
    --k3s-arg "--tls-san=k3d-$name$i-server-0@server:0" \
    $args
done

kubectl config use-context k3d-upstream



================================================
FILE: dev/setup-multi-cluster
================================================
#!/usr/bin/env bash

export DEFAULT_CONFIG="dev/env.multi-cluster-defaults"
export CUSTOM_CONFIG_FILE="env.multi-cluster"

# shellcheck source=dev/setup-cluster-config
source dev/setup-cluster-config

FLEET_E2E_DS_CLUSTER_COUNT=${FLEET_E2E_DS_CLUSTER_COUNT:-1}

# Cleans with settings sourced, so it should be rather selective.
./dev/k3d-clean

PORT_OFFSET=0
if [ -z "$external_ip" ];
then
 PORT_OFFSET=$(( RANDOM % 10001 ))
fi

./dev/setup-k3d "${FLEET_E2E_CLUSTER#k3d-}" "$PORT_OFFSET"
./dev/setup-k3ds-downstream
./dev/build-fleet
./dev/import-images-k3d
./dev/setup-fleet-multi-cluster

# needed for gitrepo tests
./dev/import-images-tests-k3d
./dev/create-zot-certs 'FleetCI-RootCA' # for OCI tests
./dev/create-secrets 'FleetCI-RootCA'
go run ./e2e/testenv/infra/main.go setup



================================================
FILE: dev/setup-rancher-clusters
================================================
#!/bin/bash

set -eux

# Check availability of Rancher CLI
if ! command -v rancher &>/dev/null && ! command -v rancher-cli &>/dev/null; then
  echo "Error: Either 'rancher' or 'rancher-cli' binary must be installed and available in PATH."
  exit 1
fi

if [ ! -d ./.github/scripts ]; then
  echo "please change the current directory to the fleet repo checkout"
  exit 1
fi

upstream_ctx="${FLEET_E2E_CLUSTER-k3d-upstream}"
downstream_ctx="${FLEET_E2E_CLUSTER_DOWNSTREAM-k3d-downstream1}"
rancherpassword="${RANCHER_PASSWORD-rancherpassword}"

public_hostname="${public_hostname:-}"
if [ -z "${public_hostname}" ]; then
  until kubectl get service -n kube-system traefik -o jsonpath='{.status.loadBalancer.ingress[0].ip}'; do sleep 3; done
  ip=$(kubectl get service -n kube-system traefik -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
  public_hostname="$ip.sslip.io"
fi

version="${1-}"
channel="${2-latest}" # latest or alpha
if [ -z "$version" ]; then
  version=$(curl -SsLf "https://releases.rancher.com/server-charts/$channel/index.yaml" | yq -r '.entries | to_entries | .[].value[] | .version' | sort -V | tail -1)
fi

kubectl config use-context "$upstream_ctx"

kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.13.1/cert-manager.yaml
kubectl wait --for=condition=Available deployment --timeout=2m -n cert-manager --all

helm upgrade rancher "https://releases.rancher.com/server-charts/${channel}/rancher-${version#v}.tgz" \
  --devel \
  --install --wait \
  --create-namespace \
  --namespace cattle-system \
  --set "extraEnv[0].name=CATTLE_SERVER_URL" \
  --set "extraEnv[0].value=https://$public_hostname" \
  --set replicas=1 \
  --set hostname="$public_hostname" \
  --set bootstrapPassword="$rancherpassword"

# wait for deployment of rancher
kubectl -n cattle-system rollout status deploy/rancher

echo "Waiting for fleet deployment to appear..."
timeout 300 bash -c 'until kubectl get deployments -n cattle-fleet-system 2>/dev/null | grep -q fleet; do sleep 2; done'

kubectl -n cattle-fleet-system rollout status deploy/fleet-controller

echo "Waiting for fleet-agent-local bundle to be ready..."
timeout 300 bash -c 'until kubectl get bundles -n fleet-local 2>/dev/null | grep -E -q "fleet-agent-local.*1/1"; do sleep 2; done'

./.github/scripts/wait-for-loadbalancer.sh

export cluster_downstream="$downstream_ctx"
export public_hostname="$public_hostname"
./.github/scripts/register-downstream-clusters.sh

# register-downstream-clusters.sh only supports fleet-default
export FLEET_E2E_NS_DOWNSTREAM=fleet-default
./.github/scripts/label-downstream-cluster.sh

echo -----------------------------------------
echo The Rancher-Interface is available at: https://${public_hostname}/dashboard/?setup=$(kubectl get secret --namespace cattle-system bootstrap-secret -o go-template='{{.data.bootstrapPassword|base64decode}}')


================================================
FILE: dev/setup-single-cluster
================================================
#!/usr/bin/env bash

export DEFAULT_CONFIG="dev/env.single-cluster-defaults"
export CUSTOM_CONFIG_FILE="env.single-cluster"

# shellcheck source=dev/setup-cluster-config
source ./dev/setup-cluster-config

if [ $1 = "--reuse" ]; then
    ./dev/remove-fleet
else
    # Cleans with settings sourced, so it should be rather selective.
    ./dev/k3d-clean

    PORT_OFFSET=0
    if [ -z "$external_ip" ];
    then
     PORT_OFFSET=$(( RANDOM % 10001 ))
    fi

    ./dev/setup-k3d "${FLEET_E2E_CLUSTER#k3d-}" "$PORT_OFFSET"
fi

./dev/build-fleet
./dev/import-images-k3d
./dev/setup-fleet "${FLEET_E2E_CLUSTER#k3d-}" '[
    {
        "id": "shard0",
        "nodeSelector": {
            "kubernetes.io/hostname": "k3d-upstream-server-0"
        }
    },
    {
        "id": "shard1"
    },
    {
        "id": "shard2",
        "nodeSelector": {
            "kubernetes.io/hostname": "k3d-upstream-server-2"
        }
    }
]'

# needed for gitrepo tests
./dev/import-images-tests-k3d
./dev/create-zot-certs 'FleetCI-RootCA' # for OCI tests
set +e # keep going if secrets already exist
./dev/create-secrets 'FleetCI-RootCA'
go run ./e2e/testenv/infra/main.go setup



================================================
FILE: dev/update-agent-k3d
================================================
#!/bin/bash

set -euxo pipefail

if [ ! -d ./cmd/fleetagent ]; then
  echo "please change the current directory to the fleet repo checkout"
  exit 1
fi

export GOOS=linux
export GOARCH="${GOARCH:-amd64}"
export CGO_ENABLED=0

# fleet agent
go build -gcflags='all=-N -l' -o "bin/fleetagent-linux-$GOARCH" ./cmd/fleetagent
docker build -f package/Dockerfile.agent -t rancher/fleet-agent:dev --build-arg="ARCH=$GOARCH" .

fleet_ctx=$(kubectl config current-context)
k3d image import rancher/fleet-agent:dev -m direct -c "${fleet_ctx#k3d-}"
kubectl delete pod -l app=fleet-agent -n cattle-fleet-local-system

upstream_ctx="${FLEET_E2E_CLUSTER-k3d-upstream}"
downstream_ctx="${FLEET_E2E_CLUSTER_DOWNSTREAM-k3d-downstream1}"
downstream_keyword="${downstream_ctx#k3d-}"
downstream_keyword="${downstream_keyword%[0-9]*}"
if [ "$upstream_ctx" != "$downstream_ctx" ]; then
  for cluster in $(k3d cluster list -o json | \
      jq -r ".[].name | select(. | contains(\"${downstream_keyword}\"))"); do
    k3d image import rancher/fleet-agent:dev -m direct -c "${cluster}"
    kubectl --context "k3d-$cluster" delete pod -l app=fleet-agent -n cattle-fleet-system
  done
fi



================================================
FILE: dev/update-controller-k3d
================================================
#!/bin/bash

set -euxo pipefail

if [ ! -d ./cmd/fleetcontroller ]; then
  echo "please change the current directory to the fleet repo checkout"
  exit 1
fi

export GOOS=linux
export GOARCH="${GOARCH:-amd64}"
export CGO_ENABLED=0

# fleetcontroller
go build -gcflags='all=-N -l' -o bin/fleetcontroller-linux-"$GOARCH" ./cmd/fleetcontroller
go build -gcflags='all=-N -l' -o "bin/fleet-linux-$GOARCH" ./cmd/fleetcli
docker build -f package/Dockerfile -t rancher/fleet:dev --build-arg="ARCH=$GOARCH"  .

fleet_ctx=$(kubectl config current-context)
k3d image import rancher/fleet:dev -m direct -c "${fleet_ctx#k3d-}"
kubectl delete pod -l app=fleet-controller -n cattle-fleet-system
kubectl delete pod -l app=gitjob -n cattle-fleet-system



================================================
FILE: dev/update-fleet-in-rancher-k3d
================================================
#!/bin/bash

set -euxo pipefail

if [ ! -d ./.github/scripts ]; then
  echo "please change the current directory to the fleet repo checkout"
  exit 1
fi

downstream_ctx="${FLEET_E2E_CLUSTER_DOWNSTREAM-k3d-downstream1}"

fleet_version="${1-0.7.0-rc.2}"
if [ "$fleet_version" == "dev" ]; then
  echo "don't forget to run dev/build-fleet before running this script"
  dev/import-images-k3d
  FLEET_E2E_CLUSTER_DOWNSTREAM="$downstream_ctx" ./.github/scripts/upgrade-rancher-fleet-to-dev-fleet.sh
  exit 0
fi

# install released fleet from url
url_crd="https://github.com/rancher/fleet/releases/download/v${fleet_version}/fleet-crd-${fleet_version}.tgz"
url="https://github.com/rancher/fleet/releases/download/v${fleet_version}/fleet-${fleet_version}.tgz"
version="v${fleet_version}"
fleetns="cattle-fleet-system"

helm upgrade fleet-crd "$url_crd" --wait -n "$fleetns"
until helm -n "$fleetns" status fleet-crd  | grep -q "STATUS: deployed"; do echo waiting for original fleet-crd chart to be deployed; sleep 1; done

helm upgrade fleet "$url" \
  --wait -n "$fleetns" \
  --set image.tag="$version" \
  --set agentImage.tag="$version" \
  --set debug=true --set debugLevel=1



================================================
FILE: docs/README.md
================================================
This directory is used for maintainer and developer documentation. The [Fleet docs site](https://fleet.rancher.io/) provides the latest end-user documentation.



================================================
FILE: docs/design.md
================================================
# Design

More information can be found in the "Component Overview" [section](https://fleet.rancher.io/architecture#component-overview) on the Fleet docs website.

## Design for Fleet Managing Fleet: Hosted Rancher, Harvester, Rancher Managing Rancher, etc.

Starting with Fleet v0.3.7 and Rancher v2.6.1, scenarios where Fleet is managing Fleet (i.e. Rancher managing Rancher) will result in _two_ `fleet-agent` deployments running every _managed_ Fleet cluster.
The agents will be communicating with two different `fleet-controller` deployments.

```
 Local Fleet Cluster                  Managed Fleet Cluster                     Downstream Cluster
┌───────────────────────────────┐    ┌────────────────────────────────────┐    ┌────────────────────────────────────┐
│                               │    │                                    │    │                                    │
│ ┌────cattle-fleet-system────┐ │    │ ┌──────cattle-fleet-system───────┐ │    │                                    │
│ │                           │ │    │ │                                │ │    │                                    │
│ │  ┌─────────────────────┐  │ │    │ │  ┌──────────────────────────┐  │ │    │                                    │
│ │  │  fleet-controller   ◄──┼─┼────┼─┼──► fleet-agent (downstream) │  │ │    │                                    │
│ │  └─────────────────────┘  │ │    │ │  └──────────────────────────┘  │ │    │                                    │
│ │                           │ │    │ │                                │ │    │                                    │ 
│ └───────────────────────────┘ │    │ │                                │ │    │                                    │
│                               │    │ │                                │ │    │                                    │
│ ┌─cattle-fleet-local-system─┐ │    │ │                                │ │    │ ┌──────cattle-fleet-system───────┐ │
│ │                           │ │    │ │                                │ │    │ │                                │ │
│ │  ┌─────────────────────┐  │ │    │ │  ┌──────────────────────────┐  │ │    │ │  ┌──────────────────────────┐  │ │
│ │  │ fleet-agent (local) │  │ │    │ │  │    fleet-controller      ◄──┼─┼────┼─┼──► fleet-agent (downstream) │  │ │
│ │  └─────────────────────┘  │ │    │ │  └──────────────────────────┘  │ │    │ │  └──────────────────────────┘  │ │
│ │                           │ │    │ │                                │ │    │ │                                │ │
│ └───────────────────────────┘ │    │ └────────────────────────────────┘ │    │ └────────────────────────────────┘ │
│                               │    │                                    │    │                                    │
└───────────────────────────────┘    │ ┌───cattle-fleet-local-system────┐ │    └────────────────────────────────────┘
                                     │ │                                │ │
                                     │ │  ┌──────────────────────────┐  │ │
                                     │ │  │   fleet-agent (local)    │  │ │
                                     │ │  └──────────────────────────┘  │ │
                                     │ │                                │ │
                                     │ └────────────────────────────────┘ │
                                     │                                    │
                                     └────────────────────────────────────┘
```

## Design for Fleet in Rancher v2.6+

Fleet is a required component of Rancher as of Rancher v2.6+.
Fleet clusters are tied directly to native Rancher object types accordingly:

```
┌───────────────────────────────────┐  ==  ┌────────────────────────────────────┐  ==  ┌──────────────────────────────────┐
│ clusters.fleet.cattle.io/v1alpha1 ├──────┤ clusters.provisioning.cattle.io/v1 ├──────┤ clusters.management.cattle.io/v3 │
└────────────────┬──────────────────┘      └───────────────────┬────────────────┘      └──────────────────────────────────┘
                 │                                             │
                 └──────────────────────┬──────────────────────┘
                                        │
                          ┌─────────────▼────────────────────────┐
                          │                                      │
       ┌──────────────────▼──────────────────────┐  ==  ┌────────▼──────┐
       │ fleetworkspaces.management.cattle.io/v3 ├──────┤ namespaces/v1 │
       └─────────────────────────────────────────┘      └───────────────┘
```

---

## Attributions

- ASCII charts created with [asciiflow](https://asciiflow.com/)



================================================
FILE: docs/performance.md
================================================
# Examining Performance Issues

Fleet differs from Rancher in one major design philosophy: nearly all "business logic" happens in the local cluster rather than in downstream clusters via agents.
The good news here is that the `fleet-controller` will tell us nearly all that we need to know via pod logs, network traffic and resource usage.
That being said, downstream `fleet-agent` deployments can perform Kubernetes API requests _back_ to the local cluster, which means that we have to monitor traffic inbound to the local cluster from our agents _as well as_ the outbound traffic we'd come to expect from the local `fleet-controller`.

While network traffic is major point of consideration, we also have to consider whether our performance issues are **compute-based**, **memory-based**, or **network-based**.
For example: you may encounter a pod with high compute usage, but that could be caused by heightened network traffic received from the _truly_ malfunctioning pod.

## Using pprof

[http pprof](https://pkg.go.dev/net/http/pprof) handlers are enabled by default with all [default profiles](https://pkg.go.dev/runtime/pprof#Profile) under the `/debug/pprof` prefix.

To collect profiling information continuously one can use https://github.com/rancherlabs/support-tools/tree/master/collection/rancher/v2.x/profile-collector



================================================
FILE: docs/qa_template.md
================================================
Pull request content for Rancher QA verification.

## Additional QA
### Issue: <link the issue or issues this PR resolves here>
< If your PR depends on changes from another pr link them here and describe why they are needed on your solution section. >
 
### Problem
< Describe the root cause of the issue you are resolving. This may include what behavior is observed and why it is not desirable. If this is a new feature describe why we need this feature and how it will be used. >
 
### Solution
< Describe what you changed to fix the issue. Relate your changes back to the original issue / feature and explain why this addresses the issue.>
 
### Testing
< Note: Confirm if the repro steps in the GitHub issue are valid, if not, please update the issue with accurate repro steps. >

### Engineering Testing
#### Manual Testing
< Describe what manual testing you did (if no testing was done, explain why). >

#### Automated Testing
<If you added/updated unit/integration/validation tests, describe what cases they cover and do not cover. >

### QA Testing Considerations
< Highlight areas or (additional) cases that QA should test w.r.t a fresh install as well as the upgrade scenarios >
 
#### Regressions Considerations
< Dedicated section to specifically call out any areas that with higher chance of regressions caused by this change, include estimation of probability of regressions >


================================================
FILE: docs/release.md
================================================
# Release

This section contains information on releasing Fleet.
**Please note: it may be sparse since it is only intended for maintainers.**

---

## Cherry Picking Bug Fixes To Releases

With releases happening on release branches, there are times where a bug fix needs to be handled on the `main` branch and pulled into a release that happens through a release branch.

All bug fixes should first happen on the `main` branch.

If a bug fix needs to be brought into a release, such as during the release candidate phase, it should be cherry picked from the `main` branch to the release branch via a pull request. The pull request should be prefixed with the major and minor version for the release (e.g., `[v0.4]`) to illustrate it's for a release branch.

After merge verify that the Github Action test runs for the release branch were successful.

## When do we branch?

We branch the next release branch release/v0.x from master only, when we start on 0.x+1 features. This should keep the distance between both branches to a minimum.

We have to make sure all the QA relevant commits are part of the release plan and their issues have the correct milestone, etc.

```
% git merge-base master release/v0.6                                                                  
2312ff8f8823320629769f9ab408472ed58c2442
% git log 2312ff8f8823320629769f9ab408472ed58c2442..master
```

After branching, we cherry pick PRs with separate issues for QA. The issues should use '[v0.x]' in their title.

## What else to do after a release

More detailed instructions, e.g. how to use the release workflows and interact with the rancher/charts repo are in the Wiki.

* generate release notes, make sure all changes are included since last release
* edit release notes, only user issues that are relevant to users, fix spelling, capitalization
* update versioned docs in fleet-docs with yarn
* adapt CI so scheduled test are run for new version



================================================
FILE: internal/bundlereader/auth.go
================================================
package bundlereader

import (
	"context"
	"crypto/sha256"
	"encoding/binary"
	"encoding/hex"
	"fmt"
	"strconv"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type Auth struct {
	Username           string `json:"username,omitempty"`
	Password           string `json:"password,omitempty"`
	CABundle           []byte `json:"caBundle,omitempty"`
	SSHPrivateKey      []byte `json:"sshPrivateKey,omitempty"`
	InsecureSkipVerify bool   `json:"insecureSkipVerify,omitempty"`
	BasicHTTP          bool   `json:"basicHTTP,omitempty"`
	// remember to update Hash() when adding/modifying fields
}

func toByte(v bool) byte {
	if v {
		return byte(1)
	}
	return byte(0)
}

func (a Auth) Hash() string {
	hash := sha256.New()

	// Use data length as delimiter to avoid possible collisions
	lenBuf := make([]byte, 8)
	writeField := func(data []byte) {
		binary.LittleEndian.PutUint64(lenBuf, uint64(len(data)))
		hash.Write(lenBuf)
		hash.Write(data)
	}

	for _, v := range [][]byte{
		[]byte(a.Username),
		[]byte(a.Password),
		a.CABundle,
		a.SSHPrivateKey,
		{toByte(a.InsecureSkipVerify)},
		{toByte(a.BasicHTTP)},
	} {
		writeField(v)
	}

	return hex.EncodeToString(hash.Sum(nil))

}

func ReadHelmAuthFromSecret(ctx context.Context, c client.Reader, req types.NamespacedName) (Auth, error) {
	if req.Name == "" {
		return Auth{}, nil
	}
	secret := &corev1.Secret{}
	err := c.Get(ctx, req, secret)
	if err != nil {
		return Auth{}, err
	}

	auth := Auth{}
	username, okUsername := secret.Data[corev1.BasicAuthUsernameKey]
	if okUsername {
		auth.Username = string(username)
	}

	password, okPasswd := secret.Data[corev1.BasicAuthPasswordKey]
	if okPasswd {
		auth.Password = string(password)
	}

	// check that username and password are both set or none is set
	if okUsername && !okPasswd {
		return Auth{}, fmt.Errorf("%s is set in the secret, but %s isn't", corev1.BasicAuthUsernameKey, corev1.BasicAuthPasswordKey)
	} else if !okUsername && okPasswd {
		return Auth{}, fmt.Errorf("%s is set in the secret, but %s isn't", corev1.BasicAuthPasswordKey, corev1.BasicAuthUsernameKey)
	}

	caBundle, ok := secret.Data["cacerts"]
	if ok {
		auth.CABundle = caBundle
	}

	// Get the values for skipping TLS and basic HTTP connections.
	// In case of error reading the values they will be considered
	// as set to false as those values are security related.
	insecureSkipVerify := false
	if value, ok := secret.Data["insecureSkipVerify"]; ok {
		boolValue, err := strconv.ParseBool(string(value))
		if err == nil {
			insecureSkipVerify = boolValue
		}
	}

	basicHTTP := false
	if value, ok := secret.Data["basicHTTP"]; ok {
		boolValue, err := strconv.ParseBool(string(value))
		if err == nil {
			basicHTTP = boolValue
		}
	}

	auth.InsecureSkipVerify = insecureSkipVerify
	auth.BasicHTTP = basicHTTP

	return auth, nil
}



================================================
FILE: internal/bundlereader/auth_test.go
================================================
package bundlereader_test

import (
	"context"
	"fmt"
	"testing"

	"github.com/stretchr/testify/assert"
	"go.uber.org/mock/gomock"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/types"

	"github.com/rancher/fleet/internal/bundlereader"
	"github.com/rancher/fleet/internal/mocks"
)

func TestReadHelmAuthFromSecret(t *testing.T) {
	cases := []struct {
		name              string
		secretData        map[string][]byte
		getError          string
		expectedAuth      bundlereader.Auth
		expectedErrNotNil bool
		expectedError     string
	}{
		{
			name:         "nothing is set",
			secretData:   map[string][]byte{},
			getError:     "",
			expectedAuth: bundlereader.Auth{
				// default values
			},
			expectedErrNotNil: false,
			expectedError:     "",
		},
		{
			name: "username, password and caBundle are set",
			secretData: map[string][]byte{
				corev1.BasicAuthUsernameKey: []byte("user"),
				corev1.BasicAuthPasswordKey: []byte("passwd"),
				"cacerts":                   []byte("test_cabundle"),
			},
			getError: "",
			expectedAuth: bundlereader.Auth{
				Username: "user",
				Password: "passwd",
				CABundle: []byte("test_cabundle"),
			},
			expectedErrNotNil: false,
			expectedError:     "",
		},
		{
			name: "username, password are set, caBundle is not",
			secretData: map[string][]byte{
				corev1.BasicAuthUsernameKey: []byte("user"),
				corev1.BasicAuthPasswordKey: []byte("passwd"),
			},
			getError: "",
			expectedAuth: bundlereader.Auth{
				Username: "user",
				Password: "passwd",
			},
			expectedErrNotNil: false,
			expectedError:     "",
		},
		{
			name: "caBundle is set, username and password are not",
			secretData: map[string][]byte{
				"cacerts": []byte("test_cabundle"),
			},
			getError: "",
			expectedAuth: bundlereader.Auth{
				CABundle: []byte("test_cabundle"),
			},
			expectedErrNotNil: false,
			expectedError:     "",
		},
		{
			name: "username, caBundle are set, password is not",
			secretData: map[string][]byte{
				corev1.BasicAuthUsernameKey: []byte("user"),
				"cacerts":                   []byte("test_cabundle"),
			},
			getError:          "",
			expectedAuth:      bundlereader.Auth{},
			expectedErrNotNil: true,
			expectedError:     "username is set in the secret, but password isn't",
		},
		{
			name: "username is set, password and caBundle are not",
			secretData: map[string][]byte{
				corev1.BasicAuthUsernameKey: []byte("user"),
			},
			getError:          "",
			expectedAuth:      bundlereader.Auth{},
			expectedErrNotNil: true,
			expectedError:     "username is set in the secret, but password isn't",
		},
		{
			name: "password, caBundle are set, username is not",
			secretData: map[string][]byte{
				corev1.BasicAuthPasswordKey: []byte("passwd"),
				"cacerts":                   []byte("test_cabundle"),
			},
			getError:          "",
			expectedAuth:      bundlereader.Auth{},
			expectedErrNotNil: true,
			expectedError:     "password is set in the secret, but username isn't",
		},
		{
			name: "password is set, username and caBundle are not",
			secretData: map[string][]byte{
				corev1.BasicAuthPasswordKey: []byte("passwd"),
			},
			getError:          "",
			expectedAuth:      bundlereader.Auth{},
			expectedErrNotNil: true,
			expectedError:     "password is set in the secret, but username isn't",
		},
		{
			name: "username, password and caBundle are set, but we get an error getting the secret",
			secretData: map[string][]byte{
				corev1.BasicAuthPasswordKey: []byte("passwd"),
			},
			getError:          "error getting secret",
			expectedAuth:      bundlereader.Auth{},
			expectedErrNotNil: true,
			expectedError:     "error getting secret",
		},
		{
			name: "insecureSkipVerify is set to true",
			secretData: map[string][]byte{
				"insecureSkipVerify": []byte("true"),
			},
			getError: "",
			expectedAuth: bundlereader.Auth{
				InsecureSkipVerify: true,
			},
			expectedErrNotNil: false,
			expectedError:     "",
		},
		{
			name: "insecureSkipVerify is set to an invalid value",
			secretData: map[string][]byte{
				"insecureSkipVerify": []byte("THIS_IS_NOT_A_VALID_VALUE"),
			},
			getError: "",
			expectedAuth: bundlereader.Auth{
				InsecureSkipVerify: false,
			},
			expectedErrNotNil: false,
			expectedError:     "",
		},
		{
			name: "basicHTTP is set to true",
			secretData: map[string][]byte{
				"basicHTTP": []byte("true"),
			},
			getError: "",
			expectedAuth: bundlereader.Auth{
				BasicHTTP: true,
			},
			expectedErrNotNil: false,
			expectedError:     "",
		},
		{
			name: "basicHTTP is set to an invalid value",
			secretData: map[string][]byte{
				"basicHTTP": []byte("THIS_IS_NOT_A_VALID_VALUE"),
			},
			getError: "",
			expectedAuth: bundlereader.Auth{
				BasicHTTP: false,
			},
			expectedErrNotNil: false,
			expectedError:     "",
		},
	}

	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	mockClient := mocks.NewMockK8sClient(mockCtrl)

	assert := assert.New(t)
	for _, c := range cases {
		if c.getError != "" {
			mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any()).DoAndReturn(
				func(_ context.Context, _ types.NamespacedName, secret *corev1.Secret, _ ...interface{}) error {
					return fmt.Errorf("%v", c.getError)
				},
			)
		} else {
			mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any()).DoAndReturn(
				func(_ context.Context, _ types.NamespacedName, secret *corev1.Secret, _ ...interface{}) error {
					secret.Data = c.secretData
					return nil
				},
			)
		}

		nsName := types.NamespacedName{Name: "test", Namespace: "test"}
		auth, err := bundlereader.ReadHelmAuthFromSecret(context.TODO(), mockClient, nsName)
		assert.Equal(c.expectedErrNotNil, err != nil)
		if err != nil && c.expectedErrNotNil {
			assert.Equal(c.expectedError, err.Error())
		}
		assert.Equal(c.expectedAuth, auth)
	}
}

func TestAuth_Hash(t *testing.T) {
	for name, baseAuth := range map[string]bundlereader.Auth{
		"no fields": {},
		"all fields": {
			Username:           "user",
			Password:           "pass",
			CABundle:           []byte("ca-data"),
			SSHPrivateKey:      []byte("ssh-key"),
			InsecureSkipVerify: true,
			BasicHTTP:          false,
		},
	} {
		t.Run(name, func(t *testing.T) {
			// Test that changing each field individually results in a new hash.
			testCases := []struct {
				name          string
				mod           func(a bundlereader.Auth) bundlereader.Auth
				auth          bundlereader.Auth
				baseHash      string
				expectedEqual bool
			}{
				{
					name:          "No changes",
					mod:           func(a bundlereader.Auth) bundlereader.Auth { return a },
					expectedEqual: true,
				},
				{
					name: "Different Username",
					mod:  func(a bundlereader.Auth) bundlereader.Auth { a.Username = "different-user"; return a },
				},
				{
					name: "Different Password",
					mod:  func(a bundlereader.Auth) bundlereader.Auth { a.Password = "different-pass"; return a },
				},
				{
					name: "Different CABundle",
					mod:  func(a bundlereader.Auth) bundlereader.Auth { a.CABundle = []byte("different-ca"); return a },
				},
				{
					name: "Different SSHPrivateKey",
					mod:  func(a bundlereader.Auth) bundlereader.Auth { a.SSHPrivateKey = []byte("different-key"); return a },
				},
				{
					name: "Different InsecureSkipVerify",
					mod:  func(a bundlereader.Auth) bundlereader.Auth { a.InsecureSkipVerify = !a.InsecureSkipVerify; return a },
				},
				{
					name: "Different BasicHTTP",
					mod:  func(a bundlereader.Auth) bundlereader.Auth { a.BasicHTTP = !a.BasicHTTP; return a },
				},
			}

			for _, tc := range testCases {
				t.Run(tc.name, func(t *testing.T) {
					baseHash := baseAuth.Hash()
					modifiedAuth := tc.mod(baseAuth)
					modifiedHash := modifiedAuth.Hash()

					if tc.expectedEqual {
						assert.Equal(t, modifiedHash, baseHash)
					} else {
						assert.NotEqual(t, modifiedHash, baseHash)
					}
				})
			}
		})
	}
}



================================================
FILE: internal/bundlereader/charturl.go
================================================
package bundlereader

import (
	"context"
	"crypto/tls"
	"crypto/x509"
	"errors"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"strings"

	"github.com/Masterminds/semver/v3"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"golang.org/x/sync/singleflight"
	repov1 "helm.sh/helm/v4/pkg/repo/v1"
	"sigs.k8s.io/yaml"

	"oras.land/oras-go/v2/registry"
	"oras.land/oras-go/v2/registry/remote"
	"oras.land/oras-go/v2/registry/remote/auth"
	"oras.land/oras-go/v2/registry/remote/errcode"
)

var concurrentIndexFetch singleflight.Group

// ChartVersion returns the version of the helm chart from a helm repo server, by
// inspecting the repo's index.yaml
func ChartVersion(ctx context.Context, location fleet.HelmOptions, a Auth) (string, error) {
	if repoURI, ok := strings.CutPrefix(location.Repo, ociURLPrefix); ok {
		client, err := getOCIRepoClient(repoURI, a)
		if err != nil {
			return "", err
		}

		tag, err := GetOCITag(ctx, client, location.Version)
		if len(tag) == 0 || err != nil {
			return "", fmt.Errorf(
				"could not find tag matching constraint %q in registry %s: %w",
				location.Version,
				location.Repo,
				err,
			)
		}
		return tag, nil
	}

	repoURL := location.Repo
	if repoURL == "" {
		return location.Version, nil
	}

	repoIndex, err := getHelmRepoIndex(ctx, repoURL, a)
	if err != nil {
		return "", err
	}
	chart, err := repoIndex.Get(location.Chart, location.Version)
	if err != nil {
		return "", err
	}

	if len(chart.URLs) == 0 {
		return "", fmt.Errorf("no URLs found for chart %s %s at %s", chart.Name, chart.Version, location.Repo)
	}

	return chart.Version, nil
}

func getOCIRepoClient(repoURI string, a Auth) (*remote.Repository, error) {
	r, err := remote.NewRepository(repoURI)
	if err != nil {
		return nil, fmt.Errorf("failed to create OCI client: %w", err)
	}

	authCli := &auth.Client{
		Client: getHTTPClient(a),
		Cache:  auth.NewCache(),
	}
	if a.Username != "" {
		cred := auth.Credential{
			Username: a.Username,
			Password: a.Password,
		}
		authCli.Credential = func(ctx context.Context, s string) (auth.Credential, error) {
			return cred, nil
		}
	}
	r.Client = authCli

	if a.BasicHTTP {
		r.PlainHTTP = true
	}

	return r, nil
}

// chartURL returns the URL to the helm chart from a helm repo server, by
// inspecting the repo's index.yaml
func chartURL(ctx context.Context, location fleet.HelmOptions, auth Auth, isHelmOps bool) (string, error) {
	if uri, ok := isOCIChart(location, isHelmOps); ok {
		return uri, nil
	}
	repoURL := location.Repo
	if repoURL == "" {
		return location.Chart, nil
	}

	// Aggregate any concurrent helm repo index retrieval for the same combination of repo URL and auth
	i, err, _ := concurrentIndexFetch.Do(auth.Hash()+repoURL, func() (interface{}, error) {
		return getHelmRepoIndex(ctx, repoURL, auth)
	})
	if err != nil {
		return "", err
	}
	repoIndex := i.(helmRepoIndex)

	chart, err := repoIndex.Get(location.Chart, location.Version)
	if err != nil {
		return "", err
	}

	if len(chart.URLs) == 0 {
		return "", fmt.Errorf("no URLs found for chart %s %s at %s", chart.Name, chart.Version, repoURL)
	}
	return toAbsoluteURLIfNeeded(repoURL, chart.URLs[0])
}

type helmRepoIndex interface {
	Get(chart, version string) (*repov1.ChartVersion, error)
}

// getHelmRepoIndex retrieves and parses the index.yaml from a base URL which can be used to find a specific chart and version
func getHelmRepoIndex(ctx context.Context, repoURL string, auth Auth) (helmRepoIndex, error) {
	indexURL, err := url.JoinPath(repoURL, "index.yaml")
	if err != nil {
		return nil, err
	}

	request, err := http.NewRequestWithContext(ctx, http.MethodGet, indexURL, nil)
	if err != nil {
		return nil, err
	}

	if auth.Username != "" && auth.Password != "" {
		request.SetBasicAuth(auth.Username, auth.Password)
	}

	client := getHTTPClient(auth)

	resp, err := client.Do(request)
	if err != nil {
		return nil, fmt.Errorf("failed to fetch %q: %w", indexURL, err)
	}
	defer resp.Body.Close()

	bytes, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, err
	}

	if resp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("failed to read helm repo from %s, error code: %v", indexURL, resp.StatusCode)
	}

	var index repov1.IndexFile
	if err := yaml.Unmarshal(bytes, &index); err != nil {
		return nil, err
	}
	index.SortEntries()
	return &index, nil
}

// GetOCITag fetches the highest available tag matching version v in repository r.
// Returns an error if the remote repository itself returns an error, for instance if the OCI repository is not found.
// If no error is returned, it is the caller's responsibility to check that the returned tag is non-empty.
func GetOCITag(ctx context.Context, r *remote.Repository, v string) (string, error) {
	constraint, err := semver.NewConstraint(v)
	if err != nil {
		return "", fmt.Errorf("failed to compute version constraint from version %q: %w", v, err)
	}

	availableTags, err := registry.Tags(ctx, r)
	if err != nil {
		var regErr errcode.Error
		if errors.As(err, &regErr) {
			err = regErr

			if regErr.Code == errcode.ErrorCodeNameUnknown {
				return "", fmt.Errorf("repository %q not found in the registry", r.Reference.Repository)
			}
		}

		return "", fmt.Errorf("failed to get available tags for version %q: %w", v, err)
	}

	var tagToResolve string
	var resolvedVersion *semver.Version

	_, err = semver.StrictNewVersion(v)
	isExactVersion := err == nil

	// As per https://github.com/opencontainers/distribution-spec/blob/v1.1.1/spec.md#listing-tags, available tags
	// are sorted in lexical order. However, the spec does not specify anything about ascending or descending order.
	for _, tag := range availableTags {
		// check for exact match before trying something more involved.
		if isExactVersion && v == tag {
			tagToResolve = tag
			break
		}

		sv, err := semver.NewVersion(tag)
		if err != nil {
			continue
		}

		if !constraint.Check(sv) {
			continue
		}

		if len(tagToResolve) == 0 || sv.GreaterThan(resolvedVersion) {
			tagToResolve = tag
			resolvedVersion = sv
		}
	}

	return tagToResolve, nil
}

func getHTTPClient(auth Auth) *http.Client {
	client := &http.Client{}

	transport := http.DefaultTransport.(*http.Transport).Clone()
	transport.TLSClientConfig = &tls.Config{
		InsecureSkipVerify: auth.InsecureSkipVerify, //nolint:gosec
	}

	if auth.CABundle != nil {
		pool, err := x509.SystemCertPool()
		if err != nil {
			pool = x509.NewCertPool()
		}
		pool.AppendCertsFromPEM(auth.CABundle)

		transport.TLSClientConfig.RootCAs = pool
		transport.TLSClientConfig.MinVersion = tls.VersionTLS12
	}

	client.Transport = transport

	return client
}

func isOCIChart(location fleet.HelmOptions, isHelmOps bool) (string, bool) {
	OCIField := location.Chart
	if isHelmOps {
		OCIField = location.Repo
	}

	if strings.HasPrefix(OCIField, ociURLPrefix) {
		return OCIField, true
	}
	return "", false
}

func toAbsoluteURLIfNeeded(baseURL, chartURL string) (string, error) {
	// Check if already absolute
	chartU, err := url.Parse(chartURL)
	if err != nil || chartU.IsAbs() {
		return chartURL, err
	}

	u, err := url.Parse(baseURL)
	if err != nil {
		return "", err
	}

	return u.ResolveReference(chartU).String(), nil
}



================================================
FILE: internal/bundlereader/charturl_test.go
================================================
//go:generate mockgen --build_flags=--mod=mod -destination=../mocks/oci_client_mock.go -package=mocks oras.land/oras-go/v2/registry/remote Client
package bundlereader_test

import (
	"bytes"
	"context"
	"io"
	"net/http"
	"strings"
	"testing"

	"github.com/rancher/fleet/internal/bundlereader"
	"github.com/rancher/fleet/internal/mocks"
	"go.uber.org/mock/gomock"

	"oras.land/oras-go/v2/registry/remote"
)

func Test_getOCITag(t *testing.T) {
	cases := []struct {
		name           string
		inputVersion   string
		respTags       string
		respStatusCode int
		expectedTag    string
		expectedErrMsg string
	}{
		{
			name:           "finds exact match",
			inputVersion:   "0.2.0",
			respTags:       `{"name": "bar", "tags":["0.1.0", "0.2.0", "0.3.0"]}`,
			respStatusCode: http.StatusOK,
			expectedTag:    "0.2.0",
		},
		{
			name:           "finds exact match in reversed output",
			inputVersion:   "0.2.0",
			respTags:       `{"name": "bar", "tags":["0.3.0", "0.2.0", "0.1.0"]}`,
			respStatusCode: http.StatusOK,
			expectedTag:    "0.2.0",
		},
		{
			name:           "finds highest match for constraint",
			inputVersion:   "0.*.0",
			respTags:       `{"name": "bar", "tags":["0.1.0", "0.1.9", "0.2.0"]}`,
			respStatusCode: http.StatusOK,
			expectedTag:    "0.2.0",
		},
		{
			name:           "finds highest match for constraint with comparisons",
			inputVersion:   "> 0.1.0, <= 1.0.0",
			respTags:       `{"name": "bar", "tags":["0.1.0", "0.2.0", "0.4.3", "0.9.9", "1.0.1"]}`,
			respStatusCode: http.StatusOK,
			expectedTag:    "0.9.9",
		},
		{
			name:           "returns empty tag and no error when no candidate is found",
			inputVersion:   "0.*.0",
			respTags:       `{"name": "bar", "tags":["1.1.0", "1.2.0", "1.4.3", "1.9.9"]}`,
			respStatusCode: http.StatusOK,
			expectedTag:    "",
		},
		{
			name:           "errors when the repository is not found",
			inputVersion:   "0.*.0",
			respTags:       `{"errors": [{"code": "MANIFEST_UNKNOWN", "message": "more stuff here"}]}`,
			expectedTag:    "",
			respStatusCode: http.StatusNotFound,
			expectedErrMsg: `failed to get available tags for version "0.*.0": manifest unknown`,
		},
		{
			name:           "outputs other errors",
			inputVersion:   "0.*.0",
			respTags:       `{"error": "blah bleh"}`,
			expectedTag:    "",
			respStatusCode: http.StatusBadRequest,
			expectedErrMsg: `failed to get available tags for version "0.*.0":`,
		},
		{
			name:           "errors when the version constraint is invalid",
			inputVersion:   "   ",
			expectedTag:    "",
			expectedErrMsg: "failed to compute version constraint",
		},
	}
	for _, c := range cases {
		t.Run(c.name, func(t *testing.T) {
			r, err := remote.NewRepository("foo/bar")
			if err != nil {
				t.Errorf("failed to instantiate test repository")
			}

			ctrl := gomock.NewController(t)
			mockCli := mocks.NewMockOCIClient(ctrl)
			r.Client = mockCli

			resp := http.Response{
				Body:       io.NopCloser(bytes.NewBufferString(c.respTags)),
				Request:    &http.Request{},
				StatusCode: c.respStatusCode,
			}

			mockCli.EXPECT().Do(gomock.Any()).Return(&resp, nil).MaxTimes(1)

			tag, err := bundlereader.GetOCITag(context.Background(), r, c.inputVersion)

			if err != nil {
				if len(c.expectedErrMsg) == 0 || !strings.Contains(err.Error(), c.expectedErrMsg) {
					t.Errorf("expected error message containing %q, got %v", c.expectedErrMsg, err)
				}
			} else if err == nil && len(c.expectedErrMsg) != 0 {
				t.Errorf("expected error message containing %q, got nil", c.expectedErrMsg)
			}

			if tag != c.expectedTag {
				t.Errorf("expected tag %q, got %q", c.expectedTag, tag)
			}
		})
	}
}



================================================
FILE: internal/bundlereader/helm.go
================================================
package bundlereader

import (
	"context"
	"fmt"
	"os"

	"github.com/rancher/fleet/internal/manifest"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// GetManifestFromHelmChart downloads the given helm chart and creates a
// manifest with its contents. This is used by the agent to deploy HelmOps.
func GetManifestFromHelmChart(ctx context.Context, c client.Reader, bd *fleet.BundleDeployment) (*manifest.Manifest, error) {
	helm := bd.Spec.Options.Helm

	if helm == nil {
		return nil, fmt.Errorf("helm options not found")
	}
	temp, err := os.MkdirTemp("", "helmop")
	if err != nil {
		return nil, err
	}
	defer os.RemoveAll(temp)

	nsName := types.NamespacedName{Namespace: bd.Namespace, Name: bd.Spec.HelmChartOptions.SecretName}
	auth, err := ReadHelmAuthFromSecret(ctx, c, nsName)
	if err != nil {
		return nil, err
	}
	auth.InsecureSkipVerify = bd.Spec.HelmChartOptions.InsecureSkipTLSverify

	chartURL, err := chartURL(ctx, *helm, auth, true)
	if err != nil {
		return nil, err
	}

	resources, err := loadDirectory(ctx,
		loadOpts{},
		directory{
			prefix:  checksum(helm),
			base:    temp,
			source:  chartURL,
			version: helm.Version,
			auth:    auth,
		},
	)
	if err != nil {
		return nil, err
	}

	return manifest.New(resources), nil
}



================================================
FILE: internal/bundlereader/helm_test.go
================================================
package bundlereader_test

import (
	"archive/tar"
	"bytes"
	"compress/gzip"
	"context"
	"crypto/sha256"
	"crypto/subtle"
	"fmt"
	"io"
	"net/http"
	"net/http/httptest"
	"os"
	"path/filepath"
	"strings"
	"testing"

	"github.com/stretchr/testify/assert"
	"go.uber.org/mock/gomock"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/types"

	"github.com/rancher/fleet/internal/bundlereader"
	"github.com/rancher/fleet/internal/mocks"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

const (
	authUsername  = "holadonpepito"
	authPassword  = "holadonjose"
	chartName     = "sleeper-chart"
	helmRepoIndex = `apiVersion: v1
entries:
  sleeper:
    - created: 2016-10-06T16:23:20.499814565-06:00
      description: Super sleeper chart
      digest: 99c76e403d752c84ead610644d4b1c2f2b453a74b921f422b9dcb8a7c8b559cd
      home: https://helm.sh/helm
      name: alpine
      sources:
      - https://github.com/helm/helm
      urls:
      - https://##URL##/sleeper-chart-0.1.0.tgz
      version: 0.1.0
generated: 2016-10-06T16:23:20.499029981-06:00`

	chartYAML = `apiVersion: v2
appVersion: 1.16.0
description: A test chart
name: sleeper-chart
type: application
version: 0.1.0`

	values = `replicaCount: 1`

	deployment = `apiVersion: apps/v1
kind: Deployment
metadata:
  name: sleeper
  labels:
    fleet: testing
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: sleeper
  template:
    metadata:
      labels:
        app: sleeper
    spec:
      containers:
        - name: {{ .Chart.Name }}
          command:
            - sleep
            - 7d
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "rancher/mirrored-library-busybox:1.34.1"
          imagePullPolicy: IfNotPresent`
)

func checksumPrefix(helm *fleet.HelmOptions) string {
	if helm == nil {
		return "none"
	}
	return fmt.Sprintf(".chart/%x", sha256.Sum256([]byte(helm.Chart+":"+helm.Repo+":"+helm.Version)))
}

func createChartDir(dir string) error {
	// create the chart directories and copy the files
	chartDir := filepath.Join(dir, chartName)
	if err := os.Mkdir(chartDir, 0755); err != nil {
		return err
	}

	templatesDir := filepath.Join(chartDir, "templates")
	if err := os.Mkdir(templatesDir, 0755); err != nil {
		return err
	}
	if err := createFileFromString(chartDir, "Chart.yaml", chartYAML); err != nil {
		return err
	}
	if err := createFileFromString(chartDir, "values.yaml", values); err != nil {
		return err
	}
	if err := createFileFromString(templatesDir, "deployment.yaml", deployment); err != nil {
		return err
	}

	return nil
}

func compressFolder(src string, buf io.Writer) error {
	zr := gzip.NewWriter(buf)
	defer zr.Close()
	tw := tar.NewWriter(zr)
	defer tw.Close()

	return filepath.Walk(src, func(file string, fi os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		header, err := tar.FileInfoHeader(fi, file)
		if err != nil {
			return err
		}

		relPath, err := filepath.Rel(src, file)
		if err != nil {
			return err
		}
		header.Name = filepath.ToSlash(relPath)

		if err := tw.WriteHeader(header); err != nil {
			return err
		}

		if !fi.IsDir() {
			data, err := os.Open(file)
			if err != nil {
				return err
			}
			defer data.Close()

			_, err = io.Copy(tw, data)
			if err != nil {
				return err
			}
		}
		return nil
	})
}

func createFileFromString(dir, fileName, data string) error {
	path := filepath.Join(dir, fileName)
	return os.WriteFile(path, []byte(data), 0644)
}

func createHelmChartGZIP() (string, string, error) {
	temp, err := os.MkdirTemp("", "charts_tmp")
	if err != nil {
		return "", "", err
	}
	defer os.RemoveAll(temp)

	if err := createChartDir(temp); err != nil {
		return "", "", err
	}

	var buf bytes.Buffer
	if err := compressFolder(temp, &buf); err != nil {
		return "", "", err
	}

	finalDir, err := os.MkdirTemp("", "chart")
	if err != nil {
		return "", "", err
	}

	gzipPath := filepath.Join(finalDir, "sleeper-chart-0.1.0.tgz")
	err = os.WriteFile(gzipPath, buf.Bytes(), os.ModePerm)
	if err != nil {
		return finalDir, "", err
	}

	return finalDir, gzipPath, nil
}

func newTLSServer(index string, withAuth bool) *httptest.Server {
	srv := httptest.NewTLSServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		if withAuth {
			username, password, ok := r.BasicAuth()
			if ok {
				usernameHash := sha256.Sum256([]byte(username))
				passwordHash := sha256.Sum256([]byte(password))
				expectedUsernameHash := sha256.Sum256([]byte(authUsername))
				expectedPasswordHash := sha256.Sum256([]byte(authPassword))

				usernameMatch := (subtle.ConstantTimeCompare(usernameHash[:], expectedUsernameHash[:]) == 1)
				passwordMatch := (subtle.ConstantTimeCompare(passwordHash[:], expectedPasswordHash[:]) == 1)

				if !usernameMatch || !passwordMatch {
					w.Header().Set("WWW-Authenticate", `Basic realm="restricted", charset="UTF-8"`)
					http.Error(w, "Unauthorized", http.StatusUnauthorized)
					return
				}
			}
		}

		w.WriteHeader(http.StatusOK)
		switch r.URL.Path {
		case "/index.yaml":
			index = strings.ReplaceAll(index, "##URL##", r.Host)
			fmt.Fprint(w, index)
		case "/sleeper-chart-0.1.0.tgz":
			dir, chartPath, err := createHelmChartGZIP()
			if dir != "" {
				defer os.RemoveAll(dir)
			}

			if err != nil {
				fmt.Printf("%v", err)
				w.WriteHeader(http.StatusInternalServerError)
				fmt.Fprint(w, err.Error())
				return
			}
			f, err := os.Open(chartPath)
			if err != nil {
				w.WriteHeader(http.StatusInternalServerError)
				fmt.Fprint(w, err.Error())
			}
			defer f.Close()

			_, err = io.Copy(w, f)
			if err != nil {
				w.WriteHeader(http.StatusInternalServerError)
				fmt.Fprint(w, err.Error())
			}
		}
	}))
	return srv
}

func TestGetManifestFromHelmChart(t *testing.T) {
	cases := []struct {
		name                string
		bd                  fleet.BundleDeployment
		readerCalls         func(*mocks.MockReader)
		requiresAuth        bool
		expectedNilManifest bool
		expectedResources   []fleet.BundleResource
		expectedErrNotNil   bool
		expectedError       string
	}{
		{
			name: "no helm options",
			bd: fleet.BundleDeployment{
				Spec: fleet.BundleDeploymentSpec{
					Options: fleet.BundleDeploymentOptions{
						Helm: nil,
					},
				},
			},
			readerCalls:         func(c *mocks.MockReader) {},
			requiresAuth:        false,
			expectedNilManifest: true,
			expectedResources:   []fleet.BundleResource{},
			expectedErrNotNil:   true,
			expectedError:       "helm options not found",
		},
		{
			name: "error reading secret",
			bd: fleet.BundleDeployment{
				Spec: fleet.BundleDeploymentSpec{
					Options: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{},
					},
					HelmChartOptions: &fleet.BundleHelmOptions{
						SecretName: "invalid-secret",
					},
				},
			},
			readerCalls: func(c *mocks.MockReader) {
				c.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any()).Return(fmt.Errorf("secret not found"))
			},
			requiresAuth:        false,
			expectedNilManifest: true,
			expectedResources:   []fleet.BundleResource{},
			expectedErrNotNil:   true,
			expectedError:       "secret not found",
		},
		{
			name: "authentication error",
			bd: fleet.BundleDeployment{
				Spec: fleet.BundleDeploymentSpec{
					Options: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							Repo: "##URL##", // will be replaced by the mock server url
						},
					},
					HelmChartOptions: &fleet.BundleHelmOptions{
						SecretName:            "secretdoesnotexist",
						InsecureSkipTLSverify: true,
					},
				},
			},
			readerCalls: func(c *mocks.MockReader) {
				c.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any()).DoAndReturn(
					func(_ context.Context, _ types.NamespacedName, secret *corev1.Secret, _ ...interface{}) error {
						secret.Data = make(map[string][]byte)
						secret.Data[corev1.BasicAuthUsernameKey] = []byte(authUsername)
						secret.Data[corev1.BasicAuthPasswordKey] = []byte("bad password")
						return nil
					},
				)
			},
			requiresAuth:        true,
			expectedNilManifest: true,
			expectedResources:   []fleet.BundleResource{},
			expectedErrNotNil:   true,
			expectedError:       "failed to read helm repo from ##URL##/index.yaml, error code: 401",
		},
		{
			name: "tls error",
			bd: fleet.BundleDeployment{
				Spec: fleet.BundleDeploymentSpec{
					Options: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							Repo: "##URL##", // will be replaced by the mock server url
						},
					},
					HelmChartOptions: &fleet.BundleHelmOptions{
						SecretName:            "secretdoesnotexist",
						InsecureSkipTLSverify: false,
					},
				},
			},
			readerCalls: func(c *mocks.MockReader) {
				c.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any()).Return(nil)
			},
			requiresAuth:        false,
			expectedNilManifest: true,
			expectedResources:   []fleet.BundleResource{},
			expectedErrNotNil:   true,
			expectedError:       "Get \"##URL##/index.yaml\": tls: failed to verify certificate: x509: certificate signed by unknown authority",
		},
		{
			name: "load directory no version specified",
			bd: fleet.BundleDeployment{
				Spec: fleet.BundleDeploymentSpec{
					Options: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							Repo:  "##URL##", // will be replaced by the mock server url
							Chart: "sleeper",
						},
					},
					HelmChartOptions: &fleet.BundleHelmOptions{
						InsecureSkipTLSverify: true,
					},
				},
			},
			readerCalls:         func(c *mocks.MockReader) {},
			requiresAuth:        false,
			expectedNilManifest: false,
			expectedResources: []fleet.BundleResource{
				{
					Name:    "sleeper-chart/templates/deployment.yaml",
					Content: deployment,
				},
				{
					Name:    "sleeper-chart/values.yaml",
					Content: values,
				},
				{
					Name:    "sleeper-chart/Chart.yaml",
					Content: chartYAML,
				},
			},
			expectedErrNotNil: false,
			expectedError:     "",
		},
	}

	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	mockUpstreamReader := mocks.NewMockReader(mockCtrl)

	assert := assert.New(t)
	for _, c := range cases {
		// set expected calls to client mock
		c.readerCalls(mockUpstreamReader)

		// start mock server for test
		srv := newTLSServer(helmRepoIndex, c.requiresAuth)
		defer srv.Close()

		resourcePrefix := ""
		if c.bd.Spec.Options.Helm != nil {
			c.bd.Spec.Options.Helm.Repo = strings.ReplaceAll(c.bd.Spec.Options.Helm.Repo, "##URL##", srv.URL)
			// resource names have a prefix that depends on helm options
			resourcePrefix = checksumPrefix(c.bd.Spec.Options.Helm)
		}
		// change the url in the error in case it is present
		c.expectedError = strings.ReplaceAll(c.expectedError, "##URL##", srv.URL)

		manifest, err := bundlereader.GetManifestFromHelmChart(context.TODO(), mockUpstreamReader, &c.bd)

		assert.Equal(c.expectedNilManifest, manifest == nil)
		assert.Equal(c.expectedErrNotNil, err != nil)
		if err != nil && c.expectedErrNotNil {
			assert.Contains(err.Error(), c.expectedError)
		}
		if manifest != nil {
			// check that all expected resources are found
			for _, expectedRes := range c.expectedResources {
				// find the resource in the expected ones
				found := false
				for _, r := range manifest.Resources {
					if fmt.Sprintf("%s/%s", resourcePrefix, expectedRes.Name) == r.Name {
						found = true
						assert.Equal(expectedRes.Content, r.Content)
					}
				}
				if !found {
					t.Errorf("expected resource %s was not found", expectedRes.Name)
				}
			}

			// check that all of the returned resources are also expected
			for _, r := range manifest.Resources {
				// find the resource in the expected ones
				found := false
				for _, expectedRes := range c.expectedResources {
					if fmt.Sprintf("%s/%s", resourcePrefix, expectedRes.Name) == r.Name {
						found = true
						assert.Equal(expectedRes.Content, r.Content)
					}
				}
				if !found {
					t.Errorf("returned resource %s was not expected", r.Name)
				}
			}
		}
	}
}



================================================
FILE: internal/bundlereader/loaddirectory.go
================================================
package bundlereader

import (
	"bufio"
	"context"
	"crypto/tls"
	"crypto/x509"
	"encoding/base64"
	"fmt"
	"io/fs"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"regexp"
	"slices"
	"strings"
	"unicode/utf8"

	"github.com/hashicorp/go-getter/v2"
	"github.com/rancher/fleet/internal/content"
	"github.com/rancher/fleet/internal/helmupdater"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"helm.sh/helm/v4/pkg/downloader"
	helmgetter "helm.sh/helm/v4/pkg/getter"
	"helm.sh/helm/v4/pkg/registry"
)

// ignoreTree represents a tree of ignored paths (read from .fleetignore files), each node being a directory.
// It provides a means for ignored paths to be propagated down the tree, but not between subdirectories of a same
// directory.
type ignoreTree struct {
	path         string
	ignoredPaths []string
	children     []*ignoreTree
}

// isIgnored checks whether any path within xt matches path, and returns true if so.
func (xt *ignoreTree) isIgnored(path string, info fs.DirEntry) (bool, error) {
	steps := xt.findNode(path, false, nil)

	for _, step := range steps {
		for _, ignoredPath := range step.ignoredPaths {
			if isAllFilesInDirPattern(ignoredPath) {
				// ignores a folder
				if info.IsDir() {
					dirNameInPattern := strings.TrimSuffix(ignoredPath, "/*")
					if dirNameInPattern == filepath.Base(path) {
						return true, nil
					}
				}
			} else {
				toIgnore, err := filepath.Match(ignoredPath, filepath.Base(path))
				if err != nil {
					return false, err
				}

				if toIgnore {
					return true, nil
				}
			}
		}
	}

	return false, nil
}

func isAllFilesInDirPattern(path string) bool {
	match, _ := regexp.MatchString("^.+/\\*", path)
	return match
}

// addNode reads a `.fleetignore` file in dir's root and adds each of its entries to ignored paths for dir.
// Returns an error if a `.fleetignore` file exists for dir but reading it fails.
func (xt *ignoreTree) addNode(dir string) error {
	toIgnore, err := readFleetIgnore(dir)
	if err != nil {
		return fmt.Errorf("read .fleetignore for %s: %w", dir, err)
	}

	if len(toIgnore) == 0 {
		return nil
	}

	steps := xt.findNode(dir, true, nil)
	if steps == nil {
		return fmt.Errorf("ignore tree node not found for path %q", dir)
	}

	destNode := steps[len(steps)-1]
	destNode.ignoredPaths = append(destNode.ignoredPaths, toIgnore...)

	return nil
}

// findNode finds the right node for path, creating that node if needed and if isDir is true.
// Returns a slice representing all relevant nodes in the path to the destination, in order of traversal from the root.
// The last element of that slice is the destination node.
func (xt *ignoreTree) findNode(path string, isDir bool, nodesRoute []*ignoreTree) []*ignoreTree {
	// The path doesn't even belong in the tree. This should never happen.
	if !strings.HasPrefix(path, xt.path) {
		return nil
	}

	nodesRoute = append(nodesRoute, xt)

	if path == xt.path {
		return nodesRoute
	}

	for _, c := range xt.children {
		if steps := c.findNode(path, isDir, nodesRoute); steps != nil {
			return append(nodesRoute, steps...)
		}
	}

	if isDir {
		xt.children = append(xt.children, &ignoreTree{path: path})

		createdChild := xt.children[len(xt.children)-1]

		return append(nodesRoute, createdChild)
	}

	return append(nodesRoute, xt)
}

// readFleetIgnore reads a possible .fleetignore file within path and returns its entries as a slice of strings.
// If no .fleetignore exists, then an empty slice and a nil error are returned.
// If an error happens while opening an existing .fleetignore file, that error is returned along with an empty slice.
func readFleetIgnore(path string) ([]string, error) {
	file, err := os.Open(filepath.Join(path, ".fleetignore"))
	if err != nil {
		// No ignored paths to add if no .fleetignore exists.
		if os.IsNotExist(err) {
			return nil, nil
		}
		return nil, err
	}

	defer file.Close()

	scanner := bufio.NewScanner(file)
	scanner.Split(bufio.ScanLines)

	var ignored []string

	trailingSpaceRegex := regexp.MustCompile(`([^\\])\s+$`)

	for scanner.Scan() {
		path := scanner.Text()

		// Trim trailing spaces unless escaped.
		path = trailingSpaceRegex.ReplaceAllString(path, "$1")

		// Ignore empty lines and comments (although they should not match any file).
		if path == "" || strings.HasPrefix(path, "#") {
			continue
		}

		ignored = append(ignored, path)
	}

	return ignored, nil
}

func loadDirectory(ctx context.Context, opts loadOpts, dir directory) ([]fleet.BundleResource, error) {
	var resources []fleet.BundleResource

	files, err := GetContent(ctx, dir.base, dir.source, dir.version, dir.auth, opts.disableDepsUpdate, opts.ignoreApplyConfigs)
	if err != nil {
		return nil, err
	}

	for name, data := range files {
		r := fleet.BundleResource{Name: name}
		if opts.compress || !utf8.Valid(data) {
			content, err := content.Base64GZ(data)
			if err != nil {
				return nil, err
			}
			r.Content = content
			r.Encoding = "base64+gz"
		} else {
			r.Content = string(data)
		}
		if dir.prefix != "" {
			r.Name = filepath.Join(dir.prefix, name)
		}
		resources = append(resources, r)
	}

	return resources, nil
}

// GetContent uses go-getter (and Helm for OCI) to read the files from directories and servers.
func GetContent(ctx context.Context, base, source, version string, auth Auth, disableDepsUpdate bool, ignoreApplyConfigs []string) (map[string][]byte, error) {
	temp, err := os.MkdirTemp("", "fleet")
	if err != nil {
		return nil, err
	}
	defer os.RemoveAll(temp)

	orgSource := source

	// go-getter does not support downloading OCI registry based files yet
	// until this is implemented we use Helm to download charts from OCI based registries
	// and provide the downloaded file to go-getter locally
	if strings.HasPrefix(source, ociURLPrefix) {
		source, err = downloadOCIChart(source, version, temp, auth)
		if err != nil {
			return nil, err
		}
	}

	temp = filepath.Join(temp, "content")

	base, err = filepath.Abs(base)
	if err != nil {
		return nil, err
	}

	if auth.SSHPrivateKey != nil {
		if !strings.ContainsAny(source, "?") {
			source += "?"
		} else {
			source += "&"
		}
		source += fmt.Sprintf("sshkey=%s", base64.StdEncoding.EncodeToString(auth.SSHPrivateKey))
	}

	customGetters := []getter.Getter{}
	for _, g := range getter.Getters {
		// Replace default HTTP(S) getter with our customized one
		if _, ok := g.(*getter.HttpGetter); ok {
			continue
		}
		customGetters = append(customGetters, g)
	}

	httpGetter := newHttpGetter(auth)
	customGetters = append(customGetters, httpGetter)

	client := &getter.Client{
		Getters: customGetters,
	}

	req := &getter.Request{
		Src:     source,
		Dst:     temp,
		Pwd:     base,
		GetMode: getter.ModeDir,
	}

	if _, err := client.Get(ctx, req); err != nil {
		return nil, err
	}

	files := map[string][]byte{}

	// dereference link if possible
	if dest, err := os.Readlink(temp); err == nil {
		temp = dest
	}

	ignoredPaths := ignoreTree{path: temp}

	err = filepath.WalkDir(temp, func(path string, info fs.DirEntry, err error) error {
		if err != nil {
			return err
		}

		name, err := filepath.Rel(temp, path)
		if err != nil {
			return err
		}

		ignore, err := ignoredPaths.isIgnored(path, info)
		if err != nil {
			return err
		}

		// ignore files containing only fleet apply config
		if slices.Contains(ignoreApplyConfigs, name) {
			return nil
		}

		if info.IsDir() {
			// If the folder is a helm chart and dependency updates are not disabled,
			// try to update possible dependencies.
			if !disableDepsUpdate && helmupdater.ChartYAMLExists(path) {
				if err = helmupdater.UpdateHelmDependencies(path); err != nil {
					return err
				}
			}
			// Skip .fleetignore'd and hidden directories
			if ignore || strings.HasPrefix(filepath.Base(path), ".") {
				return filepath.SkipDir
			}

			return ignoredPaths.addNode(path)
		}

		if ignore {
			return nil
		}

		// Skip hidden files
		if strings.HasPrefix(filepath.Base(name), ".") {
			return nil
		}

		content, err := os.ReadFile(path)
		if err != nil {
			return err
		}

		files[name] = content
		return nil
	})
	if err != nil {
		return nil, fmt.Errorf("failed to read %s relative to %s: %w", orgSource, base, err)
	}

	return files, nil
}

// downloadOCIChart uses Helm to download charts from OCI based registries
func downloadOCIChart(name, version, path string, auth Auth) (string, error) {
	var requiresLogin = auth.Username != "" && auth.Password != ""

	url, err := url.Parse(name)
	if err != nil {
		return "", err
	}

	temp, err := os.MkdirTemp("", "creds")
	if err != nil {
		return "", err
	}
	defer os.RemoveAll(temp)

	tmpGetter := newHttpGetter(auth)
	clientOptions := []registry.ClientOption{
		registry.ClientOptCredentialsFile(filepath.Join(temp, "creds.json")),
		registry.ClientOptHTTPClient(tmpGetter.Client),
	}
	if auth.BasicHTTP {
		clientOptions = append(clientOptions, registry.ClientOptPlainHTTP())
	}
	registryClient, err := registry.NewClient(clientOptions...)
	if err != nil {
		return "", err
	}

	// Helm does not support direct authentication for private OCI registries when a chart is downloaded
	// so it is necessary to login before via Helm which stores the registry token in a configuration
	// file on the system
	addr := url.Hostname()
	if requiresLogin {
		if port := url.Port(); port != "" {
			addr = fmt.Sprintf("%s:%s", addr, port)
		}

		err = registryClient.Login(
			addr,
			registry.LoginOptInsecure(auth.InsecureSkipVerify),
			registry.LoginOptBasicAuth(auth.Username, auth.Password),
		)
		if err != nil {
			return "", err
		}
	}

	getterOptions := []helmgetter.Option{}
	if auth.Username != "" && auth.Password != "" {
		getterOptions = append(getterOptions, helmgetter.WithBasicAuth(auth.Username, auth.Password))
	}
	getterOptions = append(getterOptions, helmgetter.WithInsecureSkipVerifyTLS(auth.InsecureSkipVerify))

	c := downloader.ChartDownloader{
		Verify:       downloader.VerifyNever,
		ContentCache: path, // Required in Helm v4
		Getters: helmgetter.Providers{
			helmgetter.Provider{
				Schemes: []string{registry.OCIScheme},
				New: func(options ...helmgetter.Option) (helmgetter.Getter, error) {
					return helmgetter.NewOCIGetter(helmgetter.WithRegistryClient(registryClient))
				},
			},
		},
		RegistryClient: registryClient,
		Options:        getterOptions,
	}

	saved, _, err := c.DownloadTo(name, version, path)
	if err != nil {
		return "", fmt.Errorf("helm chart download: %w", err)
	}

	// Logout to remove the token configuration file from the system again
	if requiresLogin {
		err = registryClient.Logout(addr)
		if err != nil {
			return "", err
		}
	}

	return saved, nil
}

func newHttpGetter(auth Auth) *getter.HttpGetter {
	httpGetter := &getter.HttpGetter{
		Client: &http.Client{},
	}

	if auth.Username != "" && auth.Password != "" {
		header := http.Header{}
		header.Add("Authorization", "Basic "+basicAuth(auth.Username, auth.Password))
		httpGetter.Header = header
	}

	transport := http.DefaultTransport.(*http.Transport).Clone()
	if auth.CABundle != nil {
		pool, err := x509.SystemCertPool()
		if err != nil {
			pool = x509.NewCertPool()
		}
		pool.AppendCertsFromPEM(auth.CABundle)
		transport.TLSClientConfig = &tls.Config{
			RootCAs:            pool,
			MinVersion:         tls.VersionTLS12,
			InsecureSkipVerify: auth.InsecureSkipVerify, //nolint:gosec
		}
	} else if auth.InsecureSkipVerify {
		transport.TLSClientConfig = &tls.Config{
			InsecureSkipVerify: auth.InsecureSkipVerify, //nolint:gosec
		}
	}
	httpGetter.Client.Transport = transport

	return httpGetter
}

func basicAuth(username, password string) string {
	auth := username + ":" + password
	return base64.StdEncoding.EncodeToString([]byte(auth))
}



================================================
FILE: internal/bundlereader/loaddirectory_test.go
================================================
package bundlereader_test

import (
	"context"
	"io"
	"net/http"
	"net/http/httptest"
	"os"
	"path/filepath"
	"regexp"
	"testing"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"github.com/rancher/fleet/internal/bundlereader"
)

// fsNode represents a directory structure used to model `.fleetignore` test cases.
type fsNode struct {
	name string

	contents string   // if not a directory
	children []fsNode // non-empty only in case of a directory

	isDir bool
}

func TestGetContent(t *testing.T) {
	cases := []struct {
		name               string
		directoryStructure fsNode
		expectedFiles      map[string][]byte
		source             string
		auth               bundlereader.Auth
		expectedErr        *regexp.Regexp
	}{
		{
			name: "ensure panic doesn't occur when InsecureSkipVerify is set to false (#3782)",
			directoryStructure: fsNode{
				name: "fleet.yaml",
				contents: `namespace: fleet-helm-oci-with-auth-example
		 helm:
		   chart: "oci://ghcr.io/fleetqa/fleet-qa-examples/fleet-test-configmap-chart"
		   version: "0.1.0"
		   values:
		     replicas: 2`,
			},
			source: "oci://foo/bar/baz",
			auth: bundlereader.Auth{
				Username: "foo",
				Password: "bar",
				// InsecureSkipVerify is false by default
			},
			expectedErr: regexp.MustCompile("(no such host|server misbehaving)"),
		},
		{
			name: "no .fleetignore",
			directoryStructure: fsNode{
				isDir: true,
				name:  "no-fleetignore",
				children: []fsNode{
					{
						name:     "fleet.yaml",
						contents: "foo",
					},
					{
						name:  "chart",
						isDir: true,
						children: []fsNode{
							{
								name:     "myvalues.yaml",
								contents: "bar",
							},
						},
					},
					{
						name:     "something.yaml",
						contents: "foo",
					},
				},
			},
			expectedFiles: map[string][]byte{
				"something.yaml": []byte("foo"),
			},
		},
		{
			name: "empty .fleetignore",
			directoryStructure: fsNode{
				isDir: true,
				name:  "empty-fleetignore",
				children: []fsNode{
					{
						name:     "fleet.yaml",
						contents: "foo",
					},
					{
						name:     "something.yaml",
						contents: "foo",
					},
					{
						name:     ".fleetignore",
						contents: "",
					},
				},
			},
			expectedFiles: map[string][]byte{
				"something.yaml": []byte("foo"),
			},
		},
		{
			name: "ignore lines with leading # unless escaped",
			directoryStructure: fsNode{
				isDir: true,
				name:  "comments",
				children: []fsNode{
					{
						name:     "something.yaml",
						contents: "foo",
					},
					{
						name:     "#something_else.yaml",
						contents: "bar",
					},
					{
						name:     ".fleetignore",
						contents: "#something.yaml\n\\#something_else.yaml",
					},
				},
			},
			expectedFiles: map[string][]byte{
				"something.yaml": []byte("foo"),
			},
		},
		{
			name: "simple .fleetignore",
			directoryStructure: fsNode{
				isDir: true,
				name:  "simple-fleetignore",
				children: []fsNode{
					{
						name:     "something.yaml",
						contents: "foo",
					},
					{
						name:     "something_else.yaml",
						contents: "bar",
					},
					{
						name:     ".fleetignore",
						contents: "not_here.txt\nsomething.yaml",
					},
				},
			},
			expectedFiles: map[string][]byte{
				"something_else.yaml": []byte("bar"),
			},
		},
		{
			name: "glob syntax",
			directoryStructure: fsNode{
				isDir: true,
				name:  "glob-syntax",
				children: []fsNode{
					{
						name:     "something.yaml",
						contents: "foo",
					},
					{
						name:     ".fleetignore",
						contents: "something*",
					},
				},
			},
			expectedFiles: map[string][]byte{},
		},
		{
			name: "ignore trailing spaces unless escaped",
			directoryStructure: fsNode{
				isDir: true,
				name:  "trim-space",
				children: []fsNode{
					{
						name:     "something.yaml ",
						contents: "foo",
					},
					{
						name:     "something_else.yaml  ",
						contents: "bar",
					},
					{
						name:     ".fleetignore",
						contents: "something_else.yaml\\ \\ \nsomething.yaml ",
					},
				},
			},
			expectedFiles: map[string][]byte{
				"something.yaml ": []byte("foo"),
			},
		},
		{
			name: "ignore directories",
			directoryStructure: fsNode{
				isDir: true,
				name:  "ignore-directories",
				children: []fsNode{
					{
						name:     "something.yaml",
						contents: "foo",
					},
					{
						name:     ".fleetignore",
						contents: "subdir",
					},
					{
						name:  "subdir",
						isDir: true,
						children: []fsNode{
							{
								name:     "in_dir.yaml",
								contents: "baz",
							},
						},
					},
				},
			},
			expectedFiles: map[string][]byte{
				"something.yaml": []byte("foo"),
			},
		},
		{
			name: "ignore file multiple levels below .fleetignore",
			directoryStructure: fsNode{
				isDir: true,
				name:  "ignore-file-multiple-levels",
				children: []fsNode{
					{
						name:     "something.yaml",
						contents: "foo",
					},
					{
						name:     ".fleetignore",
						contents: "in_dir.yaml",
					},
					{
						name:  "subdir",
						isDir: true,
						children: []fsNode{
							{
								name:  "subsubdir",
								isDir: true,
								children: []fsNode{
									{
										name:     "in_dir.yaml",
										contents: "bar",
									},
								},
							},
						},
					},
				},
			},
			expectedFiles: map[string][]byte{
				"something.yaml": []byte("foo"),
			},
		},
		{
			name: ".fleetignore files in neighbour dirs do not interfere with one another",
			directoryStructure: fsNode{
				isDir: true,
				name:  "multiple-files-same-level",
				children: []fsNode{
					{
						name:     "something.yaml",
						contents: "foo",
					},
					{
						name:  "subdir1",
						isDir: true,
						children: []fsNode{
							{
								name:     "in_dir.yaml",
								contents: "from dir 1",
							},
							{
								name:     ".fleetignore",
								contents: "in_dir.yaml",
							},
						},
					},
					{
						name:  "subdir2",
						isDir: true,
						children: []fsNode{
							{
								name:     "in_dir.yaml",
								contents: "from dir 2",
							},
						},
					},
				},
			},
			expectedFiles: map[string][]byte{
				"something.yaml":      []byte("foo"),
				"subdir2/in_dir.yaml": []byte("from dir 2"),
			},
		},
		{
			name: "entries from parent directories' .fleetignore files are added in lower directories",
			directoryStructure: fsNode{
				isDir: true,
				name:  "add-parent-entries",
				children: []fsNode{
					{
						name:     "something.yaml",
						contents: "foo",
					},
					{
						name:     ".fleetignore",
						contents: "ignore-always.yaml",
					},
					{
						name:  "foo",
						isDir: true,
						children: []fsNode{
							{
								name:     "ignore-always.yaml",
								contents: "will be ignored",
							},
							{
								name:     "something.yaml",
								contents: "something",
							},
						},
					},
					{
						name:  "bar",
						isDir: true,
						children: []fsNode{
							{
								name:     ".fleetignore",
								contents: "something.yaml",
							},
							{
								name:     "something.yaml",
								contents: "something",
							},
							{
								name:     "something2.yaml",
								contents: "something2",
							},
							{
								name:     "ignore-always.yaml",
								contents: "will be ignored",
							},
						},
					},
				},
			},
			expectedFiles: map[string][]byte{
				"something.yaml":      []byte("foo"),
				"foo/something.yaml":  []byte("something"),
				"bar/something2.yaml": []byte("something2"),
			},
		},
		{
			name: "root .fleetignore contains folder/* entries",
			directoryStructure: fsNode{
				isDir: true,
				name:  "root-fleetignore-all-files-in-dir",
				children: []fsNode{
					{
						name:     "something.yaml",
						contents: "foo",
					},
					{
						name:     ".fleetignore",
						contents: "foo/*\n",
					},
					{
						name:  "foo",
						isDir: true,
						children: []fsNode{
							{
								name:     "ignore-always.yaml",
								contents: "will be ignored",
							},
							{
								name:     "something.yaml",
								contents: "will be ignored",
							},
						},
					},
					{
						name:  "bar",
						isDir: true,
						children: []fsNode{
							{
								name:     "something.yaml",
								contents: "something",
							},
							{
								name:     "something2.yaml",
								contents: "something2",
							},
							{
								name:  "foo",
								isDir: true,
								children: []fsNode{
									{
										name:     "ignore.yaml",
										contents: "will be ignored",
									},
									{
										name:     "ignore2.yaml",
										contents: "will be ignored",
									},
									{
										name:     "something.yaml",
										contents: "will be ignored",
									},
								},
							},
						},
					},
				},
			},
			expectedFiles: map[string][]byte{
				"something.yaml":      []byte("foo"),
				"bar/something.yaml":  []byte("something"),
				"bar/something2.yaml": []byte("something2"),
			},
		},
		{
			name: "non root .fleetignore contains folder/* entries",
			directoryStructure: fsNode{
				isDir: true,
				name:  "non-root-fleetignore-all-files-in-dir",
				children: []fsNode{
					{
						name:     "something.yaml",
						contents: "foo",
					},
					{
						name:  "foo",
						isDir: true,
						children: []fsNode{
							{
								name:     "something1.yaml",
								contents: "something1",
							},
							{
								name:     "something2.yaml",
								contents: "something2",
							},
						},
					},
					{
						name:  "bar",
						isDir: true,
						children: []fsNode{
							{
								name:     "something.yaml",
								contents: "something",
							},
							{
								name:     "something2.yaml",
								contents: "something2",
							},
							{
								name:     ".fleetignore",
								contents: "foo/*\n",
							},
							{
								name:  "foo",
								isDir: true,
								children: []fsNode{
									{
										name:     "ignore.yaml",
										contents: "will be ignored",
									},
									{
										name:     "ignore2.yaml",
										contents: "will be ignored",
									},
									{
										name:     "something.yaml",
										contents: "will be ignored",
									},
								},
							},
						},
					},
				},
			},
			expectedFiles: map[string][]byte{
				"something.yaml":      []byte("foo"),
				"foo/something1.yaml": []byte("something1"),
				"foo/something2.yaml": []byte("something2"),
				"bar/something.yaml":  []byte("something"),
				"bar/something2.yaml": []byte("something2"),
			},
		},
		{
			name: ".fleetignore contains folder/* entry does not apply to files",
			directoryStructure: fsNode{
				isDir: true,
				name:  "fleetignore-all-files-in-dir-does-not-apply-to-files",
				children: []fsNode{
					{
						name:     "something.yaml",
						contents: "foo",
					},
					{
						name:     ".fleetignore",
						contents: "foo/*\n",
					},
					{
						name:     "foo",
						contents: "everybody was a kung-foo fighting",
					},
					{
						name:  "bar",
						isDir: true,
						children: []fsNode{
							{
								name:     "something.yaml",
								contents: "something",
							},
							{
								name:     "something2.yaml",
								contents: "something2",
							},
							{
								name:     ".fleetignore",
								contents: "foo/*\n",
							},
							{
								name:  "foo",
								isDir: true,
								children: []fsNode{
									{
										name:     "ignore.yaml",
										contents: "will be ignored",
									},
									{
										name:     "ignore2.yaml",
										contents: "will be ignored",
									},
									{
										name:     "something.yaml",
										contents: "will be ignored",
									},
								},
							},
						},
					},
				},
			},
			expectedFiles: map[string][]byte{
				"something.yaml":      []byte("foo"),
				"foo":                 []byte("everybody was a kung-foo fighting"),
				"bar/something.yaml":  []byte("something"),
				"bar/something2.yaml": []byte("something2"),
			},
		},
	}

	base, err := os.MkdirTemp("", "test-fleet")
	require.NoError(t, err)

	defer os.RemoveAll(base)

	ignoreApplyConfigs := []string{"fleet.yaml", "chart/myvalues.yaml"}

	for _, c := range cases {
		t.Run(c.name, func(t *testing.T) {

			root := createDirStruct(t, base, c.directoryStructure)

			if c.source == "" {
				c.source = root
			}
			files, err := bundlereader.GetContent(context.Background(), root, c.source, "", c.auth, false, ignoreApplyConfigs)
			if c.expectedErr == nil {
				require.NoError(t, err)
			} else if !c.expectedErr.Match([]byte(err.Error())) {
				assert.Failf(t, "expected error to match", "expected: %s, got: %s", c.expectedErr.String(), err.Error())
			}

			assert.Len(t, files, len(c.expectedFiles))
			for k, v := range c.expectedFiles {
				assert.Equal(t, v, files[k])
			}
		})
	}
}

type authTester struct {
	t    *testing.T
	want string
}

func (a *authTester) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodGet {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}
	url, err := r.URL.Parse(r.URL.String())
	if err != nil {
		http.Error(w, "Invalid URL", http.StatusBadRequest)
		return
	}
	if sskey := url.Query().Get("sshkey"); sskey != a.want {
		a.t.Errorf("wrong or no sshkey query parameter: want %s but got %s", a.want, sskey)
	}
	w.WriteHeader(http.StatusOK)
}

func TestGetContentSSHKey(t *testing.T) {
	cases := []struct {
		name, want string
		auth       bundlereader.Auth
	}{
		{
			name: "any URL with SSHPrivateKey set should be queried with sshkey query parameter",
			auth: bundlereader.Auth{
				SSHPrivateKey: []byte("foo"),
			},
			want: "Zm9v", // base64 encoding of "foo"
		},
		{
			name: "no query parameter if SSHPrivateKey is not set",
			auth: bundlereader.Auth{},
		},
	}
	for _, c := range cases {
		t.Run(c.name, func(t *testing.T) {
			authTester := &authTester{t: t, want: c.want}
			s := httptest.NewServer(authTester)
			defer s.Close()

			base, err := os.MkdirTemp("", "test-fleet")
			require.NoError(t, err)
			defer os.RemoveAll(base)

			_, _ = bundlereader.GetContent(context.Background(), base, s.URL, "", c.auth, false, []string{})
		})
	}
}

func TestGetContentOCI(t *testing.T) {
	cases := []struct {
		name    string
		source  string
		version string

		result      []string
		expectedErr string
	}{
		// Note: These tests rely on external hosts and DNS resolution.
		// We could just test the helm registry client is initialized
		// correctly, however for now these tests document different
		// scenarios nicely.
		{
			name:   "OCI URL without version",
			source: "oci://ghcr.io/rancher/fleet-test-configmap-chart",
			result: []string{
				"fleet-test-configmap-chart/Chart.yaml",
				"fleet-test-configmap-chart/values.yaml",
				"fleet-test-configmap-chart/templates/configmap.yaml",
			},
		},
		{
			name:    "OCI URL with version",
			source:  "oci://ghcr.io/rancher/fleet-test-configmap-chart",
			version: "0.1.0",
			result: []string{
				"fleet-test-configmap-chart/Chart.yaml",
				"fleet-test-configmap-chart/values.yaml",
				"fleet-test-configmap-chart/templates/configmap.yaml",
			},
		},
		{
			name:        "OCI URL with invalid version",
			source:      "oci://ghcr.io/rancher/fleet-test-configmap-chart",
			version:     "latest",
			expectedErr: "helm chart download: improper constraint: latest",
		},
		{
			name:        "Non-existing OCI URL without version",
			source:      "oci://non-existing-hostname/charts/chart",
			expectedErr: "dial tcp: lookup non-existing-hostname",
		},
		{
			name:        "Non-existing OCI URL with invalid version",
			source:      "oci://non-existing-hostname/charts/chart",
			version:     "latest",
			expectedErr: "dial tcp: lookup non-existing-hostname",
		},
		{
			name:        "OCI URL which includes version too",
			source:      "oci://ghcr.io/rancher/fleet-test-configmap-chart:1234.0",
			version:     "1.0",
			expectedErr: "chart reference and version mismatch: 1.0 is not 1234.0",
		},
		{
			name:        "Non-existing OCI URL with valid semver",
			source:      "oci://non-existing-hostname/charts/chart",
			version:     "1.0",
			expectedErr: "helm chart download: failed to perform",
		},
	}

	assert := assert.New(t)

	base, err := os.MkdirTemp("", "test-fleet")
	require.NoError(t, err)

	defer os.RemoveAll(base)

	for _, c := range cases {
		t.Run(c.name, func(t *testing.T) {
			result, err := bundlereader.GetContent(context.Background(), base, c.source, c.version, bundlereader.Auth{}, false, []string{})
			if c.expectedErr == "" {
				require.NoError(t, err)
				for k := range result {
					assert.Contains(c.result, k)
				}
			} else {
				assert.ErrorContains(err, c.expectedErr, c.name)
			}
		})
	}
}

// createDirStruct generates and populates a directory structure which root is node, placing it at basePath.
func createDirStruct(t *testing.T, basePath string, node fsNode) string {
	t.Helper()
	path := filepath.Join(basePath, node.name)

	if !node.isDir {
		f, err := os.Create(path)
		require.NoError(t, err)

		_, err = io.WriteString(f, node.contents)
		require.NoError(t, err)

		return ""
	}

	err := os.Mkdir(path, 0777)
	require.NoError(t, err)

	for _, c := range node.children {
		createDirStruct(t, path, c)
	}

	return path
}



================================================
FILE: internal/bundlereader/read.go
================================================
// Package bundlereader creates a bundle from a source and adds all the
// referenced resources, as well as image scans.
package bundlereader

import (
	"bytes"
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"strconv"

	"github.com/rancher/fleet/internal/fleetyaml"
	"github.com/rancher/fleet/internal/names"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"github.com/sirupsen/logrus"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/yaml"
)

// Options include the GitRepo overrides, which are passed via command line args
type Options struct {
	BundleFile       string
	Compress         bool
	Labels           map[string]string
	ServiceAccount   string
	TargetsFile      string
	TargetNamespace  string
	Paused           bool
	SyncGeneration   int64
	Auth             Auth
	HelmRepoURLRegex string
	KeepResources    bool
	DeleteNamespace  bool
	CorrectDrift     *fleet.CorrectDrift
}

// NewBundle reads the fleet.yaml, from stdin, or basedir, or a file in basedir.
// Then it reads/downloads all referenced resources. It returns the populated
// bundle and any existing imagescans.
func NewBundle(ctx context.Context, name, baseDir, file string, opts *Options) (*fleet.Bundle, []*fleet.ImageScan, error) {
	if baseDir == "" {
		baseDir = "."
	}

	if file == "-" {
		b, s, err := loadBundle(ctx, name, baseDir, os.Stdin, opts)
		if err != nil {
			return b, s, fmt.Errorf("failed to process bundle from STDIN: %w", err)
		}
	}

	var (
		in io.Reader
	)

	if file == "" {
		if file, err := setupIOReader(baseDir); err != nil {
			return nil, nil, fmt.Errorf("failed to open existing fleet.yaml in %q: %w", baseDir, err)
		} else if file != nil {
			in = file
			defer file.Close()
		} else {
			// Create a new buffer if opening both files resulted in "IsNotExist" errors.
			in = bytes.NewBufferString("{}")
		}
	} else {
		f, err := os.Open(filepath.Join(baseDir, file))
		if err != nil {
			return nil, nil, fmt.Errorf("failed to open file %q: %w", file, err)
		}
		defer f.Close()
		in = f
	}

	b, s, err := loadBundle(ctx, name, baseDir, in, opts)
	if err != nil {
		return b, s, fmt.Errorf("failed to process bundle: %w", err)
	}

	return b, s, nil
}

// Try accessing the documented, primary fleet.yaml extension first. If that returns an "IsNotExist" error, then we
// try the fallback extension. If we receive "IsNotExist" errors for both file extensions, then we return a "nil" file
// and a "nil" error. If either return a non-"IsNotExist" error, then we return the error immediately.
func setupIOReader(baseDir string) (*os.File, error) {
	if file, err := os.Open(fleetyaml.GetFleetYamlPath(baseDir, false)); err != nil && !os.IsNotExist(err) {
		return nil, err
	} else if err == nil {
		// File must be closed in the parent function.
		return file, nil
	}

	if file, err := os.Open(fleetyaml.GetFleetYamlPath(baseDir, true)); err != nil && !os.IsNotExist(err) {
		return nil, err
	} else if err == nil {
		// File must be closed in the parent function.
		return file, nil
	}

	return nil, nil
}

// loadBundle creates a bundle and imagescan from a base directory name and a reader (which may represent data from a
// directory structure or from standard input).
func loadBundle(ctx context.Context, name, baseDir string, bundleSpecReader io.Reader, opts *Options) (*fleet.Bundle, []*fleet.ImageScan, error) {
	if opts == nil {
		opts = &Options{}
	}

	data, err := io.ReadAll(bundleSpecReader)
	if err != nil {
		return nil, nil, err
	}

	bundle, scans, err := bundleFromDir(ctx, name, baseDir, data, opts)
	if err != nil {
		return nil, nil, err
	}

	if size, err := size(bundle); err != nil {
		return nil, nil, err
	} else if size < 1000000 {
		return bundle, scans, nil
	}

	newOpts := *opts
	newOpts.Compress = true
	return bundleFromDir(ctx, name, baseDir, data, &newOpts)
}

func size(bundle *fleet.Bundle) (int, error) {
	marshalled, err := json.Marshal(bundle)
	if err != nil {
		return 0, err
	}
	return len(marshalled), nil
}

// bundleFromDir reads the fleet.yaml from the bundleData and loads all resources
func bundleFromDir(ctx context.Context, name, baseDir string, bundleData []byte, opts *Options) (*fleet.Bundle, []*fleet.ImageScan, error) {
	if opts == nil {
		opts = &Options{}
	}

	if baseDir == "" {
		baseDir = "./"
	}

	fy := &fleet.FleetYAML{}
	if err := yaml.Unmarshal(bundleData, fy); err != nil {
		return nil, nil, fmt.Errorf("reading fleet.yaml: %w", err)
	}

	var scans []*fleet.ImageScan
	for i, scan := range fy.ImageScans {
		if scan.Image == "" {
			continue
		}
		if scan.TagName == "" {
			return nil, nil, errors.New("the name of scan is required")
		}

		scans = append(scans, &fleet.ImageScan{
			ObjectMeta: metav1.ObjectMeta{
				Name: names.SafeConcatName("imagescan", name, strconv.Itoa(i)),
			},
			Spec: scan.ImageScanSpec,
		})
	}

	fy.Targets = append(fy.Targets, fy.TargetCustomizations...)

	meta, err := readMetadata(bundleData)
	if err != nil {
		return nil, nil, err
	}

	meta.Name = name
	if fy.Name != "" {
		meta.Name = fy.Name
	}

	setTargetNames(&fy.BundleSpec)

	propagateHelmChartProperties(&fy.BundleSpec)

	resources, err := readResources(ctx, &fy.BundleSpec, opts.Compress, baseDir, opts.Auth, opts.HelmRepoURLRegex, opts.BundleFile)
	if err != nil {
		return nil, nil, fmt.Errorf("failed reading resources for %q: %w", baseDir, err)
	}

	fy.Resources = resources

	bundle := &fleet.Bundle{
		ObjectMeta: meta.ObjectMeta,
		Spec:       fy.BundleSpec,
	}

	for k, v := range opts.Labels {
		if bundle.Labels == nil {
			bundle.Labels = make(map[string]string)
		}
		bundle.Labels[k] = v
	}

	// apply additional labels from spec
	for k, v := range fy.Labels {
		if bundle.Labels == nil {
			bundle.Labels = make(map[string]string)
		}
		bundle.Labels[k] = v
	}

	if opts.ServiceAccount != "" {
		bundle.Spec.ServiceAccount = opts.ServiceAccount
	}

	bundle.Spec.ForceSyncGeneration = opts.SyncGeneration

	// Targets defined in the GitRepo are stored in the targets file, which will be used if OverrideTargets is not provided.
	// Use targets from OverrideTargets if found in the fleet.yaml.
	if fy.OverrideTargets != nil {
		logrus.Debugf("Overriding targets for Bundle '%s' ", bundle.Name)
		for _, target := range fy.OverrideTargets {
			bundle.Spec.Targets = append(bundle.Spec.Targets, fleet.BundleTarget{
				Name:                 target.Name,
				ClusterName:          target.ClusterName,
				ClusterSelector:      target.ClusterSelector,
				ClusterGroup:         target.ClusterGroup,
				ClusterGroupSelector: target.ClusterGroupSelector,
			})
			bundle.Spec.TargetRestrictions = append(bundle.Spec.TargetRestrictions, fleet.BundleTargetRestriction(target))
		}
	} else {
		bundle, err = appendTargets(bundle, opts.TargetsFile)
		if err != nil {
			return nil, nil, err
		}
	}

	if len(bundle.Spec.Targets) == 0 {
		bundle.Spec.Targets = []fleet.BundleTarget{
			{
				Name:         "default",
				ClusterGroup: "default",
			},
		}
	}

	if opts.TargetNamespace != "" {
		bundle.Spec.TargetNamespace = opts.TargetNamespace
		for i := range bundle.Spec.Targets {
			bundle.Spec.Targets[i].TargetNamespace = opts.TargetNamespace
		}
	}

	if opts.Paused {
		bundle.Spec.Paused = true
	}

	if opts.KeepResources {
		bundle.Spec.KeepResources = opts.KeepResources
	}

	if opts.DeleteNamespace {
		bundle.Spec.DeleteNamespace = opts.DeleteNamespace
	}

	if opts.CorrectDrift != nil && opts.CorrectDrift.Enabled {
		bundle.Spec.CorrectDrift = opts.CorrectDrift
	}

	return bundle, scans, nil
}

// propagateHelmChartProperties propagates root Helm chart properties to the child targets.
// This is necessary, so we can download the correct chart version for each target.
func propagateHelmChartProperties(spec *fleet.BundleSpec) {
	// Check if there is anything to propagate
	if spec.Helm == nil {
		return
	}
	for _, target := range spec.Targets {
		if target.Helm == nil {
			// This target has nothing to propagate to
			continue
		}
		if target.Helm.Repo == "" {
			target.Helm.Repo = spec.Helm.Repo
		}
		if target.Helm.Chart == "" {
			target.Helm.Chart = spec.Helm.Chart
		}
		if target.Helm.Version == "" {
			target.Helm.Version = spec.Helm.Version
		}
	}
}

func appendTargets(def *fleet.Bundle, targetsFile string) (*fleet.Bundle, error) {
	if targetsFile == "" {
		return def, nil
	}

	data, err := os.ReadFile(targetsFile)
	if err != nil {
		return nil, err
	}

	spec := &fleet.BundleSpec{}
	if err := yaml.Unmarshal(data, spec); err != nil {
		return nil, err
	}

	def.Spec.Targets = append(def.Spec.Targets, spec.Targets...)
	def.Spec.TargetRestrictions = append(def.Spec.TargetRestrictions, spec.TargetRestrictions...)

	return def, nil
}

func setTargetNames(spec *fleet.BundleSpec) {
	for i, target := range spec.Targets {
		if target.Name == "" {
			spec.Targets[i].Name = fmt.Sprintf("target%03d", i)
		}
	}
}

type bundleMeta struct {
	metav1.ObjectMeta `json:",inline,omitempty"`
}

func readMetadata(bytes []byte) (*bundleMeta, error) {
	temp := &bundleMeta{}
	return temp, yaml.Unmarshal(bytes, temp)
}



================================================
FILE: internal/bundlereader/resources.go
================================================
package bundlereader

import (
	"context"
	"crypto/sha256"
	"fmt"
	"os"
	"path/filepath"
	"regexp"
	"sort"
	"sync"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"golang.org/x/sync/errgroup"
	"golang.org/x/sync/semaphore"

	"github.com/rancher/wrangler/v3/pkg/data"

	"sigs.k8s.io/yaml"
)

const ociURLPrefix = "oci://"

// readResources reads and downloads all resources from the bundle. Resources
// can be downloaded and are spread across multiple directories.
func readResources(ctx context.Context, spec *fleet.BundleSpec, compress bool, base string, auth Auth, helmRepoURLRegex, bundleFile string) ([]fleet.BundleResource, error) {
	directories, err := addDirectory(base, ".", ".")
	if err != nil {
		return nil, err
	}

	var chartDirs []*fleet.HelmOptions

	if spec.Helm != nil && spec.Helm.Chart != "" {
		if err := parseValuesFiles(base, spec.Helm); err != nil {
			return nil, err
		}
		chartDirs = append(chartDirs, spec.Helm)
	}

	for _, target := range spec.Targets {
		if target.Helm != nil {
			err := parseValuesFiles(base, target.Helm)
			if err != nil {
				return nil, err
			}
			if target.Helm.Chart != "" {
				chartDirs = append(chartDirs, target.Helm)
			}
		}
	}

	directories, err = addRemoteCharts(ctx, directories, base, chartDirs, auth, helmRepoURLRegex)
	if err != nil {
		return nil, fmt.Errorf("failed to add directory for chart: %w", err)
	}

	// helm chart dependency update is enabled by default
	disableDepsUpdate := false
	if spec.Helm != nil {
		disableDepsUpdate = spec.Helm.DisableDependencyUpdate
	}

	loadOpts := loadOpts{
		compress:           compress,
		disableDepsUpdate:  disableDepsUpdate,
		ignoreApplyConfigs: ignoreApplyConfigs(bundleFile, spec.Helm, spec.Targets...),
	}
	resources, err := loadDirectories(ctx, loadOpts, directories...)
	if err != nil {
		return nil, err
	}

	// flatten map to slice
	var result []fleet.BundleResource
	for _, r := range resources {
		result = append(result, r...)
	}

	sort.Slice(result, func(i, j int) bool {
		return result[i].Name < result[j].Name
	})

	return result, nil
}

type loadOpts struct {
	compress           bool
	disableDepsUpdate  bool
	ignoreApplyConfigs []string
}

// ignoreApplyConfigs returns a list of config files that should not be added to the
// bundle's resources. Their contents are converted into deployment options.
// This includes:
// * bundle file (typically named fleet.yaml, but may be arbitrarily named when user-driven bundle scan is used)
// * spec.Helm.ValuesFiles
// * spec.Targets[].Helm.ValuesFiles
func ignoreApplyConfigs(bundleFile string, spec *fleet.HelmOptions, targets ...fleet.BundleTarget) []string {
	ignore := []string{"fleet.yaml", bundleFile}

	// Values files may be referenced from `fleet.yaml` files either with their file name
	// alone, or with a directory prefix, for instance for a chart directory.
	// Values files must be ignored in both cases, and determining which of the filename or full path will be needed
	// depends on where the `fleet.yaml` file lives relatively to the values file(s) which it references.
	if spec != nil {
		ignore = append(ignore, spec.ValuesFiles...)

		for _, vf := range spec.ValuesFiles {
			ignore = append(ignore, filepath.Base(vf))
		}
	}

	for _, target := range targets {
		if target.Helm == nil {
			continue
		}

		ignore = append(ignore, target.Helm.ValuesFiles...)

		for _, vf := range target.Helm.ValuesFiles {
			ignore = append(ignore, filepath.Base(vf))
		}
	}

	return ignore
}

// directory represents a directory to load resources from. The directory can
// be created from an external Helm chart, or a local path.
// One bundle can consist of multiple directories.
type directory struct {
	// prefix is the generated top level dir of the chart, e.g. '.chart/1234'
	prefix string
	// base is the directory on disk to load the files from
	base string
	// source is the chart URL to download the chart from
	source string
	// version is the version of the chart
	version string
	// auth is the auth to use for the chart URL
	auth Auth
}

func addDirectory(base, customDir, defaultDir string) ([]directory, error) {
	var directories []directory
	if customDir == "" {
		if _, err := os.Stat(filepath.Join(base, defaultDir)); os.IsNotExist(err) {
			return directories, nil
		} else if err != nil {
			return directories, err
		}
		customDir = defaultDir
	}

	return []directory{{
		prefix: defaultDir,
		base:   base,
		source: customDir,
	}}, nil
}

func parseValuesFiles(base string, chart *fleet.HelmOptions) (err error) {
	if len(chart.ValuesFiles) != 0 {
		valuesMap, err := generateValues(base, chart)
		if err != nil {
			return err
		}

		if len(valuesMap.Data) != 0 {
			chart.Values = valuesMap
		}
	}

	return nil
}

func generateValues(base string, chart *fleet.HelmOptions) (valuesMap *fleet.GenericMap, err error) {
	valuesMap = &fleet.GenericMap{}
	if chart.Values != nil {
		valuesMap = chart.Values
	}
	for _, value := range chart.ValuesFiles {
		valuesByte, err := os.ReadFile(base + "/" + value)
		if err != nil {
			return nil, fmt.Errorf("reading values file: %s/%s: %w", base, value, err)
		}
		tmpDataOpt := &fleet.GenericMap{}
		err = yaml.Unmarshal(valuesByte, tmpDataOpt)
		if err != nil {
			return nil, fmt.Errorf("reading values file: %s/%s: %w", base, value, err)
		}
		valuesMap = mergeGenericMap(valuesMap, tmpDataOpt)
	}

	return valuesMap, nil
}

func mergeGenericMap(first, second *fleet.GenericMap) *fleet.GenericMap {
	result := &fleet.GenericMap{Data: make(map[string]interface{})}
	result.Data = data.MergeMaps(first.Data, second.Data)
	return result
}

// addRemoteCharts gets the chart url from a helm repo server and returns a `directory` struct.
// For every chart that is not on disk, create a directory struct that contains the charts URL as path.
// This adds one directory per HelmOption.
func addRemoteCharts(ctx context.Context, directories []directory, base string, charts []*fleet.HelmOptions, auth Auth, helmRepoURLRegex string) ([]directory, error) {
	for _, chart := range charts {
		if _, err := os.Stat(filepath.Join(base, chart.Chart)); os.IsNotExist(err) || chart.Repo != "" {
			shouldAddAuthToRequest, err := shouldAddAuthToRequest(helmRepoURLRegex, chart.Repo, chart.Chart)
			if err != nil {
				return nil, fmt.Errorf("failed to add auth to request for %s: %w", downloadChartError(*chart), err)
			}
			auth := auth // loop-scoped variable
			if !shouldAddAuthToRequest {
				auth = Auth{}
			}

			chartURL, err := chartURL(ctx, *chart, auth, false)
			if err != nil {
				return nil, fmt.Errorf("failed to resolve URL of %s: %w", downloadChartError(*chart), err)
			}

			directories = append(directories, directory{
				prefix:  checksum(chart),
				base:    base,
				source:  chartURL,
				auth:    auth,
				version: chart.Version,
			})
		}
	}
	return directories, nil
}

func downloadChartError(c fleet.HelmOptions) string {
	return fmt.Sprintf(
		"repo=%s chart=%s version=%s",
		c.Repo,
		c.Chart,
		c.Version,
	)
}

func shouldAddAuthToRequest(helmRepoURLRegex, repo, chart string) (bool, error) {
	if helmRepoURLRegex == "" {
		return true, nil
	}
	if repo == "" {
		return regexp.MatchString(helmRepoURLRegex, chart)
	}

	return regexp.MatchString(helmRepoURLRegex, repo)
}

func checksum(helm *fleet.HelmOptions) string {
	if helm == nil {
		return "none"
	}
	return fmt.Sprintf(".chart/%x", sha256.Sum256([]byte(helm.Chart+":"+helm.Repo+":"+helm.Version)))
}

// loadDirectories loads all resources from a bundle's directories
func loadDirectories(ctx context.Context, opts loadOpts, directories ...directory) (map[string][]fleet.BundleResource, error) {
	var (
		sem    = semaphore.NewWeighted(4)
		result = map[string][]fleet.BundleResource{}
		l      = sync.Mutex{}
	)

	eg, ctx := errgroup.WithContext(ctx)

	alreadyLoaded := make(map[string]struct{})
	for _, dir := range directories {
		// Avoid loading the same directory more than once
		// We don't take auth into account because having the same source
		// with different authentication means having the same resources anyway.
		// Using a comma separator to avoid false equivalents due to combinations with empty strings.
		dirId := fmt.Sprintf("%q,%q,%q,%q", dir.prefix, dir.base, dir.source, dir.version)
		if _, ok := alreadyLoaded[dirId]; ok {
			continue
		}
		alreadyLoaded[dirId] = struct{}{}
		eg.Go(func() error {
			if err := sem.Acquire(ctx, 1); err != nil {
				return fmt.Errorf("waiting to load directory %s, %s: %w", dir.prefix, dir.base, err)
			}
			defer sem.Release(1)
			resources, err := loadDirectory(ctx, opts, dir)
			if err != nil {
				return fmt.Errorf("loading directory %s, %s: %w", dir.prefix, dir.base, err)
			}

			key := dir.prefix
			if key == "" {
				key = dir.source
			}

			l.Lock()
			result[key] = resources
			l.Unlock()
			return nil
		})
	}

	return result, eg.Wait()
}



================================================
FILE: internal/bundlereader/resources_test.go
================================================
package bundlereader

import (
	"bytes"
	"testing"

	"github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"k8s.io/apimachinery/pkg/util/yaml"
)

const (
	valuesOneYaml = `microService1:
  resources:
    limits:
      cpu: 500m
      memory: 500Mi
    requests:
      cpu: 256m
      memory: 256Mi

microService2:
  resources:
    limits:
      cpu: 500m
      memory: 500Mi
    requests:
      cpu: 256m
      memory: 256Mi
`
	valuesTwoYaml = `microService1:
  replicas: 1
microService2:
  replicas: 2`
)

func TestValueMerge(t *testing.T) {
	first := &v1alpha1.GenericMap{}
	second := &v1alpha1.GenericMap{}

	err := yaml.NewYAMLToJSONDecoder(bytes.NewBufferString(valuesOneYaml)).Decode(first)
	if err != nil {
		t.Fatalf("error during valuesOneYaml parsing %v", err)
	}

	err = yaml.NewYAMLToJSONDecoder(bytes.NewBufferString(valuesTwoYaml)).Decode(second)
	if err != nil {
		t.Fatalf("error during valuesTwoYaml parsing %v", err)
	}

	mergeMap := mergeGenericMap(first, second)

	for _, serviceName := range []string{"microService1", "microService2"} {
		serviceVals, ok := mergeMap.Data[serviceName]
		if !ok {
			t.Fatalf("unable to find parent key for service %s", serviceName)
		}
		resourceVals, ok := serviceVals.(map[string]interface{})["resources"]
		if !ok {
			t.Fatalf("unable to find key resources in values for service %s", serviceName)
		}

		limitVals, ok := resourceVals.(map[string]interface{})["limits"]
		if !ok {
			t.Fatalf("unable to find key limits in resources for service %s", serviceName)
		}

		_, ok = limitVals.(map[string]interface{})["cpu"]
		if !ok {
			t.Fatalf("unable to find key cpu in limits for service %s", serviceName)
		}

		_, ok = limitVals.(map[string]interface{})["memory"]
		if !ok {
			t.Fatalf("unable to find key memory in limits for service %s", serviceName)
		}

		requestVals, ok := resourceVals.(map[string]interface{})["requests"]
		if !ok {
			t.Fatalf("unable to find key requests in resources for service %s", serviceName)
		}

		_, ok = requestVals.(map[string]interface{})["cpu"]
		if !ok {
			t.Fatalf("unable to find key cpu in requests for service %s", serviceName)
		}

		_, ok = requestVals.(map[string]interface{})["memory"]
		if !ok {
			t.Fatalf("unable to find key memory in requests for service %s", serviceName)
		}
		_, ok = serviceVals.(map[string]interface{})["replicas"]
		if !ok {
			t.Fatalf("unable to find key replicas in values for service %s", serviceName)
		}
	}
}



================================================
FILE: internal/bundlereader/style.go
================================================
package bundlereader

import (
	"path/filepath"
	"strings"

	"github.com/rancher/fleet/internal/manifest"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

const (
	chartYAML = "Chart.yaml"
)

func joinAndClean(path, file string) string {
	return strings.TrimPrefix(filepath.Join(path, file), "/")
}

func chartPath(options fleet.BundleDeploymentOptions) (string, string) {
	if options.Helm == nil {
		return chartYAML, ""
	}

	path := options.Helm.Chart
	if len(path) == 0 {
		path = options.Helm.Repo
	}

	return joinAndClean(path, chartYAML), checksum(options.Helm) + "/"
}

func kustomizePath(options fleet.BundleDeploymentOptions) string {
	if options.Kustomize == nil || options.Kustomize.Dir == "" {
		return "kustomization.yaml"
	}
	return joinAndClean(options.Kustomize.Dir, "kustomization.yaml")
}

type Style struct {
	ChartPath     string
	KustomizePath string
	HasChartYAML  bool
	Options       fleet.BundleDeploymentOptions
}

func (s Style) IsHelm() bool {
	return s.HasChartYAML
}

func (s Style) IsKustomize() bool {
	return s.KustomizePath != ""
}

func (s Style) IsRawYAML() bool {
	return !s.IsHelm() && !s.IsKustomize()
}

func matchesExternalChartYAML(externalChartPath string, path string) bool {
	if externalChartPath == "" {
		return false
	}
	if !strings.HasPrefix(path, externalChartPath) {
		return false
	}
	parts := strings.Split(strings.TrimPrefix(path, externalChartPath), "/")
	return (len(parts) == 1 && parts[0] == chartYAML) ||
		(len(parts) == 2 && parts[1] == chartYAML)
}

func DetermineStyle(m *manifest.Manifest, options fleet.BundleDeploymentOptions) Style {
	var (
		chartPath, externalChartPath = chartPath(options)
		kustomizePath                = kustomizePath(options)
		result                       = Style{
			Options: options,
		}
	)

	for _, resource := range m.Resources {
		switch {
		case resource.Name == "":
			// ignore
		case resource.Name == chartPath:
			result.ChartPath = chartPath
			result.HasChartYAML = true
		case matchesExternalChartYAML(externalChartPath, resource.Name):
			result.ChartPath = resource.Name
			result.HasChartYAML = true
		case resource.Name == kustomizePath:
			result.KustomizePath = kustomizePath
		}
	}

	return result
}



================================================
FILE: internal/client/client.go
================================================
package client

import (
	"fmt"

	"github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io"
	fleetcontrollers "github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io/v1alpha1"

	"github.com/rancher/wrangler/v3/pkg/apply"
	"github.com/rancher/wrangler/v3/pkg/generated/controllers/core"
	corev1 "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"
	"github.com/rancher/wrangler/v3/pkg/generated/controllers/rbac"
	rbaccontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/rbac/v1"
	"github.com/rancher/wrangler/v3/pkg/kubeconfig"
)

type Getter struct {
	Kubeconfig string
	Context    string
	Namespace  string
}

func (g *Getter) Get() (*Client, error) {
	if g == nil {
		return nil, fmt.Errorf("client is not configured, please set client getter")
	}
	return newClient(g.Kubeconfig, g.Context, g.Namespace)
}

func (g *Getter) GetNamespace() string {
	return g.Namespace
}

type Client struct {
	Fleet     fleetcontrollers.Interface
	Core      corev1.Interface
	RBAC      rbaccontrollers.Interface
	Apply     apply.Apply
	Namespace string
}

func NewGetter(kubeconfig, context, namespace string) *Getter {
	return &Getter{
		Kubeconfig: kubeconfig,
		Context:    context,
		Namespace:  namespace,
	}
}

func newClient(kubeConfig, context, namespace string) (*Client, error) {
	cc := kubeconfig.GetNonInteractiveClientConfigWithContext(kubeConfig, context)
	ns, _, err := cc.Namespace()
	if err != nil {
		return nil, err
	}

	if namespace != "" {
		ns = namespace
	}

	restConfig, err := cc.ClientConfig()
	if err != nil {
		return nil, err
	}

	c := &Client{
		Namespace: ns,
	}

	fleet, err := fleet.NewFactoryFromConfig(restConfig)
	if err != nil {
		return nil, err
	}
	c.Fleet = fleet.Fleet().V1alpha1()

	core, err := core.NewFactoryFromConfig(restConfig)
	if err != nil {
		return nil, err
	}
	c.Core = core.Core().V1()

	rbac, err := rbac.NewFactoryFromConfig(restConfig)
	if err != nil {
		return nil, err
	}
	c.RBAC = rbac.Rbac().V1()

	c.Apply, err = apply.NewForConfig(restConfig)
	if err != nil {
		return nil, err
	}

	if c.Namespace == "" {
		c.Namespace = "default"
	}

	c.Apply = c.Apply.
		WithDynamicLookup().
		WithDefaultNamespace(c.Namespace).
		WithListerNamespace(c.Namespace).
		WithRestrictClusterScoped()

	return c, nil
}



================================================
FILE: internal/cmd/builder.go
================================================
package cmd

// Copied from https://github.com/rancher/wrangler-cli

import (
	"os"
	"reflect"
	"regexp"
	"strconv"
	"strings"
	"unsafe"

	"github.com/spf13/cobra"
)

var (
	caseRegexp = regexp.MustCompile("([a-z])([A-Z])")
)

type PersistentPreRunnable interface {
	PersistentPre(cmd *cobra.Command, args []string) error
}

type PreRunnable interface {
	Pre(cmd *cobra.Command, args []string) error
}

type HasHelpFunc interface {
	HelpFunc(command *cobra.Command, strings []string)
}

type Runnable interface {
	Run(cmd *cobra.Command, args []string) error
}

type customizer interface {
	Customize(cmd *cobra.Command)
}

type fieldInfo struct {
	FieldType  reflect.StructField
	FieldValue reflect.Value
}

func fields(obj interface{}) []fieldInfo {
	ptrValue := reflect.ValueOf(obj)
	objValue := ptrValue.Elem()

	var result []fieldInfo

	for i := 0; i < objValue.NumField(); i++ {
		fieldType := objValue.Type().Field(i)
		if fieldType.Anonymous && fieldType.Type.Kind() == reflect.Struct {
			result = append(result, fields(objValue.Field(i).Addr().Interface())...)
		} else if !fieldType.Anonymous {
			result = append(result, fieldInfo{
				FieldValue: objValue.Field(i),
				FieldType:  objValue.Type().Field(i),
			})
		}
	}

	return result
}

func Name(obj interface{}) string {
	ptrValue := reflect.ValueOf(obj)
	objValue := ptrValue.Elem()
	commandName := strings.Replace(objValue.Type().Name(), "Command", "", 1)
	commandName, _ = name(commandName, "", "")
	return commandName
}

// Command populates a cobra.Command object by extracting args from struct tags of the
// Runnable obj passed.  Also the Run method is assigned to the RunE of the command.
// name = Override the struct field with

func Command(obj Runnable, cmd cobra.Command) *cobra.Command {
	var (
		envs     []func()
		arrays   = map[string]reflect.Value{}
		slices   = map[string]reflect.Value{}
		maps     = map[string]reflect.Value{}
		ptrValue = reflect.ValueOf(obj)
		objValue = ptrValue.Elem()
	)

	c := cmd
	if c.Use == "" {
		c.Use = Name(obj)
	}

	for _, info := range fields(obj) {
		fieldType := info.FieldType
		v := info.FieldValue

		name, alias := name(fieldType.Name, fieldType.Tag.Get("name"), fieldType.Tag.Get("short"))
		usage := fieldType.Tag.Get("usage")
		env := strings.Split(fieldType.Tag.Get("env"), ",")
		defValue := fieldType.Tag.Get("default")
		if len(env) == 1 && env[0] == "" {
			env = nil
		}
		defInt, err := strconv.Atoi(defValue)
		if err != nil {
			defInt = 0
		}

		flags := c.PersistentFlags()
		switch fieldType.Type.Kind() {
		case reflect.Int:
			flags.IntVarP((*int)(unsafe.Pointer(v.Addr().Pointer())), name, alias, defInt, usage)
		case reflect.String:
			flags.StringVarP((*string)(unsafe.Pointer(v.Addr().Pointer())), name, alias, defValue, usage)
		case reflect.Slice:
			switch fieldType.Tag.Get("split") {
			case "false":
				arrays[name] = v
				flags.StringArrayP(name, alias, nil, usage)
			default:
				slices[name] = v
				flags.StringSliceP(name, alias, nil, usage)
			}
		case reflect.Map:
			maps[name] = v
			flags.StringSliceP(name, alias, nil, usage)
		case reflect.Bool:
			initVal := false
			if defValue == "true" {
				initVal = true
			}
			flags.BoolVarP((*bool)(unsafe.Pointer(v.Addr().Pointer())), name, alias, initVal, usage)
		default:
			panic("Unknown kind on field " + fieldType.Name + " on " + objValue.Type().Name())
		}

		for _, env := range env {
			envs = append(envs, func() {
				v := os.Getenv(env)
				if v != "" {
					fv, err := flags.GetString(name)
					if err == nil && (fv == "" || fv == defValue) {
						_ = flags.Set(name, v)
					}
				}
			})
		}
	}

	if p, ok := obj.(PersistentPreRunnable); ok {
		c.PersistentPreRunE = p.PersistentPre
	}

	if p, ok := obj.(PreRunnable); ok {
		c.PreRunE = p.Pre
	}

	if p, ok := obj.(HasHelpFunc); ok {
		c.SetHelpFunc(p.HelpFunc)
	}

	c.RunE = obj.Run
	c.PersistentPreRunE = bind(c.PersistentPreRunE, arrays, slices, maps, envs)
	c.PreRunE = bind(c.PreRunE, arrays, slices, maps, envs)
	c.RunE = bind(c.RunE, arrays, slices, maps, envs)

	cust, ok := obj.(customizer)
	if ok {
		cust.Customize(&c)
	}

	return &c
}

func assignMaps(app *cobra.Command, maps map[string]reflect.Value) error {
	for k, v := range maps {
		k = contextKey(k)
		s, err := app.Flags().GetStringSlice(k)
		if err != nil {
			return err
		}
		if s != nil {
			values := map[string]string{}
			for _, part := range s {
				parts := strings.SplitN(part, "=", 2)
				if len(parts) == 1 {
					values[parts[0]] = ""
				} else {
					values[parts[0]] = parts[1]
				}
			}
			v.Set(reflect.ValueOf(values))
		}
	}
	return nil
}

func assignSlices(app *cobra.Command, slices map[string]reflect.Value) error {
	for k, v := range slices {
		k = contextKey(k)
		s, err := app.Flags().GetStringSlice(k)
		if err != nil {
			return err
		}
		if s != nil {
			v.Set(reflect.ValueOf(s))
		}
	}
	return nil
}

func assignArrays(app *cobra.Command, arrays map[string]reflect.Value) error {
	for k, v := range arrays {
		k = contextKey(k)
		s, err := app.Flags().GetStringArray(k)
		if err != nil {
			return err
		}
		if s != nil {
			v.Set(reflect.ValueOf(s))
		}
	}
	return nil
}

func contextKey(name string) string {
	parts := strings.Split(name, ",")
	return parts[len(parts)-1]
}

func name(name, setName, short string) (string, string) {
	if setName != "" {
		return setName, short
	}
	parts := strings.Split(name, "_")
	i := len(parts) - 1
	name = caseRegexp.ReplaceAllString(parts[i], "$1-$2")
	name = strings.ToLower(name)
	result := append([]string{name}, parts[0:i]...)
	for i := 0; i < len(result); i++ {
		result[i] = strings.ToLower(result[i])
	}
	if short == "" && len(result) > 1 {
		short = result[1]
	}
	return result[0], short
}

func bind(next func(*cobra.Command, []string) error,
	arrays map[string]reflect.Value,
	slices map[string]reflect.Value,
	maps map[string]reflect.Value,
	envs []func()) func(*cobra.Command, []string) error {
	if next == nil {
		return nil
	}
	return func(cmd *cobra.Command, args []string) error {
		for _, envCallback := range envs {
			envCallback()
		}
		if err := assignArrays(cmd, arrays); err != nil {
			return err
		}
		if err := assignSlices(cmd, slices); err != nil {
			return err
		}
		if err := assignMaps(cmd, maps); err != nil {
			return err
		}

		if next != nil {
			return next(cmd, args)
		}

		return nil
	}
}



================================================
FILE: internal/cmd/debug.go
================================================
package cmd

// Copied from https://github.com/rancher/wrangler-cli

import (
	"flag"
	"fmt"

	"github.com/sirupsen/logrus"
	"go.uber.org/zap/zapcore"

	"k8s.io/klog/v2"
	crzap "sigs.k8s.io/controller-runtime/pkg/log/zap"
)

type DebugConfig struct {
	Debug      bool `usage:"Turn on debug logging"`
	DebugLevel int  `usage:"If debugging is enabled, set klog -v=X"`
}

func (c *DebugConfig) SetupDebug() error {
	logging := flag.NewFlagSet("", flag.PanicOnError)
	klog.InitFlags(logging)
	if c.Debug {
		logrus.SetLevel(logrus.DebugLevel)
		if err := logging.Parse([]string{
			fmt.Sprintf("-v=%d", c.DebugLevel),
		}); err != nil {
			return err
		}
	} else {
		if err := logging.Parse([]string{
			"-v=0",
		}); err != nil {
			return err
		}
	}

	return nil
}

// OverrideZapOpts, for compatibility override zap opts with legacy debug opts.
func (c *DebugConfig) OverrideZapOpts(zopts *crzap.Options) *crzap.Options {
	if zopts == nil {
		zopts = &crzap.Options{}
	}

	zopts.Development = c.Debug

	if c.Debug && c.DebugLevel > 0 {
		zopts.Level = zapcore.Level(c.DebugLevel * -1) //nolint:gosec // no risk, just debug level
	}

	return zopts
}



================================================
FILE: internal/cmd/options.go
================================================
package cmd

import (
	"fmt"
	"os"
	"strconv"
	"time"

	"github.com/sirupsen/logrus"
)

type LeaderElectionOptions struct {
	// LeaseDuration is the duration that non-leader candidates will
	// wait to force acquire leadership. This is measured against time of
	// last observed ack. Default is 15 seconds.
	LeaseDuration time.Duration

	// RenewDeadline is the duration that the acting controlplane will retry
	// refreshing leadership before giving up. Default is 10 seconds.
	RenewDeadline time.Duration

	// RetryPeriod is the duration the LeaderElector clients should wait
	// between tries of actions. Default is 2 seconds.
	RetryPeriod time.Duration
}

// NewLeaderElectionOptions returns a new LeaderElectionOptions struct with the
// values parsed from environment variables.
func NewLeaderElectionOptions() (LeaderElectionOptions, error) {
	return NewLeaderElectionOptionsWithPrefix("CATTLE")
}

// NewLeaderElectionOptionsWithPrefix returns a new LeaderElectionOptions struct with the values
// parsed from environment variables with the given prefix.
//
// The environment variables passed to the fleet agentmanagement container in the fleet controller
// pod need to be prefixed differently to differentiate between those which are meant to configure
// the fleet controller and those which are meant to configure the fleet agentmanagement container,
// since the fleet agentmanagement controller creates the fleet deployment (for manager initiated
// deployments).
func NewLeaderElectionOptionsWithPrefix(prefix string) (LeaderElectionOptions, error) {
	var (
		leaderOpts LeaderElectionOptions
		err        error
		name       string
	)

	name = fmt.Sprintf("%s_ELECTION_LEASE_DURATION", prefix)
	leaderOpts.LeaseDuration, err = parseEnvDuration(name)
	if err != nil {
		return leaderOpts, err
	}

	name = fmt.Sprintf("%s_ELECTION_RENEW_DEADLINE", prefix)
	leaderOpts.RenewDeadline, err = parseEnvDuration(name)
	if err != nil {
		return leaderOpts, err
	}

	name = fmt.Sprintf("%s_ELECTION_RETRY_PERIOD", prefix)
	leaderOpts.RetryPeriod, err = parseEnvDuration(name)
	if err != nil {
		return leaderOpts, err
	}

	return leaderOpts, nil
}

// ParseEnvAgentReplicaCount parses the environment variable FLEET_AGENT_REPLICA_COUNT. If the
// environment variable is not set or the value cannot be parsed, it will return 1.
func ParseEnvAgentReplicaCount() int32 {
	replicas, err := parseEnvInt32("FLEET_AGENT_REPLICA_COUNT")
	if err != nil {
		logrus.Warn("FLEET_AGENT_REPLICA_COUNT not set, defaulting to 1")
		return 1
	}
	return replicas
}

// parseEnvInt32 parses an environment variable. It returns an error if the environment variable is
// not set or if it cannot be parsed as an int32.
func parseEnvInt32(envVar string) (int32, error) {
	if d, ok := os.LookupEnv(envVar); ok {
		v, err := strconv.ParseInt(d, 10, 32)
		if err != nil {
			return 0, fmt.Errorf("failed to parse %s with int32 %s: %w", envVar, d, err)
		}
		return int32(v), nil
	}
	return 0, fmt.Errorf("environment variable %s not set", envVar)
}

func parseEnvDuration(envVar string) (time.Duration, error) {
	if d := os.Getenv(envVar); d != "" {
		v, err := time.ParseDuration(d)
		if err != nil {
			return 0, fmt.Errorf("failed to parse %s with duration %s: %w", envVar, d, err)
		}
		return v, nil
	}
	return 0, fmt.Errorf("environment variable %s not set", envVar)
}



================================================
FILE: internal/cmd/agent/clusterstatus.go
================================================
package agent

import (
	"context"
	"time"

	"k8s.io/client-go/rest"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"

	"github.com/rancher/fleet/internal/cmd/agent/clusterstatus"
	"github.com/rancher/fleet/internal/cmd/agent/register"
)

type ClusterStatusRunnable struct {
	config          *rest.Config
	namespace       string
	checkinInterval string
	agentInfo       *register.AgentInfo
}

func (cs *ClusterStatusRunnable) Start(ctx context.Context) error {
	ctrl.SetLogger(zap.New(zap.UseFlagOptions(zopts)))
	ctx = log.IntoContext(ctx, ctrl.Log)

	var err error
	var checkinInterval time.Duration
	if cs.checkinInterval != "" {
		checkinInterval, err = time.ParseDuration(cs.checkinInterval)
		if err != nil {
			return err
		}
	}

	setupLog.Info("Starting cluster status ticker", "checkin interval", checkinInterval.String(), "cluster namespace", cs.agentInfo.ClusterNamespace, "cluster name", cs.agentInfo.ClusterName)

	// use a separate client for the cluster status ticker, that does not use a cache
	client, err := client.New(cs.config, client.Options{Scheme: scheme})
	if err != nil {
		return err
	}

	go func() {
		clusterstatus.Ticker(
			ctx,
			client,
			cs.namespace,
			cs.agentInfo.ClusterNamespace,
			cs.agentInfo.ClusterName,
			checkinInterval,
		)

		<-ctx.Done()
	}()

	return nil
}



================================================
FILE: internal/cmd/agent/operator.go
================================================
package agent

import (
	"context"
	"flag"
	"fmt"
	"os"

	"github.com/rancher/fleet/internal/cmd/agent/controller"
	"github.com/rancher/fleet/internal/cmd/agent/deployer"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/cleanup"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/desiredset"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/driftdetect"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/monitor"
	"github.com/rancher/fleet/internal/cmd/agent/register"
	"github.com/rancher/fleet/internal/cmd/agent/trigger"
	"github.com/rancher/fleet/internal/config"
	"github.com/rancher/fleet/internal/helmdeployer"
	"github.com/rancher/fleet/internal/manifest"
	"github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"helm.sh/helm/v4/pkg/cli"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	"k8s.io/client-go/dynamic"
	clientgoscheme "k8s.io/client-go/kubernetes/scheme"
	"k8s.io/client-go/rest"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/cache"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/cluster"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/healthz"
	"sigs.k8s.io/controller-runtime/pkg/manager"
	metricsserver "sigs.k8s.io/controller-runtime/pkg/metrics/server"
)

var (
	scheme      = runtime.NewScheme()
	localScheme = runtime.NewScheme()
)

// defaultNamespace is the namespace to use for resources that don't specify a namespace, e.g. "default"
const defaultNamespace = "default"

func init() {
	utilruntime.Must(clientgoscheme.AddToScheme(scheme))
	utilruntime.Must(v1alpha1.AddToScheme(scheme))
	//+kubebuilder:scaffold:scheme

	utilruntime.Must(clientgoscheme.AddToScheme(localScheme))
}

// start the fleet agent
// systemNamespace is the namespace the agent is running in, e.g. cattle-fleet-system
func start(
	ctx context.Context,
	localConfig *rest.Config,
	systemNamespace string,
	agentScope string,
	checkinInterval string,
	workersOpts AgentReconcilerWorkers,
	agentInfo *register.AgentInfo,
) error {
	upstreamConfig, err := agentInfo.ClientConfig.ClientConfig()
	if err != nil {
		return fmt.Errorf("failed to get client config: %w", err)
	}
	agentConfig, err := getAgentConfig(ctx, systemNamespace, localConfig)
	if err != nil {
		return fmt.Errorf("failed to get agent config: %w", err)
	}

	// fleetNamespace is the upstream cluster namespace from AgentInfo, e.g. cluster-fleet-ID
	fleetNamespace, _, err := agentInfo.ClientConfig.Namespace()
	if err != nil {
		return fmt.Errorf("failed to get namespace from upstream cluster: %w", err)
	}

	// Start manager for upstream cluster, we do not use leader election
	setupLog.Info("listening for changes on upstream cluster", "cluster", agentInfo.ClusterName, "namespace", fleetNamespace)

	metricsAddr := ":8080"
	if d := os.Getenv("FLEET_AGENT_METRICS_BIND_ADDRESS"); d != "" {
		metricsAddr = d
	}

	probeAddr := ":8081"
	if d := os.Getenv("FLEET_AGENT_PROBE_BIND_ADDRESS"); d != "" {
		probeAddr = d
	}

	setupLog.Info("Starting controller", "metricsAddr", metricsAddr, "probeAddr", probeAddr, "systemNamespace", systemNamespace)
	mgr, err := ctrl.NewManager(upstreamConfig, ctrl.Options{
		Scheme:                 scheme,
		Metrics:                metricsserver.Options{BindAddress: metricsAddr},
		HealthProbeBindAddress: probeAddr,
		LeaderElection:         false,
		// only watch resources in the cluster namespace
		Cache: cache.Options{
			DefaultNamespaces: map[string]cache.Config{fleetNamespace: {}},
		},
	})
	if err != nil {
		setupLog.Error(err, "unable to start manager")
		return err
	}

	localCtx, cancel := context.WithCancel(ctx)
	defer cancel()

	driftChan := make(chan event.TypedGenericEvent[*v1alpha1.BundleDeployment])

	reconciler, err := newReconciler(
		ctx,
		localCtx,
		mgr,
		localConfig,
		systemNamespace,
		fleetNamespace,
		agentScope,
		*agentConfig,
		driftChan,
		workersOpts.BundleDeployment,
	)
	if err != nil {
		setupLog.Error(err, "unable to set up bundledeployment reconciler")
		return err
	}

	// Set up the bundledeployment reconciler
	if err = (reconciler).SetupWithManager(mgr); err != nil {
		setupLog.Error(err, "unable to create controller", "controller", "BundleDeployment")
		return err
	}
	//+kubebuilder:scaffold:builder

	// RawSource watches for all events from the driftdetect mini controller
	driftReconciler := &controller.DriftReconciler{
		Client: mgr.GetClient(),
		Scheme: mgr.GetScheme(),

		Deployer:    reconciler.Deployer,
		Monitor:     reconciler.Monitor,
		DriftDetect: reconciler.DriftDetect,

		DriftChan: driftChan,

		Workers: workersOpts.Drift,
	}
	if err = driftReconciler.SetupWithManager(mgr); err != nil {
		setupLog.Error(err, "unable to create controller", "controller", "BundleDeployment")
		return err
	}

	clusterStatus := &ClusterStatusRunnable{
		agentInfo:       agentInfo,
		config:          upstreamConfig,
		checkinInterval: checkinInterval,
		namespace:       systemNamespace,
	}
	if err := mgr.Add(clusterStatus); err != nil {
		setupLog.Error(err, "unable to add cluster status controller")
		return err
	}

	if err := mgr.AddHealthzCheck("healthz", healthz.Ping); err != nil {
		setupLog.Error(err, "unable to set up health check")
		return err
	}
	if err := mgr.AddReadyzCheck("readyz", healthz.Ping); err != nil {
		setupLog.Error(err, "unable to set up ready check")
		return err
	}

	setupLog.Info("starting manager")
	if err := mgr.Start(ctx); err != nil {
		setupLog.Error(err, "problem running manager")
		return err

	}

	return nil
}

func newReconciler(
	ctx context.Context,
	localCtx context.Context,
	mgr manager.Manager,
	localConfig *rest.Config,
	systemNamespace string,
	fleetNamespace string,
	agentScope string,
	agentConfig config.Config,
	driftChan chan event.TypedGenericEvent[*v1alpha1.BundleDeployment],
	workers int,
) (*controller.BundleDeploymentReconciler, error) {
	upstreamClient := mgr.GetClient()

	// Build client for local cluster
	localCluster, err := newCluster(localCtx, localConfig, ctrl.Options{
		Scheme: localScheme,
		Logger: mgr.GetLogger().WithName("local-cluster"),
	})
	if err != nil {
		setupLog.Error(err, "unable to build local cluster client")
		return nil, err
	}
	localClient := localCluster.GetClient()

	if kubeconfig := flag.Lookup("kubeconfig").Value.String(); kubeconfig != "" {
		// set KUBECONFIG env var so helm can find it
		os.Setenv("KUBECONFIG", kubeconfig)
	}

	// Build the helm deployer, which uses a getter for local cluster's client-go client for helm SDK
	helmDeployer := helmdeployer.New(
		systemNamespace,
		defaultNamespace,
		defaultNamespace,
		agentScope,
	)
	err = helmDeployer.Setup(ctx, localClient, cli.New().RESTClientGetter())
	if err != nil {
		setupLog.Error(err, "unable to setup local helm SDK client")
		return nil, err
	}

	// Build the deployer that the bundledeployment reconciler will use
	deployer := deployer.New(
		localClient,
		mgr.GetAPIReader(),
		manifest.NewLookup(),
		helmDeployer,
	)

	// Build the monitor to update the bundle deployment's status, calculates modified/non-modified
	localDynamic, err := dynamic.NewForConfig(localConfig)
	if err != nil {
		return nil, err
	}
	ds, err := desiredset.New(localConfig)
	if err != nil {
		return nil, err
	}
	monitor := monitor.New(
		localClient,
		ds,
		helmDeployer,
		defaultNamespace,
		agentScope,
	)

	// Build the drift detector for deployed resources
	trigger := trigger.New(ctx, localDynamic, localCluster.GetRESTMapper())
	driftdetect := driftdetect.New(
		trigger,
		ds,
		defaultNamespace,
		defaultNamespace,
		agentScope,
		driftChan,
	)

	// Build the clean up, which deletes helm releases
	cleanup := cleanup.New(
		upstreamClient,
		localClient.RESTMapper(),
		localDynamic,
		helmDeployer,
		fleetNamespace,
		defaultNamespace,
		agentConfig.GarbageCollectionInterval.Duration,
	)

	return &controller.BundleDeploymentReconciler{
		Client: upstreamClient,
		Reader: mgr.GetAPIReader(),

		Scheme:      mgr.GetScheme(),
		LocalClient: localClient,

		Deployer:    deployer,
		Monitor:     monitor,
		DriftDetect: driftdetect,
		Cleanup:     cleanup,

		DefaultNamespace: defaultNamespace,

		AgentScope: agentScope,

		Workers: workers,
	}, nil
}

// newCluster returns a new cluster client, see controller-runtime/pkg/manager/manager.go
// This client is for the local cluster, not the upstream cluster. The upstream
// cluster client is used by the manager to watch for changes to the
// bundledeployments.
func newCluster(ctx context.Context, config *rest.Config, options manager.Options) (cluster.Cluster, error) {
	cluster, err := cluster.New(config, func(clusterOptions *cluster.Options) {
		clusterOptions.Scheme = options.Scheme
		clusterOptions.Logger = options.Logger
	})
	if err != nil {
		return nil, err
	}
	go func() {
		err := cluster.GetCache().Start(ctx)
		if err != nil {
			setupLog.Error(err, "unable to start the cache")
			os.Exit(1)
		}
	}()
	cluster.GetCache().WaitForCacheSync(ctx)

	return cluster, nil
}

func getAgentConfig(ctx context.Context, namespace string, cfg *rest.Config) (agentConfig *config.Config, err error) {
	cfg = rest.CopyConfig(cfg)
	// disable the rate limiter
	cfg.QPS = -1
	cfg.RateLimiter = nil

	client, err := client.New(cfg, client.Options{})
	if err != nil {
		return nil, fmt.Errorf("failed to create controller-runtime client: %w", err)
	}

	configMap := &corev1.ConfigMap{}
	err = client.Get(ctx, types.NamespacedName{
		Namespace: namespace,
		Name:      config.AgentConfigName,
	}, configMap)
	if err != nil {
		return nil, fmt.Errorf("failed to look up client config %s/%s: %w", namespace, config.AgentConfigName, err)
	}

	agentConfig, err = config.ReadConfig(configMap)
	if err != nil {
		return nil, fmt.Errorf("failed to parse config from ConfigMap: %w", err)
	}

	return agentConfig, nil
}



================================================
FILE: internal/cmd/agent/register.go
================================================
package agent

import (
	"context"
	"fmt"

	"github.com/rancher/fleet/internal/cmd/agent/register"

	"k8s.io/client-go/rest"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"
)

type Register struct {
	// system namespace is the namespace, the agent runs in, e.g. cattle-fleet-system
	Namespace string
}

func (r *Register) RegisterAgent(ctx context.Context, localConfig *rest.Config) (*register.AgentInfo, error) {
	ctrl.SetLogger(zap.New(zap.UseFlagOptions(zopts)))
	ctx = log.IntoContext(ctx, ctrl.Log)

	ctx, cancel := context.WithCancel(ctx)
	defer cancel()

	// try to register with upstream fleet controller by obtaining
	// a kubeconfig for the upstream cluster
	agentInfo, err := register.Register(ctx, r.Namespace, localConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to register with upstream cluster: %w", err)
	}

	ns, _, err := agentInfo.ClientConfig.Namespace()
	if err != nil {
		return nil, fmt.Errorf("failed to get namespace from upstream cluster: %w", err)
	}

	_, err = agentInfo.ClientConfig.ClientConfig()
	if err != nil {
		return nil, fmt.Errorf("failed to get kubeconfig from upstream cluster: %w", err)
	}

	setupLog.Info("successfully registered with upstream cluster", "namespace", ns)

	return agentInfo, nil
}



================================================
FILE: internal/cmd/agent/root.go
================================================
package agent

import (
	"context"
	"flag"
	"fmt"
	glog "log"
	"net/http"
	"os"
	"strconv"

	"github.com/spf13/cobra"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/leaderelection"
	"k8s.io/client-go/tools/leaderelection/resourcelock"

	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"

	command "github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/pkg/version"
)

type UpstreamOptions struct {
	Kubeconfig string `usage:"kubeconfig file for upstream cluster"`
	Namespace  string `usage:"system namespace is the namespace, the agent runs in, e.g. cattle-fleet-system" env:"NAMESPACE"`
}

type FleetAgent struct {
	command.DebugConfig
	AgentScope string `usage:"An identifier used to scope the agent bundleID names, typically the same as namespace" env:"AGENT_SCOPE"`
	UpstreamOptions
	CheckinInterval      string `usage:"How often to post cluster status" env:"CHECKIN_INTERVAL"`
	EnableLeaderElection bool   `name:"leader-elect" default:"true" usage:"Enable leader election for the agent. Enabling this will ensure there is only one active agent."`
}

type AgentReconcilerWorkers struct {
	BundleDeployment int
	Drift            int
}

const (
	leaseLockName              = "fleet-agent"
	leaseLockNameClusterStatus = "fleet-agent-clusterstatus"
)

var (
	setupLog = ctrl.Log.WithName("setup")
	zopts    = &zap.Options{
		Development: true,
	}
)

func (a *FleetAgent) PersistentPre(cmd *cobra.Command, _ []string) error {
	if err := a.SetupDebug(); err != nil {
		return fmt.Errorf("failed to setup debug logging: %w", err)
	}
	zopts = a.OverrideZapOpts(zopts)
	return nil
}

func getUniqueIdentifier() (string, error) {
	// For in-cluster deployments.
	if podName, ok := os.LookupEnv("POD_NAME"); ok {
		return podName, nil
	}

	// For local development, combine hostname with process ID
	hostname, err := os.Hostname()
	if err != nil {
		return "", fmt.Errorf("failed to get hostname: %w", err)
	}

	pid := os.Getpid()
	return fmt.Sprintf("%s-%d", hostname, pid), nil
}

func (a *FleetAgent) Run(cmd *cobra.Command, args []string) error {
	logger := zap.New(zap.UseFlagOptions(zopts))
	ctrl.SetLogger(logger)

	if a.Namespace == "" {
		return fmt.Errorf("--namespace or env NAMESPACE is required to be set")
	}

	ctx := log.IntoContext(cmd.Context(), ctrl.Log)

	localConfig := ctrl.GetConfigOrDie()
	localClient, err := kubernetes.NewForConfig(localConfig)
	if err != nil {
		return fmt.Errorf("failed to create local client: %w", err)
	}

	identifier, err := getUniqueIdentifier()
	if err != nil {
		return fmt.Errorf("failed to get unique identifier: %w", err)
	}

	lock := resourcelock.LeaseLock{
		LeaseMeta: metav1.ObjectMeta{
			Name:      leaseLockName,
			Namespace: a.Namespace,
		},
		Client: localClient.CoordinationV1(),
		LockConfig: resourcelock.ResourceLockConfig{
			Identity: identifier,
		},
	}

	run := func(ctx context.Context) {
		// Handle agent registration.
		r := &Register{
			Namespace: a.Namespace,
		}

		agentInfo, err := r.RegisterAgent(ctx, localConfig)
		if err != nil {
			setupLog.Error(err, "failed to register with upstream cluster")
			return
		}

		workersOpts := AgentReconcilerWorkers{}

		if d := os.Getenv("BUNDLEDEPLOYMENT_RECONCILER_WORKERS"); d != "" {
			w, err := strconv.Atoi(d)
			if err != nil {
				setupLog.Error(err, "failed to parse BUNDLEDEPLOYMENT_RECONCILER_WORKERS", "value", d)
			}
			workersOpts.BundleDeployment = w
		}

		if d := os.Getenv("DRIFT_RECONCILER_WORKERS"); d != "" {
			w, err := strconv.Atoi(d)
			if err != nil {
				setupLog.Error(err, "failed to parse DRIFT_RECONCILER_WORKERS", "value", d)
			}
			workersOpts.Drift = w
		}

		if err := start(ctx, localConfig, a.Namespace, a.AgentScope, a.CheckinInterval, workersOpts, agentInfo); err != nil {
			setupLog.Error(err, "failed to start agent")
		}
	}

	if a.EnableLeaderElection {
		leaderOpts, err := command.NewLeaderElectionOptions()
		if err != nil {
			return err
		}

		if os.Getenv("FLEET_AGENT_PPROF_DISABLED") != "true" {
			go func() {
				glog.Println(http.ListenAndServe("localhost:6060", nil)) //nolint:gosec // Debugging only
			}()
		}

		leaderElectionConfig := leaderelection.LeaderElectionConfig{
			Lock:          &lock,
			LeaseDuration: leaderOpts.LeaseDuration,
			RetryPeriod:   leaderOpts.RetryPeriod,
			RenewDeadline: leaderOpts.RenewDeadline,
			Callbacks: leaderelection.LeaderCallbacks{
				OnStartedLeading: run,
				OnStoppedLeading: func() {
					select {
					case <-ctx.Done():
						// Request to terminate.
						// This must be handled gracefully to prevent Kubernetes from recording the
						// container as exiting in error when termination was requested.
						// This situation matches SIGTERM being sent, which happens when a node is shut
						// down.
						setupLog.Info("termination requested, exiting")
						os.Exit(0)
					default:
						setupLog.Info("stopped leading")
						os.Exit(1)
					}
				},
				OnNewLeader: func(identity string) {
					if identity == identifier {
						setupLog.Info("renewed leader", "identity", identity)
					} else {
						setupLog.Info("new leader", "identity", identity)
					}
				},
			},
		}

		leaderelection.RunOrDie(ctx, leaderElectionConfig)

		return nil
	}

	run(ctx)

	return nil
}

func App() *cobra.Command {
	root := command.Command(&FleetAgent{}, cobra.Command{
		Version: version.FriendlyVersion(),
	})
	// add command line flags from zap and controller-runtime, which use
	// goflags and convert them to pflags
	fs := flag.NewFlagSet("", flag.ExitOnError)
	zopts.BindFlags(fs)
	ctrl.RegisterFlags(fs)
	root.Flags().AddGoFlagSet(fs)

	return root
}



================================================
FILE: internal/cmd/agent/clusterstatus/suite_test.go
================================================
package clusterstatus_test

import (
	"testing"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

const (
	timeout = 30 * time.Second
)

func TestFleet(t *testing.T) {
	RegisterFailHandler(Fail)
	RunSpecs(t, "ClusterStatus Suite")
}

var _ = BeforeSuite(func() {
	SetDefaultEventuallyTimeout(timeout)
})



================================================
FILE: internal/cmd/agent/clusterstatus/ticker.go
================================================
// Package clusterstatus updates the cluster.fleet.cattle.io status in the upstream cluster with the current cluster status.
package clusterstatus

import (
	"context"
	"time"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"

	"github.com/rancher/wrangler/v3/pkg/ticker"

	"k8s.io/apimachinery/pkg/api/equality"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

type handler struct {
	agentNamespace   string
	clusterName      string
	clusterNamespace string
	client           client.Client
	reported         fleet.AgentStatus
}

func Ticker(ctx context.Context, client client.Client, agentNamespace string, clusterNamespace string, clusterName string, checkinInterval time.Duration) {
	logger := log.FromContext(ctx).WithName("clusterstatus").WithValues("cluster", clusterName, "interval", checkinInterval)

	h := handler{
		agentNamespace:   agentNamespace,
		clusterName:      clusterName,
		clusterNamespace: clusterNamespace,
		client:           client,
	}

	go func() {
		time.Sleep(durations.ClusterRegisterDelay)
		logger.V(1).Info("Reporting cluster status once")
		if err := h.Update(ctx); err != nil {
			logger.Error(err, "failed to report initial cluster status")
		}
	}()
	go func() {
		if checkinInterval == 0 {
			checkinInterval = durations.DefaultClusterCheckInterval
		}
		for range ticker.Context(ctx, checkinInterval) {
			logger.V(1).Info("Reporting cluster status")
			if err := h.Update(ctx); err != nil {
				logger.Error(err, "failed to report cluster status")
			}
		}
	}()
}

// Update the cluster.fleet.cattle.io status in the upstream cluster with the current cluster status
func (h *handler) Update(ctx context.Context) error {
	agentStatus := fleet.AgentStatus{
		LastSeen:  metav1.Now(),
		Namespace: h.agentNamespace,
	}

	if equality.Semantic.DeepEqual(h.reported, agentStatus) {
		return nil
	}

	cluster := &fleet.Cluster{
		ObjectMeta: metav1.ObjectMeta{
			Name:      h.clusterName,
			Namespace: h.clusterNamespace,
		},
	}

	// Create a patch with the updated status, we avoid Get as that would
	// need additional RBAC
	patch := `[{"op":"add","path":"/status/agent","value":{"lastSeen":"` +
		agentStatus.LastSeen.Format(time.RFC3339) +
		`","namespace":"` + agentStatus.Namespace +
		`"}}]`

	err := h.client.Status().Patch(ctx, cluster, client.RawPatch(types.JSONPatchType, []byte(patch)))
	if err != nil {
		return err
	}

	h.reported = agentStatus
	return nil
}



================================================
FILE: internal/cmd/agent/clusterstatus/ticker_test.go
================================================
package clusterstatus

import (
	"context"
	"encoding/json"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"

	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/client/fake"
	"sigs.k8s.io/controller-runtime/pkg/client/interceptor"
)

var clientBuilder = fake.NewClientBuilder()

var _ = Describe("ClusterStatus Ticker", func() {
	var (
		scheme *runtime.Scheme
		cancel context.CancelFunc

		ctx              context.Context
		clt              client.Client
		agentNamespace   string
		clusterName      string
		clusterNamespace string
		checkinInterval  time.Duration
	)

	BeforeEach(func() {
		scheme = runtime.NewScheme()
		utilruntime.Must(fleet.AddToScheme(scheme))

		agentNamespace = "cattle-fleet-system"
		clusterName = "cluster-name"
		clusterNamespace = "cluster-namespace"
		checkinInterval = time.Millisecond * 1

		interceptorFuncs := interceptor.Funcs{
			SubResourcePatch: func(ctx context.Context, client client.Client, subResourceName string, obj client.Object, patch client.Patch, opts ...client.SubResourcePatchOption) error {
				defer cancel()

				bytes, err := patch.Data(obj)
				if err != nil {
					return err
				}

				clusterStatus := struct {
					Status struct {
						Agent fleet.AgentStatus `json:"agent"`
					} `json:"status"`
				}{}
				if err = json.Unmarshal(bytes, &clusterStatus); err != nil {
					return err
				}

				Expect(clusterStatus.Status.Agent.LastSeen.Time).To(BeTemporally("~", time.Now(), time.Minute*5),
					"time stamp should have been updated within the last 5 minutes")
				return nil
			},
		}

		ctx = context.Background()
		cluster := &fleet.Cluster{
			TypeMeta: metav1.TypeMeta{
				Kind:       "Cluster",
				APIVersion: "fleet.cattle.io/v1alpha1",
			},
			ObjectMeta: metav1.ObjectMeta{
				Name:      clusterName,
				Namespace: clusterNamespace,
			},
			Status: fleet.ClusterStatus{
				Agent: fleet.AgentStatus{
					LastSeen: metav1.Time{Time: time.Now().Add(-time.Hour * 24)},
				},
			},
		}
		clt = clientBuilder.
			WithScheme(scheme).
			WithObjects(cluster).
			WithStatusSubresource(cluster).
			WithInterceptorFuncs(interceptorFuncs).
			Build()

		// Make sure the test is eventually aborted if the context is not canceled on success.
		ctx, cancel = context.WithDeadline(ctx, time.Now().Add(time.Second*30))
	})

	It("should patch the cluster status after checkinInterval", func() {
		Ticker(ctx, clt, agentNamespace, clusterNamespace, clusterName, checkinInterval)
		<-ctx.Done()
	})
})



================================================
FILE: internal/cmd/agent/controller/bundledeployment_controller.go
================================================
package controller

import (
	"context"
	"fmt"
	"strings"
	"time"

	"github.com/rancher/fleet/internal/cmd/agent/deployer"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/cleanup"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/driftdetect"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/monitor"
	"github.com/rancher/fleet/internal/experimental"
	"github.com/rancher/fleet/internal/helmvalues"
	"github.com/rancher/fleet/internal/namespaces"
	fleetv1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"

	"github.com/go-logr/logr"
	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	errutil "k8s.io/apimachinery/pkg/util/errors"
	"k8s.io/apimachinery/pkg/util/wait"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
)

// BundleDeploymentReconciler reconciles a BundleDeployment object, by
// deploying the bundle as a helm release.
type BundleDeploymentReconciler struct {
	client.Client
	Reader client.Reader

	Scheme *runtime.Scheme

	// LocalClient is the client for the cluster the agent is running on.
	LocalClient client.Client

	Deployer    *deployer.Deployer
	Monitor     *monitor.Monitor
	DriftDetect *driftdetect.DriftDetect
	Cleanup     *cleanup.Cleanup

	DefaultNamespace string

	// AgentInfo is the labelSuffix used by the helm deployer
	AgentScope string

	Workers int
}

var DefaultRetry = wait.Backoff{
	Steps:    5,
	Duration: 5 * time.Second,
	Factor:   1.0,
	Jitter:   0.1,
}

// SetupWithManager sets up the controller with the Manager.
func (r *BundleDeploymentReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&fleetv1.BundleDeployment{}).
		WithEventFilter(
			// we do not trigger for status changes
			predicate.Or(
				// Note: These predicates prevent cache
				// syncPeriod from triggering reconcile, since
				// cache sync is an Update event.
				predicate.GenerationChangedPredicate{},
				predicate.AnnotationChangedPredicate{},
				predicate.LabelChangedPredicate{},
				predicate.Funcs{
					// except for changes to status.Refresh
					UpdateFunc: func(e event.UpdateEvent) bool {
						n := e.ObjectNew.(*fleetv1.BundleDeployment)
						o := e.ObjectOld.(*fleetv1.BundleDeployment)
						if n == nil || o == nil {
							return false
						}
						return n.Status.SyncGeneration != o.Status.SyncGeneration
					},
					DeleteFunc: func(e event.DeleteEvent) bool {
						return true
					},
				},
			)).
		WithOptions(controller.Options{MaxConcurrentReconciles: r.Workers}).
		Complete(r)
}

//+kubebuilder:rbac:groups=fleet.cattle.io,resources=bundledeployments,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=bundledeployments/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=bundledeployments/finalizers,verbs=update

// Reconcile compares the state specified by the BundleDeployment object
// against the actual state, and decides if the bundle should be deployed.
// The deployed resources are then monitored for drift.
// It also updates the status of the BundleDeployment object with the results.
//
// For more details, check Reconcile and its Result here:
// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.19.0/pkg/reconcile
//
//nolint:gocyclo // refactoring this would make it harder to understand
func (r *BundleDeploymentReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx).WithName("bundledeployment")
	ctx = log.IntoContext(ctx, logger)
	key := req.String()

	// get latest BundleDeployment from cluster
	bd := &fleetv1.BundleDeployment{}
	err := r.Get(ctx, req.NamespacedName, bd)
	if apierrors.IsNotFound(err) {
		// This actually deletes the helm releases if a bundledeployment is deleted or orphaned
		logger.V(1).Info("BundleDeployment deleted, cleaning up helm releases")
		if err := r.Cleanup.CleanupReleases(ctx, key, nil); err != nil {
			logger.Error(err, "Failed to clean up missing bundledeployment", "key", key)
		}

		return ctrl.Result{}, nil
	} else if err != nil {
		return ctrl.Result{}, err
	}
	orig := bd.DeepCopy()

	if bd.Spec.Paused {
		logger.V(1).Info("Bundle paused, clearing drift detection")
		err := r.DriftDetect.Clear(req.String())

		return ctrl.Result{}, err
	}
	if bd.Spec.OffSchedule {
		logger.V(1).Info("Bundle not in schedule, clearing drift detection")
		err := r.DriftDetect.Clear(req.String())

		return ctrl.Result{}, err
	}

	// load the bundledeployment options from the secret, if present
	if bd.Spec.ValuesHash != "" {
		secret := &corev1.Secret{}
		if err := r.Reader.Get(ctx, client.ObjectKey{Namespace: bd.Namespace, Name: bd.Name}, secret); err != nil {
			return ctrl.Result{}, err
		}

		h := helmvalues.HashOptions(secret.Data[helmvalues.ValuesKey], secret.Data[helmvalues.StagedValuesKey])
		if h != bd.Spec.ValuesHash {
			return ctrl.Result{}, fmt.Errorf("retrying, hash mismatch between secret and bundledeployment: actual %s != expected %s", h, bd.Spec.ValuesHash)
		}

		if err := helmvalues.SetOptions(bd, secret.Data); err != nil {
			return ctrl.Result{}, err
		}
	}

	forceDeploy, err := r.copyResourcesFromUpstream(ctx, bd, logger)
	if err != nil {
		return ctrl.Result{}, err
	}

	var merr []error

	// helm deploy the bundledeployment
	if status, err := r.Deployer.DeployBundle(ctx, bd, forceDeploy); err != nil {
		logger.V(1).Info("Failed to deploy bundle", "status", status, "error", err)

		// do not use the returned status, instead set the condition and possibly a timestamp
		bd.Status = setCondition(bd.Status, err, monitor.Cond(fleetv1.BundleDeploymentConditionDeployed))

		merr = append(merr, fmt.Errorf("failed deploying bundle: %w", err))
	} else {
		logger.V(1).Info("Bundle deployed", "status", status)
		bd.Status = setCondition(status, nil, monitor.Cond(fleetv1.BundleDeploymentConditionDeployed))
	}

	// retrieve the resources from the helm history.
	// if we can't retrieve the resources, we don't need to try any of the other operations and requeue now
	resources, err := r.Deployer.Resources(bd.Name, bd.Status.Release)
	if err != nil {
		logger.V(1).Info("Failed to retrieve bundledeployment's resources")
		if statusErr := r.updateStatus(ctx, orig, bd); statusErr != nil {
			merr = append(merr, err)
			merr = append(merr, fmt.Errorf("failed to update the status: %w", statusErr))
		}
		return ctrl.Result{}, errutil.NewAggregate(merr)
	}

	if monitor.ShouldUpdateStatus(bd) {
		// update the bundledeployment status and check if we deploy an agent
		status, err := r.Monitor.UpdateStatus(ctx, bd, resources)
		if err != nil {
			logger.Error(err, "Cannot monitor deployed bundle")

			// if there is an error, do not use the returned status from monitor
			bd.Status = setCondition(bd.Status, err, monitor.Cond(fleetv1.BundleDeploymentConditionMonitored))
			merr = append(merr, fmt.Errorf("failed updating status: %w", err))
		} else {
			// we add to the status from deployer.DeployBundle
			bd.Status = setCondition(status, nil, monitor.Cond(fleetv1.BundleDeploymentConditionMonitored))
		}

		if len(bd.Status.ModifiedStatus) > 0 && monitor.ShouldRedeployAgent(bd) {
			bd.Status.AppliedDeploymentID = ""
			if err := r.Cleanup.OldAgent(ctx, status.ModifiedStatus); err != nil {
				merr = append(merr, fmt.Errorf("failed cleaning old agent: %w", err))
			}
		}
	}

	// update our driftdetect mini controller, which watches deployed resources for drift
	if err := r.DriftDetect.Refresh(ctx, req.String(), bd, resources); err != nil {
		logger.V(1).Info("Failed to refresh drift detection", "step", "drift", "error", err)
		merr = append(merr, fmt.Errorf("failed refreshing drift detection: %w", err))
	}

	// Check if this bundle deployment has overlapping resources with a previously deleted bundle (Overwrites
	// field): if so, requeue to ensure the corresponding Helm release, and therefore its resources, are
	// reinstalled.
	// Overlaps are checked by namespace, kind and name. Resource contents do not matter in this context, as a
	// resource would be deleted by Helm deleting its parent release based on its kind, name and namespace.
	// This requires deleting the release beforehand, to force a new installation as the deployer would otherwise
	// skip re-installing an existing release with no version change.
	// See fleet#3770 for more context.
	if len(orig.Status.ModifiedStatus) > 0 && len(orig.Spec.Options.Overwrites) > 0 {
		for _, ms := range orig.Status.ModifiedStatus {
			if !ms.Create { // missing
				continue
			}
			for _, ow := range orig.Spec.Options.Overwrites {
				if ow.Kind == ms.Kind && ow.Name == ms.Name {
					logger.V(1).Info(
						"Triggering new deployment to overwrite missing resource",
						"kind", ow.Kind,
						"name", ow.Name,
						"namespace", ow.Namespace,
					)

					// Uninstall the release to allow a new reconcile loop to re-install it,
					// resolving the missing resource(s) issue.
					if err := r.Cleanup.CleanupReleases(ctx, key, nil); err != nil {
						logger.V(1).Info("Failed to clean up releases before triggering new deployment", "error", err)
					}

					return ctrl.Result{RequeueAfter: durations.DefaultRequeueAfter}, nil
				}
			}
		}
	}

	if err := r.Cleanup.CleanupReleases(ctx, key, bd); err != nil {
		logger.V(1).Info("Failed to clean up bundledeployment releases", "error", err)
	}

	// final status update
	logger.V(1).Info("Reconcile finished, updating the bundledeployment status")
	if err := r.updateStatus(ctx, orig, bd); apierrors.IsNotFound(err) {
		merr = append(merr, fmt.Errorf("bundledeployment has been deleted: %w", err))
	} else if err != nil {
		merr = append(merr, fmt.Errorf("failed final update to bundledeployment status: %w", err))
	}

	return ctrl.Result{}, errutil.NewAggregate(merr)
}

// copyResourcesFromUpstream copies bd's DownstreamResources, from the downstream cluster's namespace on the management
// cluster to the destination namespace on the downstream cluster, creating that namespace if needed.
// If bd does not have any DownstreamResources, this method does not issue any API server calls.
//
//nolint:dupl // Same pattern between secrets and config map, but different logic.
func (r *BundleDeploymentReconciler) copyResourcesFromUpstream(
	ctx context.Context,
	bd *fleetv1.BundleDeployment,
	logger logr.Logger,
) (bool, error) {
	if !experimental.CopyResourcesDownstreamEnabled() {
		return false, nil
	}

	if len(bd.Spec.Options.DownstreamResources) == 0 {
		return false, nil
	}

	destNS := namespaces.GetDeploymentNS(r.DefaultNamespace, bd.Spec.Options)

	ns := corev1.Namespace{ObjectMeta: metav1.ObjectMeta{Name: destNS}}
	if err := r.LocalClient.Get(ctx, types.NamespacedName{Name: ns.Name}, &ns); apierrors.IsNotFound(err) {
		if err := r.LocalClient.Create(ctx, &ns); err != nil {
			logger.Info(err.Error())
			return false, err
		}

		logger.V(1).Info("Created namespace to copy resources from upstream", "namespace", ns.Name)
	}

	requiresBDUpdate := false

	for _, rsc := range bd.Spec.Options.DownstreamResources {
		switch strings.ToLower(rsc.Kind) {
		case "secret":
			var s corev1.Secret
			if err := r.Reader.Get(ctx, client.ObjectKey{Namespace: bd.Namespace, Name: rsc.Name}, &s); err != nil {
				// The bundle deployment is actually created by the bundle reconciler _before_
				// these objects are copied to the cluster's namespace, hence retries should happen if
				// they are not found.

				return false, fmt.Errorf(
					"could not get secret %s/%s from upstream namespace for copying: %w",
					bd.Namespace,
					rsc.Name,
					err,
				)
			}

			s.Namespace = destNS
			s.ResourceVersion = ""
			if s.Labels == nil {
				s.Labels = map[string]string{}
			}
			s.Labels[fleetv1.BundleDeploymentOwnershipLabel] = bd.Name

			updated := s.DeepCopy()
			op, err := controllerutil.CreateOrUpdate(ctx, r.LocalClient, &s, func() error {
				s.Data = updated.Data
				s.StringData = updated.StringData

				return nil
			})
			if err != nil {
				return false, fmt.Errorf("failed to create or update secret %s/%s downstream: %w", bd.Namespace, rsc.Name, err)
			}

			requiresBDUpdate = op == controllerutil.OperationResultUpdated

		case "configmap":
			var cm corev1.ConfigMap
			if err := r.Reader.Get(ctx, client.ObjectKey{Namespace: bd.Namespace, Name: rsc.Name}, &cm); err != nil {
				// The bundle deployment is actually created by the bundle reconciler _before_
				// these objects are copied to the cluster's namespace, hence retries should happen if
				// they are not found.

				return false, fmt.Errorf(
					"could not get config map %s/%s from upstream namespace for copying: %w",
					bd.Namespace,
					rsc.Name,
					err,
				)
			}
			cm.Namespace = destNS
			cm.ResourceVersion = ""
			if cm.Labels == nil {
				cm.Labels = map[string]string{}
			}
			cm.Labels[fleetv1.BundleDeploymentOwnershipLabel] = bd.Name

			updated := cm.DeepCopy()
			op, err := controllerutil.CreateOrUpdate(ctx, r.LocalClient, &cm, func() error {
				cm.Data = updated.Data
				cm.BinaryData = updated.BinaryData

				return nil
			})
			if err != nil {
				return false, fmt.Errorf("failed to create or update configmap %s/%s downstream: %w", bd.Namespace, rsc.Name, err)
			}

			requiresBDUpdate = op == controllerutil.OperationResultUpdated
		default:
			return false, fmt.Errorf("unknown resource type for copy to downstream cluster: %q", rsc.Kind)
		}
	}

	return requiresBDUpdate, nil
}

func (r *BundleDeploymentReconciler) updateStatus(ctx context.Context, orig *fleetv1.BundleDeployment, obj *fleetv1.BundleDeployment) error {
	statusPatch := client.MergeFrom(orig)
	if patchData, err := statusPatch.Data(obj); err == nil && string(patchData) == "{}" {
		// skip update if patch is empty
		return nil
	}
	return r.Status().Patch(ctx, obj, statusPatch)
}

// setCondition sets the condition and updates the timestamp, if the condition changed
func setCondition(newStatus fleetv1.BundleDeploymentStatus, err error, cond monitor.Cond) fleetv1.BundleDeploymentStatus {
	cond.SetError(&newStatus, "", ignoreConflict(err))
	return newStatus
}

func ignoreConflict(err error) error {
	if apierrors.IsConflict(err) {
		return nil
	}
	return err
}



================================================
FILE: internal/cmd/agent/controller/drift_controller.go
================================================
package controller

import (
	"context"
	"fmt"
	"time"

	"github.com/rancher/fleet/internal/cmd/agent/deployer"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/driftdetect"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/monitor"
	fleetv1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"github.com/go-logr/logr"
	"github.com/rancher/wrangler/v3/pkg/condition"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	errutil "k8s.io/apimachinery/pkg/util/errors"
	"k8s.io/client-go/util/workqueue"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"
	"sigs.k8s.io/controller-runtime/pkg/source"
)

type DriftReconciler struct {
	client.Client
	Scheme *runtime.Scheme

	Deployer    *deployer.Deployer
	Monitor     *monitor.Monitor
	DriftDetect *driftdetect.DriftDetect

	DriftChan chan event.TypedGenericEvent[*fleetv1.BundleDeployment]

	Workers int
}

// enqueueDelay is used as an artificial delay for enqueuing BundleDeployment reconciliation requests
// This allows aggregating multiple consecutive events on deployed resources, reducing the number of BundleDeployment (and Bundle) reconciliations at the cost of introducing a delay in the notification
const enqueueDelay = 5 * time.Second

// SetupWithManager sets up the controller with the Manager.
func (r *DriftReconciler) SetupWithManager(mgr ctrl.Manager) error {
	src := source.Channel(r.DriftChan, enqueueRequestHandlerWithDelay(enqueueDelay))
	return ctrl.NewControllerManagedBy(mgr).
		Named("drift-reconciler").
		WatchesRawSource(src).
		WithOptions(controller.Options{MaxConcurrentReconciles: r.Workers}).
		Complete(r)

}

// Reconcile is triggered via a channel from the driftdetect mini controller,
// which watches deployed resources for drift. It does so by creating a plan
// and comparing it to the current state.
// It will update the status of the BundleDeployment and correct drift if enabled.
//
// For more details, check Reconcile and its Result here:
// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.19.0/pkg/reconcile
func (r *DriftReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx).WithName("drift")
	ctx = log.IntoContext(ctx, logger)

	// get latest BundleDeployment from cluster
	bd := &fleetv1.BundleDeployment{}
	err := r.Get(ctx, req.NamespacedName, bd)
	if apierrors.IsNotFound(err) {
		return ctrl.Result{}, nil
	} else if err != nil {
		return ctrl.Result{}, err
	}

	orig := bd.DeepCopy()
	if bd.Spec.CorrectDrift != nil {
		logger = logger.WithValues("enabled", bd.Spec.CorrectDrift.Enabled, "force", bd.Spec.CorrectDrift.Force)
	}

	if bd.Spec.Paused {
		logger.V(1).Info("Bundle paused, clearing drift detection")
		err := r.DriftDetect.Clear(req.String())

		return ctrl.Result{}, err
	}
	if bd.Spec.OffSchedule {
		logger.V(1).Info("Bundle not in schedule, clearing drift detection")
		err := r.DriftDetect.Clear(req.String())

		return ctrl.Result{}, err
	}

	merr := []error{}

	// retrieve the resources from the helm history.
	// if we can't retrieve the resources, we don't need to try any of the other operations and requeue now
	resources, err := r.Deployer.Resources(bd.Name, bd.Status.Release)
	if err != nil {
		logger.V(1).Info("Failed to retrieve bundledeployment's resources")
		return ctrl.Result{}, err
	}

	// return early if the bundledeployment is still being installed
	if !monitor.ShouldUpdateStatus(bd) {
		logger.V(1).Info("BundleDeployment is still being installed")
		return ctrl.Result{}, nil
	}

	// update the bundledeployment status from the helm resource list
	bd.Status, err = r.Monitor.UpdateStatus(ctx, bd, resources)
	if err != nil {
		logger.Error(err, "Cannot monitor deployed bundle")
	}

	// run drift correction
	if len(bd.Status.ModifiedStatus) > 0 && bd.Spec.CorrectDrift != nil && bd.Spec.CorrectDrift.Enabled {
		logger.V(1).Info("Removing external changes")
		if release, err := r.Deployer.RemoveExternalChanges(ctx, bd); err != nil {
			merr = append(merr, fmt.Errorf("failed reconciling drift: %w", err))
			// Propagate drift correction error to bundle deployment status.
			condition.Cond(fleetv1.BundleDeploymentConditionReady).SetError(&bd.Status, "", err)
		} else {
			bd.Status.Release = release
		}
	}

	// final status update
	if err := r.updateStatus(ctx, logger, orig, bd); err != nil {
		if apierrors.IsNotFound(err) {
			merr = append(merr, fmt.Errorf("bundledeployment has been deleted: %w", err))
		} else {
			merr = append(merr, fmt.Errorf("failed final update to bundledeployment status: %w", err))
		}
	}

	return ctrl.Result{}, errutil.NewAggregate(merr)
}

func (r *DriftReconciler) updateStatus(ctx context.Context, logger logr.Logger, orig *fleetv1.BundleDeployment, obj *fleetv1.BundleDeployment) error {
	statusPatch := client.MergeFrom(orig)

	// Pre-calculate patch contents, to skip request if it's empty
	patchData, err := statusPatch.Data(obj)
	if err == nil && string(patchData) == "{}" {
		return nil
	}

	if err := r.Status().Patch(ctx, obj, statusPatch); err != nil {
		return err
	}

	logger.V(1).Info("Reconcile finished, bundledeployment status updated", "statusPatch", string(patchData))
	return nil
}

// enqueueRequestHandlerWithDelay implements a TypedEventHandler that introduces a constant delay in the resources being enqueued
// Due to how workqueue.TypedDelayingInterface's AddAfter is implemented, successive calls with the same key are aggregated.
// Only implemented for Generic events, as this is only meant to be used from with source.Channel to receive internal events, not from an informer
func enqueueRequestHandlerWithDelay(delay time.Duration) handler.TypedEventHandler[*fleetv1.BundleDeployment, reconcile.Request] {
	return &handler.TypedFuncs[*fleetv1.BundleDeployment, reconcile.Request]{
		GenericFunc: func(ctx context.Context, e event.TypedGenericEvent[*fleetv1.BundleDeployment], w workqueue.TypedRateLimitingInterface[reconcile.Request]) {
			if e.Object == nil {
				log.FromContext(ctx).Error(nil, "GenericEvent received with no metadata", "event", e)
				return
			}
			w.AddAfter(reconcile.Request{NamespacedName: types.NamespacedName{
				Name:      e.Object.GetName(),
				Namespace: e.Object.GetNamespace(),
			}}, delay)
		},
	}

}



================================================
FILE: internal/cmd/agent/deployer/deployer.go
================================================
package deployer

import (
	"context"
	"errors"
	"fmt"
	"reflect"
	"regexp"
	"strings"

	"github.com/rancher/fleet/internal/bundlereader"
	"github.com/rancher/fleet/internal/helmdeployer"
	"github.com/rancher/fleet/internal/manifest"
	"github.com/rancher/fleet/internal/ocistorage"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"github.com/rancher/wrangler/v3/pkg/condition"
	"github.com/rancher/wrangler/v3/pkg/kv"

	"github.com/go-logr/logr"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

type Deployer struct {
	client         client.Client
	upstreamClient client.Reader
	lookup         Lookup
	helm           *helmdeployer.Helm
}

type Lookup interface {
	Get(ctx context.Context, client client.Reader, id string) (*manifest.Manifest, error)
}

func New(localClient client.Client, upstreamClient client.Reader, lookup Lookup, deployer *helmdeployer.Helm) *Deployer {
	return &Deployer{
		client:         localClient,
		upstreamClient: upstreamClient,
		lookup:         lookup,
		helm:           deployer,
	}
}

func (d *Deployer) Resources(name string, releaseID string) (*helmdeployer.Resources, error) {
	return d.helm.Resources(name, releaseID)
}

func (d *Deployer) RemoveExternalChanges(ctx context.Context, bd *fleet.BundleDeployment) (string, error) {
	return d.helm.RemoveExternalChanges(ctx, bd)
}

// DeployBundle deploys the bundle deployment with the helm SDK. It does not
// mutate bd, instead it returns the modified status
// If force is true, bd will be upgraded even if its contents have not changed; this is useful for
// applying changes coming from external resources, such as those referenced through valuesFrom.
func (d *Deployer) DeployBundle(
	ctx context.Context,
	bd *fleet.BundleDeployment,
	force bool,
) (fleet.BundleDeploymentStatus, error) {
	status := bd.Status
	logger := log.FromContext(ctx).WithName("deploy-bundle").WithValues("deploymentID", bd.Spec.DeploymentID, "appliedDeploymentID", status.AppliedDeploymentID)

	if err := d.checkDependency(ctx, bd); err != nil {
		logger.V(1).Info("Bundle has a dependency that is not ready", "error", err)
		return status, err
	}

	releaseID, err := d.helmdeploy(ctx, logger, bd, force)

	if err != nil {
		// When an error from DeployBundle is returned it causes DeployBundle
		// to requeue and keep trying to deploy on a loop. If there is something
		// wrong with the deployed manifests this will be a loop that re-deploying
		// cannot fix. Here we catch those errors and update the status to note
		// the problem while skipping the constant requeuing.
		if do, newStatus := deployErrToStatus(err, status); do {
			// Setting the release to an empty string removes the previous
			// release name. When a deployment fails the release name is not
			// returned. Keeping the old release name can lead to other functions
			// looking up old data in the history and presenting the wrong status.
			// For example, the deployManager.Deploy function will find the old
			// release and not return an error. It will set everything as if the
			// current one is running properly.
			newStatus.Release = ""
			newStatus.AppliedDeploymentID = bd.Spec.DeploymentID
			return newStatus, nil
		}
		return status, err
	}
	status.Release = releaseID
	status.AppliedDeploymentID = bd.Spec.DeploymentID

	if err := d.setNamespaceLabelsAndAnnotations(ctx, bd, releaseID); err != nil {
		return fleet.BundleDeploymentStatus{}, err
	}

	// Setting the error to nil clears any existing error
	condition.Cond(fleet.BundleDeploymentConditionInstalled).SetError(&status, "", nil)
	return status, nil
}

// Deploy the bundle deployment, i.e. with helmdeployer.
// This loads the manifest and the contents from the upstream cluster.
// If force is true, checks on whether the bundle deployment exists will be skipped, leading to the bundle deployment
// being updated even if its deployment ID has not changed.
func (d *Deployer) helmdeploy(ctx context.Context, logger logr.Logger, bd *fleet.BundleDeployment, force bool) (string, error) {
	if !force && bd.Spec.DeploymentID == bd.Status.AppliedDeploymentID {
		if ok, err := d.helm.EnsureInstalled(bd.Name, bd.Status.Release); err != nil {
			return "", err
		} else if ok {
			return bd.Status.Release, nil
		}
	}
	manifestID, _ := kv.Split(bd.Spec.DeploymentID, ":")
	var (
		m   *manifest.Manifest
		err error
	)
	switch {
	case bd.Spec.OCIContents:
		oci := ocistorage.NewOCIWrapper()
		secretID := client.ObjectKey{Name: manifestID, Namespace: bd.Namespace}
		opts, err := ocistorage.ReadOptsFromSecret(ctx, d.upstreamClient, secretID)
		if err != nil {
			return "", err
		}
		m, err = oci.PullManifest(ctx, opts, manifestID)
		if err != nil {
			return "", err
		}
		// Verify that the calculated manifestID for the manifest
		// we just downloaded matches the expected one.
		// Otherwise, the manifest will be considered incorrect or corrupted.
		actualID, err := m.ID()
		if err != nil {
			return "", err
		}
		if actualID != manifestID {
			return "", fmt.Errorf("invalid or corrupt manifest. Expecting id: %q, got %q", manifestID, actualID)
		}
	case bd.Spec.HelmChartOptions != nil:
		m, err = bundlereader.GetManifestFromHelmChart(ctx, d.upstreamClient, bd)
		if err != nil {
			return "", err
		}
	default:
		m, err = d.lookup.Get(ctx, d.upstreamClient, manifestID)
		if err != nil {
			return "", err
		}
	}

	m.Commit = bd.Labels[fleet.CommitLabel]
	release, err := d.helm.Deploy(ctx, bd.Name, m, bd.Spec.Options)
	if err != nil {
		return "", err
	}

	resourceID := helmdeployer.ReleaseToResourceID(release)

	logger.Info("Deployed bundle", "release", resourceID, "DeploymentID", bd.Spec.DeploymentID)

	return resourceID, nil
}

// setNamespaceLabelsAndAnnotations updates the namespace for the release, applying all labels and annotations to that namespace as configured in the bundle spec.
func (d *Deployer) setNamespaceLabelsAndAnnotations(ctx context.Context, bd *fleet.BundleDeployment, releaseID string) error {
	if bd.Spec.Options.NamespaceLabels == nil && bd.Spec.Options.NamespaceAnnotations == nil {
		return nil
	}

	ns, err := d.fetchNamespace(ctx, releaseID)
	if err != nil {
		return err
	}

	if reflect.DeepEqual(bd.Spec.Options.NamespaceLabels, ns.Labels) && reflect.DeepEqual(bd.Spec.Options.NamespaceAnnotations, ns.Annotations) {
		return nil
	}

	if bd.Spec.Options.NamespaceLabels != nil {
		addLabelsFromOptions(ns.Labels, bd.Spec.Options.NamespaceLabels)
	}
	if bd.Spec.Options.NamespaceAnnotations != nil {
		if ns.Annotations == nil {
			ns.Annotations = map[string]string{}
		}
		addAnnotationsFromOptions(ns.Annotations, bd.Spec.Options.NamespaceAnnotations)
	}
	err = d.updateNamespace(ctx, ns)
	if err != nil {
		return err
	}

	return nil
}

// updateNamespace updates a namespace resource in the cluster.
func (d *Deployer) updateNamespace(ctx context.Context, ns *corev1.Namespace) error {
	err := d.client.Update(ctx, ns)
	if err != nil {
		return err
	}

	return nil
}

// fetchNamespace gets the namespace matching the release ID. Returns an error if none is found.
// releaseID is composed of release.Namespace/release.Name/release.Version
func (d *Deployer) fetchNamespace(ctx context.Context, releaseID string) (*corev1.Namespace, error) {
	namespace := strings.Split(releaseID, "/")[0]
	ns := &corev1.Namespace{}
	err := d.client.Get(ctx, types.NamespacedName{Name: namespace}, ns)
	if err != nil {
		return nil, err
	}
	return ns, nil
}

// addLabelsFromOptions updates nsLabels so that it only contains all labels specified in optLabels, plus the `kubernetes.io/metadata.name` labels added by kubernetes when creating the namespace.
func addLabelsFromOptions(nsLabels map[string]string, optLabels map[string]string) {
	for k, v := range optLabels {
		nsLabels[k] = v
	}

	// Delete labels not defined in the options.
	// Keep the `kubernetes.io/metadata.name` label as it is added by kubernetes when creating the namespace.
	for k := range nsLabels {
		if _, ok := optLabels[k]; k != corev1.LabelMetadataName && !ok {
			delete(nsLabels, k)
		}
	}
}

// addAnnotationsFromOptions updates nsAnnotations so that it only contains all annotations specified in optAnnotations.
func addAnnotationsFromOptions(nsAnnotations map[string]string, optAnnotations map[string]string) {
	for k, v := range optAnnotations {
		nsAnnotations[k] = v
	}

	// Delete Annotations not defined in the options.
	for k := range nsAnnotations {
		if _, ok := optAnnotations[k]; !ok {
			delete(nsAnnotations, k)
		}
	}
}

// deployErrToStatus converts an error into a status update
func deployErrToStatus(err error, status fleet.BundleDeploymentStatus) (bool, fleet.BundleDeploymentStatus) {
	if err == nil {
		return false, status
	}

	msg := err.Error()

	// The following error conditions are turned into a status
	// Note: these error strings are returned by the Helm SDK and its dependencies
	re := regexp.MustCompile(
		"(timed out waiting for the condition)|" + // a Helm wait occurs and it times out
			"(error validating data)|" + // manifests fail to pass validation
			"(chart requires kubeVersion)|" + // kubeVersion mismatch
			"(annotation validation error)|" + // annotations fail to pass validation
			"(failed, and has been rolled back due to atomic being set)|" + // atomic is set and a rollback occurs
			"(YAML parse error)|" + // YAML is broken in source files
			"(Forbidden: updates to [0-9A-Za-z]+ spec for fields other than [0-9A-Za-z ']+ are forbidden)|" + // trying to update fields that cannot be updated
			"(Forbidden: spec is immutable after creation)|" + // trying to modify immutable spec
			"(chart requires kubeVersion: [0-9A-Za-z\\.\\-<>=]+ which is incompatible with Kubernetes)", // trying to deploy to incompatible Kubernetes
	)
	if re.MatchString(msg) {
		status.Ready = false
		status.NonModified = true

		// The ready status is displayed throughout the UI. Setting this as well
		// as installed enables the status to be displayed when looking at the
		// CRD or a UI build on that.
		readyError := fmt.Errorf("not ready: %s", msg)
		condition.Cond(fleet.BundleDeploymentConditionReady).SetError(&status, "", readyError)

		// Deployed and Monitored conditions are handled in the reconciler.
		// They are true if the deployer returns no error and false if
		// an error is returned. When an error is returned they are
		// requeued. To capture the state of an error that is not
		// returned a new condition is being captured. Ready is the
		// condition that displays for status in general and it is used
		// for the readiness of resources. Only when we cannot capture
		// the status of resources, like here, can use use it for a
		// message like the above. The Installed condition lets us have
		// a condition to capture the error that can be bubbled up for
		// Bundles and Gitrepos to consume.
		installError := fmt.Errorf("not installed: %s", msg)
		condition.Cond(fleet.BundleDeploymentConditionInstalled).SetError(&status, "", installError)

		return true, status
	}

	// The case that the bundle is already in an error state. A previous
	// condition with the error should already be applied.
	if errors.Is(err, helmdeployer.ErrNoResourceID) {
		return true, status
	}

	return false, status
}

func (d *Deployer) checkDependency(ctx context.Context, bd *fleet.BundleDeployment) error {
	var depBundleList []string
	bundleNamespace := bd.Labels[fleet.BundleNamespaceLabel]
	for _, depend := range bd.Spec.DependsOn {
		// skip empty BundleRef definitions. Possible if there is a typo in the yaml
		if depend.Name != "" || depend.Selector != nil {
			ls := &metav1.LabelSelector{}
			if depend.Selector != nil {
				ls = depend.Selector
			}

			// depend.Name is just a shortcut for matchLabels: {bundle-name: name}
			if depend.Name != "" {
				ls = metav1.AddLabelToSelector(ls, fleet.BundleLabel, depend.Name)
				ls = metav1.AddLabelToSelector(ls, fleet.BundleNamespaceLabel, bundleNamespace)
			}

			selector, err := metav1.LabelSelectorAsSelector(ls)
			if err != nil {
				return err
			}

			bds := fleet.BundleDeploymentList{}
			err = d.upstreamClient.List(ctx, &bds, client.MatchingLabelsSelector{Selector: selector}, client.InNamespace(bd.Namespace))
			if err != nil {
				return err
			}

			if len(bds.Items) == 0 {
				return fmt.Errorf("list bundledeployments: no bundles matching labels %s in namespace %s", selector.String(), bundleNamespace)
			}

			for _, depBundle := range bds.Items {
				c := condition.Cond("Ready")
				if c.IsTrue(depBundle) {
					continue
				} else {
					depBundleList = append(depBundleList, depBundle.Name)
				}
			}
		}
	}

	if len(depBundleList) != 0 {
		return fmt.Errorf("dependent bundle(s) are not ready: %v", depBundleList)
	}

	return nil
}



================================================
FILE: internal/cmd/agent/deployer/deployer_test.go
================================================
package deployer

import (
	"context"
	"testing"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	clientgoscheme "k8s.io/client-go/kubernetes/scheme"
	"sigs.k8s.io/controller-runtime/pkg/client/fake"
)

func TestSetNamespaceLabelsAndAnnotations(t *testing.T) {
	tests := map[string]struct {
		bd         *fleet.BundleDeployment
		ns         corev1.Namespace
		release    string
		expectedNs corev1.Namespace
	}{
		"Empty sets of NamespaceLabels and NamespaceAnnotations are supported": {
			bd: &fleet.BundleDeployment{Spec: fleet.BundleDeploymentSpec{
				Options: fleet.BundleDeploymentOptions{
					NamespaceLabels:      nil, // equivalent to map[string]string{}
					NamespaceAnnotations: nil,
				},
			}},
			ns: corev1.Namespace{
				ObjectMeta: metav1.ObjectMeta{
					Name:   "namespace",
					Labels: map[string]string{"kubernetes.io/metadata.name": "namespace"},
				},
			},
			release: "namespace/foo/bar",
			expectedNs: corev1.Namespace{
				ObjectMeta: metav1.ObjectMeta{
					Name:        "namespace",
					Labels:      map[string]string{"kubernetes.io/metadata.name": "namespace"},
					Annotations: nil,
				},
			},
		},

		"NamespaceLabels and NamespaceAnnotations are appended": {
			bd: &fleet.BundleDeployment{Spec: fleet.BundleDeploymentSpec{
				Options: fleet.BundleDeploymentOptions{
					NamespaceLabels:      map[string]string{"optLabel1": "optValue1", "optLabel2": "optValue2"},
					NamespaceAnnotations: map[string]string{"optAnn1": "optValue1"},
				},
			}},
			ns: corev1.Namespace{
				ObjectMeta: metav1.ObjectMeta{
					Name:   "namespace",
					Labels: map[string]string{"kubernetes.io/metadata.name": "namespace"},
				},
			},
			release: "namespace/foo/bar",
			expectedNs: corev1.Namespace{
				ObjectMeta: metav1.ObjectMeta{
					Name:        "namespace",
					Labels:      map[string]string{"kubernetes.io/metadata.name": "namespace", "optLabel1": "optValue1", "optLabel2": "optValue2"},
					Annotations: map[string]string{"optAnn1": "optValue1"},
				},
			},
		},

		"NamespaceLabels and NamespaceAnnotations removes entries that are not in the options, except the name label": {
			bd: &fleet.BundleDeployment{Spec: fleet.BundleDeploymentSpec{
				Options: fleet.BundleDeploymentOptions{
					NamespaceLabels:      map[string]string{"optLabel": "optValue"},
					NamespaceAnnotations: map[string]string{},
				},
			}},
			ns: corev1.Namespace{
				ObjectMeta: metav1.ObjectMeta{
					Name:        "namespace",
					Labels:      map[string]string{"nsLabel": "nsValue", "kubernetes.io/metadata.name": "namespace"},
					Annotations: map[string]string{"nsAnn": "nsValue"},
				},
			},
			release: "namespace/foo/bar",
			expectedNs: corev1.Namespace{
				ObjectMeta: metav1.ObjectMeta{
					Name:        "namespace",
					Labels:      map[string]string{"optLabel": "optValue", "kubernetes.io/metadata.name": "namespace"},
					Annotations: map[string]string{},
				},
			},
		},

		"NamespaceLabels and NamespaceAnnotations updates existing values": {
			bd: &fleet.BundleDeployment{Spec: fleet.BundleDeploymentSpec{
				Options: fleet.BundleDeploymentOptions{
					NamespaceLabels:      map[string]string{"bdLabel": "labelUpdated"},
					NamespaceAnnotations: map[string]string{"bdAnn": "annUpdated"},
				},
			}},
			ns: corev1.Namespace{
				ObjectMeta: metav1.ObjectMeta{
					Name:        "namespace",
					Labels:      map[string]string{"bdLabel": "nsValue", "kubernetes.io/metadata.name": "namespace"},
					Annotations: map[string]string{"bdAnn": "nsValue"},
				},
			},
			release: "namespace/foo/bar",
			expectedNs: corev1.Namespace{
				ObjectMeta: metav1.ObjectMeta{
					Name:        "namespace",
					Labels:      map[string]string{"bdLabel": "labelUpdated", "kubernetes.io/metadata.name": "namespace"},
					Annotations: map[string]string{"bdAnn": "annUpdated"},
				},
			},
		},
	}

	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			scheme := runtime.NewScheme()
			utilruntime.Must(clientgoscheme.AddToScheme(scheme))
			client := fake.NewClientBuilder().WithScheme(scheme).WithObjects(&test.ns).Build()
			h := Deployer{
				client: client,
			}
			err := h.setNamespaceLabelsAndAnnotations(context.Background(), test.bd, test.release)
			if err != nil {
				t.Errorf("expected nil error: got %v", err)
			}

			ns := &corev1.Namespace{}
			err = client.Get(context.Background(), types.NamespacedName{Name: test.ns.Name}, ns)
			if err != nil {
				t.Errorf("expected nil error: got %v", err)
			}
			for k, v := range test.expectedNs.Labels {
				if ns.Labels[k] != v {
					t.Errorf("expected label %s: %s, got %s", k, v, ns.Labels[k])
				}
			}
			for k, v := range test.expectedNs.Annotations {
				if ns.Annotations[k] != v {
					t.Errorf("expected annotation %s: %s, got %s", k, v, ns.Annotations[k])
				}
			}
		})
	}
}

func TestSetNamespaceLabelsAndAnnotationsError(t *testing.T) {
	bd := &fleet.BundleDeployment{Spec: fleet.BundleDeploymentSpec{
		Options: fleet.BundleDeploymentOptions{
			NamespaceLabels:      map[string]string{"optLabel1": "optValue1", "optLabel2": "optValue2"},
			NamespaceAnnotations: map[string]string{"optAnn1": "optValue1"},
		},
	}}
	release := "test/foo/bar"

	scheme := runtime.NewScheme()
	utilruntime.Must(clientgoscheme.AddToScheme(scheme))
	client := fake.NewClientBuilder().WithScheme(scheme).Build()
	h := Deployer{
		client: client,
	}

	err := h.setNamespaceLabelsAndAnnotations(context.Background(), bd, release)

	if !apierrors.IsNotFound(err) {
		t.Errorf("expected not found error: got %v", err)
	}
}



================================================
FILE: internal/cmd/agent/deployer/cleanup/cleanup.go
================================================
package cleanup

import (
	"context"
	"fmt"
	"sync"
	"time"

	"github.com/go-logr/logr"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/kv"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/merr"
	"github.com/rancher/fleet/internal/helmdeployer"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"

	apierror "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/api/meta"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/apimachinery/pkg/util/wait"
	"k8s.io/client-go/dynamic"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

type HelmDeployer interface {
	NewListAction() helmdeployer.ListAction
	ListDeployments(list helmdeployer.ListAction) ([]helmdeployer.DeployedBundle, error)
	DeleteRelease(ctx context.Context, deployed helmdeployer.DeployedBundle) error
	Delete(ctx context.Context, name string) error
}

type Cleanup struct {
	client           client.Client
	fleetNamespace   string
	defaultNamespace string
	helmDeployer     HelmDeployer
	cleanupOnce      sync.Once

	mapper meta.RESTMapper
	// localDynamicClient is a dynamic client for the cluster the agent is running on (local cluster).
	localDynamicClient *dynamic.DynamicClient

	garbageCollectionInterval time.Duration
}

func New(
	upstream client.Client,
	mapper meta.RESTMapper,
	localDynamicClient *dynamic.DynamicClient,
	deployer HelmDeployer,
	fleetNamespace string,
	defaultNamespace string,
	garbageCollectionInterval time.Duration,
) *Cleanup {
	if garbageCollectionInterval == 0 {
		garbageCollectionInterval = durations.GarbageCollect
	}

	return &Cleanup{
		client:                    upstream,
		mapper:                    mapper,
		localDynamicClient:        localDynamicClient,
		helmDeployer:              deployer,
		fleetNamespace:            fleetNamespace,
		defaultNamespace:          defaultNamespace,
		garbageCollectionInterval: garbageCollectionInterval,
	}
}

func (c *Cleanup) OldAgent(ctx context.Context, modifiedStatuses []fleet.ModifiedStatus) error {
	logger := log.FromContext(ctx).WithName("cleanup-old-agent")
	var errs []error
	for _, modified := range modifiedStatuses {
		if modified.Delete {
			gvk := schema.FromAPIVersionAndKind(modified.APIVersion, modified.Kind)
			mapping, err := c.mapper.RESTMapping(gvk.GroupKind(), gvk.Version)
			if err != nil {
				errs = append(errs, fmt.Errorf("mapping resource for %s for agent cleanup: %w", gvk, err))
				continue
			}

			logger.Info("Removing old agent resource", "namespace", modified.Namespace, "name", modified.Name, "gvk", gvk)
			err = c.localDynamicClient.Resource(mapping.Resource).Namespace(modified.Namespace).Delete(ctx, modified.Name, metav1.DeleteOptions{})
			if err != nil {
				errs = append(errs, fmt.Errorf("deleting %s/%s for %s for agent cleanup: %w", modified.Namespace, modified.Name, gvk, err))
				continue
			}
		}
	}
	return merr.NewErrors(errs...)
}

func (c *Cleanup) CleanupReleases(ctx context.Context, key string, bd *fleet.BundleDeployment) error {
	c.cleanupOnce.Do(func() {
		go c.garbageCollect(ctx)
	})

	if bd != nil {
		return nil
	}
	return c.delete(ctx, key)
}

func (c *Cleanup) garbageCollect(ctx context.Context) {
	logger := log.FromContext(ctx).WithName("garbage-collect")
	for {
		if err := c.cleanup(ctx, logger); err != nil {
			logger.Error(err, "failed to cleanup orphaned releases")
		}
		select {
		case <-ctx.Done():
			return
		case <-time.After(wait.Jitter(c.garbageCollectionInterval, 1.0)):
		}
	}
}

func (c *Cleanup) cleanup(ctx context.Context, logger logr.Logger) error {
	deployed, err := c.helmDeployer.ListDeployments(c.helmDeployer.NewListAction())
	if err != nil {
		return err
	}

	for _, deployed := range deployed {
		bundleDeployment := &fleet.BundleDeployment{}
		err := c.client.Get(ctx, types.NamespacedName{Namespace: c.fleetNamespace, Name: deployed.BundleID}, bundleDeployment)
		if apierror.IsNotFound(err) {
			// found a helm secret, but no bundle deployment, so uninstall the release
			logger.Info("Deleting orphan bundle ID, helm uninstall", "bundleID", deployed.BundleID, "release", deployed.ReleaseName)
			if err := c.helmDeployer.DeleteRelease(ctx, deployed); err != nil {
				return err
			}

			return nil
		} else if err != nil {
			return err
		}

		key := releaseKey(c.defaultNamespace, bundleDeployment)
		if key != deployed.ReleaseName {
			// found helm secret and bundle deployment for BundleID, but release name doesn't match, so delete the release
			logger.Info("Deleting unknown bundle ID, helm uninstall", "bundleID", deployed.BundleID, "release", deployed.ReleaseName, "expectedRelease", key)
			if err := c.helmDeployer.DeleteRelease(ctx, deployed); err != nil {
				return err
			}
		}
	}

	return nil
}

func (c *Cleanup) delete(ctx context.Context, bundleDeploymentKey string) error {
	_, name := kv.RSplit(bundleDeploymentKey, "/")
	return c.helmDeployer.Delete(ctx, name)
}

// releaseKey returns a deploymentKey from namespace+releaseName
func releaseKey(ns string, bd *fleet.BundleDeployment) string {
	if bd.Spec.Options.TargetNamespace != "" {
		ns = bd.Spec.Options.TargetNamespace
	} else if bd.Spec.Options.DefaultNamespace != "" {
		ns = bd.Spec.Options.DefaultNamespace
	}

	if bd.Spec.Options.Helm == nil || bd.Spec.Options.Helm.ReleaseName == "" {
		return ns + "/" + bd.Name
	}
	return ns + "/" + bd.Spec.Options.Helm.ReleaseName
}



================================================
FILE: internal/cmd/agent/deployer/cleanup/cleanup_test.go
================================================
//go:generate mockgen --build_flags=--mod=mod -destination=../../../../mocks/helm_deployer_mock.go -package=mocks github.com/rancher/fleet/internal/cmd/agent/deployer/cleanup HelmDeployer

package cleanup

import (
	"context"
	"fmt"
	"testing"
	"time"

	"github.com/rancher/fleet/internal/helmdeployer"
	"github.com/rancher/fleet/internal/mocks"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"go.uber.org/mock/gomock"
	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

func TestCleanupReleases(t *testing.T) {
	fleetNS := "foo"   // Used to get bundle deployments by bundle ID
	defaultNS := "bar" // Used to compute the expected release key

	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()

	deployedBundles := []helmdeployer.DeployedBundle{
		{
			BundleID:    "ID1",
			ReleaseName: fmt.Sprintf("%s/TestRelease1", defaultNS),
		},
		{
			BundleID:    "ID2",
			ReleaseName: fmt.Sprintf("%s/TestRelease2", defaultNS),
		},
		{
			BundleID:    "ID3",
			ReleaseName: fmt.Sprintf("%s/TestRelease3", defaultNS),
		},
	}

	mockClient := mocks.NewMockK8sClient(mockCtrl)
	bd := &fleet.BundleDeployment{}
	mockClient.EXPECT().Get(gomock.Any(), types.NamespacedName{Namespace: fleetNS, Name: "ID1"}, bd).DoAndReturn(
		func(_ context.Context, _ types.NamespacedName, bd *fleet.BundleDeployment, _ ...interface{}) error {
			bd.Spec.Options.TargetNamespace = defaultNS
			bd.Spec.Options.Helm = &fleet.HelmOptions{
				ReleaseName: "TestRelease1", // will be kept
			}

			return nil
		},
	)

	mockClient.EXPECT().Get(gomock.Any(), types.NamespacedName{Namespace: fleetNS, Name: "ID2"}, bd).DoAndReturn(
		func(_ context.Context, _ types.NamespacedName, bd *fleet.BundleDeployment, _ ...interface{}) error {
			bd.Spec.Options.TargetNamespace = defaultNS
			bd.Spec.Options.Helm = &fleet.HelmOptions{
				ReleaseName: "TestRelease2-old", // will be deleted
			}

			return nil
		},
	)

	mockClient.EXPECT().Get(gomock.Any(), types.NamespacedName{Namespace: fleetNS, Name: "ID3"}, bd).DoAndReturn(
		func(_ context.Context, _ types.NamespacedName, bd *fleet.BundleDeployment, _ ...interface{}) error {
			bd.Spec.Options.TargetNamespace = defaultNS + "-old" // will be deleted
			bd.Spec.Options.Helm = &fleet.HelmOptions{
				ReleaseName: "TestRelease3",
			}

			return nil
		},
	)

	mockHelmDeployer := mocks.NewMockHelmDeployer(mockCtrl)
	mockHelmDeployer.EXPECT().NewListAction()
	mockHelmDeployer.EXPECT().ListDeployments(gomock.Any()).Return(deployedBundles, nil)
	mockHelmDeployer.EXPECT().DeleteRelease(gomock.Any(), deployedBundles[1]).Return(nil)
	mockHelmDeployer.EXPECT().DeleteRelease(gomock.Any(), deployedBundles[2]).Return(nil)

	cleanup := New(mockClient, nil, nil, mockHelmDeployer, fleetNS, defaultNS, 1*time.Second)

	err := cleanup.cleanup(context.Background(), log.FromContext(context.Background()).WithName("test"))

	if err != nil {
		t.Errorf("cleanup failed: %v", err)
	}
}



================================================
FILE: internal/cmd/agent/deployer/data/data.go
================================================
package data

import "github.com/rancher/fleet/internal/cmd/agent/deployer/data/convert"

type List []map[string]interface{}

type Object map[string]interface{}

func (o Object) Map(names ...string) Object {
	v := GetValueN(o, names...)
	m := convert.ToMapInterface(v)
	return m
}

func (o Object) Slice(names ...string) (result []Object) {
	v := GetValueN(o, names...)
	for _, item := range convert.ToInterfaceSlice(v) {
		result = append(result, convert.ToMapInterface(item))
	}
	return
}

func (o Object) String(names ...string) string {
	v := GetValueN(o, names...)
	return convert.ToString(v)
}

func (o Object) StringSlice(names ...string) []string {
	v := GetValueN(o, names...)
	return convert.ToStringSlice(v)
}

func (o Object) Bool(key ...string) bool {
	return convert.ToBool(GetValueN(o, key...))
}



================================================
FILE: internal/cmd/agent/deployer/data/values.go
================================================
// Package data contains functions for working with unstructured values like []interface or map[string]interface{}.
// It allows reading/writing to these values without having to convert to structured items.
package data

func GetValueN(data map[string]interface{}, keys ...string) interface{} {
	val, _ := getValue(data, keys...)
	return val
}

// getValue works similar to GetValueFromAny, but can only process maps. Kept this way to avoid breaking changes with
// the previous interface, GetValueFromAny should be used in most cases since that can handle slices as well.
func getValue(data map[string]interface{}, keys ...string) (interface{}, bool) {
	for i, key := range keys {
		if i == len(keys)-1 {
			val, ok := data[key]
			return val, ok
		}
		data, _ = data[key].(map[string]interface{})
	}
	return nil, false
}



================================================
FILE: internal/cmd/agent/deployer/data/convert/convert.go
================================================
package convert

import (
	"bytes"
	"encoding/json"
	"errors"
	"fmt"
	"strings"
	"time"

	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
)

func singular(value interface{}) interface{} {
	if slice, ok := value.([]string); ok {
		if len(slice) == 0 {
			return nil
		}
		return slice[0]
	}
	if slice, ok := value.([]interface{}); ok {
		if len(slice) == 0 {
			return nil
		}
		return slice[0]
	}
	return value
}

func toStringNoTrim(value interface{}) string {
	if t, ok := value.(time.Time); ok {
		return t.Format(time.RFC3339)
	}
	single := singular(value)
	if single == nil {
		return ""
	}
	return fmt.Sprint(single)
}

func ToString(value interface{}) string {
	return strings.TrimSpace(toStringNoTrim(value))
}

func ToTimestamp(value interface{}) (int64, error) {
	str := ToString(value)
	if str == "" {
		return 0, errors.New("invalid date")
	}
	t, err := time.Parse(time.RFC3339, str)
	if err != nil {
		return 0, err
	}
	return t.UnixNano() / 1000000, nil
}

func ToBool(value interface{}) bool {
	value = singular(value)

	b, ok := value.(bool)
	if ok {
		return b
	}

	str := strings.ToLower(ToString(value))
	return str == "true" || str == "t" || str == "yes" || str == "y"
}

func ToInterfaceSlice(obj interface{}) []interface{} {
	if v, ok := obj.([]interface{}); ok {
		return v
	}
	return nil
}

func ToMapSlice(obj interface{}) []map[string]interface{} {
	if v, ok := obj.([]map[string]interface{}); ok {
		return v
	}
	vs, _ := obj.([]interface{})
	var result []map[string]interface{}
	for _, item := range vs {
		if v, ok := item.(map[string]interface{}); ok {
			result = append(result, v)
		} else {
			return nil
		}
	}

	return result
}

func ToStringSlice(data interface{}) []string {
	if v, ok := data.([]string); ok {
		return v
	}
	if v, ok := data.([]interface{}); ok {
		var result []string
		for _, item := range v {
			result = append(result, ToString(item))
		}
		return result
	}
	if v, ok := data.(string); ok {
		return []string{v}
	}
	return nil
}

func ToMapInterface(obj interface{}) map[string]interface{} {
	v, _ := obj.(map[string]interface{})
	return v
}

func ToObj(data interface{}, into interface{}) error {
	bytes, err := json.Marshal(data)
	if err != nil {
		return err
	}
	return json.Unmarshal(bytes, into)
}

func EncodeToMap(obj interface{}) (map[string]interface{}, error) {
	if m, ok := obj.(map[string]interface{}); ok {
		return m, nil
	}

	if unstr, ok := obj.(*unstructured.Unstructured); ok {
		return unstr.Object, nil
	}

	b, err := json.Marshal(obj)
	if err != nil {
		return nil, err
	}
	result := map[string]interface{}{}
	dec := json.NewDecoder(bytes.NewBuffer(b))
	dec.UseNumber()
	return result, dec.Decode(&result)
}



================================================
FILE: internal/cmd/agent/deployer/desiredset/clients.go
================================================
package desiredset

import (
	"fmt"
	"sync"

	"k8s.io/apimachinery/pkg/runtime/schema"

	"k8s.io/client-go/discovery"
	"k8s.io/client-go/dynamic"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/cache"
)

const (
	defaultNamespace = "default"
)

type ClientFactory func(gvr schema.GroupVersionResource) (dynamic.NamespaceableResourceInterface, error)

type InformerFactory interface {
	Get(gvk schema.GroupVersionKind, gvr schema.GroupVersionResource) (cache.SharedIndexInformer, error)
}

type InformerGetter interface {
	Informer() cache.SharedIndexInformer
	GroupVersionKind() schema.GroupVersionKind
}

func newForConfig(cfg *rest.Config) (*Client, error) {
	discovery, err := discovery.NewDiscoveryClientForConfig(cfg)
	if err != nil {
		return nil, err
	}

	cf := newClientFactory(cfg)
	return &Client{
		clients: &clients{
			clientFactory: cf,
			discovery:     discovery,
			namespaced:    map[schema.GroupVersionKind]bool{},
			gvkToGVR:      map[schema.GroupVersionKind]schema.GroupVersionResource{},
			clients:       map[schema.GroupVersionKind]dynamic.NamespaceableResourceInterface{},
		},
		informers: map[schema.GroupVersionKind]cache.SharedIndexInformer{},
	}, nil
}

type Client struct {
	clients   *clients
	informers map[schema.GroupVersionKind]cache.SharedIndexInformer
}

type clients struct {
	sync.Mutex

	clientFactory ClientFactory
	discovery     discovery.DiscoveryInterface
	namespaced    map[schema.GroupVersionKind]bool
	gvkToGVR      map[schema.GroupVersionKind]schema.GroupVersionResource
	clients       map[schema.GroupVersionKind]dynamic.NamespaceableResourceInterface
}

func (c *clients) IsNamespaced(gvk schema.GroupVersionKind) (bool, error) {
	c.Lock()
	ok, exists := c.namespaced[gvk]
	c.Unlock()

	if exists {
		return ok, nil
	}
	_, err := c.client(gvk)
	if err != nil {
		return false, err
	}

	c.Lock()
	defer c.Unlock()
	return c.namespaced[gvk], nil
}

func (c *clients) client(gvk schema.GroupVersionKind) (dynamic.NamespaceableResourceInterface, error) {
	c.Lock()
	defer c.Unlock()

	if client, ok := c.clients[gvk]; ok {
		return client, nil
	}

	resources, err := c.discovery.ServerResourcesForGroupVersion(gvk.GroupVersion().String())
	if err != nil {
		return nil, err
	}

	for _, resource := range resources.APIResources {
		if resource.Kind != gvk.Kind {
			continue
		}

		client, err := c.clientFactory(gvk.GroupVersion().WithResource(resource.Name))
		if err != nil {
			return nil, err
		}

		c.namespaced[gvk] = resource.Namespaced
		c.clients[gvk] = client
		c.gvkToGVR[gvk] = gvk.GroupVersion().WithResource(resource.Name)
		return client, nil
	}

	return nil, fmt.Errorf("failed to discover client for %s", gvk)
}

func newClientFactory(config *rest.Config) ClientFactory {
	return func(gvr schema.GroupVersionResource) (dynamic.NamespaceableResourceInterface, error) {
		client, err := dynamic.NewForConfig(config)
		if err != nil {
			return nil, err
		}

		return client.Resource(gvr), nil
	}
}



================================================
FILE: internal/cmd/agent/deployer/desiredset/desiredset.go
================================================
package desiredset

import (
	"context"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/merr"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/objectset"

	"k8s.io/apimachinery/pkg/runtime"
)

// Indexer name added for cached types
const byHash = "wrangler.byObjectSetHash"

type desiredSet struct {
	client           *Client
	defaultNamespace string

	ratelimitingQps float32

	remove     bool
	setID      string
	objs       *objectset.ObjectSet
	errs       []error
	onlyDelete bool

	plan Plan
}

func newDesiredSet(client *Client) desiredSet {
	return desiredSet{
		client:           client,
		defaultNamespace: defaultNamespace,
		ratelimitingQps:  1,
	}
}

func (o *desiredSet) addErr(err error) error {
	o.errs = append(o.errs, err)
	return o.Err()
}

func (o desiredSet) Err() error {
	return merr.NewErrors(append(o.errs, o.objs.Err())...)
}

func (o *desiredSet) setup(ns string, setID string, objs ...runtime.Object) {
	if ns == "" {
		o.defaultNamespace = defaultNamespace
	} else {
		o.defaultNamespace = ns
	}

	o.setID = setID

	o.objs = objectset.NewObjectSet()
	o.objs.Add(objs...)

	o.plan.Create = objectset.ObjectKeyByGVK{}
	o.plan.Update = PatchByGVK{}
	o.plan.Delete = objectset.ObjectKeyByGVK{}
}

func (o desiredSet) dryRun(ctx context.Context) (Plan, error) {
	err := o.apply(ctx)
	return o.plan, err
}

func (o desiredSet) dryRunDelete(ctx context.Context) (objectset.ObjectKeyByGVK, error) {
	o.onlyDelete = true
	err := o.apply(ctx)
	return o.plan.Delete, err
}



================================================
FILE: internal/cmd/agent/deployer/desiredset/desiredset_apply.go
================================================
package desiredset

import (
	"context"
	"crypto/sha1" //nolint:gosec // non crypto usage
	"encoding/hex"
	"fmt"
	"sync"
	"time"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/objectset"

	"k8s.io/apimachinery/pkg/api/meta"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/apimachinery/pkg/selection"
	"k8s.io/client-go/util/flowcontrol"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

const (
	LabelApplied   = "objectset.rio.cattle.io/applied"
	LabelID        = "objectset.rio.cattle.io/id"
	LabelGVK       = "objectset.rio.cattle.io/owner-gvk"
	LabelName      = "objectset.rio.cattle.io/owner-name"
	LabelNamespace = "objectset.rio.cattle.io/owner-namespace"
	LabelHash      = "objectset.rio.cattle.io/hash"
	LabelPrefix    = "objectset.rio.cattle.io/"
	LabelPrune     = "objectset.rio.cattle.io/prune"
)

var (
	hashOrder = []string{
		LabelID,
		LabelGVK,
		LabelName,
		LabelNamespace,
	}
	rls     = map[string]flowcontrol.RateLimiter{}
	rlsLock sync.Mutex
)

func (o *desiredSet) getRateLimit(labelHash string) flowcontrol.RateLimiter {
	var rl flowcontrol.RateLimiter

	rlsLock.Lock()
	defer rlsLock.Unlock()
	if o.remove {
		delete(rls, labelHash)
	} else {
		rl = rls[labelHash]
		if rl == nil {
			rl = flowcontrol.NewTokenBucketRateLimiter(o.ratelimitingQps, 10)
			rls[labelHash] = rl
		}
	}

	return rl
}

func (o *desiredSet) apply(ctx context.Context) error {
	if o.objs == nil || o.objs.Len() == 0 {
		o.remove = true
	}

	if err := o.Err(); err != nil {
		return err
	}

	labelSet, annotationSet, err := GetLabelsAndAnnotations(o.setID)
	if err != nil {
		return o.addErr(err)
	}

	rl := o.getRateLimit(labelSet[LabelHash])
	if rl != nil {
		t := time.Now()
		rl.Accept()
		if d := time.Since(t); d.Seconds() > 1 {
			logger := log.FromContext(ctx)
			logger.Info("rate limited", "setID", o.setID, "labels", labelSet, "duration", d)
		}
	}

	objList, err := o.injectLabelsAndAnnotations(labelSet, annotationSet)
	if err != nil {
		return o.addErr(err)
	}

	objs := o.collect(objList)

	sel, err := getSelector(labelSet)
	if err != nil {
		return o.addErr(err)
	}

	for _, gvk := range o.objs.GVKOrder(o.knownGVK()...) {
		o.process(ctx, sel, gvk, objs[gvk])
	}

	return o.Err()
}

func (o *desiredSet) knownGVK() (ret []schema.GroupVersionKind) {
	return
}

func (o *desiredSet) collect(objList []runtime.Object) objectset.ObjectByGVK {
	result := objectset.ObjectByGVK{}
	for _, obj := range objList {
		_, _ = result.Add(obj)
	}
	return result
}

func getSelector(labelSet map[string]string) (labels.Selector, error) {
	req, err := labels.NewRequirement(LabelHash, selection.Equals, []string{labelSet[LabelHash]})
	if err != nil {
		return nil, err
	}
	return labels.NewSelector().Add(*req), nil
}

func (o *desiredSet) injectLabelsAndAnnotations(labels, annotations map[string]string) ([]runtime.Object, error) {
	var result []runtime.Object

	for _, objMap := range o.objs.ObjectsByGVK() {
		for key, obj := range objMap {
			obj = obj.DeepCopyObject()
			meta, err := meta.Accessor(obj)
			if err != nil {
				return nil, fmt.Errorf("failed to get metadata for %s: %w", key, err)
			}

			setLabels(meta, labels)
			setAnnotations(meta, annotations)

			result = append(result, obj)
		}
	}

	return result, nil
}

func setAnnotations(meta metav1.Object, annotations map[string]string) {
	objAnn := meta.GetAnnotations()
	if objAnn == nil {
		objAnn = map[string]string{}
	}
	delete(objAnn, LabelApplied)
	for k, v := range annotations {
		objAnn[k] = v
	}
	meta.SetAnnotations(objAnn)
}

func setLabels(meta metav1.Object, labels map[string]string) {
	objLabels := meta.GetLabels()
	if objLabels == nil {
		objLabels = map[string]string{}
	}
	for k, v := range labels {
		objLabels[k] = v
	}
	meta.SetLabels(objLabels)
}

func objectSetHash(labels map[string]string) string {
	dig := sha1.New() //nolint:gosec // non crypto usage
	for _, key := range hashOrder {
		dig.Write([]byte(labels[key]))
	}
	return hex.EncodeToString(dig.Sum(nil))
}



================================================
FILE: internal/cmd/agent/deployer/desiredset/desiredset_compare.go
================================================
package desiredset

import (
	"bytes"
	"compress/gzip"
	"encoding/base64"
	"fmt"
	"strings"

	"github.com/go-logr/logr"

	data2 "github.com/rancher/fleet/internal/cmd/agent/deployer/data"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/data/convert"

	"k8s.io/apimachinery/pkg/api/meta"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/apimachinery/pkg/util/json"
	"k8s.io/apimachinery/pkg/util/jsonmergepatch"
	"k8s.io/apimachinery/pkg/util/strategicpatch"
)

var (
	knownListKeys = map[string]bool{
		"apiVersion":    true,
		"containerPort": true,
		"devicePath":    true,
		"ip":            true,
		"kind":          true,
		"mountPath":     true,
		"name":          true,
		"port":          true,
		"topologyKey":   true,
		"type":          true,
	}
)

func prepareObjectForCreate(gvk schema.GroupVersionKind, obj runtime.Object) (runtime.Object, error) {
	serialized, err := serializeApplied(obj)
	if err != nil {
		return nil, err
	}

	obj = obj.DeepCopyObject()
	m, err := meta.Accessor(obj)
	if err != nil {
		return nil, err
	}
	annotations := m.GetAnnotations()
	if annotations == nil {
		annotations = map[string]string{}
	}

	annotations[LabelApplied] = appliedToAnnotation(serialized)
	m.SetAnnotations(annotations)

	typed, err := meta.TypeAccessor(obj)
	if err != nil {
		return nil, err
	}

	apiVersion, kind := gvk.ToAPIVersionAndKind()
	typed.SetAPIVersion(apiVersion)
	typed.SetKind(kind)

	return obj, nil
}

func modifiedObj(gvk schema.GroupVersionKind, newObject runtime.Object) ([]byte, error) {
	newObject, err := prepareObjectForCreate(gvk, newObject)
	if err != nil {
		return nil, err
	}

	modified, err := json.Marshal(newObject)

	return modified, err
}

func emptyMaps(data map[string]interface{}, keys ...string) bool {
	for _, key := range append(keys, "__invalid_key__") {
		if len(data) == 0 {
			// map is empty so all children are empty too
			return true
		} else if len(data) > 1 {
			// map has more than one key so not empty
			return false
		}

		value, ok := data[key]
		if !ok {
			// map has one key but not what we are expecting so not considered empty
			return false
		}

		data = convert.ToMapInterface(value)
	}

	return true
}

func sanitizePatch(patch []byte, removeObjectSetAnnotation bool) ([]byte, error) {
	mod := false
	data := map[string]interface{}{}
	err := json.Unmarshal(patch, &data)
	if err != nil {
		return nil, err
	}

	if _, ok := data["kind"]; ok {
		mod = true
		delete(data, "kind")
	}

	if _, ok := data["apiVersion"]; ok {
		mod = true
		delete(data, "apiVersion")
	}

	if _, ok := data["status"]; ok {
		mod = true
		delete(data, "status")
	}

	if deleted := removeCreationTimestamp(data); deleted {
		mod = true
	}

	if removeObjectSetAnnotation {
		metadata := convert.ToMapInterface(data2.GetValueN(data, "metadata"))
		annotations := convert.ToMapInterface(data2.GetValueN(data, "metadata", "annotations"))
		for k := range annotations {
			if strings.HasPrefix(k, LabelPrefix) {
				mod = true
				delete(annotations, k)
			}
		}
		if mod && len(annotations) == 0 {
			delete(metadata, "annotations")
			if len(metadata) == 0 {
				delete(data, "metadata")
			}
		}
	}

	if emptyMaps(data, "metadata", "annotations") {
		return []byte("{}"), nil
	}

	if !mod {
		return patch, nil
	}

	return json.Marshal(data)
}

func (o *desiredSet) applyPatch(logger logr.Logger, gvk schema.GroupVersionKind, oldObject, newObject runtime.Object) (bool, error) {
	oldMetadata, err := meta.Accessor(oldObject)
	if err != nil {
		return false, err
	}

	modified, err := modifiedObj(gvk, newObject)
	if err != nil {
		return false, err
	}

	current, err := json.Marshal(oldObject)
	if err != nil {
		return false, err
	}

	patch, err := doPatch(logger, gvk, modified, current)
	if err != nil {
		return false, fmt.Errorf("patch generation: %w", err)
	}

	if string(patch) == "{}" {
		return false, nil
	}

	patch, err = sanitizePatch(patch, false)
	if err != nil {
		return false, err
	}

	if string(patch) == "{}" {
		return false, nil
	}

	logger.V(4).Info("DesiredSet - Looking at patch for fields", "patch", string(patch), "modified", string(modified), "current", string(current))

	patch, err = sanitizePatch(patch, true)
	if err != nil {
		return false, err
	}

	if string(patch) != "{}" {
		o.plan.Update.Set(gvk, oldMetadata.GetNamespace(), oldMetadata.GetName(), string(patch))
		logger.V(1).Info("DesiredSet - Updated plan with patch", "patch", string(patch))
	}

	return true, nil
}

func (o *desiredSet) compareObjects(logger logr.Logger, gvk schema.GroupVersionKind, oldObject runtime.Object, newObject runtime.Object) error {
	if ran, err := o.applyPatch(logger, gvk, oldObject, newObject); err != nil {
		return err
	} else if !ran {
		logger.V(1).Info("DesiredSet - No change")
	}

	return nil
}

func removeCreationTimestamp(data map[string]interface{}) bool {
	metadata, ok := data["metadata"]
	if !ok {
		return false
	}

	data = convert.ToMapInterface(metadata)
	if _, ok := data["creationTimestamp"]; ok {
		delete(data, "creationTimestamp")
		return true
	}

	return false
}

func pruneList(data []interface{}) []interface{} {
	result := make([]interface{}, 0, len(data))
	for _, v := range data {
		switch typed := v.(type) {
		case map[string]interface{}:
			result = append(result, pruneValues(typed, true))
		case []interface{}:
			result = append(result, pruneList(typed))
		default:
			result = append(result, v)
		}
	}
	return result
}

func pruneValues(data map[string]interface{}, isList bool) map[string]interface{} {
	result := map[string]interface{}{}
	for k, v := range data {
		switch typed := v.(type) {
		case map[string]interface{}:
			result[k] = pruneValues(typed, false)
		case []interface{}:
			result[k] = pruneList(typed)
		default:
			if isList && knownListKeys[k] {
				result[k] = v
			} else {
				switch x := v.(type) {
				case string:
					if len(x) > 64 {
						result[k] = x[:64]
					} else {
						result[k] = v
					}
				case []byte:
					result[k] = nil
				default:
					result[k] = v
				}
			}
		}
	}
	return result
}

func serializeApplied(obj runtime.Object) ([]byte, error) {
	data, err := convert.EncodeToMap(obj)
	if err != nil {
		return nil, err
	}
	data = pruneValues(data, false)
	return json.Marshal(data)
}

func appliedToAnnotation(b []byte) string {
	buf := &bytes.Buffer{}
	w := gzip.NewWriter(buf)
	if _, err := w.Write(b); err != nil {
		return string(b)
	}
	if err := w.Close(); err != nil {
		return string(b)
	}
	return base64.RawStdEncoding.EncodeToString(buf.Bytes())
}

// doPatch is adapted from "kubectl apply"
func doPatch(logger logr.Logger, gvk schema.GroupVersionKind, modified, current []byte) ([]byte, error) {
	var (
		patchType types.PatchType
		patch     []byte
	)

	patchType, lookupPatchMeta, err := getMergeStyle(gvk)
	if err != nil {
		return nil, err
	}

	if patchType == types.StrategicMergePatchType {
		patch, err = strategicpatch.CreateThreeWayMergePatch(nil, modified, current, lookupPatchMeta, true)
	} else {
		patch, err = jsonmergepatch.CreateThreeWayJSONMergePatch(nil, modified, current)
	}

	if err != nil {
		logger.V(1).Info("Failed to calculate patch", "gvk", gvk, "error", err)
	}

	return patch, err
}



================================================
FILE: internal/cmd/agent/deployer/desiredset/desiredset_process.go
================================================
package desiredset

import (
	"context"
	"fmt"
	"sort"
	"sync"

	"sigs.k8s.io/controller-runtime/pkg/log"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/merr"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/objectset"

	"golang.org/x/sync/errgroup"

	"k8s.io/apimachinery/pkg/api/meta"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/dynamic"
	"k8s.io/client-go/tools/cache"
)

func (o *desiredSet) getControllerAndClient(gvk schema.GroupVersionKind) (cache.SharedIndexInformer, dynamic.NamespaceableResourceInterface, error) {
	// client needs to be accessed first so that the gvk->gvr mapping gets cached
	client, err := o.client.clients.client(gvk)
	if err != nil {
		return nil, nil, err
	}

	return o.client.informers[gvk], client, nil
}

func (o *desiredSet) adjustNamespace(objs objectset.ObjectByKey) error {
	for k, v := range objs {
		if k.Namespace != "" {
			continue
		}

		v = v.DeepCopyObject()
		meta, err := meta.Accessor(v)
		if err != nil {
			return err
		}

		meta.SetNamespace(o.defaultNamespace)
		delete(objs, k)
		k.Namespace = o.defaultNamespace
		objs[k] = v
	}

	return nil
}

func (o *desiredSet) clearNamespace(objs objectset.ObjectByKey) error {
	for k, v := range objs {
		if k.Namespace == "" {
			continue
		}

		v = v.DeepCopyObject()
		meta, err := meta.Accessor(v)
		if err != nil {
			return err
		}

		meta.SetNamespace("")

		delete(objs, k)
		k.Namespace = ""
		objs[k] = v
	}

	return nil
}

func (o *desiredSet) filterCrossVersion(gvk schema.GroupVersionKind, keys []objectset.ObjectKey) []objectset.ObjectKey {
	result := make([]objectset.ObjectKey, 0, len(keys))
	gk := gvk.GroupKind()
	for _, key := range keys {
		if o.objs.Contains(gk, key) {
			continue
		}
		if key.Namespace == o.defaultNamespace && o.objs.Contains(gk, objectset.ObjectKey{Name: key.Name}) {
			continue
		}
		result = append(result, key)
	}
	return result
}

func (o *desiredSet) process(ctx context.Context, set labels.Selector, gvk schema.GroupVersionKind, objs objectset.ObjectByKey) {
	controller, client, err := o.getControllerAndClient(gvk)
	if err != nil {
		_ = o.addErr(err)
		return
	}

	nsed, err := o.client.clients.IsNamespaced(gvk)
	if err != nil {
		_ = o.addErr(err)
		return
	}

	if nsed {
		if err := o.adjustNamespace(objs); err != nil {
			_ = o.addErr(err)
			return
		}
	} else {
		if err := o.clearNamespace(objs); err != nil {
			_ = o.addErr(err)
			return
		}
	}

	existing, err := o.list(ctx, controller, client, set, objs)
	if err != nil {
		_ = o.addErr(fmt.Errorf("failed to list %s for %s: %w", gvk, o.setID, err))
		return
	}

	toCreate, toDelete, toUpdate := compareSets(existing, objs)

	// check for resources in the objectset but under a different version of the same group/kind
	toDelete = o.filterCrossVersion(gvk, toDelete)

	o.plan.Create[gvk] = toCreate
	o.plan.Delete[gvk] = toDelete

	// this is not needed for driftdetect.allResources, so PlanDelete exits early
	if o.onlyDelete {
		return
	}

	logger := log.FromContext(ctx).WithValues("setID", o.setID, "gvk", gvk)
	for _, k := range toUpdate {
		oldObject := existing[k]
		newObject := objs[k]

		oldMetadata, err := meta.Accessor(oldObject)
		if err != nil {
			_ = o.addErr(fmt.Errorf("failed to update patch %s for %s, access meta: %w", gvk, o.setID, err))
		}

		o.plan.Objects = append(o.plan.Objects, oldObject)

		logger := logger.WithValues("name", oldMetadata.GetName(), "namespace", oldMetadata.GetNamespace())
		err = o.compareObjects(logger, gvk, oldObject, newObject)
		if err != nil {
			_ = o.addErr(fmt.Errorf("failed to update patch %s for %s: %w", gvk, o.setID, err))
		}
	}
}

func (o *desiredSet) list(ctx context.Context, informer cache.SharedIndexInformer, client dynamic.NamespaceableResourceInterface, selector labels.Selector, desiredObjects objectset.ObjectByKey) (map[objectset.ObjectKey]runtime.Object, error) {
	var (
		errs []error
		objs = objectset.ObjectByKey{}
	)

	if informer == nil {
		// If a lister namespace is set, assume all objects belong to the listerNamespace.  If the
		// desiredSet has an owner but no lister namespace, list objects from all namespaces to ensure
		// we're cleaning up any owned resources.  Otherwise, search only objects from the namespaces
		// used by the objects.  Note: desiredSets without owners will never return objects to delete;
		// deletion requires an owner to track object references across multiple apply runs.
		var namespaces = desiredObjects.Namespaces()

		// no owner or lister namespace intentionally restricted; only search in specified namespaces
		err := multiNamespaceList(ctx, namespaces, client, selector, func(obj unstructured.Unstructured) {
			if err := addObjectToMap(objs, &obj); err != nil {
				errs = append(errs, err)
			}
		})
		if err != nil {
			errs = append(errs, err)
		}

		return objs, merr.NewErrors(errs...)
	}

	var namespace string

	// Special case for listing only by hash using indexers
	indexer := informer.GetIndexer()
	if hash, ok := getIndexableHash(indexer, selector); ok {
		return listByHash(indexer, hash, namespace)
	}

	if err := cache.ListAllByNamespace(indexer, namespace, selector, func(obj interface{}) {
		if err := addObjectToMap(objs, obj); err != nil {
			errs = append(errs, err)
		}
	}); err != nil {
		errs = append(errs, err)
	}

	return objs, merr.NewErrors(errs...)
}

func shouldPrune(obj runtime.Object) bool {
	meta, err := meta.Accessor(obj)
	if err != nil {
		return true
	}
	return meta.GetLabels()[LabelPrune] != "false"
}

func compareSets(existingSet, newSet objectset.ObjectByKey) (toCreate, toDelete, toUpdate []objectset.ObjectKey) {
	for k := range newSet {
		if _, ok := existingSet[k]; ok {
			toUpdate = append(toUpdate, k)
		} else {
			toCreate = append(toCreate, k)
		}
	}

	for k, obj := range existingSet {
		if _, ok := newSet[k]; !ok {
			if shouldPrune(obj) {
				toDelete = append(toDelete, k)
			}
		}
	}

	sortObjectKeys(toCreate)
	sortObjectKeys(toDelete)
	sortObjectKeys(toUpdate)

	return
}

func sortObjectKeys(keys []objectset.ObjectKey) {
	sort.Slice(keys, func(i, j int) bool {
		return keys[i].String() < keys[j].String()
	})
}

func addObjectToMap(objs objectset.ObjectByKey, obj interface{}) error {
	metadata, err := meta.Accessor(obj)
	if err != nil {
		return err
	}

	objs[objectset.ObjectKey{
		Namespace: metadata.GetNamespace(),
		Name:      metadata.GetName(),
	}] = obj.(runtime.Object)

	return nil
}

// multiNamespaceList lists objects across all given namespaces, because requests are concurrent it is possible for appendFn to be called before errors are reported.
func multiNamespaceList(ctx context.Context, namespaces []string, baseClient dynamic.NamespaceableResourceInterface, selector labels.Selector, appendFn func(obj unstructured.Unstructured)) error {
	var mu sync.Mutex
	wg, _ctx := errgroup.WithContext(ctx)

	// list all namespaces concurrently
	for _, namespace := range namespaces {
		wg.Go(func() error {
			list, err := baseClient.Namespace(namespace).List(_ctx, v1.ListOptions{
				LabelSelector: selector.String(),
			})
			if err != nil {
				return err
			}

			mu.Lock()
			for _, obj := range list.Items {
				appendFn(obj)
			}
			mu.Unlock()

			return nil
		})
	}

	return wg.Wait()
}

// getIndexableHash detects if provided selector can be replaced by using the hash index, if configured, in which case returns the hash value
func getIndexableHash(indexer cache.Indexer, selector labels.Selector) (string, bool) {
	// Check if indexer was added
	if indexer == nil || indexer.GetIndexers()[byHash] == nil {
		return "", false
	}

	// Check specific case of listing with exact hash label selector
	if req, selectable := selector.Requirements(); len(req) != 1 || !selectable {
		return "", false
	}

	return selector.RequiresExactMatch(LabelHash)
}

// inNamespace checks whether a given object is a Kubernetes object and is part of the provided namespace
func inNamespace(namespace string, obj interface{}) bool {
	metadata, err := meta.Accessor(obj)
	return err == nil && metadata.GetNamespace() == namespace
}

// listByHash use a pre-configured indexer to list objects of a certain type by their hash label
func listByHash(indexer cache.Indexer, hash string, namespace string) (map[objectset.ObjectKey]runtime.Object, error) {
	var (
		errs []error
		objs = objectset.ObjectByKey{}
	)
	res, err := indexer.ByIndex(byHash, hash)
	if err != nil {
		return nil, err
	}
	for _, obj := range res {
		if namespace != "" && !inNamespace(namespace, obj) {
			continue
		}
		if err := addObjectToMap(objs, obj); err != nil {
			errs = append(errs, err)
		}
	}
	return objs, merr.NewErrors(errs...)
}



================================================
FILE: internal/cmd/agent/deployer/desiredset/desiredset_process_test.go
================================================
package desiredset

import (
	"context"
	"errors"
	"strings"
	"testing"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	appsv1 "k8s.io/api/apps/v1"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/client-go/dynamic/fake"
	k8stesting "k8s.io/client-go/testing"
	"k8s.io/client-go/tools/cache"
)

func Test_multiNamespaceList(t *testing.T) {
	results := map[string]*unstructured.UnstructuredList{
		"ns1": {Items: []unstructured.Unstructured{
			{Object: map[string]interface{}{"name": "o1", "namespace": "ns1"}},
			{Object: map[string]interface{}{"name": "o2", "namespace": "ns1"}},
			{Object: map[string]interface{}{"name": "o3", "namespace": "ns1"}},
		}},
		"ns2": {Items: []unstructured.Unstructured{
			{Object: map[string]interface{}{"name": "o4", "namespace": "ns2"}},
			{Object: map[string]interface{}{"name": "o5", "namespace": "ns2"}},
		}},
		"ns3": {Items: []unstructured.Unstructured{}},
	}
	s := runtime.NewScheme()
	err := appsv1.SchemeBuilder.AddToScheme(s)
	require.NoError(t, err, "Failed to build schema.")
	baseClient := fake.NewSimpleDynamicClient(s)
	baseClient.PrependReactor("list", "*", func(action k8stesting.Action) (handled bool, ret runtime.Object, err error) {
		if strings.Contains(action.GetNamespace(), "error") {
			return true, nil, errors.New("simulated failure")
		}

		return true, results[action.GetNamespace()], nil
	})

	type args struct {
		namespaces []string
	}
	tests := []struct {
		name          string
		args          args
		expectedCalls int
		expectError   bool
	}{
		{
			name: "no namespaces",
			args: args{
				namespaces: []string{},
			},
			expectError:   false,
			expectedCalls: 0,
		},
		{
			name: "1 namespace",
			args: args{
				namespaces: []string{"ns1"},
			},
			expectError:   false,
			expectedCalls: 3,
		},
		{
			name: "many namespaces",
			args: args{
				namespaces: []string{"ns1", "ns2", "ns3"},
			},
			expectError:   false,
			expectedCalls: 5,
		},
		{
			name: "1 namespace error",
			args: args{
				namespaces: []string{"error", "ns2", "ns3"},
			},
			expectError:   true,
			expectedCalls: -1,
		},
		{
			name: "many namespace errors",
			args: args{
				namespaces: []string{"error", "error1", "error2"},
			},
			expectError:   true,
			expectedCalls: -1,
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			var calls int
			err := multiNamespaceList(context.TODO(), tt.args.namespaces, baseClient.Resource(appsv1.SchemeGroupVersion.WithResource("deployments")), labels.NewSelector(), func(obj unstructured.Unstructured) {
				calls += 1
			})

			if tt.expectError {
				require.Error(t, err)
			} else {
				require.NoError(t, err)
			}

			if tt.expectedCalls >= 0 {
				assert.Equal(t, tt.expectedCalls, calls)
			}
		})
	}
}

func Test_getIndexableHash(t *testing.T) {
	const hash = "somehash"
	hashSelector, err := getSelector(map[string]string{LabelHash: hash})
	if err != nil {
		t.Fatal(err)
	}
	envLabelSelector, err := metav1.LabelSelectorAsSelector(&metav1.LabelSelector{MatchLabels: map[string]string{"env": "dev"}})
	if err != nil {
		t.Fatal(err)
	}

	indexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{byHash: func(obj interface{}) ([]string, error) {
		return nil, nil
	}})
	type args struct {
		indexer  cache.Indexer
		selector labels.Selector
	}
	tests := []struct {
		name     string
		args     args
		wantHash string
		want     bool
	}{
		{name: "indexer configured", args: args{
			indexer:  indexer,
			selector: hashSelector,
		}, wantHash: hash, want: true},
		{name: "indexer not configured", args: args{
			indexer:  cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{}),
			selector: hashSelector,
		}, wantHash: "", want: false},
		{name: "using Everything selector", args: args{
			indexer:  indexer,
			selector: labels.Everything(),
		}, wantHash: "", want: false},
		{name: "using other label selectors", args: args{
			indexer:  indexer,
			selector: envLabelSelector,
		}, wantHash: "", want: false},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			gotHash, got := getIndexableHash(tt.args.indexer, tt.args.selector)
			assert.Equalf(t, tt.wantHash, gotHash, "getIndexableHash(%v, %v)", tt.args.indexer, tt.args.selector)
			assert.Equalf(t, tt.want, got, "getIndexableHash(%v, %v)", tt.args.indexer, tt.args.selector)
		})
	}
}

func Test_inNamespace(t *testing.T) {
	type args struct {
		namespace string
		obj       interface{}
	}
	tests := []struct {
		name string
		args args
		want bool
	}{
		{name: "object in namespace", args: args{
			namespace: "ns", obj: &metav1.ObjectMeta{
				Namespace: "ns",
			},
		}, want: true},
		{name: "object not in namespace", args: args{
			namespace: "ns", obj: &metav1.ObjectMeta{
				Namespace: "another-ns",
			},
		}, want: false},
		{name: "object not namespaced", args: args{
			namespace: "ns", obj: &corev1.Namespace{},
		}, want: false},
		{name: "non k8s object", args: args{
			namespace: "ns", obj: &struct{}{},
		}, want: false},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			assert.Equalf(t, tt.want, inNamespace(tt.args.namespace, tt.args.obj), "inNamespace(%v, %v)", tt.args.namespace, tt.args.obj)
		})
	}
}



================================================
FILE: internal/cmd/agent/deployer/desiredset/diff.go
================================================
package desiredset

import (
	"encoding/json"
	"slices"

	jsonpatch "github.com/evanphx/json-patch"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/internal/diff"
	argo "github.com/rancher/fleet/internal/cmd/agent/deployer/internal/normalizers"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/internal/resource"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/merr"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/normalizers"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/objectset"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
)

// Diff factors the bundledeployment's bundle diff patches into the plan from
// DryRun. This way, the status of the bundledeployment can be updated
// accurately.
func Diff(plan Plan, bd *fleet.BundleDeployment, ns string, objs ...runtime.Object) (Plan, error) {
	desired := objectset.NewObjectSet(objs...).ObjectsByGVK()
	live := objectset.NewObjectSet(plan.Objects...).ObjectsByGVK()

	norms, err := newNormalizers(live, bd)
	if err != nil {
		return plan, err
	}

	var errs []error
	if bd.Spec.Options.Diff != nil {
		toIgnore := objectset.ObjectKeyByGVK{}
		for _, patch := range bd.Spec.Options.Diff.ComparePatches {
			for _, op := range patch.Operations {
				gvk := schema.FromAPIVersionAndKind(patch.APIVersion, patch.Kind)

				if op.Op == fleet.IgnoreOp {
					key := objectset.ObjectKey{
						Name:      patch.Name,
						Namespace: patch.Namespace,
					}

					if _, ok := toIgnore[gvk]; !ok {
						toIgnore[gvk] = []objectset.ObjectKey{}
					}

					toIgnore[gvk] = append(toIgnore[gvk], key)
				}
			}
		}
		for gvk, objs := range plan.Create {
			if _, ok := toIgnore[gvk]; !ok {
				continue
			}
			for _, key := range objs {
				if idx := slices.Index(toIgnore[gvk], key); idx >= 0 {
					plan.Create[gvk] = slices.Delete(plan.Create[gvk], idx, idx+1)
					continue
				}
			}
		}
	}
	for gvk, objs := range plan.Update {
		for key := range objs {
			desiredObj := desired[gvk][key]
			if desiredObj == nil {
				desiredKey := key
				// if different namespace options to guess if resource is namespaced or not
				if desiredKey.Namespace == "" {
					desiredKey.Namespace = ns
				} else {
					desiredKey.Namespace = ""
				}
				desiredObj = desired[gvk][desiredKey]
				if desiredObj == nil {
					continue
				}
			}
			desiredObj.(*unstructured.Unstructured).SetNamespace(key.Namespace)

			actualObj := live[gvk][key]
			if actualObj == nil {
				continue
			}

			diffResult, err := diff.Diff(desiredObj.(*unstructured.Unstructured), actualObj.(*unstructured.Unstructured),
				diff.WithNormalizer(norms),
				diff.IgnoreAggregatedRoles(true))
			if err != nil {
				errs = append(errs, err)
				continue
			}
			if !diffResult.Modified {
				delete(plan.Update[gvk], key)
				continue
			}
			patch, err := jsonpatch.CreateMergePatch(diffResult.NormalizedLive, diffResult.PredictedLive)
			if err != nil {
				errs = append(errs, err)
				continue
			}
			// this will overwrite an existing entry in the Update map
			plan.Update.Set(gvk, key.Namespace, key.Name, string(patch))
		}
		if len(errs) > 0 {
			return plan, merr.NewErrors(errs...)
		}
	}
	return plan, nil
}

// newNormalizers creates a normalizer that removes fields from resources.
// The normalizer is composed of:
//
//   - StatusNormalizer
//   - MutatingWebhookNormalizer
//   - ValidatingWebhookNormalizer
//   - normalizers.NewIgnoreNormalizer (patch.JsonPointers)
//   - normalizers.NewKnownTypesNormalizer (rollout.argoproj.io)
//   - patch.Operations
func newNormalizers(live objectset.ObjectByGVK, bd *fleet.BundleDeployment) (diff.Normalizer, error) {
	var ignore []resource.ResourceIgnoreDifferences
	jsonPatchNorm := &normalizers.JSONPatchNormalizer{}

	if bd.Spec.Options.Diff != nil {
		for _, patch := range bd.Spec.Options.Diff.ComparePatches {
			groupVersion, err := schema.ParseGroupVersion(patch.APIVersion)
			if err != nil {
				return nil, err
			}
			ignore = append(ignore, resource.ResourceIgnoreDifferences{
				Namespace:    patch.Namespace,
				Name:         patch.Name,
				Kind:         patch.Kind,
				Group:        groupVersion.Group,
				JSONPointers: patch.JsonPointers,
			})

			for _, op := range patch.Operations {
				// compile each operation by itself so that one failing operation doesn't block the others
				patchData, err := json.Marshal([]interface{}{op})
				if err != nil {
					return nil, err
				}

				if op.Op == fleet.IgnoreOp {
					continue
				}

				gvk := schema.FromAPIVersionAndKind(patch.APIVersion, patch.Kind)
				key := objectset.ObjectKey{
					Name:      patch.Name,
					Namespace: patch.Namespace,
				}
				jsonPatchNorm.Add(gvk, key, patchData)
			}
		}
	}

	ignoreNormalizer, err := argo.NewIgnoreNormalizer(ignore, nil)
	if err != nil {
		return nil, err
	}

	knownTypesNorm, err := argo.NewKnownTypesNormalizer(nil)
	if err != nil {
		return nil, err
	}

	return normalizers.New(live, ignoreNormalizer, knownTypesNorm, jsonPatchNorm), nil
}



================================================
FILE: internal/cmd/agent/deployer/desiredset/diff_test.go
================================================
package desiredset_test

import (
	"testing"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/desiredset"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/objectset"
	"github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
)

func Test_Diff_IgnoreResources(t *testing.T) {
	ns := "fleet-local"

	gvk := schema.GroupVersionKind{
		Group:   "",
		Version: "bar",
		Kind:    "foo",
	}
	plan := desiredset.Plan{
		Create: objectset.ObjectKeyByGVK{
			gvk: []objectset.ObjectKey{
				{
					Name:      "baz",
					Namespace: ns,
				},
				{
					Name:      "other", // should be left untouched, not ignored
					Namespace: ns,
				},
			},
		},
	}
	bd := v1alpha1.BundleDeployment{
		Spec: v1alpha1.BundleDeploymentSpec{
			Options: v1alpha1.BundleDeploymentOptions{
				Diff: &v1alpha1.DiffOptions{
					ComparePatches: []v1alpha1.ComparePatch{
						{
							Kind:       "foo",
							APIVersion: "bar",
							Namespace:  ns,
							Name:       "baz",
							Operations: []v1alpha1.Operation{
								{
									Op: "ignore",
								},
							},
						},
					},
				},
			},
		},
	}

	objs := []runtime.Object{}

	lenBefore := len(plan.Create[gvk])

	_, err := desiredset.Diff(plan, &bd, ns, objs...)
	if err != nil {
		t.Errorf("unexpected error: %v", err)
	}

	if len(plan.Create[gvk]) != lenBefore-1 {
		t.Errorf("unexpected plan.Create length: expected %d, got %d", lenBefore-1, len(plan.Create[gvk]))
	}
}



================================================
FILE: internal/cmd/agent/deployer/desiredset/plan.go
================================================
package desiredset

import (
	"context"
	"fmt"
	"strings"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/objectset"
	"github.com/rancher/fleet/internal/config"
	"github.com/rancher/fleet/internal/names"

	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/rest"
)

type PatchByGVK map[schema.GroupVersionKind]map[objectset.ObjectKey]string

func (p PatchByGVK) Set(gvk schema.GroupVersionKind, namespace, name, patch string) {
	d, ok := p[gvk]
	if !ok {
		d = map[objectset.ObjectKey]string{}
		p[gvk] = d
	}
	d[objectset.ObjectKey{
		Name:      name,
		Namespace: namespace,
	}] = patch
}

type Plan struct {
	Create objectset.ObjectKeyByGVK

	// Delete contains existing objects that are not in the desired state,
	// unless their prune label is set to "false".
	Delete objectset.ObjectKeyByGVK

	// Update contains objects, already existing in the cluster, that have
	// changes. The patch would restore their desired state, as represented
	// in the helm manifest's resources passed into Plan(..., objs).
	Update PatchByGVK

	// Objects contains objects, already existing in the cluster, that have
	// valid metadata
	Objects []runtime.Object
}

func New(config *rest.Config) (*Client, error) {
	client, err := newForConfig(config)
	if err != nil {
		return nil, err
	}
	return client, nil
}

// Plan does a dry run of the apply to get the difference between the
// desired and live state. It needs a client.
// This adds the "objectset.rio.cattle.io/applied" annotation, which is used for tracking changes.
func (a *Client) Plan(ctx context.Context, defaultNS string, setID string, objs ...runtime.Object) (Plan, error) {
	ds := newDesiredSet(a)
	ds.setup(defaultNS, setID, objs...)
	return ds.dryRun(ctx)
}

func (a *Client) PlanDelete(ctx context.Context, defaultNS string, setID string, objs ...runtime.Object) (objectset.ObjectKeyByGVK, error) {
	ds := newDesiredSet(a)
	ds.setup(defaultNS, setID, objs...)
	return ds.dryRunDelete(ctx)
}

// GetSetID constructs a identifier from the provided args, bundleID "fleet-agent" is special
func GetSetID(bundleID, labelPrefix, labelSuffix string) string {
	// bundle is fleet-agent bundle, we need to use setID fleet-agent-bootstrap since it was applied with import controller
	if strings.HasPrefix(bundleID, "fleet-agent") {
		if labelSuffix == "" {
			return config.AgentBootstrapConfigName
		}
		return names.SafeConcatName(config.AgentBootstrapConfigName, labelSuffix)
	}
	if labelSuffix != "" {
		return names.SafeConcatName(labelPrefix, bundleID, labelSuffix)
	}
	return names.SafeConcatName(labelPrefix, bundleID)
}

// GetLabelsAndAnnotations returns the labels and annotations, like
// "objectset.rio.cattle.io/hash" and owners, to be able to use apply.DryRun
func GetLabelsAndAnnotations(setID string) (map[string]string, map[string]string, error) {
	if setID == "" {
		return nil, nil, fmt.Errorf("set ID or owner must be set")
	}

	annotations := map[string]string{
		LabelID: setID,
	}

	labels := map[string]string{
		LabelHash: objectSetHash(annotations),
	}

	return labels, annotations, nil
}



================================================
FILE: internal/cmd/agent/deployer/desiredset/style.go
================================================
package desiredset

import (
	"sync"

	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/apimachinery/pkg/util/strategicpatch"
	"k8s.io/client-go/kubernetes/scheme"
)

var (
	patchCache     = map[schema.GroupVersionKind]patchCacheEntry{}
	patchCacheLock = sync.Mutex{}
)

type patchCacheEntry struct {
	patchType types.PatchType
	lookup    strategicpatch.LookupPatchMeta
}

func getMergeStyle(gvk schema.GroupVersionKind) (types.PatchType, strategicpatch.LookupPatchMeta, error) {
	var (
		patchType       types.PatchType
		lookupPatchMeta strategicpatch.LookupPatchMeta
	)

	patchCacheLock.Lock()
	entry, ok := patchCache[gvk]
	patchCacheLock.Unlock()

	if ok {
		return entry.patchType, entry.lookup, nil
	}

	versionedObject, err := scheme.Scheme.New(gvk)

	switch {
	case runtime.IsNotRegisteredError(err) || gvk.Kind == "CustomResourceDefinition":
		patchType = types.MergePatchType
	case err != nil:
		return patchType, nil, err
	default:
		patchType = types.StrategicMergePatchType
		lookupPatchMeta, err = strategicpatch.NewPatchMetaFromStruct(versionedObject)
		if err != nil {
			return patchType, nil, err
		}
	}

	patchCacheLock.Lock()
	patchCache[gvk] = patchCacheEntry{
		patchType: patchType,
		lookup:    lookupPatchMeta,
	}
	patchCacheLock.Unlock()

	return patchType, lookupPatchMeta, nil
}



================================================
FILE: internal/cmd/agent/deployer/driftdetect/driftdetect.go
================================================
package driftdetect

import (
	"context"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/desiredset"
	"github.com/rancher/fleet/internal/cmd/agent/trigger"
	"github.com/rancher/fleet/internal/helmdeployer"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

type DriftDetect struct {
	// Trigger watches deployed resources on the local cluster.
	trigger *trigger.Trigger

	desiredset       *desiredset.Client
	defaultNamespace string
	labelPrefix      string
	labelSuffix      string

	driftChan chan event.TypedGenericEvent[*fleet.BundleDeployment]
}

func New(
	trigger *trigger.Trigger,
	desiredset *desiredset.Client,
	defaultNamespace string,
	labelPrefix string,
	labelSuffix string,
	driftChan chan event.TypedGenericEvent[*fleet.BundleDeployment],
) *DriftDetect {
	return &DriftDetect{
		trigger:          trigger,
		desiredset:       desiredset,
		defaultNamespace: defaultNamespace,
		labelPrefix:      labelPrefix,
		labelSuffix:      labelSuffix,
		driftChan:        driftChan,
	}
}

func (d *DriftDetect) Clear(bdKey string) error {
	return d.trigger.Clear(bdKey)
}

// Refresh triggers a sync of all resources of the provided bd which may have drifted from their desired state.
func (d *DriftDetect) Refresh(ctx context.Context, bdKey string, bd *fleet.BundleDeployment, resources *helmdeployer.Resources) error {
	logger := log.FromContext(ctx).WithName("drift-detect").WithValues("initialResourceVersion", bd.ResourceVersion)
	logger.V(1).Info("Refreshing drift detection")

	resources, err := d.allResources(ctx, bd, resources)
	if err != nil {
		return err
	}

	if resources == nil {
		return nil
	}

	handler := func(key string) {
		logger.V(1).Info("Notifying driftdetect reconciler of a resource change", "triggeredBy", key)
		d.driftChan <- event.TypedGenericEvent[*fleet.BundleDeployment]{Object: bd}

	}

	// Adding bundledeployment's resource list to the trigger-controller's watch list
	return d.trigger.OnChange(bdKey, resources.DefaultNamespace, handler, resources.Objects...)
}

// allResources returns the resources that are deployed by the bundle deployment,
// according to the helm release history. It adds to be deleted resources to
// the list, by comparing the desired state to the actual state with apply.
func (d *DriftDetect) allResources(ctx context.Context, bd *fleet.BundleDeployment, resources *helmdeployer.Resources) (*helmdeployer.Resources, error) {
	ns := resources.DefaultNamespace
	if ns == "" {
		ns = d.defaultNamespace
	}

	plan, err := d.desiredset.PlanDelete(ctx, ns, desiredset.GetSetID(bd.Name, d.labelPrefix, d.labelSuffix), resources.Objects...)
	if err != nil {
		return nil, err
	}

	for gvk, keys := range plan {
		for _, key := range keys {
			u := &unstructured.Unstructured{}
			u.SetGroupVersionKind(gvk)
			u.SetNamespace(key.Namespace)
			u.SetName(key.Name)
			resources.Objects = append(resources.Objects, u)
		}
	}

	return resources, nil
}



================================================
FILE: internal/cmd/agent/deployer/internal/diff/diff.go
================================================
// +vendored argoproj/gitops-engine/pkg/diff/diff.go
/*
The package provide functions that allows to compare set of Kubernetes resources using the logic equivalent to
`kubectl diff`.
*/
package diff

import (
	"bytes"
	"encoding/json"
	"errors"
	"fmt"
	"reflect"

	jsonpatch "github.com/evanphx/json-patch"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/util/jsonmergepatch"
	"k8s.io/apimachinery/pkg/util/strategicpatch"
	"k8s.io/client-go/kubernetes/scheme"

	jsonutil "github.com/rancher/fleet/internal/cmd/agent/deployer/internal/diff/json"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/internal/diff/kubernetes_vendor/pkg/api/v1/endpoints"
	kubescheme "github.com/rancher/fleet/internal/cmd/agent/deployer/internal/diff/scheme"
)

const couldNotMarshalErrMsg = "Could not unmarshal to object of type %s: %v"

// Holds diffing result of two resources
type DiffResult struct {
	// Modified is set to true if resources are not matching
	Modified bool
	// Contains YAML representation of a live resource with applied normalizations
	NormalizedLive []byte
	// Contains "expected" YAML representation of a live resource
	PredictedLive []byte
}

// Holds result of two resources sets comparison
type DiffResultList struct {
	Diffs    []DiffResult
	Modified bool
}

type noopNormalizer struct {
}

func (n *noopNormalizer) Normalize(un *unstructured.Unstructured) error {
	return nil
}

// Normalizer updates resource before comparing it
type Normalizer interface {
	Normalize(un *unstructured.Unstructured) error
}

// GetNoopNormalizer returns normalizer that does not apply any resource modifications
func GetNoopNormalizer() Normalizer {
	return &noopNormalizer{}
}

// Diff performs a diff on two unstructured objects. If the live object happens to have a
// "kubectl.kubernetes.io/last-applied-configuration", then perform a three way diff.
func Diff(config, live *unstructured.Unstructured, opts ...Option) (*DiffResult, error) {
	o := applyOptions(opts)
	if config != nil {
		config = remarshal(config, o)
		Normalize(config, opts...)
	}
	if live != nil {
		live = remarshal(live, o)
		Normalize(live, opts...)
	}
	orig, err := GetLastAppliedConfigAnnotation(live)
	if err != nil {
		o.log.V(1).Info(fmt.Sprintf("Failed to get last applied configuration: %v", err))
	} else if orig != nil && config != nil {
		Normalize(orig, opts...)
		dr, err := ThreeWayDiff(orig, config, live)
		if err == nil {
			return dr, nil
		}
		o.log.V(1).Info(fmt.Sprintf("three-way diff calculation failed: %v. Falling back to two-way diff", err))
	}
	return TwoWayDiff(config, live)
}

// TwoWayDiff performs a three-way diff and uses specified config as a recently applied config
func TwoWayDiff(config, live *unstructured.Unstructured) (*DiffResult, error) {
	switch {
	case live != nil && config != nil:
		return ThreeWayDiff(config, config.DeepCopy(), live)
	case live != nil:
		liveData, err := json.Marshal(live)
		if err != nil {
			return nil, err
		}
		return &DiffResult{Modified: false, NormalizedLive: liveData, PredictedLive: []byte("null")}, nil
	case config != nil:
		predictedLiveData, err := json.Marshal(config.Object)
		if err != nil {
			return nil, err
		}
		return &DiffResult{Modified: true, NormalizedLive: []byte("null"), PredictedLive: predictedLiveData}, nil
	default:
		return nil, errors.New("both live and config are null objects")
	}
}

// generateSchemeDefaultPatch runs the scheme default functions on the given parameter, and
// return a patch representing the delta vs the origin parameter object.
func generateSchemeDefaultPatch(kubeObj runtime.Object) ([]byte, error) {

	// 1) Call scheme defaulter functions on a clone of our k8s resource object
	patched := kubeObj.DeepCopyObject()
	kubescheme.Scheme.Default(patched)

	// 2) Compare the original object (pre-defaulter funcs) with patched object (post-default funcs),
	// and generate a patch that can be applied against the original
	patch, success, err := CreateTwoWayMergePatch(kubeObj, patched, kubeObj.DeepCopyObject())

	// Ignore empty patch: this only means that kubescheme.Scheme.Default(...) made no changes.
	if string(patch) == "{}" && err == nil {
		success = true
	}
	if err != nil || !success {
		if err == nil && !success {
			err = errors.New("empty result")
		}
		return nil, err
	}

	return patch, err
}

// applyPatch executes kubernetes server side patch:
// uses corresponding data structure, applies appropriate defaults and executes strategic merge patch
func applyPatch(liveBytes []byte, patchBytes []byte, newVersionedObject func() (runtime.Object, error)) ([]byte, []byte, error) {

	// Construct an empty instance of the object we are applying a patch against
	predictedLive, err := newVersionedObject()
	if err != nil {
		return nil, nil, err
	}

	// Apply the patchBytes patch against liveBytes, using predictedLive to indicate the k8s data type
	predictedLiveBytes, err := strategicpatch.StrategicMergePatch(liveBytes, patchBytes, predictedLive)
	if err != nil {
		return nil, nil, err
	}

	// Unmarshal predictedLiveBytes into predictedLive; note that this will discard JSON fields in predictedLiveBytes
	// which are not in the predictedLive struct. predictedLive is thus "tainted" and we should not use it directly.
	if err = json.Unmarshal(predictedLiveBytes, &predictedLive); err == nil {

		// 1) Calls 'kubescheme.Scheme.Default(predictedLive)' and generates a patch containing the delta of that
		// call, which can then be applied to predictedLiveBytes.
		//
		// Why do we do this? Since predictedLive is "tainted" (missing extra fields), we cannot use it to populate
		// predictedLiveBytes, BUT we still need predictedLive itself in order to call the default scheme functions.
		// So, we call the default scheme functions on the "tainted" struct, to generate a patch, and then
		// apply that patch to the untainted JSON.
		patch, err := generateSchemeDefaultPatch(predictedLive)
		if err != nil {
			return nil, nil, err
		}

		// 2) Apply the default-funcs patch against the original "untainted" JSON
		// This allows us to apply the scheme default values generated above, against JSON that does not fully conform
		// to its k8s resource type (eg the JSON may contain those invalid fields that we do not wish to discard).
		predictedLiveBytes, err = strategicpatch.StrategicMergePatch(predictedLiveBytes, patch, predictedLive.DeepCopyObject())
		if err != nil {
			return nil, nil, err
		}

		// 3) Unmarshall into a map[string]interface{}, then back into byte[], to ensure the fields
		// are sorted in a consistent order (we do the same below, so that they can be
		// lexicographically compared with one another)
		var result map[string]interface{}
		err = json.Unmarshal(predictedLiveBytes, &result)
		if err != nil {
			return nil, nil, err
		}
		predictedLiveBytes, err = json.Marshal(result)
		if err != nil {
			return nil, nil, err
		}
	}

	live, err := newVersionedObject()
	if err != nil {
		return nil, nil, err
	}

	// As above, unknown JSON fields in liveBytes will be discarded in the unmarshalling to 'live'.
	// However, this is much less likely since liveBytes is coming from a live k8s instance which
	// has already accepted those resources. Regardless, we still treat 'live' as tainted.
	if err = json.Unmarshal(liveBytes, live); err == nil {

		// As above, indirectly apply the schema defaults against liveBytes
		patch, err := generateSchemeDefaultPatch(live)
		if err != nil {
			return nil, nil, err
		}
		liveBytes, err = strategicpatch.StrategicMergePatch(liveBytes, patch, live.DeepCopyObject())
		if err != nil {
			return nil, nil, err
		}

		// Ensure the fields are sorted in a consistent order (as above)
		var result map[string]interface{}
		err = json.Unmarshal(liveBytes, &result)
		if err != nil {
			return nil, nil, err
		}
		liveBytes, err = json.Marshal(result)
		if err != nil {
			return nil, nil, err
		}

	}

	return liveBytes, predictedLiveBytes, nil
}

// ThreeWayDiff performs a diff with the understanding of how to incorporate the
// last-applied-configuration annotation in the diff.
// Inputs are assumed to be stripped of type information
func ThreeWayDiff(orig, config, live *unstructured.Unstructured) (*DiffResult, error) {
	orig = removeNamespaceAnnotation(orig)
	config = removeNamespaceAnnotation(config)

	// 1. calculate a 3-way merge patch
	patchBytes, newVersionedObject, err := threeWayMergePatch(orig, config, live)
	if err != nil {
		return nil, err
	}

	// 2. get expected live object by applying the patch against the live object
	liveBytes, err := json.Marshal(live)
	if err != nil {
		return nil, err
	}

	var predictedLiveBytes []byte
	// If orig/config/live represents a registered scheme...
	if newVersionedObject != nil {
		// Apply patch while applying scheme defaults
		liveBytes, predictedLiveBytes, err = applyPatch(liveBytes, patchBytes, newVersionedObject)
		if err != nil {
			return nil, err
		}
	} else {
		// Otherwise, merge patch directly as JSON
		predictedLiveBytes, err = jsonpatch.MergePatch(liveBytes, patchBytes)
		if err != nil {
			return nil, err
		}
	}

	predictedLive := &unstructured.Unstructured{}
	err = json.Unmarshal(predictedLiveBytes, predictedLive)
	if err != nil {
		return nil, err
	}

	// 3. compare live and expected live object
	dr := DiffResult{
		PredictedLive:  predictedLiveBytes,
		NormalizedLive: liveBytes,
		Modified:       string(predictedLiveBytes) != string(liveBytes),
	}
	return &dr, nil
}

// stripTypeInformation strips any type information (e.g. float64 vs. int) from the unstructured
// object by remarshalling the object. This is important for diffing since it will cause godiff
// to report a false difference.
func stripTypeInformation(un *unstructured.Unstructured) *unstructured.Unstructured {
	unBytes, err := json.Marshal(un)
	if err != nil {
		panic(err)
	}
	var newUn unstructured.Unstructured
	err = json.Unmarshal(unBytes, &newUn)
	if err != nil {
		panic(err)
	}
	return &newUn
}

// removeNamespaceAnnotation remove the namespace and an empty annotation map from the metadata.
// The namespace field is present in live (namespaced) objects, but not necessarily present in
// config or last-applied. This results in a diff which we don't care about. We delete the two so
// that the diff is more relevant.
func removeNamespaceAnnotation(orig *unstructured.Unstructured) *unstructured.Unstructured {
	orig = orig.DeepCopy()
	if metadataIf, ok := orig.Object["metadata"]; ok {
		metadata := metadataIf.(map[string]interface{})
		delete(metadata, "namespace")
		if annotationsIf, ok := metadata["annotations"]; ok {
			shouldDelete := false
			if annotationsIf == nil {
				shouldDelete = true
			} else {
				annotation, ok := annotationsIf.(map[string]interface{})
				if ok && len(annotation) == 0 {
					shouldDelete = true
				}
			}
			if shouldDelete {
				delete(metadata, "annotations")
			}
		}
	}
	return orig
}

// StatefulSet requires special handling since it embeds PersistentVolumeClaim resource.
// K8S API server applies additional default field which we cannot reproduce on client side.
// So workaround is to remove all "defaulted" fields from 'volumeClaimTemplates' of live resource.
func statefulSetWorkaround(orig, live *unstructured.Unstructured) *unstructured.Unstructured {
	origTemplate, ok, err := unstructured.NestedSlice(orig.Object, "spec", "volumeClaimTemplates")
	if !ok || err != nil {
		return live
	}

	liveTemplate, ok, err := unstructured.NestedSlice(live.Object, "spec", "volumeClaimTemplates")
	if !ok || err != nil {
		return live
	}
	live = live.DeepCopy()

	_ = unstructured.SetNestedField(live.Object, jsonutil.RemoveListFields(origTemplate, liveTemplate), "spec", "volumeClaimTemplates")
	return live
}

func threeWayMergePatch(orig, config, live *unstructured.Unstructured) ([]byte, func() (runtime.Object, error), error) {
	origBytes, err := json.Marshal(orig.Object)
	if err != nil {
		return nil, nil, err
	}
	configBytes, err := json.Marshal(config.Object)
	if err != nil {
		return nil, nil, err
	}

	if versionedObject, err := scheme.Scheme.New(orig.GroupVersionKind()); err == nil {
		gk := orig.GroupVersionKind().GroupKind()
		if (gk.Group == "apps" || gk.Group == "extensions") && gk.Kind == "StatefulSet" {
			live = statefulSetWorkaround(orig, live)
		}

		liveBytes, err := json.Marshal(live.Object)
		if err != nil {
			return nil, nil, err
		}

		lookupPatchMeta, err := strategicpatch.NewPatchMetaFromStruct(versionedObject)
		if err != nil {
			return nil, nil, err
		}
		patch, err := strategicpatch.CreateThreeWayMergePatch(origBytes, configBytes, liveBytes, lookupPatchMeta, true)
		if err != nil {
			return nil, nil, err
		}
		newVersionedObject := func() (runtime.Object, error) {
			return scheme.Scheme.New(orig.GroupVersionKind())
		}
		return patch, newVersionedObject, nil
	} else {
		// Remove defaulted fields from the live object.
		// This subtracts any extra fields in the live object which are not present in last-applied-configuration.
		live = &unstructured.Unstructured{Object: jsonutil.RemoveMapFields(orig.Object, live.Object)}

		liveBytes, err := json.Marshal(live.Object)
		if err != nil {
			return nil, nil, err
		}

		patch, err := jsonmergepatch.CreateThreeWayJSONMergePatch(origBytes, configBytes, liveBytes)
		if err != nil {
			return nil, nil, err
		}
		return patch, nil, nil
	}
}

func GetLastAppliedConfigAnnotation(live *unstructured.Unstructured) (*unstructured.Unstructured, error) {
	if live == nil {
		return nil, nil
	}
	annotations := live.GetAnnotations()
	lastAppliedStr, ok := annotations[corev1.LastAppliedConfigAnnotation]
	if !ok {
		return nil, nil
	}
	var obj unstructured.Unstructured
	err := json.Unmarshal([]byte(lastAppliedStr), &obj)
	if err != nil {
		return nil, fmt.Errorf("failed to unmarshal %s in %s: %w", corev1.LastAppliedConfigAnnotation, live.GetName(), err)
	}
	return &obj, nil
}

func Normalize(un *unstructured.Unstructured, opts ...Option) {
	if un == nil {
		return
	}
	o := applyOptions(opts)

	// creationTimestamp is sometimes set to null in the config when exported (e.g. SealedSecrets)
	// Removing the field allows a cleaner diff.
	unstructured.RemoveNestedField(un.Object, "metadata", "creationTimestamp")

	gvk := un.GroupVersionKind()
	switch {
	case gvk.Group == "" && gvk.Kind == "Secret":
		NormalizeSecret(un, opts...)
	case gvk.Group == "rbac.authorization.k8s.io" && (gvk.Kind == "ClusterRole" || gvk.Kind == "Role"):
		normalizeRole(un, o)
	case gvk.Group == "" && gvk.Kind == "Endpoints":
		normalizeEndpoint(un, o)
	}

	err := o.normalizer.Normalize(un)
	if err != nil {
		o.log.Error(err, fmt.Sprintf("Failed to normalize %s/%s/%s", un.GroupVersionKind(), un.GetNamespace(), un.GetName()))
	}
}

// NormalizeSecret mutates the supplied object and encodes stringData to data, and converts nils to
// empty strings. If the object is not a secret, or is an invalid secret, then returns the same object.
func NormalizeSecret(un *unstructured.Unstructured, opts ...Option) {
	if un == nil {
		return
	}
	gvk := un.GroupVersionKind()
	if gvk.Group != "" || gvk.Kind != "Secret" {
		return
	}
	o := applyOptions(opts)
	var secret corev1.Secret
	err := runtime.DefaultUnstructuredConverter.FromUnstructured(un.Object, &secret)
	if err != nil {
		o.log.Error(err, "Failed to convert from unstructured into Secret")
		return
	}
	// We normalize nils to empty string to handle: https://github.com/argoproj/argo-cd/issues/943
	for k, v := range secret.Data {
		if len(v) == 0 {
			secret.Data[k] = []byte("")
		}
	}
	if len(secret.StringData) > 0 {
		if secret.Data == nil {
			secret.Data = make(map[string][]byte)
		}
		for k, v := range secret.StringData {
			secret.Data[k] = []byte(v)
		}
		delete(un.Object, "stringData")
	}
	newObj, err := runtime.DefaultUnstructuredConverter.ToUnstructured(&secret)
	if err != nil {
		o.log.Error(err, "object unable to convert from secret")
		return
	}
	if secret.Data != nil {
		err = unstructured.SetNestedField(un.Object, newObj["data"], "data")
		if err != nil {
			o.log.Error(err, "failed to set secret.data")
			return
		}
	}
}

// normalizeEndpoint normalizes endpoint meaning that EndpointSubsets are sorted lexicographically.
// Note: The Endpoints API (core/v1) is deprecated in favor of EndpointSlice (discovery.k8s.io/v1).
// Endpoints are supported for backward compatibility as users may still deploy them.
func normalizeEndpoint(un *unstructured.Unstructured, o options) {
	if un == nil {
		return
	}
	gvk := un.GroupVersionKind()
	if gvk.Group != "" || gvk.Kind != "Endpoints" {
		return
	}
	//nolint:staticcheck // Endpoints is deprecated but still supported; see fleet#3760.
	var ep corev1.Endpoints
	err := runtime.DefaultUnstructuredConverter.FromUnstructured(un.Object, &ep)
	if err != nil {
		o.log.Error(err, "Failed to convert from unstructured into Endpoints")
		return
	}

	// add default protocol to subsets ports if it is empty
	for s := range ep.Subsets {
		subset := &ep.Subsets[s]
		for p := range subset.Ports {
			port := &subset.Ports[p]
			if port.Protocol == "" {
				port.Protocol = corev1.ProtocolTCP
			}
		}
	}

	endpoints.SortSubsets(ep.Subsets)

	newObj, err := runtime.DefaultUnstructuredConverter.ToUnstructured(&ep)
	if err != nil {
		o.log.Info(fmt.Sprintf(couldNotMarshalErrMsg, gvk, err))
		return
	}
	un.Object = newObj
}

// normalizeRole mutates the supplied Role/ClusterRole and sets rules to null if it is an empty list or an aggregated role
func normalizeRole(un *unstructured.Unstructured, o options) {
	if un == nil {
		return
	}
	gvk := un.GroupVersionKind()
	if gvk.Group != "rbac.authorization.k8s.io" || (gvk.Kind != "Role" && gvk.Kind != "ClusterRole") {
		return
	}

	// Check whether the role we're checking is an aggregation role. If it is, we ignore any differences in rules.
	if o.ignoreAggregatedRoles {
		aggrIf, ok := un.Object["aggregationRule"]
		if ok {
			_, ok = aggrIf.(map[string]interface{})
			if !ok {
				o.log.Info(fmt.Sprintf("Malformed aggregationRule in resource '%s', won't modify.", un.GetName()))
			} else {
				un.Object["rules"] = nil
			}
		}
	}

	rulesIf, ok := un.Object["rules"]
	if !ok {
		return
	}
	rules, ok := rulesIf.([]interface{})
	if !ok {
		return
	}
	if rules != nil && len(rules) == 0 {
		un.Object["rules"] = nil
	}

}

// CreateTwoWayMergePatch is a helper to construct a two-way merge patch from objects (instead of bytes)
func CreateTwoWayMergePatch(orig, new, dataStruct interface{}) ([]byte, bool, error) {
	origBytes, err := json.Marshal(orig)
	if err != nil {
		return nil, false, err
	}
	newBytes, err := json.Marshal(new)
	if err != nil {
		return nil, false, err
	}
	patch, err := strategicpatch.CreateTwoWayMergePatch(origBytes, newBytes, dataStruct)
	if err != nil {
		return nil, false, err
	}
	return patch, string(patch) != "{}", nil
}

// HideSecretData replaces secret data values in specified target, live secrets and in last applied configuration of live secret with stars. Also preserves differences between
// target, live and last applied config values. E.g. if all three are equal the values would be replaced with same number of stars. If all the are different then number of stars
// in replacement should be different.
func HideSecretData(target *unstructured.Unstructured, live *unstructured.Unstructured) (*unstructured.Unstructured, *unstructured.Unstructured, error) {
	var orig *unstructured.Unstructured
	if live != nil {
		orig, _ = GetLastAppliedConfigAnnotation(live)
		live = live.DeepCopy()
	}
	if target != nil {
		target = target.DeepCopy()
	}

	keys := map[string]bool{}
	for _, obj := range []*unstructured.Unstructured{target, live, orig} {
		if obj == nil {
			continue
		}
		NormalizeSecret(obj)
		if data, found, err := unstructured.NestedMap(obj.Object, "data"); found && err == nil {
			for k := range data {
				keys[k] = true
			}
		}
	}

	for k := range keys {
		// we use "+" rather than the more common "*"
		nextReplacement := "++++++++"
		valToReplacement := make(map[string]string)
		for _, obj := range []*unstructured.Unstructured{target, live, orig} {
			var data map[string]interface{}
			if obj != nil {
				// handles an edge case when secret data has nil value
				// https://github.com/argoproj/argo-cd/issues/5584
				dataValue, ok := obj.Object["data"]
				if ok {
					if dataValue == nil {
						continue
					}
				}
				var err error
				data, _, err = unstructured.NestedMap(obj.Object, "data")
				if err != nil {
					return nil, nil, fmt.Errorf("unstructured.NestedMap error: %w", err)
				}
			}
			if data == nil {
				data = make(map[string]interface{})
			}
			valData, ok := data[k]
			if !ok {
				continue
			}
			val := toString(valData)
			replacement, ok := valToReplacement[val]
			if !ok {
				replacement = nextReplacement
				nextReplacement += "++++"
				valToReplacement[val] = replacement
			}
			data[k] = replacement
			err := unstructured.SetNestedField(obj.Object, data, "data")
			if err != nil {
				return nil, nil, fmt.Errorf("unstructured.SetNestedField error: %w", err)
			}
		}
	}
	if live != nil && orig != nil {
		annotations := live.GetAnnotations()
		if annotations == nil {
			annotations = make(map[string]string)
		}
		lastAppliedData, err := json.Marshal(orig)
		if err != nil {
			return nil, nil, fmt.Errorf("error marshaling json: %w", err)
		}
		annotations[corev1.LastAppliedConfigAnnotation] = string(lastAppliedData)
		live.SetAnnotations(annotations)
	}
	return target, live, nil
}

func toString(val interface{}) string {
	if val == nil {
		return ""
	}
	return fmt.Sprintf("%s", val)
}

// remarshal checks resource kind and version and re-marshal using corresponding struct custom marshaller.
// This ensures that expected resource state is formatter same as actual resource state in kubernetes
// and allows to find differences between actual and target states more accurately.
// Remarshalling also strips any type information (e.g. float64 vs. int) from the unstructured
// object. This is important for diffing since it will cause godiff to report a false difference.
func remarshal(obj *unstructured.Unstructured, o options) *unstructured.Unstructured {
	obj = stripTypeInformation(obj)
	data, err := json.Marshal(obj)
	if err != nil {
		panic(err)
	}
	gvk := obj.GroupVersionKind()
	item, err := scheme.Scheme.New(obj.GroupVersionKind())
	if err != nil {
		// This is common. the scheme is not registered
		o.log.V(1).Info(fmt.Sprintf("Could not create new object of type %s: %v", gvk, err))
		return obj
	}
	// This will drop any omitempty fields, perform resource conversion etc...
	unmarshalledObj := reflect.New(reflect.TypeOf(item).Elem()).Interface()
	// Unmarshal data into unmarshalledObj, but detect if there are any unknown fields that are not
	// found in the target GVK object.
	decoder := json.NewDecoder(bytes.NewReader(data))
	decoder.DisallowUnknownFields()
	if err := decoder.Decode(&unmarshalledObj); err != nil {
		// Likely a field present in obj that is not present in the GVK type, or user
		// may have specified an invalid spec in git, so return original object
		o.log.V(1).Info(fmt.Sprintf(couldNotMarshalErrMsg, gvk, err))
		return obj
	}
	unstrBody, err := runtime.DefaultUnstructuredConverter.ToUnstructured(unmarshalledObj)
	if err != nil {
		o.log.V(1).Info(fmt.Sprintf(couldNotMarshalErrMsg, gvk, err))
		return obj
	}
	// Remove all default values specified by custom formatter (e.g. creationTimestamp)
	unstrBody = jsonutil.RemoveMapFields(obj.Object, unstrBody)
	return &unstructured.Unstructured{Object: unstrBody}
}



================================================
FILE: internal/cmd/agent/deployer/internal/diff/diff_options.go
================================================
// +vendored argoproj/gitops-engine/pkg/diff/diff_options.go
package diff

import (
	"github.com/go-logr/logr"
	"k8s.io/klog/v2/textlogger"
)

type Option func(*options)

// Holds diffing settings
type options struct {
	// If set to true then differences caused by aggregated roles in RBAC resources are ignored.
	ignoreAggregatedRoles bool
	normalizer            Normalizer
	log                   logr.Logger
}

func applyOptions(opts []Option) options {
	o := options{
		ignoreAggregatedRoles: false,
		normalizer:            GetNoopNormalizer(),
		log:                   textlogger.NewLogger(textlogger.NewConfig()),
	}
	for _, opt := range opts {
		opt(&o)
	}
	return o
}

func IgnoreAggregatedRoles(ignore bool) Option {
	return func(o *options) {
		o.ignoreAggregatedRoles = ignore
	}
}

func WithNormalizer(normalizer Normalizer) Option {
	return func(o *options) {
		o.normalizer = normalizer
	}
}

func WithLogr(log logr.Logger) Option {
	return func(o *options) {
		o.log = log
	}
}



================================================
FILE: internal/cmd/agent/deployer/internal/diff/json/json.go
================================================
// +vendored argoproj/gitops-engine/pkg/utils/json/json.go
package json

// https://github.com/ksonnet/ksonnet/blob/master/pkg/kubecfg/diff.go
func removeFields(config, live interface{}) interface{} {
	switch c := config.(type) {
	case map[string]interface{}:
		l, ok := live.(map[string]interface{})
		if ok {
			return RemoveMapFields(c, l)
		} else {
			return live
		}
	case []interface{}:
		l, ok := live.([]interface{})
		if ok {
			return RemoveListFields(c, l)
		} else {
			return live
		}
	default:
		return live
	}

}

// RemoveMapFields remove all non-existent fields in the live that don't exist in the config
func RemoveMapFields(config, live map[string]interface{}) map[string]interface{} {
	result := map[string]interface{}{}
	for k, v1 := range config {
		v2, ok := live[k]
		if !ok {
			continue
		}
		if v2 != nil {
			v2 = removeFields(v1, v2)
		}
		result[k] = v2
	}
	return result
}

func RemoveListFields(config, live []interface{}) []interface{} {
	// If live is longer than config, then the extra elements at the end of the
	// list will be returned as-is so they appear in the diff.
	result := make([]interface{}, 0, len(live))
	for i, v2 := range live {
		if len(config) > i {
			if v2 != nil {
				v2 = removeFields(config[i], v2)
			}
			result = append(result, v2)
		} else {
			result = append(result, v2)
		}
	}
	return result
}



================================================
FILE: internal/cmd/agent/deployer/internal/diff/scheme/scheme.go
================================================
// +vendored https://github.com/argoproj/gitops-engine/blob/master/pkg/utils/kube/scheme/scheme.go
package scheme

import (
	"k8s.io/kubernetes/pkg/api/legacyscheme"

	_ "k8s.io/kubernetes/pkg/apis/admission/install"
	_ "k8s.io/kubernetes/pkg/apis/admissionregistration/install"
	_ "k8s.io/kubernetes/pkg/apis/apps/install"
	_ "k8s.io/kubernetes/pkg/apis/authentication/install"
	_ "k8s.io/kubernetes/pkg/apis/authorization/install"
	_ "k8s.io/kubernetes/pkg/apis/autoscaling/install"
	_ "k8s.io/kubernetes/pkg/apis/batch/install"
	_ "k8s.io/kubernetes/pkg/apis/certificates/install"
	_ "k8s.io/kubernetes/pkg/apis/coordination/install"
	_ "k8s.io/kubernetes/pkg/apis/core/install"
	_ "k8s.io/kubernetes/pkg/apis/discovery/install"
	_ "k8s.io/kubernetes/pkg/apis/events/install"
	_ "k8s.io/kubernetes/pkg/apis/extensions/install"
	_ "k8s.io/kubernetes/pkg/apis/flowcontrol/install"
	_ "k8s.io/kubernetes/pkg/apis/imagepolicy/install"
	_ "k8s.io/kubernetes/pkg/apis/networking/install"
	_ "k8s.io/kubernetes/pkg/apis/node/install"
	_ "k8s.io/kubernetes/pkg/apis/policy/install"
	_ "k8s.io/kubernetes/pkg/apis/rbac/install"
	_ "k8s.io/kubernetes/pkg/apis/scheduling/install"
	_ "k8s.io/kubernetes/pkg/apis/storage/install"
)

var Scheme = legacyscheme.Scheme



================================================
FILE: internal/cmd/agent/deployer/internal/normalizers/diff_normalizer.go
================================================
// Package normalizers contains normalizers for resources. Normalizers are used to modify resources before they are compared.
// This includes the "ignore" normalizer, which removes a matched path and the knownTypes normalizer.
//
// +vendored argoproj/argo-cd/util/argo/normalizers/diff_normalizer.go
package normalizers

import (
	"encoding/json"

	jsonpatch "github.com/evanphx/json-patch"
	log "github.com/sirupsen/logrus"

	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime/schema"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/internal/diff"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/internal/normalizers/glob"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/internal/resource"
)

type normalizerPatch struct {
	groupKind schema.GroupKind
	namespace string
	name      string
	patch     jsonpatch.Patch
}

type ignoreNormalizer struct {
	patches []normalizerPatch
}

// NewIgnoreNormalizer creates diff normalizer which removes ignored fields according to given application spec and resource overrides
func NewIgnoreNormalizer(ignore []resource.ResourceIgnoreDifferences, overrides map[string]resource.ResourceOverride) (diff.Normalizer, error) {
	for key, override := range overrides {
		group, kind, err := getGroupKindForOverrideKey(key)
		if err != nil {
			log.Warn(err)
		}
		if len(override.IgnoreDifferences.JSONPointers) > 0 {
			ignore = append(ignore, resource.ResourceIgnoreDifferences{
				Group:        group,
				Kind:         kind,
				JSONPointers: override.IgnoreDifferences.JSONPointers,
			})
		}
	}
	patches := make([]normalizerPatch, 0)
	for i := range ignore {
		for _, path := range ignore[i].JSONPointers {
			patchData, err := json.Marshal([]map[string]string{{"op": "remove", "path": path}})
			if err != nil {
				return nil, err
			}
			patch, err := jsonpatch.DecodePatch(patchData)
			if err != nil {
				return nil, err
			}
			patches = append(patches, normalizerPatch{
				groupKind: schema.GroupKind{Group: ignore[i].Group, Kind: ignore[i].Kind},
				name:      ignore[i].Name,
				namespace: ignore[i].Namespace,
				patch:     patch,
			})
		}

	}
	return &ignoreNormalizer{patches: patches}, nil
}

// Normalize removes fields from supplied resource using json paths from matching items of specified resources ignored differences list
func (n *ignoreNormalizer) Normalize(un *unstructured.Unstructured) error {
	matched := make([]normalizerPatch, 0)
	for _, patch := range n.patches {
		groupKind := un.GroupVersionKind().GroupKind()

		if glob.Match(patch.groupKind.Group, groupKind.Group) &&
			glob.Match(patch.groupKind.Kind, groupKind.Kind) &&
			(patch.name == "" || patch.name == un.GetName()) &&
			(patch.namespace == "" || patch.namespace == un.GetNamespace()) {

			matched = append(matched, patch)
		}
	}
	if len(matched) == 0 {
		return nil
	}

	docData, err := json.Marshal(un)
	if err != nil {
		return err
	}

	for _, patch := range matched {
		patchedData, err := patch.patch.Apply(docData)
		if err != nil {
			log.Debugf("Failed to apply normalization: %v", err)
			continue
		}
		docData = patchedData
	}

	err = json.Unmarshal(docData, un)
	if err != nil {
		return err
	}
	return nil
}



================================================
FILE: internal/cmd/agent/deployer/internal/normalizers/knowntypes_normalizer.go
================================================
// +vendored argoproj/argo-cd/util/argo/normalizers/knowntypes_normalizer.go
package normalizers

import (
	"encoding/json"
	"fmt"
	"strings"

	log "github.com/sirupsen/logrus"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/internal/resource"
	v1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime/schema"
)

var knownTypes = map[string]func() interface{}{}

const Group string = "argoproj.io"

type knownTypeField struct {
	fieldPath  []string
	newFieldFn func() interface{}
}

type knownTypesNormalizer struct {
	typeFields map[schema.GroupKind][]knownTypeField
}

// NewKnownTypesNormalizer create a normalizer that re-format custom resource fields using built-in Kubernetes types.
func NewKnownTypesNormalizer(overrides map[string]resource.ResourceOverride) (*knownTypesNormalizer, error) {
	normalizer := knownTypesNormalizer{typeFields: map[schema.GroupKind][]knownTypeField{}}
	for key, override := range overrides {
		group, kind, err := getGroupKindForOverrideKey(key)
		if err != nil {
			log.Warn(err)
		}
		gk := schema.GroupKind{Group: group, Kind: kind}
		for _, f := range override.KnownTypeFields {
			if err := normalizer.addKnownField(gk, f.Field, f.Type); err != nil {
				log.Warnf("Failed to configure known field normalizer: %v", err)
			}
		}
	}
	normalizer.ensureDefaultCRDsConfigured()
	return &normalizer, nil
}

func (n *knownTypesNormalizer) ensureDefaultCRDsConfigured() {
	rolloutGK := schema.GroupKind{Group: Group, Kind: "Rollout"}
	if _, ok := n.typeFields[rolloutGK]; !ok {
		n.typeFields[rolloutGK] = []knownTypeField{{
			fieldPath: []string{"spec", "template", "spec"},
			newFieldFn: func() interface{} {
				return &v1.PodSpec{}
			},
		}}
	}
}

func getGroupKindForOverrideKey(key string) (string, string, error) {
	var group, kind string
	parts := strings.Split(key, "/")

	switch len(parts) {
	case 2:
		group = parts[0]
		kind = parts[1]
	case 1:
		kind = parts[0]
	default:
		return "", "", fmt.Errorf("override key must be <group>/<kind> or <kind>, got: '%s' ", key)
	}
	return group, kind, nil
}

func (n *knownTypesNormalizer) addKnownField(gk schema.GroupKind, fieldPath string, typePath string) error {
	newFieldFn, ok := knownTypes[typePath]
	if !ok {
		return fmt.Errorf("type '%s' is not supported", typePath)
	}
	n.typeFields[gk] = append(n.typeFields[gk], knownTypeField{
		fieldPath:  strings.Split(fieldPath, "."),
		newFieldFn: newFieldFn,
	})
	return nil
}

func normalize(obj map[string]interface{}, field knownTypeField, fieldPath []string) error {
	for i := range fieldPath {
		if nestedField, ok, err := unstructured.NestedFieldNoCopy(obj, fieldPath[:i+1]...); err == nil && ok {
			items, ok := nestedField.([]interface{})
			if !ok {
				continue
			}
			for j := range items {
				item, ok := items[j].(map[string]interface{})
				if !ok {
					continue
				}

				subPath := fieldPath[i+1:]
				if len(subPath) == 0 {
					newItem, err := nremarshal(item, field)
					if err != nil {
						return err
					}
					items[j] = newItem
				} else {
					if err = normalize(item, field, subPath); err != nil {
						return err
					}
				}
			}
			return unstructured.SetNestedSlice(obj, items, fieldPath[:i+1]...)
		}
	}

	if fieldVal, ok, err := unstructured.NestedMap(obj, fieldPath...); ok && err == nil {
		newFieldVal, err := nremarshal(fieldVal, field)
		if err != nil {
			return err
		}
		err = unstructured.SetNestedField(obj, newFieldVal, fieldPath...)
		if err != nil {
			return err
		}
	}

	return nil
}

func nremarshal(fieldVal map[string]interface{}, field knownTypeField) (map[string]interface{}, error) {
	data, err := json.Marshal(fieldVal)
	if err != nil {
		return nil, err
	}
	typedValue := field.newFieldFn()
	err = json.Unmarshal(data, typedValue)
	if err != nil {
		return nil, err
	}
	data, err = json.Marshal(typedValue)
	if err != nil {
		return nil, err
	}
	newFieldVal := map[string]interface{}{}
	err = json.Unmarshal(data, &newFieldVal)
	if err != nil {
		return nil, err
	}
	return newFieldVal, nil
}

// Normalize re-format custom resource fields using built-in Kubernetes types JSON marshaler.
// This technique allows avoiding false drift detections in CRDs that import data structures from Kubernetes codebase.
func (n *knownTypesNormalizer) Normalize(un *unstructured.Unstructured) error {
	if fields, ok := n.typeFields[un.GroupVersionKind().GroupKind()]; ok {
		for _, field := range fields {
			err := normalize(un.Object, field, field.fieldPath)
			if err != nil {
				return err
			}
		}
	}
	return nil
}



================================================
FILE: internal/cmd/agent/deployer/internal/normalizers/glob/glob.go
================================================
// +vendored argoproj/argo-cd/util/glob/glob.go
package glob

import (
	"github.com/gobwas/glob"
	log "github.com/sirupsen/logrus"
)

func Match(pattern, text string, separators ...rune) bool {
	compiledGlob, err := glob.Compile(pattern, separators...)
	if err != nil {
		log.Warnf("failed to compile pattern %s due to error %v", pattern, err)
		return false
	}
	return compiledGlob.Match(text)
}



================================================
FILE: internal/cmd/agent/deployer/internal/resource/ignore.go
================================================
// +vendored argoproj/argo-cd/pkg/apis/application/v1alpha1/types.go
package resource

import (
	"encoding/json"

	"sigs.k8s.io/yaml"
)

// ResourceIgnoreDifferences contains resource filter and list of json paths which should be ignored during comparison with live state.
type ResourceIgnoreDifferences struct {
	Group        string   `json:"group,omitempty" protobuf:"bytes,1,opt,name=group"`
	Kind         string   `json:"kind" protobuf:"bytes,2,opt,name=kind"`
	Name         string   `json:"name,omitempty" protobuf:"bytes,3,opt,name=name"`
	Namespace    string   `json:"namespace,omitempty" protobuf:"bytes,4,opt,name=namespace"`
	JSONPointers []string `json:"jsonPointers" protobuf:"bytes,5,opt,name=jsonPointers"`
}

// KnownTypeField contains mapping between CRD field and known Kubernetes type
type KnownTypeField struct {
	Field string `json:"field,omitempty" protobuf:"bytes,1,opt,name=field"`
	Type  string `json:"type,omitempty" protobuf:"bytes,2,opt,name=type"`
}

type OverrideIgnoreDiff struct {
	JSONPointers []string `json:"jsonPointers" protobuf:"bytes,1,rep,name=jSONPointers"`
}

// ResourceOverride holds configuration to customize resource diffing and health assessment
type ResourceOverride struct {
	HealthLua         string             `protobuf:"bytes,1,opt,name=healthLua"`
	Actions           string             `protobuf:"bytes,3,opt,name=actions"`
	IgnoreDifferences OverrideIgnoreDiff `protobuf:"bytes,2,opt,name=ignoreDifferences"`
	KnownTypeFields   []KnownTypeField   `protobuf:"bytes,4,opt,name=knownTypeFields"`
}

type rawResourceOverride struct {
	HealthLua         string           `json:"health.lua,omitempty"`
	Actions           string           `json:"actions,omitempty"`
	IgnoreDifferences string           `json:"ignoreDifferences,omitempty"`
	KnownTypeFields   []KnownTypeField `json:"knownTypeFields,omitempty"`
}

func (s *ResourceOverride) UnmarshalJSON(data []byte) error {
	raw := &rawResourceOverride{}
	if err := json.Unmarshal(data, &raw); err != nil {
		return err
	}
	s.KnownTypeFields = raw.KnownTypeFields
	s.HealthLua = raw.HealthLua
	s.Actions = raw.Actions
	return yaml.Unmarshal([]byte(raw.IgnoreDifferences), &s.IgnoreDifferences)
}

func (s ResourceOverride) MarshalJSON() ([]byte, error) {
	ignoreDifferencesData, err := yaml.Marshal(s.IgnoreDifferences)
	if err != nil {
		return nil, err
	}
	raw := &rawResourceOverride{s.HealthLua, s.Actions, string(ignoreDifferencesData), s.KnownTypeFields}
	return json.Marshal(raw)
}

func (o *ResourceOverride) GetActions() (ResourceActions, error) {
	var actions ResourceActions
	err := yaml.Unmarshal([]byte(o.Actions), &actions)
	if err != nil {
		return actions, err
	}
	return actions, nil
}

type ResourceActions struct {
	ActionDiscoveryLua string                     `json:"discovery.lua,omitempty" yaml:"discovery.lua,omitempty" protobuf:"bytes,1,opt,name=actionDiscoveryLua"`
	Definitions        []ResourceActionDefinition `json:"definitions,omitempty" protobuf:"bytes,2,rep,name=definitions"`
}

type ResourceActionDefinition struct {
	Name      string `json:"name" protobuf:"bytes,1,opt,name=name"`
	ActionLua string `json:"action.lua" yaml:"action.lua" protobuf:"bytes,2,opt,name=actionLua"`
}



================================================
FILE: internal/cmd/agent/deployer/kv/split.go
================================================
package kv

import "strings"

func Split(s, sep string) (string, string) {
	parts := strings.SplitN(s, sep, 2)
	return strings.TrimSpace(parts[0]), strings.TrimSpace(safeIndex(parts, 1))
}

// Like split but if there is only one item return "", item
func RSplit(s, sep string) (string, string) {
	parts := strings.SplitN(s, sep, 2)
	if len(parts) == 1 {
		return "", strings.TrimSpace(parts[0])
	}
	return strings.TrimSpace(parts[0]), strings.TrimSpace(safeIndex(parts, 1))
}

func safeIndex(parts []string, idx int) string {
	if len(parts) <= idx {
		return ""
	}
	return parts[idx]
}



================================================
FILE: internal/cmd/agent/deployer/merr/error.go
================================================
package merr

import "bytes"

type Errors []error

func (e Errors) Err() error {
	return NewErrors(e...)
}

func (e Errors) Error() string {
	buf := bytes.NewBuffer(nil)
	for _, err := range e {
		if buf.Len() > 0 {
			buf.WriteString(", ")
		}
		buf.WriteString(err.Error())
	}

	return buf.String()
}

func NewErrors(inErrors ...error) error {
	var errors []error
	for _, err := range inErrors {
		if err != nil {
			errors = append(errors, err)
		}
	}

	if len(errors) == 0 {
		return nil
	} else if len(errors) == 1 {
		return errors[0]
	}
	return Errors(errors)
}



================================================
FILE: internal/cmd/agent/deployer/monitor/condition.go
================================================
package monitor

import (
	"errors"
	"reflect"
	"time"

	"github.com/rancher/lasso/pkg/controller"

	"github.com/sirupsen/logrus"
)

type Cond string

var ErrSkip = controller.ErrIgnore

func (c Cond) SetError(obj interface{}, reason string, err error) {
	if err == nil || errors.Is(err, ErrSkip) {
		c.True(obj)
		c.Message(obj, "")
		c.Reason(obj, reason)
		return
	}
	if reason == "" {
		reason = "Error"
	}
	c.False(obj)
	c.Message(obj, err.Error())
	c.Reason(obj, reason)
}

func (c Cond) True(obj interface{}) {
	setStatus(obj, string(c), "True")
}

func (c Cond) IsTrue(obj interface{}) bool {
	return getStatus(obj, string(c)) == "True"
}

func (c Cond) False(obj interface{}) {
	setStatus(obj, string(c), "False")
}

func (c Cond) IsFalse(obj interface{}) bool {
	return getStatus(obj, string(c)) == "False"
}

func (c Cond) Reason(obj interface{}, reason string) {
	cond := findOrCreateCond(obj, string(c))
	getFieldValue(cond, "Reason").SetString(reason)
}

func (c Cond) GetReason(obj interface{}) string {
	cond := findOrNotCreateCond(obj, string(c))
	if cond == nil {
		return ""
	}
	return getFieldValue(*cond, "Reason").String()
}

func (c Cond) Message(obj interface{}, message string) {
	cond := findOrCreateCond(obj, string(c))
	setValue(cond, "Message", message)
}

func (c Cond) GetMessage(obj interface{}) string {
	cond := findOrNotCreateCond(obj, string(c))
	if cond == nil {
		return ""
	}
	return getFieldValue(*cond, "Message").String()
}

func touchTS(value reflect.Value) {
	now := time.Now().UTC().Format(time.RFC3339)
	getFieldValue(value, "LastUpdateTime").SetString(now)
}

func getStatus(obj interface{}, condName string) string {
	cond := findOrNotCreateCond(obj, condName)
	if cond == nil {
		return ""
	}
	return getFieldValue(*cond, "Status").String()
}

func setStatus(obj interface{}, condName, status string) {
	if reflect.TypeOf(obj).Kind() != reflect.Ptr {
		panic("obj passed must be a pointer")
	}
	cond := findOrCreateCond(obj, condName)
	setValue(cond, "Status", status)
}

func setValue(cond reflect.Value, fieldName, newValue string) {
	value := getFieldValue(cond, fieldName)
	if value.String() != newValue {
		value.SetString(newValue)
		touchTS(cond)
	}
}

func findOrNotCreateCond(obj interface{}, condName string) *reflect.Value {
	condSlice := getValue(obj, "Status", "Conditions")
	if !condSlice.IsValid() {
		condSlice = getValue(obj, "Conditions")
	}
	return findCond(obj, condSlice, condName)
}

func findOrCreateCond(obj interface{}, condName string) reflect.Value {
	condSlice := getValue(obj, "Status", "Conditions")
	if !condSlice.IsValid() {
		condSlice = getValue(obj, "Conditions")
	}
	cond := findCond(obj, condSlice, condName)
	if cond != nil {
		return *cond
	}

	newCond := reflect.New(condSlice.Type().Elem()).Elem()
	newCond.FieldByName("Type").SetString(condName)
	newCond.FieldByName("Status").SetString("Unknown")
	condSlice.Set(reflect.Append(condSlice, newCond))
	return *findCond(obj, condSlice, condName)
}

func findCond(obj interface{}, val reflect.Value, name string) *reflect.Value {
	defer func() {
		if recover() != nil {
			logrus.Fatalf("failed to find .Status.Conditions field on %v", reflect.TypeOf(obj))
		}
	}()

	for i := 0; i < val.Len(); i++ {
		cond := val.Index(i)
		typeVal := getFieldValue(cond, "Type")
		if typeVal.String() == name {
			return &cond
		}
	}

	return nil
}

func getValue(obj interface{}, name ...string) reflect.Value {
	if obj == nil || len(name) == 0 {
		return reflect.Value{}
	}
	v := reflect.ValueOf(obj)
	t := v.Type()
	if t.Kind() == reflect.Ptr {
		v = v.Elem()
	}

	field := v.FieldByName(name[0])
	if len(name) == 1 {
		return field
	}
	return getFieldValue(field, name[1:]...)
}

func getFieldValue(v reflect.Value, name ...string) reflect.Value {
	if !v.IsValid() {
		return v
	}
	if len(name) == 0 {
		return v
	}
	field := v.FieldByName(name[0])
	if len(name) == 1 {
		return field
	}
	return getFieldValue(field, name[1:]...)
}



================================================
FILE: internal/cmd/agent/deployer/monitor/conditions_test.go
================================================
package monitor

import (
	"errors"
	"testing"

	"github.com/google/go-cmp/cmp"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	v1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
)

func TestExcludeIgnoredConditions(t *testing.T) {
	podInitializedAndNotReady := v1.Pod{Status: v1.PodStatus{
		Conditions: []v1.PodCondition{{Type: v1.PodReady, Status: v1.ConditionFalse}, {Type: v1.PodInitialized, Status: v1.ConditionTrue}},
	}}
	uPodInitializedAndNotReady, err := runtime.DefaultUnstructuredConverter.ToUnstructured(&podInitializedAndNotReady)
	if err != nil {
		t.Errorf("can't convert podInitializedAndNotReady to unstructured: %v", err)
	}
	podInitialized := v1.Pod{Status: v1.PodStatus{
		Conditions: []v1.PodCondition{{Type: v1.PodInitialized, Status: v1.ConditionTrue}},
	}}
	uPodInitialized, err := runtime.DefaultUnstructuredConverter.ToUnstructured(&podInitialized)
	if err != nil {
		t.Errorf("can't convert podInitialized to unstructured: %v", err)
	}
	tests := map[string]struct {
		obj           *unstructured.Unstructured
		ignoreOptions *fleet.IgnoreOptions
		expectedObj   *unstructured.Unstructured
		expectedErr   error
	}{
		"nothing is changed with empty IgnoreOptions": {
			obj:           &unstructured.Unstructured{Object: uPodInitializedAndNotReady},
			ignoreOptions: &fleet.IgnoreOptions{},
			expectedObj:   &unstructured.Unstructured{Object: uPodInitializedAndNotReady},
			expectedErr:   nil,
		},
		"nothing is changed with nil IgnoreOptions": {
			obj:           &unstructured.Unstructured{Object: uPodInitializedAndNotReady},
			ignoreOptions: nil,
			expectedObj:   &unstructured.Unstructured{Object: uPodInitializedAndNotReady},
			expectedErr:   nil,
		},
		"nothing is changed when IgnoreOptions don't match any condition": {
			obj:           &unstructured.Unstructured{Object: uPodInitializedAndNotReady},
			ignoreOptions: &fleet.IgnoreOptions{Conditions: []map[string]string{{"Not": "Found"}}},
			expectedObj:   &unstructured.Unstructured{Object: uPodInitializedAndNotReady},
			expectedErr:   nil,
		},
		"'Type: Ready' condition is excluded when IgnoreOptions contains 'Type: Ready' condition": {
			obj:           &unstructured.Unstructured{Object: uPodInitializedAndNotReady},
			ignoreOptions: &fleet.IgnoreOptions{Conditions: []map[string]string{{"type": "Ready"}}},
			expectedObj:   &unstructured.Unstructured{Object: uPodInitialized},
			expectedErr:   nil,
		},
		"'Type: Ready' condition is excluded when IgnoreOptions contains 'Type: Ready, status: False' condition": {
			obj:           &unstructured.Unstructured{Object: uPodInitializedAndNotReady},
			ignoreOptions: &fleet.IgnoreOptions{Conditions: []map[string]string{{"type": "Ready", "status": "False"}}},
			expectedObj:   &unstructured.Unstructured{Object: uPodInitialized},
			expectedErr:   nil,
		},
		"nothing is changed when IgnoreOptions contains 'type: Ready, status: True' condition": {
			obj:           &unstructured.Unstructured{Object: uPodInitializedAndNotReady},
			ignoreOptions: &fleet.IgnoreOptions{Conditions: []map[string]string{{"type": "Ready", "status": "True"}}},
			expectedObj:   &unstructured.Unstructured{Object: uPodInitializedAndNotReady},
			expectedErr:   nil,
		},
	}

	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			obj := test.obj
			err := excludeIgnoredConditions(obj, test.ignoreOptions)
			if !errors.Is(err, test.expectedErr) {
				t.Errorf("expected error doesn't match: expected %v, got %v", test.expectedErr, err)
			}
			if !cmp.Equal(obj, test.expectedObj) {
				t.Errorf("objects don't match: expected %v, got %v", test.expectedObj, obj)
			}
		})
	}
}



================================================
FILE: internal/cmd/agent/deployer/monitor/updatestatus.go
================================================
// Package monitor provides functionality for monitoring and updating the status of a bundle deployment.
// It includes functions for determining whether the agent should be redeployed, whether the status should be updated,
// and for updating the status based on the resources and helm release history.
package monitor

import (
	"context"
	"errors"
	"fmt"
	"sort"
	"strings"

	apierrors "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/api/meta"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/desiredset"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/objectset"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/summary"
	"github.com/rancher/fleet/internal/helmdeployer"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

// limit the length of nonReady and modified resources
const resourcesDetailsMaxLength = 10

type Monitor struct {
	client     client.Client
	desiredset *desiredset.Client

	deployer *helmdeployer.Helm

	defaultNamespace string
	labelPrefix      string
	labelSuffix      string
}

func New(client client.Client, ds *desiredset.Client, deployer *helmdeployer.Helm, defaultNamespace string, labelSuffix string) *Monitor {
	return &Monitor{
		client:           client,
		desiredset:       ds,
		deployer:         deployer,
		defaultNamespace: defaultNamespace,
		labelPrefix:      defaultNamespace,
		labelSuffix:      labelSuffix,
	}
}

func ShouldRedeployAgent(bd *fleet.BundleDeployment) bool {
	if isAgent(bd) {
		return true
	}
	if bd.Spec.Options.ForceSyncGeneration <= 0 {
		return false
	}
	if bd.Status.SyncGeneration == nil {
		return true
	}
	return *bd.Status.SyncGeneration != bd.Spec.Options.ForceSyncGeneration
}

func isAgent(bd *fleet.BundleDeployment) bool {
	return strings.HasPrefix(bd.Name, "fleet-agent")
}

// ShouldUpdateStatus skips resource and ready status updates if the bundle
// deployment is unchanged or not installed yet.
func ShouldUpdateStatus(bd *fleet.BundleDeployment) bool {
	if bd.Spec.DeploymentID != bd.Status.AppliedDeploymentID {
		return false
	}

	// If the bundle failed to install the status should not be updated. Updating
	// here would remove the condition message that was previously set on it.
	if Cond(fleet.BundleDeploymentConditionInstalled).IsFalse(bd) {
		return false
	}

	return true
}

// UpdateStatus sets the status of the bundledeployment based on the resources from the helm release history and the live state.
// In the status it updates: Ready, NonReadyStatus, IncompleteState, NonReadyStatus, NonModified, ModifiedStatus, Resources and ResourceCounts fields.
// Additionally it sets the Ready condition either from the NonReadyStatus or the NonModified status field.
func (m *Monitor) UpdateStatus(ctx context.Context, bd *fleet.BundleDeployment, resources *helmdeployer.Resources) (fleet.BundleDeploymentStatus, error) {
	logger := log.FromContext(ctx).WithName("update-status")
	ctx = log.IntoContext(ctx, logger)

	// updateFromPreviousDeployment mutates bd.Status, so copy it first
	origStatus := *bd.Status.DeepCopy()
	bd = bd.DeepCopy()
	err := m.updateFromPreviousDeployment(ctx, bd, resources)
	if err != nil {

		// Returning an error will cause UpdateStatus to requeue in a loop.
		// When there is no resourceID the error should be on the status. Without
		// the ID we do not have the information to lookup the resources to
		// compute the plan and discover the state of resources.
		if errors.Is(err, helmdeployer.ErrNoResourceID) {
			return origStatus, nil
		}

		return origStatus, err
	}

	status := bd.Status
	status.SyncGeneration = &bd.Spec.Options.ForceSyncGeneration

	readyError := readyError(status)
	Cond(fleet.BundleDeploymentConditionReady).SetError(&status, "", readyError)
	if readyError != nil {
		logger.Info("Status not ready according to nonModified and nonReady", "nonModified", status.NonModified, "nonReady", status.NonReadyStatus)
	} else {
		logger.V(1).Info("Status ready, Ready condition set to true")
	}

	removePrivateFields(&status)
	return status, nil
}

// removePrivateFields removes fields from the status, which won't be marshalled to JSON.
// They would however trigger a status update in apply
func removePrivateFields(s1 *fleet.BundleDeploymentStatus) {
	for id := range s1.NonReadyStatus {
		s1.NonReadyStatus[id].Summary.Relationships = nil
		s1.NonReadyStatus[id].Summary.Attributes = nil
	}
}

// readyError returns an error based on the provided status.
// That error is non-nil if the status corresponds to a non-ready or modified state of the bundle deployment.
func readyError(status fleet.BundleDeploymentStatus) error {
	if status.Ready && status.NonModified {
		return nil
	}

	var msg string
	if !status.Ready {
		msg = "not ready"
		if len(status.NonReadyStatus) > 0 {
			msg = status.NonReadyStatus[0].String()
		}
	} else if !status.NonModified {
		msg = "out of sync"
		if len(status.ModifiedStatus) > 0 {
			msg = status.ModifiedStatus[0].String()
		}
	}

	return errors.New(msg)
}

// updateFromPreviousDeployment updates the status with information from the
// helm release history and an apply dry run.
// Modified resources are resources that have changed from the previous helm release.
func (m *Monitor) updateFromPreviousDeployment(ctx context.Context, bd *fleet.BundleDeployment, resources *helmdeployer.Resources) error {
	resourcesPreviousRelease, err := m.deployer.ResourcesFromPreviousReleaseVersion(bd.Name, bd.Status.Release)
	if err != nil {
		return err
	}

	ns := resources.DefaultNamespace
	if ns == "" {
		ns = m.defaultNamespace
	}

	// resources.Objects contains the desired state of the resources from helm history
	plan, err := m.desiredset.Plan(ctx, ns, desiredset.GetSetID(bd.Name, m.labelPrefix, m.labelSuffix), resources.Objects...)
	if err != nil {
		return err
	}

	// dryrun.Diff only takes plan.Update into account. plan.Update
	// contains objects which have changes to existing values. Adding a new
	// key to a map is not considered an update.
	plan, err = desiredset.Diff(plan, bd, resources.DefaultNamespace, resources.Objects...)
	if err != nil {
		return err
	}

	nonReadyResources := nonReady(ctx, plan, bd.Spec.Options.IgnoreOptions)
	modifiedResources := modified(ctx, m.client, plan, resourcesPreviousRelease)
	allResources, err := toBundleDeploymentResources(m.client, plan.Objects, resources.DefaultNamespace)
	if err != nil {
		return err
	}

	updateFromResources(&bd.Status, allResources, nonReadyResources, modifiedResources)
	return nil
}

func toBundleDeploymentResources(client client.Client, objs []runtime.Object, defaultNamespace string) ([]fleet.BundleDeploymentResource, error) {
	res := make([]fleet.BundleDeploymentResource, 0, len(objs))
	for _, obj := range objs {
		ma, err := meta.Accessor(obj)
		if err != nil {
			return nil, err
		}

		ns := ma.GetNamespace()
		gvk := obj.GetObjectKind().GroupVersionKind()
		if ns == "" && isNamespaced(client.RESTMapper(), gvk) {
			ns = defaultNamespace
		}

		version, kind := gvk.ToAPIVersionAndKind()
		res = append(res, fleet.BundleDeploymentResource{
			Kind:       kind,
			APIVersion: version,
			Namespace:  ns,
			Name:       ma.GetName(),
			CreatedAt:  ma.GetCreationTimestamp(),
		})
	}
	return res, nil
}

func updateFromResources(bdStatus *fleet.BundleDeploymentStatus, resources []fleet.BundleDeploymentResource, nonReadyResources []fleet.NonReadyStatus, modifiedResources []fleet.ModifiedStatus) {
	bdStatus.Ready = len(nonReadyResources) == 0
	bdStatus.NonReadyStatus = nonReadyResources
	if len(bdStatus.NonReadyStatus) > resourcesDetailsMaxLength {
		bdStatus.IncompleteState = true
		bdStatus.NonReadyStatus = nonReadyResources[:resourcesDetailsMaxLength]
	}

	bdStatus.NonModified = len(modifiedResources) == 0
	bdStatus.ModifiedStatus = modifiedResources
	if len(bdStatus.ModifiedStatus) > resourcesDetailsMaxLength {
		bdStatus.IncompleteState = true
		bdStatus.ModifiedStatus = modifiedResources[:resourcesDetailsMaxLength]
	}

	bdStatus.Resources = resources
	bdStatus.ResourceCounts = calculateResourceCounts(resources, nonReadyResources, modifiedResources)
}

func calculateResourceCounts(all []fleet.BundleDeploymentResource, nonReady []fleet.NonReadyStatus, modified []fleet.ModifiedStatus) fleet.ResourceCounts {
	// Create a map with all different resource keys, then remove modified or non-ready keys
	resourceKeys := make(map[fleet.ResourceKey]struct{}, len(all))
	for _, r := range all {
		resourceKeys[fleet.ResourceKey{
			Kind:       r.Kind,
			APIVersion: r.APIVersion,
			Namespace:  r.Namespace,
			Name:       r.Name,
		}] = struct{}{}
	}

	// The agent must have enough visibility to determine the exact state of every resource.
	// e.g. "WaitApplied" or "Unknown" states do not make sense in this context
	counts := fleet.ResourceCounts{
		DesiredReady: calculateDesiredReady(resourceKeys, modified),
	}
	for _, r := range modified {
		switch {
		case r.Create:
			counts.Missing++
		case r.Delete:
			counts.Orphaned++
		default:
			counts.Modified++
		}
		delete(resourceKeys, fleet.ResourceKey{
			Kind:       r.Kind,
			APIVersion: r.APIVersion,
			Namespace:  r.Namespace,
			Name:       r.Name,
		})
	}
	for _, r := range nonReady {
		key := fleet.ResourceKey{
			Kind:       r.Kind,
			APIVersion: r.APIVersion,
			Namespace:  r.Namespace,
			Name:       r.Name,
		}
		// If not present, it was already accounted for as "modified"
		if _, ok := resourceKeys[key]; ok {
			counts.NotReady++
			delete(resourceKeys, key)
		}
	}

	// Remaining keys are considered ready
	counts.Ready = len(resourceKeys)

	return counts
}

// calculateDesiredReady retrieves the number of total resources to be deployed.
// A ResourceKey set is obtained from plan.Objects, which only includes living objects in the cluster, so it needs to be extended with  resources in "Missing" state.
func calculateDesiredReady(liveResourceKeys map[fleet.ResourceKey]struct{}, modified []fleet.ModifiedStatus) int {
	desired := len(liveResourceKeys)
	for _, r := range modified {
		if !r.Create {
			continue
		}
		// Missing resource state
		// Increase desired count if not already present in the resource keys set
		if _, ok := liveResourceKeys[fleet.ResourceKey{
			Kind:       r.Kind,
			APIVersion: r.APIVersion,
			Namespace:  r.Namespace,
			Name:       r.Name,
		}]; !ok {
			desired++
		}
	}
	return desired
}

func nonReady(ctx context.Context, plan desiredset.Plan, ignoreOptions *fleet.IgnoreOptions) (result []fleet.NonReadyStatus) {
	logger := log.FromContext(ctx)
	defer func() {
		sort.Slice(result, func(i, j int) bool {
			return result[i].UID < result[j].UID
		})
	}()

	for _, obj := range plan.Objects {
		if u, ok := obj.(*unstructured.Unstructured); ok {
			if ignoreOptions != nil && ignoreOptions.Conditions != nil {
				if err := excludeIgnoredConditions(u, ignoreOptions); err != nil {
					logger.Error(err, "failed to ignore conditions")
				}
			}

			sum := summary.Summarize(u)
			if !sum.IsReady() {
				result = append(result, fleet.NonReadyStatus{
					UID:        u.GetUID(),
					Kind:       u.GetKind(),
					APIVersion: u.GetAPIVersion(),
					Namespace:  u.GetNamespace(),
					Name:       u.GetName(),
					Summary:    sum,
				})
			}
		}
	}

	return result
}

// modified returns a list of modified statuses based on the provided plan and previous release resources.
// The function iterates through the plan's create, delete, and update actions and constructs a modified status
// for each resource.
// If the number of modified statuses exceeds 10, the function stops and returns the current result.
func modified(ctx context.Context, c client.Client, plan desiredset.Plan, resourcesPreviousRelease *helmdeployer.Resources) (result []fleet.ModifiedStatus) {
	logger := log.FromContext(ctx)
	defer func() {
		sort.Slice(result, func(i, j int) bool {
			return sortKey(result[i]) < sortKey(result[j])
		})
	}()
	for gvk, keys := range plan.Create {
		apiVersion, kind := gvk.ToAPIVersionAndKind()
		for _, key := range keys {
			obj := &unstructured.Unstructured{}
			obj.SetGroupVersionKind(gvk)
			key := client.ObjectKey{
				Namespace: key.Namespace,
				Name:      key.Name,
			}
			err := c.Get(ctx, key, obj)

			exists := !apierrors.IsNotFound(err)

			if exists {
				logger.Info("Resource of BundleDeployment not owned by us",
					"resourceName", key.Name,
					"resourceKind", kind,
					"resourceApiVersion", apiVersion,
					"resourceNamespace", key.Namespace,
					"resourceLabels", obj.GetLabels(),
					"resourceAnnotations", obj.GetAnnotations(),
				)
			}

			result = append(result, fleet.ModifiedStatus{
				Kind:       kind,
				APIVersion: apiVersion,
				Namespace:  key.Namespace,
				Name:       key.Name,
				Create:     true,
				Exist:      exists,
			})
		}
	}

	for gvk, keys := range plan.Delete {
		apiVersion, kind := gvk.ToAPIVersionAndKind()
		for _, key := range keys {
			// Check if resource was in a previous release. It is possible that some operators copy the
			// objectset.rio.cattle.io/hash label into a dynamically created objects. We need to skip these resources
			// because they are not part of the release, and they would appear as orphaned.
			// https://github.com/rancher/fleet/issues/1141
			if isResourceInPreviousRelease(key, kind, resourcesPreviousRelease.Objects) {
				result = append(result, fleet.ModifiedStatus{
					Kind:       kind,
					APIVersion: apiVersion,
					Namespace:  key.Namespace,
					Name:       key.Name,
					Delete:     true,
				})
			}
		}
	}

	for gvk, patches := range plan.Update {
		apiVersion, kind := gvk.ToAPIVersionAndKind()
		for key, patch := range patches {
			result = append(result, fleet.ModifiedStatus{
				Kind:       kind,
				APIVersion: apiVersion,
				Namespace:  key.Namespace,
				Name:       key.Name,
				Patch:      patch,
			})
		}
	}

	return result
}

func isResourceInPreviousRelease(key objectset.ObjectKey, kind string, objsPreviousRelease []runtime.Object) bool {
	for _, obj := range objsPreviousRelease {
		metadata, _ := meta.Accessor(obj)
		if obj.GetObjectKind().GroupVersionKind().Kind == kind && metadata.GetName() == key.Name {
			return true
		}
	}

	return false
}

// excludeIgnoredConditions removes the conditions that are included in ignoreOptions from the object passed as a parameter
func excludeIgnoredConditions(obj *unstructured.Unstructured, ignoreOptions *fleet.IgnoreOptions) error {
	if ignoreOptions == nil {
		return nil
	}

	conditions, _, err := unstructured.NestedSlice(obj.Object, "status", "conditions")
	if err != nil {
		return err
	}
	conditionsWithoutIgnored := make([]interface{}, 0)

	for _, condition := range conditions {
		condition, ok := condition.(map[string]interface{})
		if !ok {
			return fmt.Errorf("condition: %#v can't be converted to map[string]interface{}", condition)
		}
		excludeCondition := false
		for _, ignoredCondition := range ignoreOptions.Conditions {
			if shouldExcludeCondition(condition, ignoredCondition) {
				excludeCondition = true
				break
			}
		}
		if !excludeCondition {
			conditionsWithoutIgnored = append(conditionsWithoutIgnored, condition)
		}
	}

	err = unstructured.SetNestedSlice(obj.Object, conditionsWithoutIgnored, "status", "conditions")
	if err != nil {
		return err
	}

	return nil
}

// shouldExcludeCondition returns true if all the elements of ignoredConditions are inside conditions
func shouldExcludeCondition(conditions map[string]interface{}, ignoredConditions map[string]string) bool {
	if len(ignoredConditions) > len(conditions) {
		return false
	}

	for k, v := range ignoredConditions {
		if vc, found := conditions[k]; !found || vc != v {
			return false
		}
	}

	return true
}

func isNamespaced(mapper meta.RESTMapper, gvk schema.GroupVersionKind) bool {
	mapping, err := mapper.RESTMapping(gvk.GroupKind(), gvk.Version)
	if err != nil {
		return true
	}
	return mapping.Scope.Name() == meta.RESTScopeNameNamespace
}

func sortKey(f fleet.ModifiedStatus) string {
	return f.APIVersion + "/" + f.Kind + "/" + f.Namespace + "/" + f.Name
}



================================================
FILE: internal/cmd/agent/deployer/monitor/updatestatus_test.go
================================================
package monitor

import (
	"fmt"
	"testing"

	fleetv1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1/summary"
	"github.com/stretchr/testify/assert"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

func Test_updateFromResources(t *testing.T) {
	type args struct {
		resources []fleet.BundleDeploymentResource
		nonReady  []fleet.NonReadyStatus
		modified  []fleet.ModifiedStatus
	}
	tests := []struct {
		name   string
		args   args
		assert func(*testing.T, fleet.BundleDeploymentStatus)
	}{
		{
			name: "all ready",
			args: args{
				resources: []fleet.BundleDeploymentResource{
					{
						Kind:       "ConfigMap",
						APIVersion: "v1",
						Namespace:  "testns",
						Name:       "testcm",
					},
					{
						Kind:       "Secret",
						APIVersion: "v1",
						Namespace:  "testns",
						Name:       "testsecret",
					},
				},
			},
			assert: func(t *testing.T, status fleet.BundleDeploymentStatus) {
				t.Helper()
				assert.Equal(t, fleet.ResourceCounts{DesiredReady: 2, Ready: 2}, status.ResourceCounts)
				assert.Truef(t, status.Ready, "unexpected ready status")
				assert.Truef(t, status.NonModified, "unexpected non-modified status")
				assert.Lenf(t, status.Resources, 2, "unexpected resources length")
				assert.Emptyf(t, status.NonReadyStatus, "expected non-ready status to be empty")
				assert.Emptyf(t, status.ModifiedStatus, "expected modified status to be empty")
			},
		},
		{
			name: "orphaned",
			args: args{
				resources: []fleet.BundleDeploymentResource{
					{
						Kind:       "ConfigMap",
						APIVersion: "v1",
						Namespace:  "testns",
						Name:       "testcm",
					},
				},
				modified: []fleet.ModifiedStatus{
					{
						Kind:       "Secret",
						APIVersion: "v1",
						Namespace:  "testns",
						Name:       "testsecret",
						Delete:     true,
					},
				},
			},
			assert: func(t *testing.T, status fleet.BundleDeploymentStatus) {
				t.Helper()
				assert.Equal(t, fleet.ResourceCounts{DesiredReady: 1, Ready: 1, Orphaned: 1}, status.ResourceCounts)
				assert.Truef(t, status.Ready, "unexpected ready status")
				assert.Falsef(t, status.NonModified, "unexpected non-modified status")
				assert.Lenf(t, status.Resources, 1, "unexpected resources length")
				assert.Len(t, status.ModifiedStatus, 1, "incorrect modified status length")
				assert.True(t, status.ModifiedStatus[0].Delete)
				assert.Emptyf(t, status.NonReadyStatus, "expected non-ready status to be empty")
			},
		},
		{
			name: "missing",
			args: args{
				resources: []fleet.BundleDeploymentResource{},
				modified: []fleet.ModifiedStatus{
					{
						Kind:       "ConfigMap",
						APIVersion: "v1",
						Namespace:  "testns",
						Name:       "testcm",
						Create:     true,
					},
				},
			},
			assert: func(t *testing.T, status fleet.BundleDeploymentStatus) {
				t.Helper()
				assert.Equal(t, fleet.ResourceCounts{DesiredReady: 1, Missing: 1}, status.ResourceCounts)
				assert.Truef(t, status.Ready, "unexpected ready status")
				assert.Falsef(t, status.NonModified, "unexpected non-modified status")
				assert.Emptyf(t, status.Resources, "expected resources to be empty")
				assert.Len(t, status.ModifiedStatus, 1, "incorrect modified status length")
				assert.True(t, status.ModifiedStatus[0].Create)
				assert.Emptyf(t, status.NonReadyStatus, "expected non-ready status to be empty")
			},
		},
		{
			name: "modified",
			args: args{
				resources: []fleet.BundleDeploymentResource{
					{
						Kind:       "ConfigMap",
						APIVersion: "v1",
						Namespace:  "testns",
						Name:       "testcm",
					},
				},
				modified: []fleet.ModifiedStatus{
					{
						Kind:       "ConfigMap",
						APIVersion: "v1",
						Namespace:  "testns",
						Name:       "testcm",
						Patch:      `{"data": {"foo": "bar"}`,
					},
				},
			},
			assert: func(t *testing.T, status fleet.BundleDeploymentStatus) {
				t.Helper()
				assert.Equal(t, fleet.ResourceCounts{DesiredReady: 1, Modified: 1}, status.ResourceCounts)
				assert.Truef(t, status.Ready, "unexpected ready status")
				assert.Falsef(t, status.NonModified, "unexpected non-modified status")
				assert.Lenf(t, status.Resources, 1, "unexpected resources length")
				assert.Len(t, status.ModifiedStatus, 1, "incorrect modified status length")
				assert.NotEmpty(t, status.ModifiedStatus[0].Patch)
				assert.Emptyf(t, status.NonReadyStatus, "expected non-ready status to be empty")
			},
		},
		{
			name: "missing and non-ready",
			args: args{
				resources: []fleet.BundleDeploymentResource{
					{
						Kind:       "Pod",
						APIVersion: "v1",
						Namespace:  "testns",
						Name:       "testpod",
					},
				},
				modified: []fleet.ModifiedStatus{
					{
						Kind:       "ConfigMap",
						APIVersion: "v1",
						Namespace:  "testns",
						Name:       "testcm",
						Create:     true,
					},
				},
				nonReady: []fleet.NonReadyStatus{
					{
						Kind:       "Pod",
						APIVersion: "v1",
						Namespace:  "testns",
						Name:       "testpod",
						Summary: fleetv1.Summary{
							State:   "Evicted",
							Error:   true,
							Message: []string{"no space left on device"},
						},
					},
				},
			},
			assert: func(t *testing.T, status fleet.BundleDeploymentStatus) {
				t.Helper()
				assert.Equal(t, fleet.ResourceCounts{DesiredReady: 2, Missing: 1, NotReady: 1}, status.ResourceCounts)
				assert.Falsef(t, status.Ready, "unexpected ready status")
				assert.Falsef(t, status.NonModified, "unexpected non-modified status")
				assert.Lenf(t, status.Resources, 1, "unexpected resources length")
				assert.Len(t, status.NonReadyStatus, 1, "incorrect non-ready status length")
				assert.NotEmptyf(t, status.NonReadyStatus[0].Summary, "unexpected empty summary for non-ready resource")
				assert.Len(t, status.ModifiedStatus, 1, "incorrect modified status length")
				assert.True(t, status.ModifiedStatus[0].Create)
			},
		},
		{
			name: "non-ready and modified status lists have a max length",
			args: args{
				resources: func(n int) []fleet.BundleDeploymentResource {
					pods := make([]fleet.BundleDeploymentResource, n)
					for x := range n {
						pods[x] = fleet.BundleDeploymentResource{
							Kind:       "Pod",
							APIVersion: "v1",
							Namespace:  "testns",
							Name:       fmt.Sprintf("pod-%d", x),
						}
					}
					return pods
				}(12),
				nonReady: func(n int) []fleet.NonReadyStatus {
					pods := make([]fleet.NonReadyStatus, n)
					for x := range n {
						pods[x] = fleet.NonReadyStatus{
							Kind:       "Pod",
							APIVersion: "v1",
							Namespace:  "testns",
							Name:       fmt.Sprintf("pod-%d", x),
							Summary: fleetv1.Summary{
								State: "Evicted",
							},
						}
					}
					return pods
				}(12),
				modified: func(n int) []fleet.ModifiedStatus {
					cms := make([]fleet.ModifiedStatus, n)
					for x := range n {
						cms[x] = fleet.ModifiedStatus{
							Kind:       "ConfigMap",
							APIVersion: "v1",
							Namespace:  "testns",
							Name:       fmt.Sprintf("testcm-%d", x),
							Create:     true,
						}
					}
					return cms
				}(12),
			},
			assert: func(t *testing.T, status fleet.BundleDeploymentStatus) {
				t.Helper()
				assert.Equal(t, fleet.ResourceCounts{DesiredReady: 24, Missing: 12, NotReady: 12}, status.ResourceCounts)
				assert.Falsef(t, status.Ready, "unexpected ready status")
				assert.Falsef(t, status.NonModified, "unexpected non-modified status")
				assert.Lenf(t, status.Resources, 12, "unexpected resources length")

				assert.Len(t, status.NonReadyStatus, 10, "non-ready status length exceeds maximum")
				assert.Len(t, status.ModifiedStatus, 10, "incorrect modified exceeds maximum")
			},
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			var status fleet.BundleDeploymentStatus
			updateFromResources(&status, tt.args.resources, tt.args.nonReady, tt.args.modified)

			tt.assert(t, status)
		})
	}
}



================================================
FILE: internal/cmd/agent/deployer/normalizers/jsonpatch.go
================================================
package normalizers

import (
	jsonpatch "github.com/evanphx/json-patch"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/objectset"
	"github.com/sirupsen/logrus"

	"k8s.io/apimachinery/pkg/api/meta"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime/schema"
)

type JSONPatch []byte

type JSONPatchNormalizer struct {
	patch map[schema.GroupVersionKind]map[objectset.ObjectKey][]JSONPatch
}

func (j *JSONPatchNormalizer) Add(gvk schema.GroupVersionKind, key objectset.ObjectKey, patch JSONPatch) {
	if j.patch == nil {
		j.patch = map[schema.GroupVersionKind]map[objectset.ObjectKey][]JSONPatch{}
	}
	if _, ok := j.patch[gvk]; !ok {
		j.patch[gvk] = map[objectset.ObjectKey][]JSONPatch{}
	}
	if _, ok := j.patch[gvk][key]; !ok {
		j.patch[gvk][key] = []JSONPatch{}
	}
	j.patch[gvk][key] = append(j.patch[gvk][key], patch)
}

func (j JSONPatchNormalizer) Normalize(un *unstructured.Unstructured) error {
	if un == nil {
		return nil
	}
	gvk := un.GroupVersionKind()
	metaObj, err := meta.Accessor(un)
	if err != nil {
		logrus.Errorf("Failed to normalize obj with json patch, error: %v", err)
		return nil
	}
	key := objectset.ObjectKey{
		Namespace: metaObj.GetNamespace(),
		Name:      metaObj.GetName(),
	}

	if !j.hasPatches(gvk, key) {
		// If there are no patches, skip marshalling and unmarshalling
		return nil
	}

	jsondata, err := un.MarshalJSON()
	if err != nil {
		logrus.Errorf("Failed to normalize obj with json patch, error: %v", err)
		return nil
	}
	patched := applyPatches(jsondata, j.patch[gvk][key])
	if err := un.UnmarshalJSON(patched); err != nil {
		logrus.Errorf("Failed to normalize obj with json patch, error: %v", err)
		return nil
	}
	return nil
}

func (j *JSONPatchNormalizer) hasPatches(gvk schema.GroupVersionKind, key objectset.ObjectKey) bool {
	gvkPatches, ok := j.patch[gvk]
	if !ok {
		return false
	}
	keyPatches, ok := gvkPatches[key]
	if !ok {
		return false
	}
	if len(keyPatches) == 0 {
		return false
	}
	return true
}

func applyPatches(jsondata []byte, patches []JSONPatch) []byte {
	for _, patch := range patches {
		p, err := jsonpatch.DecodePatch(patch)
		if err != nil {
			logrus.Errorf("Failed to normalize obj with json patch, error: %v", err)
			return nil
		}
		jsondata, err = p.Apply(jsondata)
		if err != nil {
			logrus.Errorf("Failed to normalize obj with json patch, error: %v", err)
			return nil
		}
	}
	return jsondata
}



================================================
FILE: internal/cmd/agent/deployer/normalizers/mutatingwebhook.go
================================================
package normalizers

import (
	"github.com/sirupsen/logrus"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/objectset"

	adregv1 "k8s.io/api/admissionregistration/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
)

type MutatingWebhookNormalizer struct {
	Live objectset.ObjectByGVK
}

func (m *MutatingWebhookNormalizer) Normalize(un *unstructured.Unstructured) error {
	if un == nil {
		return nil
	}
	gvk := un.GroupVersionKind()
	if gvk.Group != adregv1.GroupName || gvk.Kind != "MutatingWebhookConfiguration" {
		return nil
	}

	return m.convertMutatingWebhookV1(un)
}

func (m *MutatingWebhookNormalizer) convertMutatingWebhookV1(un *unstructured.Unstructured) error {
	var webhook adregv1.MutatingWebhookConfiguration
	err := runtime.DefaultUnstructuredConverter.FromUnstructured(un.Object, &webhook)
	if err != nil {
		logrus.Errorf("Failed to convert unstructured to webhook, err: %v", err)
		return nil
	}

	for i, config := range webhook.Webhooks {
		if webhook.UID == "" && string(config.ClientConfig.CABundle) == "\n" {
			live := lookupLive(un.GroupVersionKind(), un.GetName(), un.GetNamespace(), m.Live)
			liveWebhook, ok := live.(*unstructured.Unstructured)
			if !ok {
				continue
			}
			if err := setMutatingWebhookV1CacertNil(liveWebhook, i); err != nil {
				logrus.Errorf("Failed to normalize webhook cacert, err: %v", err)
				return nil
			}
		}
	}
	return nil
}

func setMutatingWebhookV1CacertNil(un *unstructured.Unstructured, index int) error {
	var webhook adregv1.MutatingWebhookConfiguration
	err := runtime.DefaultUnstructuredConverter.FromUnstructured(un.Object, &webhook)
	if err != nil {
		logrus.Errorf("Failed to convert unstructured to webhook, err: %v", err)
		return err
	}

	if index >= len(webhook.Webhooks) {
		return nil
	}
	webhook.Webhooks[index].ClientConfig.CABundle = nil
	newObj, err := runtime.DefaultUnstructuredConverter.ToUnstructured(&webhook)
	if err != nil {
		logrus.Errorf("Failed to convert unstructured to webhook, err: %v", err)
		return err
	}
	if webhook.Webhooks != nil {
		if err = unstructured.SetNestedField(un.Object, newObj["webhooks"], "webhooks"); err != nil {
			logrus.Errorf("MutatingWebhook normalization error: %v", err)
			return err
		}
	}
	return nil
}

func lookupLive(gvk schema.GroupVersionKind, name, namespace string, live objectset.ObjectByGVK) runtime.Object {
	key := objectset.ObjectKey{
		Namespace: namespace,
		Name:      name,
	}
	return live[gvk][key]
}



================================================
FILE: internal/cmd/agent/deployer/normalizers/norm.go
================================================
package normalizers

import (
	"github.com/rancher/fleet/internal/cmd/agent/deployer/internal/diff"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/objectset"

	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
)

type Norm struct {
	normalizers []diff.Normalizer
}

func (n Norm) Normalize(un *unstructured.Unstructured) error {
	for _, normalizer := range n.normalizers {
		if err := normalizer.Normalize(un); err != nil {
			return err
		}
	}
	return nil
}

func New(lives objectset.ObjectByGVK, additions ...diff.Normalizer) Norm {
	n := Norm{
		normalizers: []diff.Normalizer{
			// Status fields are normally subresources which can't be influenced by resource updates
			&StatusNormalizer{},
			&MutatingWebhookNormalizer{
				Live: lives,
			},
			&ValidatingWebhookNormalizer{
				Live: lives,
			},
		},
	}

	n.normalizers = append(n.normalizers, additions...)

	return n
}



================================================
FILE: internal/cmd/agent/deployer/normalizers/status.go
================================================
package normalizers

import (
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
)

// StatusNormalizer removes a top-level "status" fields from the object, if present
type StatusNormalizer struct{}

func (StatusNormalizer) Normalize(un *unstructured.Unstructured) error {
	unstructured.RemoveNestedField(un.Object, "status")
	return nil
}



================================================
FILE: internal/cmd/agent/deployer/normalizers/status_test.go
================================================
package normalizers

import (
	"errors"
	"testing"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
)

func TestStatusNormalizer_Normalize(t *testing.T) {
	tests := []struct {
		name  string
		obj   runtime.Object
		check func(object runtime.Object) error
	}{
		{
			name: "object with status",
			obj: &corev1.Pod{
				Status: corev1.PodStatus{
					PodIP: "1.2.3.4",
				},
			},
			check: func(obj runtime.Object) error {
				if obj.(*corev1.Pod).Status.PodIP != "" {
					return errors.New("status was not removed")
				}
				return nil
			},
		},
		{
			name:  "object without status",
			obj:   &corev1.ConfigMap{},
			check: func(_ runtime.Object) error { return nil },
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			un, err := runtime.DefaultUnstructuredConverter.ToUnstructured(tt.obj)
			if err != nil {
				t.Fatal(err)
			}
			if err := (StatusNormalizer{}).Normalize(&unstructured.Unstructured{Object: un}); err != nil {
				t.Fatal(err)
			}
			if err := runtime.DefaultUnstructuredConverter.FromUnstructured(un, tt.obj); err != nil {
				t.Fatal(err)
			}
			if err := tt.check(tt.obj); err != nil {
				t.Error(err)
			}
		})
	}
}



================================================
FILE: internal/cmd/agent/deployer/normalizers/validatingwebhook.go
================================================
package normalizers

import (
	"github.com/sirupsen/logrus"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/objectset"

	adregv1 "k8s.io/api/admissionregistration/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
)

type ValidatingWebhookNormalizer struct {
	Live objectset.ObjectByGVK
}

func (v *ValidatingWebhookNormalizer) Normalize(un *unstructured.Unstructured) error {
	if un == nil {
		return nil
	}
	gvk := un.GroupVersionKind()
	if gvk.Group != adregv1.GroupName || gvk.Kind != "ValidatingWebhookConfiguration" {
		return nil
	}

	return v.convertValidatingWebhookV1(un)
}

func (v *ValidatingWebhookNormalizer) convertValidatingWebhookV1(un *unstructured.Unstructured) error {
	var webhook adregv1.ValidatingWebhookConfiguration
	err := runtime.DefaultUnstructuredConverter.FromUnstructured(un.Object, &webhook)
	if err != nil {
		logrus.Errorf("Failed to convert unstructured to webhook, err: %v", err)
		return nil
	}

	for i, config := range webhook.Webhooks {
		if webhook.UID == "" && string(config.ClientConfig.CABundle) == "\n" {
			live := lookupLive(un.GroupVersionKind(), un.GetName(), un.GetNamespace(), v.Live)
			liveWebhook, ok := live.(*unstructured.Unstructured)
			if !ok {
				continue
			}
			if err := setValidatingWebhookV1CacertNil(liveWebhook, i); err != nil {
				logrus.Errorf("Failed to normalize webhook cacert, err: %v", err)
				return nil
			}
		}
	}
	return nil
}

func setValidatingWebhookV1CacertNil(un *unstructured.Unstructured, index int) error {
	var webhook adregv1.ValidatingWebhookConfiguration
	err := runtime.DefaultUnstructuredConverter.FromUnstructured(un.Object, &webhook)
	if err != nil {
		logrus.Errorf("Failed to convert unstructured to webhook, err: %v", err)
		return err
	}

	if index >= len(webhook.Webhooks) {
		return nil
	}
	webhook.Webhooks[index].ClientConfig.CABundle = nil
	newObj, err := runtime.DefaultUnstructuredConverter.ToUnstructured(&webhook)
	if err != nil {
		logrus.Errorf("Failed to convert unstructured to webhook, err: %v", err)
		return err
	}
	if webhook.Webhooks != nil {
		if err = unstructured.SetNestedField(un.Object, newObj["webhooks"], "webhooks"); err != nil {
			logrus.Errorf("ValidatingWebhook normalization error: %v", err)
			return err
		}
	}
	return nil
}



================================================
FILE: internal/cmd/agent/deployer/objectset/objectset.go
================================================
package objectset

import (
	"fmt"
	"reflect"
	"sort"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/merr"

	"github.com/rancher/wrangler/v3/pkg/schemes"

	"k8s.io/apimachinery/pkg/api/meta"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
)

type ObjectKey struct {
	Name      string
	Namespace string
}

func NewObjectKey(obj v1.Object) ObjectKey {
	return ObjectKey{
		Namespace: obj.GetNamespace(),
		Name:      obj.GetName(),
	}
}

func (o ObjectKey) String() string {
	if o.Namespace == "" {
		return o.Name
	}
	return fmt.Sprintf("%s/%s", o.Namespace, o.Name)
}

type ObjectKeyByGVK map[schema.GroupVersionKind][]ObjectKey

type ObjectByGVK map[schema.GroupVersionKind]map[ObjectKey]runtime.Object

func (o ObjectByGVK) Add(obj runtime.Object) (schema.GroupVersionKind, error) {
	metadata, err := meta.Accessor(obj)
	if err != nil {
		return schema.GroupVersionKind{}, err
	}

	gvk, err := getGVK(obj)
	if err != nil {
		return schema.GroupVersionKind{}, err
	}

	objs := o[gvk]
	if objs == nil {
		objs = ObjectByKey{}
		o[gvk] = objs
	}

	objs[ObjectKey{
		Namespace: metadata.GetNamespace(),
		Name:      metadata.GetName(),
	}] = obj

	return gvk, nil
}

type ObjectSet struct {
	errs        []error
	objects     ObjectByGVK
	objectsByGK ObjectByGK
	order       []runtime.Object
	gvkOrder    []schema.GroupVersionKind
	gvkSeen     map[schema.GroupVersionKind]bool
}

func NewObjectSet(objs ...runtime.Object) *ObjectSet {
	os := &ObjectSet{
		objects:     ObjectByGVK{},
		objectsByGK: ObjectByGK{},
		gvkSeen:     map[schema.GroupVersionKind]bool{},
	}
	os.Add(objs...)
	return os
}

func (o *ObjectSet) ObjectsByGVK() ObjectByGVK {
	if o == nil {
		return nil
	}
	return o.objects
}

func (o *ObjectSet) Contains(gk schema.GroupKind, key ObjectKey) bool {
	_, ok := o.objectsByGK[gk][key]
	return ok
}

func (o *ObjectSet) Add(objs ...runtime.Object) *ObjectSet {
	for _, obj := range objs {
		o.add(obj)
	}
	return o
}

func (o *ObjectSet) add(obj runtime.Object) {
	if obj == nil || reflect.ValueOf(obj).IsNil() {
		return
	}

	gvk, err := o.objects.Add(obj)
	if err != nil {
		o.err(fmt.Errorf("failed to add %T: %w", obj, err))
		return
	}

	_, err = o.objectsByGK.add(obj)
	if err != nil {
		o.err(fmt.Errorf("failed to add %T: %w", obj, err))
		return
	}

	o.order = append(o.order, obj)
	if !o.gvkSeen[gvk] {
		o.gvkSeen[gvk] = true
		o.gvkOrder = append(o.gvkOrder, gvk)
	}
}

func (o *ObjectSet) err(err error) {
	o.errs = append(o.errs, err)
}

func (o *ObjectSet) Err() error {
	return merr.NewErrors(o.errs...)
}

func (o *ObjectSet) Len() int {
	return len(o.objects)
}

func (o *ObjectSet) GVKOrder(known ...schema.GroupVersionKind) []schema.GroupVersionKind {
	var rest []schema.GroupVersionKind

	for _, gvk := range known {
		if o.gvkSeen[gvk] {
			continue
		}
		rest = append(rest, gvk)
	}

	sort.Slice(rest, func(i, j int) bool {
		return rest[i].String() < rest[j].String()
	})

	return append(o.gvkOrder, rest...)
}

type ObjectByKey map[ObjectKey]runtime.Object

func (o ObjectByKey) Namespaces() []string {
	namespaces := Set{}
	for objKey := range o {
		namespaces.Add(objKey.Namespace)
	}
	return namespaces.Values()
}

type ObjectByGK map[schema.GroupKind]map[ObjectKey]runtime.Object

func (o ObjectByGK) add(obj runtime.Object) (schema.GroupKind, error) {
	metadata, err := meta.Accessor(obj)
	if err != nil {
		return schema.GroupKind{}, err
	}

	gvk, err := getGVK(obj)
	if err != nil {
		return schema.GroupKind{}, err
	}

	gk := gvk.GroupKind()

	objs := o[gk]
	if objs == nil {
		objs = ObjectByKey{}
		o[gk] = objs
	}

	objs[ObjectKey{
		Namespace: metadata.GetNamespace(),
		Name:      metadata.GetName(),
	}] = obj

	return gk, nil
}

func getGVK(obj runtime.Object) (schema.GroupVersionKind, error) {
	gvk := obj.GetObjectKind().GroupVersionKind()
	if gvk.Kind != "" {
		return gvk, nil
	}

	gvks, _, err := schemes.All.ObjectKinds(obj)
	if err != nil {
		return schema.GroupVersionKind{}, fmt.Errorf("failed to find gvk for %T, you may need to import the wrangler generated controller package: %w", obj, err)
	}

	if len(gvks) == 0 {
		return schema.GroupVersionKind{}, fmt.Errorf("failed to find gvk for %T", obj)
	}

	return gvks[0], nil
}



================================================
FILE: internal/cmd/agent/deployer/objectset/objectset_test.go
================================================
package objectset

import (
	"testing"

	"github.com/stretchr/testify/assert"
)

func TestObjectByKey_Namespaces(t *testing.T) {
	tests := []struct {
		name           string
		objects        ObjectByKey
		wantNamespaces []string
	}{
		{
			name:           "empty",
			objects:        ObjectByKey{},
			wantNamespaces: nil,
		},
		{
			name: "1 namespace",
			objects: ObjectByKey{
				ObjectKey{Namespace: "ns1", Name: "a"}: nil,
				ObjectKey{Namespace: "ns1", Name: "b"}: nil,
			},
			wantNamespaces: []string{"ns1"},
		},
		{
			name: "many namespaces",
			objects: ObjectByKey{
				ObjectKey{Namespace: "ns1", Name: "a"}: nil,
				ObjectKey{Namespace: "ns2", Name: "b"}: nil,
			},
			wantNamespaces: []string{"ns1", "ns2"},
		},
		{
			name: "many namespaces with duplicates",
			objects: ObjectByKey{
				ObjectKey{Namespace: "ns1", Name: "a"}: nil,
				ObjectKey{Namespace: "ns2", Name: "b"}: nil,
				ObjectKey{Namespace: "ns1", Name: "c"}: nil,
			},
			wantNamespaces: []string{"ns1", "ns2"},
		},
		{
			name: "missing namespace",
			objects: ObjectByKey{
				ObjectKey{Namespace: "ns1", Name: "a"}: nil,
				ObjectKey{Name: "b"}:                   nil,
			},
			wantNamespaces: []string{"", "ns1"},
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			gotNamespaces := tt.objects.Namespaces()
			assert.ElementsMatchf(t, tt.wantNamespaces, gotNamespaces, "Namespaces() = %v, want %v", gotNamespaces, tt.wantNamespaces)
		})
	}
}



================================================
FILE: internal/cmd/agent/deployer/objectset/stringset.go
================================================
package objectset

var empty struct{}

// Set is an exceptionally simple `set` implementation for strings.
// It is not threadsafe, but can be used in place of a simple `map[string]struct{}`
// as long as you don't want to do too much with it.
type Set struct {
	m map[string]struct{}
}

func (s *Set) Add(ss ...string) {
	if s.m == nil {
		s.m = make(map[string]struct{}, len(ss))
	}
	for _, k := range ss {
		s.m[k] = empty
	}
}

func (s *Set) Values() []string {
	i := 0
	keys := make([]string, len(s.m))
	for key := range s.m {
		keys[i] = key
		i++
	}

	return keys
}



================================================
FILE: internal/cmd/agent/deployer/summary/cattletypes.go
================================================
package summary

import (
	"github.com/rancher/fleet/internal/cmd/agent/deployer/data"
	fleetv1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1/summary"
)

func checkCattleTypes(obj data.Object, condition []Condition, summary fleetv1.Summary) fleetv1.Summary {
	return checkRelease(obj, condition, summary)
}

func checkRelease(obj data.Object, _ []Condition, summary fleetv1.Summary) fleetv1.Summary {
	if !isKind(obj, "App", "catalog.cattle.io") {
		return summary
	}
	if obj.String("status", "summary", "state") != "deployed" {
		return summary
	}
	for _, resources := range obj.Slice("spec", "resources") {
		summary.Relationships = append(summary.Relationships, fleetv1.Relationship{
			Name:       resources.String("name"),
			Kind:       resources.String("kind"),
			APIVersion: resources.String("apiVersion"),
			Type:       "helmresource",
		})
	}
	return summary
}



================================================
FILE: internal/cmd/agent/deployer/summary/condition.go
================================================
package summary

import (
	"encoding/json"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/data"
)

func getRawConditions(obj data.Object) []data.Object {
	statusAnn := obj.String("metadata", "annotations", "cattle.io/status")
	if statusAnn != "" {
		status := data.Object{}
		if err := json.Unmarshal([]byte(statusAnn), &status); err == nil {
			return append(obj.Slice("status", "conditions"), status.Slice("conditions")...)
		}
	}
	return obj.Slice("status", "conditions")
}

func getConditions(obj data.Object) (result []Condition) {
	for _, condition := range getRawConditions(obj) {
		result = append(result, Condition{Object: condition})
	}
	return
}

type Condition struct {
	data.Object
}

func (c Condition) Type() string {
	return c.String("type")
}

func (c Condition) Status() string {
	return c.String("status")
}

func (c Condition) Reason() string {
	return c.String("reason")
}

func (c Condition) Message() string {
	return c.String("message")
}

func (c Condition) Equals(other Condition) bool {
	return c.Type() == other.Type() &&
		c.Status() == other.Status() &&
		c.Reason() == other.Reason() &&
		c.Message() == other.Message()
}



================================================
FILE: internal/cmd/agent/deployer/summary/coretypes.go
================================================
package summary

import (
	"github.com/rancher/fleet/internal/cmd/agent/deployer/data"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/data/convert"
	fleetv1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1/summary"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func checkHasPodTemplate(obj data.Object, condition []Condition, summary fleetv1.Summary) fleetv1.Summary {
	template := obj.Map("spec", "template")
	if template == nil {
		return summary
	}

	if !isKind(obj, "ReplicaSet", "apps/", "extension/") &&
		!isKind(obj, "DaemonSet", "apps/", "extension/") &&
		!isKind(obj, "StatefulSet", "apps/", "extension/") &&
		!isKind(obj, "Deployment", "apps/", "extension/") &&
		!isKind(obj, "Job", "batch/") &&
		!isKind(obj, "Service") {
		return summary
	}

	return checkPodTemplate(template, condition, summary)
}

func checkHasPodSelector(obj data.Object, condition []Condition, summary fleetv1.Summary) fleetv1.Summary {
	selector := obj.Map("spec", "selector")
	if selector == nil {
		return summary
	}

	if !isKind(obj, "ReplicaSet", "apps/", "extension/") &&
		!isKind(obj, "DaemonSet", "apps/", "extension/") &&
		!isKind(obj, "StatefulSet", "apps/", "extension/") &&
		!isKind(obj, "Deployment", "apps/", "extension/") &&
		!isKind(obj, "Job", "batch/") &&
		!isKind(obj, "Service") {
		return summary
	}

	_, hasMatch := selector["matchLabels"]
	if !hasMatch {
		_, hasMatch = selector["matchExpressions"]
	}
	sel := metav1.LabelSelector{}
	if hasMatch {
		if err := convert.ToObj(selector, &sel); err != nil {
			return summary
		}
	} else {
		sel.MatchLabels = map[string]string{}
		for k, v := range selector {
			sel.MatchLabels[k] = convert.ToString(v)
		}
	}

	t := "creates"
	if obj["kind"] == "Service" {
		t = "selects"
	}

	summary.Relationships = append(summary.Relationships, fleetv1.Relationship{
		Kind:       "Pod",
		APIVersion: "v1",
		Type:       t,
		Selector:   &sel,
	})
	return summary
}

func checkPod(obj data.Object, condition []Condition, summary fleetv1.Summary) fleetv1.Summary {
	if !isKind(obj, "Pod") {
		return summary
	}
	if obj.String("kind") != "Pod" || obj.String("apiVersion") != "v1" {
		return summary
	}
	return checkPodTemplate(obj, condition, summary)
}

func checkPodTemplate(obj data.Object, condition []Condition, summary fleetv1.Summary) fleetv1.Summary {
	summary = checkPodConfigMaps(obj, condition, summary)
	summary = checkPodSecrets(obj, condition, summary)
	summary = checkPodServiceAccount(obj, condition, summary)
	summary = checkPodProjectedVolume(obj, condition, summary)
	summary = checkPodPullSecret(obj, condition, summary)
	return summary
}

func checkPodPullSecret(obj data.Object, _ []Condition, summary fleetv1.Summary) fleetv1.Summary {
	for _, pullSecret := range obj.Slice("imagePullSecrets") {
		if name := pullSecret.String("name"); name != "" {
			summary.Relationships = append(summary.Relationships, fleetv1.Relationship{
				Name:       name,
				Kind:       "Secret",
				APIVersion: "v1",
				Type:       "uses",
			})
		}
	}
	return summary
}

func checkPodProjectedVolume(obj data.Object, _ []Condition, summary fleetv1.Summary) fleetv1.Summary {
	for _, vol := range obj.Slice("spec", "volumes") {
		for _, source := range vol.Slice("projected", "sources") {
			if secretName := source.String("secret", "name"); secretName != "" {
				summary.Relationships = append(summary.Relationships, fleetv1.Relationship{
					Name:       secretName,
					Kind:       "Secret",
					APIVersion: "v1",
					Type:       "uses",
				})
			}
			if configMap := source.String("configMap", "name"); configMap != "" {
				summary.Relationships = append(summary.Relationships, fleetv1.Relationship{
					Name:       configMap,
					Kind:       "ConfigMap",
					APIVersion: "v1",
					Type:       "uses",
				})
			}
		}
	}
	return summary
}

func addEnvRef(summary fleetv1.Summary, names map[string]bool, obj data.Object, fieldPrefix, kind string) fleetv1.Summary {
	for _, container := range obj.Slice("spec", "containers") {
		for _, env := range container.Slice("envFrom") {
			name := env.String(fieldPrefix+"Ref", "name")
			if name == "" || names[name] {
				continue
			}
			names[name] = true
			summary.Relationships = append(summary.Relationships, fleetv1.Relationship{
				Name:       name,
				Kind:       kind,
				APIVersion: "v1",
				Type:       "uses",
			})
		}
		for _, env := range container.Slice("env") {
			name := env.String("valueFrom", fieldPrefix+"KeyRef", "name")
			if name == "" || names[name] {
				continue
			}
			names[name] = true
			summary.Relationships = append(summary.Relationships, fleetv1.Relationship{
				Name:       name,
				Kind:       kind,
				APIVersion: "v1",
				Type:       "uses",
			})
		}
	}

	return summary
}

func checkPodConfigMaps(obj data.Object, _ []Condition, summary fleetv1.Summary) fleetv1.Summary {
	names := map[string]bool{}
	for _, vol := range obj.Slice("spec", "volumes") {
		name := vol.String("configMap", "name")
		if name == "" || names[name] {
			continue
		}
		names[name] = true
		summary.Relationships = append(summary.Relationships, fleetv1.Relationship{
			Name:       name,
			Kind:       "ConfigMap",
			APIVersion: "v1",
			Type:       "uses",
		})
	}
	summary = addEnvRef(summary, names, obj, "configMap", "ConfigMap")
	return summary
}

func checkPodSecrets(obj data.Object, _ []Condition, summary fleetv1.Summary) fleetv1.Summary {
	names := map[string]bool{}
	for _, vol := range obj.Slice("spec", "volumes") {
		name := vol.String("secret", "secretName")
		if name == "" || names[name] {
			continue
		}
		names[name] = true
		summary.Relationships = append(summary.Relationships, fleetv1.Relationship{
			Name:       name,
			Kind:       "Secret",
			APIVersion: "v1",
			Type:       "uses",
		})
	}
	summary = addEnvRef(summary, names, obj, "secret", "Secret")
	return summary
}

func checkPodServiceAccount(obj data.Object, _ []Condition, summary fleetv1.Summary) fleetv1.Summary {
	saName := obj.String("spec", "serviceAccountName")
	summary.Relationships = append(summary.Relationships, fleetv1.Relationship{
		Name:       saName,
		Kind:       "ServiceAccount",
		APIVersion: "v1",
		Type:       "uses",
	})
	return summary

}



================================================
FILE: internal/cmd/agent/deployer/summary/doc.go
================================================
// Package imported from github.com/rancher/wrangler, with a few local tweaks (e.g. status messages deduplication)
// Current version: v3.2.1
package summary



================================================
FILE: internal/cmd/agent/deployer/summary/gvk.go
================================================
package summary

import (
	"encoding/json"
	"fmt"
	"regexp"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/apimachinery/pkg/util/sets"
)

var (
	gvkRegExp = regexp.MustCompile(`^(.*?)/(.*),\s*Kind=(.+)$`)
)

// conditionTypeStatusJSON is a custom JSON to map a complex object into a standard JSON object. It maps Groups,
// Versions and Kinds to Conditions, Types, and Status, indicating with a flag if a certain condition with specific
// status represents an error or not. It is expected to be something like:
//
//	{
//		"gvk": 			"helm.cattle.io/v1, Kind=HelmChart",
//		"conditionMappings": [
//			{
//				"type": "JobCreated"	// This means JobCreated is mostly informational and True or False
//			},				// doesn't mean error
//			{
//				"type": "Failed",	// This means Failed is considered error if it's status is True
//				"status": ["True"]
//			},
//		}
//	}
type conditionTypeStatusJSON struct {
	GVK               string                     `json:"gvk"`
	ConditionMappings []conditionStatusErrorJSON `json:"conditionMappings"`
}

type conditionStatusErrorJSON struct {
	Type   string                   `json:"type"`
	Status []metav1.ConditionStatus `json:"status,omitempty"`
}

type ConditionTypeStatusErrorMapping map[schema.GroupVersionKind]map[string]sets.Set[metav1.ConditionStatus]

func (m ConditionTypeStatusErrorMapping) MarshalJSON() ([]byte, error) {
	output := []conditionTypeStatusJSON{}
	for gvk, mapping := range m {
		typeStatus := conditionTypeStatusJSON{GVK: gvk.String()}
		for condition, statuses := range mapping {
			typeStatus.ConditionMappings = append(typeStatus.ConditionMappings, conditionStatusErrorJSON{
				Type:   condition,
				Status: statuses.UnsortedList(),
			})
		}
		output = append(output, typeStatus)
	}
	return json.Marshal(output)
}

func (m ConditionTypeStatusErrorMapping) UnmarshalJSON(data []byte) error {
	var conditionMappingsJSON []conditionTypeStatusJSON
	err := json.Unmarshal(data, &conditionMappingsJSON)
	if err != nil {
		return err
	}

	for _, mapping := range conditionMappingsJSON {
		// checking if mapping.GVK is in the right format: <group>/[version], Kind=[kind]
		mx := gvkRegExp.FindStringSubmatch(mapping.GVK)
		if len(mx) == 0 {
			return fmt.Errorf("gvk parsing failed: wrong GVK format: <%s>", mapping.GVK)
		}

		conditionMappings := map[string]sets.Set[metav1.ConditionStatus]{}
		for _, condition := range mapping.ConditionMappings {
			conditionMappings[condition.Type] = sets.New[metav1.ConditionStatus](condition.Status...)
		}

		m[schema.GroupVersionKind{
			Group:   mx[1],
			Version: mx[2],
			Kind:    mx[3],
		}] = conditionMappings
	}
	return nil
}



================================================
FILE: internal/cmd/agent/deployer/summary/gvk_test.go
================================================
package summary

import (
	"encoding/json"
	"sort"
	"testing"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/sets"
)

func TestConditionalTypeStatusErrorMapping_MarshalJSON(t *testing.T) {
	testCases := []struct {
		name        string
		input       ConditionTypeStatusErrorMapping
		expected    []byte
		expectedErr error
	}{
		{
			name: "usual case",
			input: ConditionTypeStatusErrorMapping{
				{Group: "helm.cattle.io", Version: "v1", Kind: "HelmChart"}: {
					"JobCreated": sets.New[metav1.ConditionStatus](),
					"Failed":     sets.New[metav1.ConditionStatus](metav1.ConditionTrue),
				},
			},
			expected: []byte(`
			[
				{
					"gvk": "helm.cattle.io/v1, Kind=HelmChart",
					"conditionMappings": [
						{
							"type": "JobCreated"
						},
						{
							"type": "Failed",
							"status": ["True"]
						}
					]
				}
			]
			`),
			expectedErr: nil,
		},
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			output, err := json.Marshal(&tc.input)

			if tc.expectedErr == nil {
				require.NoError(t, err)
			} else {
				require.Error(t, err)
			}

			actual := []conditionTypeStatusJSON{}
			expected := []conditionTypeStatusJSON{}
			require.NoError(t, json.Unmarshal(output, &actual))
			assert.NoError(t, json.Unmarshal(tc.expected, &expected))

			sort.Slice(actual, func(i, j int) bool { return actual[i].GVK < actual[j].GVK })
			sort.Slice(expected, func(i, j int) bool { return expected[i].GVK < expected[j].GVK })

			for _, act := range actual {
				sort.Slice(act.ConditionMappings, func(i, j int) bool {
					return act.ConditionMappings[i].Type < act.ConditionMappings[j].Type
				})

				for _, mappings := range act.ConditionMappings {
					sort.Slice(mappings.Status, func(i, j int) bool {
						return mappings.Status[i] < mappings.Status[j]
					})
				}
			}

			for _, exp := range expected {
				sort.Slice(exp.ConditionMappings, func(i, j int) bool {
					return exp.ConditionMappings[i].Type < exp.ConditionMappings[j].Type
				})

				for _, mappings := range exp.ConditionMappings {
					sort.Slice(mappings.Status, func(i, j int) bool {
						return mappings.Status[i] < mappings.Status[j]
					})
				}
			}

			assert.Equal(t, expected, actual)
		})
	}
}

func TestConditionalTypeStatusErrorMapping_UnmarshalJSON(t *testing.T) {
	testCases := []struct {
		name            string
		input           []byte
		expected        ConditionTypeStatusErrorMapping
		errorIsExpected bool
	}{
		{
			name: "usual case",
			input: []byte(`
			[
				{
					"gvk": "helm.cattle.io/v1, Kind=HelmChart",
					"conditionMappings": [
						{
							"type": "JobCreated"
						},
						{
							"type": "Failed",
							"status": ["True"]
						}
					]
				}
			]
			`),
			expected: ConditionTypeStatusErrorMapping{
				{Group: "helm.cattle.io", Version: "v1", Kind: "HelmChart"}: {
					"JobCreated": sets.New[metav1.ConditionStatus](),
					"Failed":     sets.New[metav1.ConditionStatus](metav1.ConditionTrue),
				},
			},
			errorIsExpected: false,
		},
		{
			name: "core types (no group)",
			input: []byte(`
			[
				{
					"gvk": "/v1, Kind=Node",
					"conditionMappings": [
						{
							"type": "Ready",
							"status": ["False"]
						}
					]
				}
			]
			`),
			expected: ConditionTypeStatusErrorMapping{
				{Group: "", Version: "v1", Kind: "Node"}: {
					"Ready": sets.New[metav1.ConditionStatus](metav1.ConditionFalse),
				},
			},
			errorIsExpected: false,
		},
		{
			name: "wrong GVK format",
			input: []byte(`
			[
				{
					"gvk": "wrong GVK format",
					"conditionMappings": [
						{
							"type": "Ready",
							"status": ["False"]
						}
					]
				}
			]
			`),
			expected:        ConditionTypeStatusErrorMapping{},
			errorIsExpected: true,
		},
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			gvkConditionMappings := ConditionTypeStatusErrorMapping{}

			err := json.Unmarshal(tc.input, &gvkConditionMappings)
			if tc.errorIsExpected {
				require.Error(t, err)
			} else {
				require.NoError(t, err)
			}

			assert.Equal(t, tc.expected, gvkConditionMappings)
		})
	}
}



================================================
FILE: internal/cmd/agent/deployer/summary/suite_test.go
================================================
package summary_test

import (
	"testing"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

func TestSummary(t *testing.T) {
	RegisterFailHandler(Fail)
	RunSpecs(t, "Summary Suite")
}



================================================
FILE: internal/cmd/agent/deployer/summary/summarize.go
================================================
package summary

import (
	"strings"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/data"
	fleetv1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1/summary"

	unstructured2 "github.com/rancher/wrangler/v3/pkg/unstructured"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
)

func dedupMessage(messages []string) []string {
	seen := map[string]bool{}
	var result []string

	for _, message := range messages {
		message = strings.TrimSpace(message)
		if message == "" {
			continue
		}
		if seen[message] {
			continue
		}
		seen[message] = true
		result = append(result, message)
	}

	return result
}

func Summarize(runtimeObj runtime.Object) fleetv1.Summary {
	var (
		obj     data.Object
		err     error
		summary fleetv1.Summary
	)

	if s, ok := runtimeObj.(*SummarizedObject); ok {
		return s.Summary
	}

	unstr, ok := runtimeObj.(*unstructured.Unstructured)
	if !ok {
		unstr, err = unstructured2.ToUnstructured(runtimeObj)
		if err != nil {
			return summary
		}
	}

	if unstr != nil {
		obj = unstr.Object
	}

	conditions := getConditions(obj)

	for _, summarizer := range Summarizers {
		summary = summarizer(obj, conditions, summary)
	}

	if summary.State == "" {
		summary.State = "active"
	}

	summary.State = strings.ToLower(summary.State)
	summary.Message = dedupMessage(summary.Message)
	return summary
}



================================================
FILE: internal/cmd/agent/deployer/summary/summarized.go
================================================
package summary

import (
	fleetv1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1/summary"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

type SummarizedObject struct {
	metav1.PartialObjectMetadata
	fleetv1.Summary
}



================================================
FILE: internal/cmd/agent/deployer/summary/summarizers.go
================================================
package summary

import (
	"encoding/json"
	"os"
	"strings"
	"time"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/data"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/data/convert"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/kv"
	fleetv1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1/summary"

	"github.com/sirupsen/logrus"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/apimachinery/pkg/util/sets"
	kstatus "sigs.k8s.io/cli-utils/pkg/kstatus/status"
)

const (
	kindSep                    = ", Kind="
	reason                     = "%REASON%"
	checkGVKErrorMappingEnvVar = "CATTLE_WRANGLER_CHECK_GVK_ERROR_MAPPING"
)

var (
	// True ==
	// False == error
	// Unknown == transitioning
	TransitioningUnknown = map[string]string{
		"Active":                      "activating",
		"AddonDeploy":                 "provisioning",
		"AgentDeployed":               "provisioning",
		"BackingNamespaceCreated":     "configuring",
		"Built":                       "building",
		"CertsGenerated":              "provisioning",
		"ConfigOK":                    "configuring",
		"Created":                     "creating",
		"CreatorMadeOwner":            "configuring",
		"DefaultNamespaceAssigned":    "configuring",
		"DefaultNetworkPolicyCreated": "configuring",
		"DefaultProjectCreated":       "configuring",
		"DockerProvisioned":           "provisioning",
		"Deployed":                    "deploying",
		"Drained":                     "draining",
		"Downloaded":                  "downloading",
		"etcd":                        "provisioning",
		"Inactive":                    "deactivating",
		"Initialized":                 "initializing",
		"Installed":                   "installing",
		"NodesCreated":                "provisioning",
		"Pending":                     "pending",
		"PodScheduled":                "scheduling",
		"Provisioned":                 "provisioning",
		"Reconciled":                  "reconciling",
		"Refreshed":                   "refreshed",
		"Registered":                  "registering",
		"Removed":                     "removing",
		"Saved":                       "saving",
		"Updated":                     "updating",
		"Updating":                    "updating",
		"Upgraded":                    "upgrading",
		"Waiting":                     "waiting",
		"InitialRolesPopulated":       "activating",
		"ScalingActive":               "pending",
		"AbleToScale":                 "pending",
		"RunCompleted":                "running",
		"Processed":                   "processed",
	}

	// For given GVK, This condition Type and this Status, indicates an error or not
	// e.g.: GVK: helm.cattle.io/v1, HelmChart
	//		--> JobCreated: [], indicates True or False are not errors
	//		--> Failed: ["True"], indicates "True" status is considered error
	//		--> Worked: ["False"], indicates "False" status is considered error
	//		--> Unknown: ["True", "False"] indicated "True" or "False" are considered errors
	GVKConditionErrorMapping = ConditionTypeStatusErrorMapping{
		{Group: "helm.cattle.io", Version: "v1", Kind: "HelmChart"}: {
			"JobCreated": sets.New[metav1.ConditionStatus](),
			"Failed":     sets.New(metav1.ConditionTrue),
		},
		{Group: "", Version: "v1", Kind: "Node"}: {
			"OutOfDisk":          sets.New(metav1.ConditionTrue),
			"MemoryPressure":     sets.New(metav1.ConditionTrue),
			"DiskPressure":       sets.New(metav1.ConditionTrue),
			"NetworkUnavailable": sets.New(metav1.ConditionTrue),
		},
		{Group: "apps", Version: "v1", Kind: "Deployment"}: {
			"ReplicaFailure": sets.New(metav1.ConditionTrue),
			"Progressing":    sets.New(metav1.ConditionFalse),
		},
		{Group: "apps", Version: "v1", Kind: "ReplicaSet"}: {
			"ReplicaFailure": sets.New(metav1.ConditionTrue),
		},

		// FALLBACK: In case we cannot match any Groups, Versions and Kinds then we fallback to this mapping.
		{Group: "", Version: "", Kind: ""}: {
			"Stalled": sets.New(metav1.ConditionTrue),
			"Failed":  sets.New(metav1.ConditionTrue),
		},
	}

	// True ==
	// False == transitioning
	// Unknown == error
	TransitioningFalse = map[string]string{
		"Completed":           "activating",
		"Ready":               "unavailable",
		"Available":           "updating",
		"BootstrapReady":      reason,
		"InfrastructureReady": reason,
		"NodeHealthy":         reason,
	}

	// True == transitioning
	// False ==
	// Unknown == error
	TransitioningTrue = map[string]string{
		"Reconciling": "reconciling",
	}

	Summarizers []Summarizer
)

type Summarizer func(obj data.Object, conditions []Condition, summary fleetv1.Summary) fleetv1.Summary

func init() {
	Summarizers = []Summarizer{
		checkStatusSummary,
		checkErrors,
		checkTransitioning,
		checkActive,
		checkPhase,
		checkInitializing,
		checkRemoving,
		checkStandard,
		checkLoadBalancer,
		checkPod,
		checkHasPodSelector,
		checkHasPodTemplate,
		checkOwner,
		checkApplyOwned,
		checkCattleTypes,
		kStatusSummarizer,
	}

	initializeCheckErrors()
}

func initializeCheckErrors() {
	gvkConfig := os.Getenv(checkGVKErrorMappingEnvVar)
	if gvkConfig != "" {
		logrus.Debugf("GVK Error Mapping Provided")
		gvkErrorMapping := ConditionTypeStatusErrorMapping{}
		if err := json.Unmarshal([]byte(gvkConfig), &gvkErrorMapping); err != nil {
			logrus.Errorln("Unable to parse GVK config: ", err.Error())
			return
		}

		// Merging GVK + Conditions
		//
		// IMPORTANT: In case you add a condition that exists already, we replace the set that holds the Status
		// completely of that condition by yours, this makes it possible to deactivate certain statuses for
		// debugging reasons.
		//
		// eg.:
		//
		// Existing one:
		//
		// helm.cattle.io, Kind=HelmChart
		// JobCreated => []
		// Failed => ["True"]
		//
		// In case you set Failed = ["False"] and add Ready = ["False"]:
		//
		// helm.cattle.io, Kind=HelmChart
		// JobCreated => []			<<<= not changed
		// Failed => ["False"]		<<<= replaced completely the set.
		// Ready => ["False"] 		<<<= merged to existing conditions.
		//
		// So, we've merged the conditions, but not the status set values.
		for gvk, newConditionsMap := range gvkErrorMapping {
			if _, exists := GVKConditionErrorMapping[gvk]; !exists {
				GVKConditionErrorMapping[gvk] = map[string]sets.Set[metav1.ConditionStatus]{}
			}

			existingConditionsMap := GVKConditionErrorMapping[gvk]
			for condition, errorMapping := range newConditionsMap {
				existingConditionsMap[condition] = errorMapping
			}
			GVKConditionErrorMapping[gvk] = existingConditionsMap
		}
		logrus.Debugf("GVK Error Mapping Set")
		return
	}
	logrus.Debugf("GVK Error Mapping not provided, using predefined values")
}

func checkOwner(obj data.Object, conditions []Condition, summary fleetv1.Summary) fleetv1.Summary {
	ustr := &unstructured.Unstructured{
		Object: obj,
	}
	for _, ownerref := range ustr.GetOwnerReferences() {
		rel := fleetv1.Relationship{
			Name:       ownerref.Name,
			Kind:       ownerref.Kind,
			APIVersion: ownerref.APIVersion,
			Type:       "owner",
			Inbound:    true,
		}
		if ownerref.Controller != nil && *ownerref.Controller {
			rel.ControlledBy = true
		}

		summary.Relationships = append(summary.Relationships, rel)
	}

	return summary
}

func checkStatusSummary(obj data.Object, _ []Condition, summary fleetv1.Summary) fleetv1.Summary {
	summaryObj := obj.Map("status", "display")
	if len(summaryObj) == 0 {
		summaryObj = obj.Map("status", "summary")
		if len(summaryObj) == 0 {
			return summary
		}
	}
	obj = summaryObj

	if _, ok := obj["state"]; ok {
		summary.State = obj.String("state")
	}
	if _, ok := obj["transitioning"]; ok {
		summary.Transitioning = obj.Bool("transitioning")
	}
	if _, ok := obj["error"]; ok {
		summary.Error = obj.Bool("error")
	}
	if _, ok := obj["message"]; ok {
		summary.Message = append(summary.Message, obj.String("message"))
	}

	return summary
}

func checkStandard(obj data.Object, _ []Condition, summary fleetv1.Summary) fleetv1.Summary {
	if summary.State != "" {
		return summary
	}

	// this is a hack to not call the standard summarizers on norman mapped objects
	if strings.HasPrefix(obj.String("type"), "/") {
		return summary
	}

	result, err := kstatus.Compute(&unstructured.Unstructured{Object: obj})
	if err != nil {
		return summary
	}

	switch result.Status {
	case kstatus.InProgressStatus:
		summary.State = "in-progress"
		summary.Message = append(summary.Message, result.Message)
		summary.Transitioning = true
	case kstatus.FailedStatus:
		summary.State = "failed"
		summary.Message = append(summary.Message, result.Message)
		summary.Error = true
	case kstatus.CurrentStatus:
		summary.State = "active"
		summary.Message = append(summary.Message, result.Message)
	case kstatus.TerminatingStatus:
		summary.State = "removing"
		summary.Message = append(summary.Message, result.Message)
		summary.Transitioning = true
	}

	return summary
}

func checkErrors(data data.Object, conditions []Condition, summary fleetv1.Summary) fleetv1.Summary {
	if len(conditions) == 0 {
		return summary
	}

	ustr := &unstructured.Unstructured{
		Object: data,
	}

	conditionMapping, found := GVKConditionErrorMapping[ustr.GroupVersionKind()]
	if !found {
		conditionMapping = GVKConditionErrorMapping[schema.GroupVersionKind{}]
	}

	for _, c := range conditions {
		status, found := conditionMapping[c.Type()]
		reasonIsError := c.Reason() == "Error"

		if !found && !reasonIsError {
			continue
		}

		if reasonIsError || status.Has(metav1.ConditionStatus(c.Status())) {
			summary.Error = true
			summary.Message = append(summary.Message, c.Message())
			if summary.State == "active" || summary.State == "" {
				summary.State = "error"
			}
		}
	}

	return summary
}

func checkTransitioning(_ data.Object, conditions []Condition, summary fleetv1.Summary) fleetv1.Summary {
	for _, c := range conditions {
		newState, ok := TransitioningUnknown[c.Type()]
		if !ok {
			continue
		}

		if c.Status() == "False" {
			summary.Error = true
			summary.State = newState
			summary.Message = append(summary.Message, c.Message())
		} else if c.Status() == "Unknown" && summary.State == "" {
			summary.Transitioning = true
			summary.State = newState
			summary.Message = append(summary.Message, c.Message())
		}
	}

	for _, c := range conditions {
		if summary.State != "" {
			break
		}
		newState, ok := TransitioningTrue[c.Type()]
		if !ok {
			continue
		}
		if c.Status() == "True" {
			summary.Transitioning = true
			summary.State = newState
			summary.Message = append(summary.Message, c.Message())
		}
	}

	ready := true
	readyMessage := ""
	for _, c := range conditions {
		if summary.State != "" {
			break
		}

		if c.Type() == "Ready" && c.Status() == "False" {
			ready = false
			readyMessage = c.Message()
			continue
		}
		newState, ok := TransitioningFalse[c.Type()]
		if !ok {
			continue
		}
		if newState == reason {
			newState = c.Reason()
		}
		if c.Status() == "False" {
			summary.Transitioning = true
			summary.State = newState
			summary.Message = append(summary.Message, c.Message())
		} else if c.Status() == "Unknown" {
			summary.Error = true
			summary.State = newState
			summary.Message = append(summary.Message, c.Message())
		}
	}

	if summary.State == "" && !ready {
		summary.Transitioning = true
		summary.State = "unavailable"
		summary.Message = append(summary.Message, readyMessage)
	}

	return summary
}

func checkActive(obj data.Object, _ []Condition, summary fleetv1.Summary) fleetv1.Summary {
	if summary.State != "" {
		return summary
	}

	switch obj.String("spec", "active") {
	case "true":
		summary.State = "active"
	case "false":
		summary.State = "inactive"
	}

	return summary
}

func checkPhase(obj data.Object, _ []Condition, summary fleetv1.Summary) fleetv1.Summary {
	phase := obj.String("status", "phase")
	switch {
	case phase == "Succeeded":
		summary.State = "succeeded"
		summary.Transitioning = false
	case phase == "Bound":
		summary.State = "bound"
		summary.Transitioning = false
	case phase != "" && summary.State == "":
		summary.State = phase
	}
	return summary
}

func checkInitializing(obj data.Object, conditions []Condition, summary fleetv1.Summary) fleetv1.Summary {
	apiVersion := obj.String("apiVersion")
	_, hasConditions := obj.Map("status")["conditions"]
	if summary.State == "" && hasConditions && len(conditions) == 0 && strings.Contains(apiVersion, "cattle.io") {
		val := obj.String("metadata", "created")
		if i, err := convert.ToTimestamp(val); err == nil {
			if time.Unix(i/1000, 0).Add(5 * time.Second).After(time.Now()) {
				summary.State = "initializing"
				summary.Transitioning = true
			}
		}
	}
	return summary
}

func checkRemoving(obj data.Object, conditions []Condition, summary fleetv1.Summary) fleetv1.Summary {
	removed := obj.String("metadata", "removed")
	if removed == "" {
		return summary
	}

	summary.State = "removing"
	summary.Transitioning = true

	finalizers := obj.StringSlice("metadata", "finalizers")
	if len(finalizers) == 0 {
		finalizers = obj.StringSlice("spec", "finalizers")
	}

	for _, cond := range conditions {
		if cond.Type() == "Removed" && (cond.Status() == "Unknown" || cond.Status() == "False") && cond.Message() != "" {
			summary.Message = append(summary.Message, cond.Message())
		}
	}

	if len(finalizers) == 0 {
		return summary
	}

	_, f := kv.RSplit(finalizers[0], "controller.cattle.io/")
	if f == "foregroundDeletion" {
		f = "object cleanup"
	}

	summary.Message = append(summary.Message, "waiting on "+f)
	if i, err := convert.ToTimestamp(removed); err == nil {
		if time.Unix(i/1000, 0).Add(5 * time.Minute).Before(time.Now()) {
			summary.Error = true
		}
	}

	return summary
}

func checkLoadBalancer(obj data.Object, _ []Condition, summary fleetv1.Summary) fleetv1.Summary {
	if (summary.State == "active" || summary.State == "") &&
		obj.String("kind") == "Service" &&
		(obj.String("spec", "serviceKind") == "LoadBalancer" ||
			obj.String("spec", "type") == "LoadBalancer") {
		addresses := obj.Slice("status", "loadBalancer", "ingress")
		if len(addresses) == 0 {
			summary.State = "pending"
			summary.Transitioning = true
			summary.Message = append(summary.Message, "Load balancer is being provisioned")
		}
	}

	return summary
}

func isKind(obj data.Object, kind string, apiGroups ...string) bool {
	if obj.String("kind") != kind {
		return false
	}

	if len(apiGroups) == 0 {
		return obj.String("apiVersion") == "v1"
	}

	if len(apiGroups) == 0 {
		apiGroups = []string{""}
	}

	for _, group := range apiGroups {
		switch {
		case group == "":
			if obj.String("apiVersion") == "v1" {
				return true
			}
		case group[len(group)-1] == '/':
			if strings.HasPrefix(obj.String("apiVersion"), group) {
				return true
			}
		default:
			if obj.String("apiVersion") != group {
				return true
			}
		}
	}

	return false
}

func checkApplyOwned(obj data.Object, conditions []Condition, summary fleetv1.Summary) fleetv1.Summary {
	if len(obj.Slice("metadata", "ownerReferences")) > 0 {
		return summary
	}

	annotations := obj.Map("metadata", "annotations")
	gvkString := convert.ToString(annotations["objectset.rio.cattle.io/owner-gvk"])
	i := strings.Index(gvkString, kindSep)
	if i <= 0 {
		return summary
	}

	name := convert.ToString(annotations["objectset.rio.cattle.io/owner-name"])
	namespace := convert.ToString(annotations["objectset.rio.cattle.io/owner-namespace"])

	apiVersion := gvkString[:i]
	kind := gvkString[i+len(kindSep):]

	rel := fleetv1.Relationship{
		Name:       name,
		Namespace:  namespace,
		Kind:       kind,
		APIVersion: apiVersion,
		Type:       "applies",
		Inbound:    true,
	}

	summary.Relationships = append(summary.Relationships, rel)

	return summary
}

func kStatusSummarizer(obj data.Object, _ []Condition, summary fleetv1.Summary) fleetv1.Summary {
	result, err := kstatus.Compute(&unstructured.Unstructured{Object: obj})
	if err != nil {
		return summary
	}

	switch result.Status {
	case kstatus.InProgressStatus:
		summary.Transitioning = true
	case kstatus.FailedStatus:
		summary.Error = true
	case kstatus.CurrentStatus:
	case kstatus.TerminatingStatus:
		summary.Transitioning = true
	case kstatus.UnknownStatus:
	}

	if result.Message != "" {
		// Deduplicate status messages (https://github.com/rancher/fleet/issues/2859)
		messages := make(map[string]bool)
		var resultMessages []string
		for _, message := range strings.Split(result.Message, "; ") {
			if _, ok := messages[message]; ok {
				continue
			}
			messages[message] = true
			resultMessages = append(resultMessages, message)
		}

		summary.Message = append(summary.Message, strings.Join(resultMessages, "; "))
	}

	return summary
}



================================================
FILE: internal/cmd/agent/deployer/summary/summarizers_test.go
================================================
package summary

import (
	"os"
	"testing"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/data"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1/summary"
	"github.com/stretchr/testify/assert"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

func TestCheckErrors(t *testing.T) {
	type input struct {
		data       data.Object
		conditions []Condition
		summary    fleet.Summary
	}

	type output struct {
		summary fleet.Summary
	}

	testCases := []struct {
		name           string
		loadConditions func()
		input          input
		expected       output
	}{
		{
			name: "gvk not detected - summary remains the same",
			input: input{
				data: data.Object{},
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			expected: output{
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
		},
		{
			name: "gvk not found - summary remains the same",
			input: input{
				data: data.Object{
					"apiVersion": "sample.cattle.io/v1",
					"kind":       "Sample",
				},
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			expected: output{
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
		},
		{
			name: "gvk found, no conditions provided",
			input: input{
				data: data.Object{
					"apiVersion": "helm.cattle.io/v1",
					"kind":       "HelmChart",
				},
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			expected: output{
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
		},
		{
			name: "gvk found, condition not found",
			input: input{
				data: data.Object{
					"apiVersion": "helm.cattle.io/v1",
					"kind":       "HelmChart",
				},
				conditions: []Condition{
					newCondition("JobFailed", "True", "", ""),
				},
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			expected: output{
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
		},
		{
			name: "gvk found, condition is error",
			input: input{
				data: data.Object{
					"apiVersion": "helm.cattle.io/v1",
					"kind":       "HelmChart",
				},
				conditions: []Condition{
					newCondition("Failed", "True", "", "Helm Install Error"),
				},
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			expected: output{
				summary: fleet.Summary{
					State: "testing",
					Error: true,
					Message: []string{
						"Helm Install Error",
					},
				},
			},
		},
		{
			name: "gvk found, condition is not an error",
			input: input{
				data: data.Object{
					"apiVersion": "helm.cattle.io/v1",
					"kind":       "HelmChart",
				},
				conditions: []Condition{
					newCondition("Failed", "False", "", ""),
				},
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			expected: output{
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
		},
		{
			name: "load conditions - gvk not found",
			input: input{
				data: data.Object{
					"apiVersion": "helm.cattle.io/v1",
					"kind":       "HelmChart",
				},
				conditions: []Condition{
					newCondition("Failed", "False", "", ""),
				},
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			expected: output{
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			loadConditions: func() {
				os.Setenv(checkGVKErrorMappingEnvVar, `
					[
						{
							"gvk": "sample.cattle.io/v1, Kind=Sample",
							"conditionMappings": [
								{
									"type": "Failed",
									"status": ["True"]
								}
							]
						}
					]
				`)
			},
		},
		{
			name: "load conditions - gvk found - condition is only informational",
			input: input{
				data: data.Object{
					"apiVersion": "sample.cattle.io/v1",
					"kind":       "Sample",
				},
				conditions: []Condition{
					newCondition("Created", "True", "", ""),
				},
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			expected: output{
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			loadConditions: func() {
				os.Setenv(checkGVKErrorMappingEnvVar, `
					[
						{
							"gvk": "sample.cattle.io/v1, Kind=Sample",
							"conditionMappings": [
								{
									"type": "Created",
									"status": []
								}
							]
						}
					]
				`)
			},
		},
		{
			name: "load conditions - gvk found - is not an error",
			input: input{
				data: data.Object{
					"apiVersion": "sample.cattle.io/v1",
					"kind":       "Sample",
				},
				conditions: []Condition{
					newCondition("Failed", "False", "", ""),
				},
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			expected: output{
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
		},
		{
			name: "load conditions - gvk found - is error",
			input: input{
				data: data.Object{
					"apiVersion": "sample.cattle.io/v1",
					"kind":       "Sample",
				},
				conditions: []Condition{
					newCondition("Failed", "True", "", "Sample Failure"),
				},
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			expected: output{
				summary: fleet.Summary{
					State: "testing",
					Error: true,
					Message: []string{
						"Sample Failure",
					},
				},
			},
		},
		{
			name: "load conditions - gvk found - is error but should be ignored",
			input: input{
				data: data.Object{
					"apiVersion": "sample.cattle.io/v1",
					"kind":       "Sample",
				},
				conditions: []Condition{
					newCondition("Failed", "True", "", ""),
				},
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			expected: output{
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			loadConditions: func() {
				os.Setenv(checkGVKErrorMappingEnvVar, `
					[
						{
							"gvk": "sample.cattle.io/v1, Kind=Sample",
							"conditionMappings": [
								{
									"type": "Failed",
                                    "status": []
								}
							]
						}
					]
				`)
			},
		},
		{
			name: "load conditions - gvk found - is not error but should be treated as error",
			input: input{
				data: data.Object{
					"apiVersion": "sample.cattle.io/v1",
					"kind":       "Sample",
				},
				conditions: []Condition{
					newCondition("Foo", "True", "", ""),
				},
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			expected: output{
				summary: fleet.Summary{
					State:   "testing",
					Error:   true,
					Message: []string{""},
				},
			},
			loadConditions: func() {
				os.Setenv(checkGVKErrorMappingEnvVar, `
					[
						{
							"gvk": "sample.cattle.io/v1, Kind=Sample",
							"conditionMappings": [
								{
									"type": "Foo",
                                    "status": ["True"]
								}
							]
						}
					]
				`)
			},
		},
		{
			name: "fallback conditions",
			input: input{
				data: data.Object{
					"apiVersion": "fallback.cattle.io/v1",
					"kind":       "Fallback",
				},
				conditions: []Condition{
					newCondition("Failed", "True", "", "Sample Failure"),
				},
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			expected: output{
				summary: fleet.Summary{
					State: "testing",
					Error: true,
					Message: []string{
						"Sample Failure",
					},
				},
			},
		},
		{
			name: "condition has error at reason field",
			input: input{
				data: data.Object{
					"apiVersion": "sample.cattle.io/v1",
					"kind":       "Sample",
				},
				conditions: []Condition{
					newCondition("SampleFailed", "True", "Error", "Error in Reason"),
				},
				summary: fleet.Summary{
					State: "testing",
					Error: false,
				},
			},
			expected: output{
				summary: fleet.Summary{
					State: "testing",
					Error: true,
					Message: []string{
						"Error in Reason",
					},
				},
			},
		},
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			if tc.loadConditions != nil {
				tc.loadConditions()
			}
			initializeCheckErrors()
			summary := checkErrors(tc.input.data, tc.input.conditions, tc.input.summary)

			assert.Equal(t, tc.expected.summary, summary)
		})
	}

}

func newCondition(conditionType, status, reason, message string) Condition {
	return Condition{
		Object: map[string]interface{}{
			"type":    conditionType,
			"status":  status,
			"reason":  reason,
			"message": message,
		},
	}
}

var _ = Describe("Summary", func() {
	When("testing kStatusSummarizer", func() {
		newObj := func(message string) data.Object {
			return data.Object{
				"status": map[string]interface{}{
					"conditions": []interface{}{
						map[string]interface{}{
							"type":    "Ready",
							"status":  "False",
							"message": message,
						},
					},
				},
			}
		}

		It("should deduplicate messages", func() {
			obj := newObj("message1; message2; message1; message1; message2")
			smr := kStatusSummarizer(obj, nil, fleet.Summary{})
			Expect(smr.Message).To(Equal([]string{"message1; message2"}))
			Expect(smr.Transitioning).To(BeTrue())
			Expect(smr.Error).To(BeFalse())
		})

		It("should not alter the message that doesn't contain duplicates", func() {
			obj := newObj("message1; message2; message3")
			smr := kStatusSummarizer(obj, nil, fleet.Summary{})
			Expect(smr.Message).To(Equal([]string{"message1; message2; message3"}))
		})

		It("should not deduplicate messages with the wrong separator", func() {
			separators := []string{", ", " ", " , ", "[", "]", "{", "}"}
			for _, sep := range separators {
				obj := newObj("message1" + sep + "message1")
				smr := kStatusSummarizer(obj, nil, fleet.Summary{})
				Expect(smr.Message).To(Equal([]string{"message1" + sep + "message1"}))
			}
		})

		It("should not fail on an empty message", func() {
			obj := newObj("")
			smr := kStatusSummarizer(obj, nil, fleet.Summary{})
			Expect(smr.Message).To(BeEmpty())
		})
	})
})



================================================
FILE: internal/cmd/agent/register/register.go
================================================
package register

import (
	"context"
	"fmt"
	"net/http"
	"os"
	"time"

	"github.com/sirupsen/logrus"

	"github.com/rancher/fleet/internal/config"
	"github.com/rancher/fleet/internal/registration"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"
	fleetcontrollers "github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io"

	"github.com/rancher/wrangler/v3/pkg/generated/controllers/core"
	corecontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"
	"github.com/rancher/wrangler/v3/pkg/randomtoken"

	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	clientcmdapi "k8s.io/client-go/tools/clientcmd/api"
	"sigs.k8s.io/yaml"
)

const (
	CredName            = "fleet-agent" // same as AgentConfigName
	Kubeconfig          = "kubeconfig"
	Token               = "token"
	Values              = "values"
	DeploymentNamespace = "deploymentNamespace"
	ClusterNamespace    = "clusterNamespace"
	ClusterName         = "clusterName"
)

type AgentInfo struct {
	// ClusterNamespace is the namespace on upstream, e.g. cluster-fleet-ID
	ClusterNamespace string
	ClusterName      string
	ClientConfig     clientcmd.ClientConfig
}

// Register creates a fleet-agent secret with the upstream kubeconfig, by
// running the registration process with the upstream cluster.
// For the initial registration, the fleet-agent-bootstrap secret must exist
// on the local cluster.
func Register(ctx context.Context, namespace string, config *rest.Config) (*AgentInfo, error) {
	for {
		cfg, err := tryRegister(ctx, namespace, config)
		if err == nil {
			return cfg, nil
		}
		logrus.Errorf("Failed to register agent: %v", err)
		select {
		case <-ctx.Done():
			return nil, ctx.Err()
		case <-time.After(durations.AgentRegistrationRetry):
		}
	}
}

// tryRegister makes sure the secret cattle-fleet-system/fleet-agent is
// populated and the contained kubeconfig is working
func tryRegister(ctx context.Context, namespace string, cfg *rest.Config) (*AgentInfo, error) {
	cfg = rest.CopyConfig(cfg)
	// disable the rate limiter
	cfg.QPS = -1
	cfg.RateLimiter = nil

	k8s, err := core.NewFactoryFromConfig(cfg)
	if err != nil {
		return nil, err
	}

	secret, err := k8s.Core().V1().Secret().Get(namespace, CredName, metav1.GetOptions{})
	if apierrors.IsNotFound(err) {
		logrus.Warn("Cannot find fleet-agent secret, running registration")
		// fallback to local cattle-fleet-system/fleet-agent-bootstrap
		secret, err = runRegistration(ctx, k8s.Core().V1(), namespace)
		if err != nil {
			return nil, fmt.Errorf("registration failed: %w", err)
		}
	} else if err != nil {
		return nil, err
	} else if err := testClientConfig(secret.Data[Kubeconfig]); err != nil {
		// skip testClientConfig check if previous error, or IsNotFound fallback succeeded
		logrus.Errorf("Current credential failed, falling back to reregistering: %v", err)
		secret, err = runRegistration(ctx, k8s.Core().V1(), namespace)
		if err != nil {
			return nil, fmt.Errorf("re-registration failed: %w", err)
		}
	}

	clientConfig, err := clientcmd.NewClientConfigFromBytes(secret.Data[Kubeconfig])
	if err != nil {
		return nil, err
	}

	// delete the fleet-agent-bootstrap cred
	_ = k8s.Core().V1().Secret().Delete(namespace, config.AgentBootstrapConfigName, nil)
	return &AgentInfo{
		ClusterNamespace: string(secret.Data[ClusterNamespace]),
		ClusterName:      string(secret.Data[ClusterName]),
		ClientConfig:     clientConfig,
	}, nil
}

// coreInterface is a subset of corecontrollers.Interface
type coreInterface interface {
	ConfigMap() corecontrollers.ConfigMapController
	Namespace() corecontrollers.NamespaceController
	Secret() corecontrollers.SecretController
}

// runRegistration reads the cattle-fleet-system/fleet-agent-bootstrap secret and
// waits for the registration secret to appear on the management cluster to
// create a new fleet-agent secret.
// It uses the token provided in fleet-agent-bootstrap to build a
// kubeconfig and create a ClusterRegistration on the management cluster.
// Then it waits up to 30 minutes for the registration secret
// "c-clientID-clientRandom" to appear in the systemRegistrationNamespace on
// the management cluster.
// Finally uses the client from the config (service account: fleet-agent), to
// update the "fleet-agent" secret with a new kubeconfig from the registration
// secret. The new kubeconfig can then be used to query bundledeployments.
func runRegistration(ctx context.Context, k8s coreInterface, namespace string) (*corev1.Secret, error) {
	secret, err := k8s.Secret().Get(namespace, config.AgentBootstrapConfigName, metav1.GetOptions{})
	if err != nil {
		return nil, fmt.Errorf("looking up secret %s/%s: %w", namespace, config.AgentBootstrapConfigName, err)
	}

	cfg, err := config.Lookup(ctx, secret.Namespace, config.AgentConfigName, k8s.ConfigMap())
	if err != nil {
		return nil, fmt.Errorf("failed to look up client config %s/%s: %w", secret.Namespace, config.AgentConfigName, err)
	}

	clientConfig, undoBypass := createClientConfigFromSecret(ctx, secret, cfg.AgentTLSMode == config.AgentTLSModeSystemStore)
	defer undoBypass()

	ns, _, err := clientConfig.Namespace()
	if err != nil {
		return nil, err
	}

	kc, err := clientConfig.ClientConfig()
	if err != nil {
		return nil, err
	}

	fleetK8s, err := kubernetes.NewForConfig(kc)
	if err != nil {
		return nil, err
	}

	fc, err := fleetcontrollers.NewFactoryFromConfig(kc)
	if err != nil {
		return nil, err
	}

	token, err := randomtoken.Generate()
	if err != nil {
		return nil, err
	}

	clientID := ""
	if cfg.ClientID != "" {
		clientID = cfg.ClientID
	} else {
		kubeSystem, err := k8s.Namespace().Get("kube-system", metav1.GetOptions{})
		if err != nil {
			return nil, fmt.Errorf("cannot retrieve our kubeSystem.UID: %w", err)
		}

		// no configured id, client id is now "clusterID"
		clientID = string(kubeSystem.UID)
	}

	// add the name of the pod that created the registration for debugging
	if cfg.Labels == nil {
		cfg.Labels = map[string]string{}
	}
	cfg.Labels["fleet.cattle.io/created-by-agent-pod"] = os.Getenv("HOSTNAME")

	logrus.Infof("Creating clusterregistration with id '%s' for new token", clientID)
	request, err := fc.Fleet().V1alpha1().ClusterRegistration().Create(&fleet.ClusterRegistration{
		ObjectMeta: metav1.ObjectMeta{
			GenerateName: "request-",
			Namespace:    ns,
		},
		Spec: fleet.ClusterRegistrationSpec{
			ClientID:      clientID,
			ClientRandom:  token,
			ClusterLabels: cfg.Labels,
		},
	})
	if err != nil {
		return nil, fmt.Errorf("cannot create clusterregistration on management cluster for cluster id '%s': %w", clientID, err)
	}

	secretName := registration.SecretName(request.Spec.ClientID, request.Spec.ClientRandom)
	secretNamespace := string(values(secret.Data)["systemRegistrationNamespace"])
	timeout := time.After(durations.CreateClusterSecretTimeout)

	for {
		select {
		case <-timeout:
			return nil, fmt.Errorf("timeout waiting for registration secret '%s/%s' on management cluster", secretNamespace, secretName)
		case <-ctx.Done():
			return nil, ctx.Err()
		case <-time.After(durations.ClusterSecretRetry):
		}

		newSecret, err := fleetK8s.CoreV1().Secrets(secretNamespace).Get(ctx, secretName, metav1.GetOptions{})
		if err != nil {
			logrus.Infof("Waiting for secret '%s/%s' on management cluster for request '%s/%s': %v", secretNamespace, secretName, request.Namespace, request.Name, err)
			continue
		}

		newToken := newSecret.Data[Token]
		clusterNamespace := newSecret.Data[ClusterNamespace]
		clusterName := newSecret.Data[ClusterName]
		// this is set to cluster.Status.Namespace
		deploymentNamespace := newSecret.Data[DeploymentNamespace]

		newKubeconfig, err := updateClientConfig(clientConfig, string(newToken), string(deploymentNamespace))
		if err != nil {
			return nil, err
		}

		if err := testClientConfig(newKubeconfig); err != nil {
			return nil, fmt.Errorf("new client config cannot list bundledeployments on management cluster: %w", err)
		}

		// fleet-agent secret
		agentSecret := &corev1.Secret{
			ObjectMeta: metav1.ObjectMeta{
				Name:      CredName,
				Namespace: secret.Namespace,
			},
			Data: map[string][]byte{
				Kubeconfig:          newKubeconfig,
				DeploymentNamespace: deploymentNamespace,
				ClusterNamespace:    clusterNamespace,
				ClusterName:         clusterName,
			},
		}

		secret, err := k8s.Secret().Create(agentSecret)
		if apierrors.IsAlreadyExists(err) {
			if err = k8s.Secret().Delete(agentSecret.Namespace, agentSecret.Name, &metav1.DeleteOptions{}); err != nil {
				return nil, err
			}
			secret, err = k8s.Secret().Create(agentSecret)
		}
		if err != nil {
			err = fmt.Errorf("failed to create 'fleet-agent' secret: %w", err)
		}
		return secret, err
	}
}

func values(data map[string][]byte) map[string][]byte {
	values := data[Values]
	if len(values) == 0 {
		return data
	}
	// never reached? FIXME maybe use config.KubeConfigValuesKey or config.ImportTokenSecretValuesKey

	newData := map[string]interface{}{}
	if err := yaml.Unmarshal(values, &newData); err != nil {
		return data
	}

	data = map[string][]byte{}
	for k, v := range newData {
		if s, ok := v.(string); ok {
			data[k] = []byte(s)
		}
	}
	return data
}

// createClientConfigFromSecret reads the fleet-agent-bootstrap secret and
// creates a clientConfig to access the upstream cluster
func createClientConfigFromSecret(ctx context.Context, secret *corev1.Secret, trustSystemStoreCAs bool) (clientcmd.ClientConfig, func()) {
	data := values(secret.Data)
	apiServerURL := string(data[config.APIServerURLKey])
	apiServerCA := data[config.APIServerCAKey]
	namespace := string(data[ClusterNamespace])
	token := string(data[Token])

	undoBypass := func() {}

	if trustSystemStoreCAs { // Save a request to the API server URL if system CAs are not to be trusted.
		// NOTE(manno): client-go will use the system trust store even if a CA is configured. So, why do this?
		req, err := http.NewRequestWithContext(ctx, http.MethodGet, apiServerURL, nil)
		if err == nil {
			if resp, err := http.DefaultClient.Do(req); err == nil {
				resp.Body.Close()
				apiServerCA = nil
			}
		}
	} else {
		undoBypass = config.BypassSystemCAStore()
	}

	cfg := clientcmdapi.Config{
		Clusters: map[string]*clientcmdapi.Cluster{
			"cluster": {
				Server:                   apiServerURL,
				CertificateAuthorityData: apiServerCA,
			},
		},
		AuthInfos: map[string]*clientcmdapi.AuthInfo{
			"user": {
				Token: token,
			},
		},
		Contexts: map[string]*clientcmdapi.Context{
			"default": {
				Cluster:   "cluster",
				AuthInfo:  "user",
				Namespace: namespace,
			},
		},
		CurrentContext: "default",
	}

	return clientcmd.NewDefaultClientConfig(cfg, &clientcmd.ConfigOverrides{}), undoBypass
}

func testClientConfig(cfg []byte) error {
	cc, err := clientcmd.NewClientConfigFromBytes(cfg)
	if err != nil {
		return err
	}

	ns, _, err := cc.Namespace()
	if err != nil {
		return err
	}

	rest, err := cc.ClientConfig()
	if err != nil {
		return err
	}

	fc, err := fleetcontrollers.NewFactoryFromConfig(rest)
	if err != nil {
		return err
	}

	_, err = fc.Fleet().V1alpha1().BundleDeployment().List(ns, metav1.ListOptions{})
	return err
}

func updateClientConfig(cc clientcmd.ClientConfig, token, ns string) ([]byte, error) {
	raw, err := cc.RawConfig()
	if err != nil {
		return nil, err
	}
	for _, v := range raw.AuthInfos {
		v.Token = token
	}
	for _, v := range raw.Contexts {
		v.Namespace = ns
	}

	return clientcmd.Write(raw)
}



================================================
FILE: internal/cmd/agent/register/register_test.go
================================================
package register

import (
	"context"
	"net/http"
	"testing"
	"time"

	corecontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"
	"github.com/rancher/wrangler/v3/pkg/generic/fake"
	"github.com/stretchr/testify/assert"
	"go.uber.org/mock/gomock"
	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

type mockCoreInterface struct {
	corecontrollers.ConfigMapController
	corecontrollers.NamespaceController
	corecontrollers.SecretController
}

func (c mockCoreInterface) ConfigMap() corecontrollers.ConfigMapController {
	return c.ConfigMapController
}
func (c mockCoreInterface) Namespace() corecontrollers.NamespaceController {
	return c.NamespaceController
}
func (c mockCoreInterface) Secret() corecontrollers.SecretController {
	return c.SecretController
}

// This is a smoke test, preventing regressions for https://github.com/rancher/rancher/issues/43012 where adding a label
// to cluster registration labels would panic, causing the Fleet agent to crash, if the map of labels was nil.
func TestRunRegistrationLabelSmokeTest(t *testing.T) {
	namespace := "my-namespace"
	secret := corev1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Namespace: namespace,
		},
		Data: map[string][]byte{
			"apiServerURL":     []byte("https://42.42.42.42:4242"),
			"clusterNamespace": []byte(namespace),
		},
	}

	ctrl := gomock.NewController(t)
	mockSecretController := fake.NewMockControllerInterface[*corev1.Secret, *corev1.SecretList](ctrl)
	mockSecretController.EXPECT().Get(namespace, "fleet-agent-bootstrap", metav1.GetOptions{}).Return(&secret, nil)

	mockConfigMapController := fake.NewMockControllerInterface[*corev1.ConfigMap, *corev1.ConfigMapList](ctrl)
	mockConfigMapController.EXPECT().Get(secret.Namespace, "fleet-agent", metav1.GetOptions{}).
		Return(nil, &apierrors.StatusError{ErrStatus: metav1.Status{Reason: metav1.StatusReasonNotFound}})

	mockNamespaceController := fake.NewMockNonNamespacedControllerInterface[*corev1.Namespace, *corev1.NamespaceList](ctrl)
	mockNamespaceController.EXPECT().Get("kube-system", metav1.GetOptions{}).Return(&corev1.Namespace{}, nil)

	mockCoreController := mockCoreInterface{mockConfigMapController, mockNamespaceController, mockSecretController}

	http.DefaultClient.Timeout = 100 * time.Millisecond // no need to wait longer, the API server URL is a dummy.
	agentSecret, err := runRegistration(context.Background(), mockCoreController, namespace)

	assert.Nil(t, agentSecret)
	// expecting an error at cluster registration creation time because the API server URL is a dummy.
	// this may be a timeout or a simple 'connection refused' error.
	assert.Regexp(t, "cannot create clusterregistration.*", err.Error())
}



================================================
FILE: internal/cmd/agent/trigger/watcher.go
================================================
// Package trigger watches a set of deployed resources and triggers a callback when one of them is deleted.
package trigger

import (
	"context"
	"sync"
	"time"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/objectset"
	"github.com/rancher/fleet/pkg/durations"

	"k8s.io/apimachinery/pkg/api/meta"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/apimachinery/pkg/watch"
	"k8s.io/client-go/dynamic"
)

type triggerFunc func(key string)

type Trigger struct {
	sync.RWMutex

	ctx        context.Context
	objectSets map[string]*objectset.ObjectSet
	watches    map[schema.GroupVersionKind]*watcher
	triggers   map[schema.GroupVersionKind]map[objectset.ObjectKey]map[string]triggerFunc
	restMapper meta.RESTMapper
	client     dynamic.Interface
}

func New(ctx context.Context, client dynamic.Interface, restMapper meta.RESTMapper) *Trigger {
	return &Trigger{
		ctx:        ctx,
		objectSets: map[string]*objectset.ObjectSet{},
		watches:    map[schema.GroupVersionKind]*watcher{},
		triggers:   map[schema.GroupVersionKind]map[objectset.ObjectKey]map[string]triggerFunc{},
		restMapper: restMapper,
		client:     client,
	}
}

func (t *Trigger) gvr(gvk schema.GroupVersionKind) (schema.GroupVersionResource, bool, error) {
	mapping, err := t.restMapper.RESTMapping(gvk.GroupKind(), gvk.Version)
	if err != nil {
		return schema.GroupVersionResource{}, false, err
	}
	return mapping.Resource, mapping.Scope.Name() == meta.RESTScopeNameNamespace, nil
}

func (t *Trigger) Clear(key string) error {
	return t.OnChange(key, "", nil)
}

func setNamespace(nsed bool, key objectset.ObjectKey, defaultNamespace string) objectset.ObjectKey {
	if nsed {
		if key.Namespace == "" {
			key.Namespace = defaultNamespace
		}
	} else {
		key.Namespace = ""
	}
	return key
}

func (t *Trigger) OnChange(key string, defaultNamespace string, trigger triggerFunc, objs ...runtime.Object) error {
	t.Lock()
	defer t.Unlock()

	os := objectset.NewObjectSet(objs...)
	oldOS := t.objectSets[key]
	gvkNSed := map[schema.GroupVersionKind]bool{}

	for gvk := range os.ObjectsByGVK() {
		gvr, nsed, err := t.gvr(gvk)
		if err != nil {
			return err
		}
		gvkNSed[gvk] = nsed
		t.watch(gvk, gvr)
	}

	for gvk, objs := range oldOS.ObjectsByGVK() {
		t.unwatch(gvk)
		for objectKey := range objs {
			objectKey = setNamespace(gvkNSed[gvk], objectKey, defaultNamespace)
			delete(t.triggers[gvk][objectKey], key)
		}
	}

	for gvk, objs := range os.ObjectsByGVK() {
		for objectKey := range objs {
			objectKey = setNamespace(gvkNSed[gvk], objectKey, defaultNamespace)
			objectKeys, ok := t.triggers[gvk]
			if !ok {
				objectKeys = map[objectset.ObjectKey]map[string]triggerFunc{}
				t.triggers[gvk] = objectKeys
			}
			funcs, ok := objectKeys[objectKey]
			if !ok {
				funcs = map[string]triggerFunc{}
				objectKeys[objectKey] = funcs
			}
			funcs[key] = trigger
		}
	}

	// prune
	for k, v := range t.triggers {
		for k, v2 := range v {
			if len(v2) == 0 {
				delete(v, k)
			}
		}
		if len(v) == 0 {
			delete(t.triggers, k)
		}
	}

	if len(objs) == 0 {
		delete(t.objectSets, key)
	} else {
		t.objectSets[key] = os
	}

	return nil
}

func (t *Trigger) call(gvk schema.GroupVersionKind, key objectset.ObjectKey) {
	t.RLock()
	defer t.RUnlock()

	for _, f := range t.triggers[gvk][key] {
		f(key.String())
	}
}

func (t *Trigger) watch(gvk schema.GroupVersionKind, gvr schema.GroupVersionResource) {
	gvkWatcher, ok := t.watches[gvk]
	if ok {
		gvkWatcher.count++
	} else {
		gvkWatcher = &watcher{
			client: t.client,
			t:      t,
			gvr:    gvr,
			gvk:    gvk,
			count:  1,
		}
		go gvkWatcher.Start(t.ctx)
		t.watches[gvk] = gvkWatcher
	}
}

func (t *Trigger) unwatch(gvk schema.GroupVersionKind) {
	gvkWatcher, ok := t.watches[gvk]
	if !ok {
		return
	}
	gvkWatcher.count--
	if gvkWatcher.count <= 0 {
		gvkWatcher.Stop()
		delete(t.watches, gvk)
	}
}

type watcher struct {
	sync.Mutex

	client  dynamic.Interface
	gvk     schema.GroupVersionKind
	gvr     schema.GroupVersionResource
	count   int
	stopped bool
	w       watch.Interface
	t       *Trigger
}

func (w *watcher) Start(ctx context.Context) {
	resourceVersion := ""
	for {
		w.Lock()
		if w.stopped {
			w.Unlock()
			break
		}
		w.Unlock()

		time.Sleep(durations.TriggerSleep)
		resp, err := w.client.Resource(w.gvr).Watch(ctx, metav1.ListOptions{
			AllowWatchBookmarks: true,
			ResourceVersion:     resourceVersion,
		})
		if err != nil {
			resourceVersion = ""
			continue
		}

		w.Lock()
		w.w = resp
		w.Unlock()

		for event := range resp.ResultChan() {
			meta, err := meta.Accessor(event.Object)
			var key objectset.ObjectKey
			if err == nil {
				resourceVersion = meta.GetResourceVersion()
				key.Name = meta.GetName()
				key.Namespace = meta.GetNamespace()
			}

			switch event.Type {
			case watch.Added:
				fallthrough
			case watch.Modified:
				fallthrough
			case watch.Deleted:
				w.t.call(w.gvk, key)
			}
		}
	}
}

func (w *watcher) Stop() {
	w.Lock()
	defer w.Unlock()
	w.stopped = true
	if w.w != nil {
		w.w.Stop()
	}
}



================================================
FILE: internal/cmd/cli/apply.go
================================================
package cli

import (
	"bytes"
	"fmt"
	"os"
	"os/exec"
	"strings"

	"github.com/sirupsen/logrus"
	"github.com/spf13/cobra"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"

	"github.com/rancher/fleet/internal/bundlereader"
	command "github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/cmd/cli/apply"
	"github.com/rancher/fleet/internal/cmd/cli/writer"
	ssh "github.com/rancher/fleet/internal/ssh"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/util/yaml"
	"k8s.io/client-go/kubernetes"
	typedv1core "k8s.io/client-go/kubernetes/typed/core/v1"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/record"
)

type readFile func(name string) ([]byte, error)

// NewApply returns a subcommand to create bundles from directories
func NewApply() *cobra.Command {
	return command.Command(&Apply{}, cobra.Command{
		Use:   "apply [flags] BUNDLE_NAME PATH...",
		Short: "Create bundles from directories, and output them or apply them on a cluster",
	})
}

type Apply struct {
	FleetClient
	BundleInputArgs
	OutputArgsNoDefault
	Label                        map[string]string `usage:"Labels to apply to created bundles" short:"l"`
	TargetsFile                  string            `usage:"Addition source of targets and restrictions to be append"`
	Compress                     bool              `usage:"Force all resources to be compress" short:"c"`
	ServiceAccount               string            `usage:"Service account to assign to bundle created" short:"a"`
	SyncGeneration               int               `usage:"Generation number used to force sync the deployment"`
	TargetNamespace              string            `usage:"Ensure this bundle goes to this target namespace"`
	Paused                       bool              `usage:"Create bundles in a paused state"`
	Commit                       string            `usage:"Commit to assign to the bundle" env:"COMMIT"`
	Username                     string            `usage:"Basic auth username for helm repo" env:"HELM_USERNAME"`
	PasswordFile                 string            `usage:"Path of file containing basic auth password for helm repo"`
	CACertsFile                  string            `usage:"Path of custom cacerts for helm repo" name:"cacerts-file"`
	SSHPrivateKeyFile            string            `usage:"Path of ssh-private-key for helm repo" name:"ssh-privatekey-file"`
	HelmRepoURLRegex             string            `usage:"Helm credentials will be used if the helm repo matches this regex. Credentials will always be used if this is empty or not provided" name:"helm-repo-url-regex"`
	KeepResources                bool              `usage:"Keep resources created after the GitRepo or Bundle is deleted" name:"keep-resources"`
	DeleteNamespace              bool              `usage:"Delete GitRepo target namespace after the GitRepo or Bundle is deleted" name:"delete-namespace"`
	HelmCredentialsByPathFile    string            `usage:"Path of file containing helm credentials for paths" name:"helm-credentials-by-path-file"`
	HelmBasicHTTP                bool              `usage:"Uses plain HTTP connections when downloading from helm repositories" name:"helm-basic-http"`
	HelmInsecureSkipTLS          bool              `usage:"Skip TLS verification when downloading from helm repositories" name:"helm-insecure-skip-tls"`
	CorrectDrift                 bool              `usage:"Rollback any change made from outside of Fleet" name:"correct-drift"`
	CorrectDriftForce            bool              `usage:"Use --force when correcting drift. Resources can be deleted and recreated" name:"correct-drift-force"`
	CorrectDriftKeepFailHistory  bool              `usage:"Keep helm history for failed rollbacks" name:"correct-drift-keep-fail-history"`
	OCIRegistrySecret            string            `usage:"OCI storage registry secret name" name:"oci-registry-secret"`
	DrivenScan                   bool              `usage:"Use driven scan. Bundles are defined by the user" name:"driven-scan"`
	DrivenScanSeparator          string            `usage:"Separator to use for bundle folder and options file" name:"driven-scan-sep" default:":"`
	BundleCreationMaxConcurrency int               `usage:"Maximum number of concurrent bundle creation routines" name:"bundle-creation-max-concurrency" default:"4" env:"FLEET_BUNDLE_CREATION_MAX_CONCURRENCY"`
}

func (r *Apply) PersistentPre(_ *cobra.Command, _ []string) error {
	if err := r.SetupDebug(); err != nil {
		return fmt.Errorf("failed to set up debug logging: %w", err)
	}

	return nil
}

func (a *Apply) Run(cmd *cobra.Command, args []string) error {
	// Apply retries on conflict errors.
	// We could have race conditions updating the Bundle in high load situations
	var err error
	retries, err := apply.GetOnConflictRetries()
	if err != nil {
		logrus.Errorf("failed parsing env variable %s, using defaults, err: %v", apply.FleetApplyConflictRetriesEnv, err)
	}
	for range retries {
		err = a.run(cmd, args)
		if !errors.IsConflict(err) {
			break
		}
	}

	return err
}

func (a *Apply) run(cmd *cobra.Command, args []string) error {
	labels := a.Label
	if a.Commit == "" {
		a.Commit = currentCommit()
	}
	if a.Commit != "" {
		if labels == nil {
			labels = map[string]string{}
		}
		labels[fleet.CommitLabel] = a.Commit
	}

	name := ""
	opts := apply.Options{
		Namespace:                    a.Namespace,
		BundleFile:                   a.BundleFile,
		Output:                       writer.NewDefaultNone(a.Output),
		Compress:                     a.Compress,
		ServiceAccount:               a.ServiceAccount,
		Labels:                       a.Label,
		TargetsFile:                  a.TargetsFile,
		TargetNamespace:              a.TargetNamespace,
		Paused:                       a.Paused,
		SyncGeneration:               int64(a.SyncGeneration),
		HelmRepoURLRegex:             a.HelmRepoURLRegex,
		KeepResources:                a.KeepResources,
		DeleteNamespace:              a.DeleteNamespace,
		CorrectDrift:                 a.CorrectDrift,
		CorrectDriftForce:            a.CorrectDriftForce,
		CorrectDriftKeepFailHistory:  a.CorrectDriftKeepFailHistory,
		DrivenScan:                   a.DrivenScan,
		DrivenScanSeparator:          a.DrivenScanSeparator,
		OCIRegistrySecret:            a.OCIRegistrySecret,
		BundleCreationMaxConcurrency: a.BundleCreationMaxConcurrency,
	}

	knownHostsPath, err := writeTmpKnownHosts()
	if err != nil {
		return err
	}

	defer os.RemoveAll(knownHostsPath)

	if err := a.addAuthToOpts(&opts, os.ReadFile, a.HelmBasicHTTP, a.HelmInsecureSkipTLS); err != nil {
		return fmt.Errorf("adding auth to opts: %w", err)
	}

	switch {
	case a.File == "-":
		opts.BundleReader = os.Stdin
		if len(args) != 1 {
			return fmt.Errorf("the bundle name is required as the first argument")
		}
		name = args[0]
	case a.File != "":
		f, err := os.Open(a.File)
		if err != nil {
			return err
		}
		defer f.Close()
		opts.BundleReader = f
		if len(args) != 1 {
			return fmt.Errorf("the bundle name is required as the first argument")
		}
		name = args[0]
	case len(args) < 1:
		return fmt.Errorf("at least one arguments is required BUNDLE_NAME")
	default:
		name = args[0]
		args = args[1:]
	}

	restoreEnv, err := setEnv(knownHostsPath)
	if err != nil {
		return fmt.Errorf("setting git SSH command env var for known hosts: %w", err)
	}

	defer restoreEnv() //nolint: errcheck // best-effort

	ctx := cmd.Context()
	cfg := ctrl.GetConfigOrDie()
	client, err := client.New(cfg, client.Options{Scheme: scheme})
	if err != nil {
		return err
	}
	recorder, err := getEventRecorder(cfg, "fleet-apply")
	if err != nil {
		return err
	}

	if opts.DrivenScan {
		return apply.CreateBundlesDriven(ctx, client, recorder, name, args, opts)
	}
	return apply.CreateBundles(ctx, client, recorder, name, args, opts)
}

// addAuthToOpts adds auth if provided as arguments. It will look first for HelmCredentialsByPathFile. If HelmCredentialsByPathFile
// is not provided it means that the same helm secret should be used for all helm repositories, then it will look for
// Username, PasswordFile, CACertsFile and SSHPrivateKeyFile.
// It will also set the values for using basic HTTP connections and skipping TLS.
func (a *Apply) addAuthToOpts(opts *apply.Options, readFile readFile, helmBasicHTTP, helmInsecureSkipTLS bool) error {
	if a.HelmCredentialsByPathFile != "" {
		file, err := readFile(a.HelmCredentialsByPathFile)
		if err != nil && !os.IsNotExist(err) {
			return err
		}
		var authByPath map[string]bundlereader.Auth
		err = yaml.NewYAMLToJSONDecoder(bytes.NewBuffer(file)).Decode(&authByPath)
		if err != nil {
			return err
		}
		opts.AuthByPath = authByPath

		return nil
	}

	if a.Username != "" && a.PasswordFile != "" {
		password, err := readFile(a.PasswordFile)
		if err != nil && !os.IsNotExist(err) {
			return err
		}

		opts.Auth.Username = a.Username
		opts.Auth.Password = string(password)
	}
	if a.CACertsFile != "" {
		cabundle, err := readFile(a.CACertsFile)
		if err != nil && !os.IsNotExist(err) {
			return err
		}
		opts.Auth.CABundle = cabundle
	}
	if a.SSHPrivateKeyFile != "" {
		privateKey, err := readFile(a.SSHPrivateKeyFile)
		if err != nil && !os.IsNotExist(err) {
			return err
		}
		opts.Auth.SSHPrivateKey = privateKey
	}

	opts.Auth.BasicHTTP = helmBasicHTTP
	opts.Auth.InsecureSkipVerify = helmInsecureSkipTLS

	return nil
}

func currentCommit() string {
	cmd := exec.Command("git", "rev-parse", "HEAD") //nolint:noctx // TODO: refactor to use go-git's ResolveRevision
	buf := &bytes.Buffer{}
	cmd.Stdout = buf
	err := cmd.Run()
	if err == nil {
		return strings.TrimSpace(buf.String())
	}
	return ""
}

// writeTmpKnownHosts creates a temporary file and writes known_hosts data to it, if such data is available from
// environment variable `FLEET_KNOWN_HOSTS`.
// It returns the name of the file and any error which may have happened while creating the file or writing to it.
func writeTmpKnownHosts() (string, error) {
	knownHosts, isSet := os.LookupEnv(ssh.KnownHostsEnvVar)
	if !isSet || knownHosts == "" {
		return "", nil
	}

	f, err := os.CreateTemp("", "known_hosts")
	if err != nil {
		return "", err
	}

	knownHostsPath := f.Name()

	if err := os.WriteFile(knownHostsPath, []byte(knownHosts), 0600); err != nil {
		return "", fmt.Errorf(
			"failed to write value of %q env var to known_hosts file %s: %w",
			ssh.KnownHostsEnvVar,
			knownHostsPath,
			err,
		)
	}

	return knownHostsPath, nil
}

// setEnv sets the `GIT_SSH_COMMAND` environment variable with a known_hosts flag pointing to the provided
// knownHostsPath. It takes care of preserving existing flags in the existing value of the environment variable, if any,
// except for other user known_hosts file flags.
// It returns a function to restore the environment variable to its initial value, and any error that might have
// occurred in the process.
func setEnv(knownHostsPath string) (func() error, error) {
	commandEnvVar := "GIT_SSH_COMMAND"
	flagName := "UserKnownHostsFile"

	initialCommand, isSet := os.LookupEnv(commandEnvVar)

	fail := func(err error) (func() error, error) {
		return func() error { return nil }, err
	}

	if !isSet {
		if err := os.Setenv(commandEnvVar, fmt.Sprintf("ssh -o %s=%s", flagName, knownHostsPath)); err != nil {
			return fail(err)
		}

		return func() error { return os.Unsetenv(commandEnvVar) }, nil
	}

	// Check if `UserKnownHostsFile` is already present (case-insensitive), even multiple times, and skip it if so.
	var newSSHCommand strings.Builder
	options := strings.Split(initialCommand, " -o ")
	for _, opt := range options {
		kv := strings.Split(opt, "=")
		if len(kv) != 2 { // first element, pre `-o`, or other flag
			if _, err := newSSHCommand.WriteString(opt); err != nil {
				return fail(err)
			}

			continue
		}

		if strings.EqualFold(kv[0], flagName) { // case-insensitive comparison
			continue
		}

		if _, err := newSSHCommand.WriteString(fmt.Sprintf(" -o %s", opt)); err != nil {
			return fail(err)
		}
	}

	if _, err := newSSHCommand.WriteString(fmt.Sprintf(" -o %s=%s", flagName, knownHostsPath)); err != nil {
		return fail(err)
	}

	if err := os.Setenv(commandEnvVar, newSSHCommand.String()); err != nil {
		return fail(err)
	}

	restore := func() error {
		return os.Setenv(commandEnvVar, initialCommand)
	}

	return restore, nil
}

func getEventRecorder(config *rest.Config, componentName string) (record.EventRecorder, error) {
	clientset, err := kubernetes.NewForConfig(config)
	if err != nil {
		return nil, err
	}

	broadcaster := record.NewBroadcaster()
	broadcaster.StartStructuredLogging(0)
	broadcaster.StartRecordingToSink(&typedv1core.EventSinkImpl{
		Interface: clientset.CoreV1().Events(""),
	})

	return broadcaster.NewRecorder(scheme, corev1.EventSource{Component: componentName}), nil
}



================================================
FILE: internal/cmd/cli/apply_test.go
================================================
package cli

import (
	"errors"
	"os"
	"testing"

	"github.com/google/go-cmp/cmp"
	"sigs.k8s.io/yaml"

	"github.com/rancher/fleet/internal/bundlereader"
	"github.com/rancher/fleet/internal/cmd/cli/apply"
)

const (
	username = "user"

	password_file    = "password_file"
	password_content = "pass"

	caCerts_file    = "caCerts_file"
	caCerts_content = "caCerts"

	sshPrivateKey_file    = "sshPrivateKey_file"
	sshPrivateKey_content = "sshPrivateKey"

	helmSecretsNameByPath_file = "helmSecretsNameByPath_file"
)

var helmSecretsNameByPath_content = map[string]bundlereader.Auth{"path": {Username: username, Password: password_content}}

func TestSetEnv(t *testing.T) {
	tests := map[string]struct {
		envValue              string
		knownHostsPath        string
		expectedGitSSHCommand string
		expectedErr           error
	}{
		"unset env var": {
			knownHostsPath:        "/foo/bar",
			expectedGitSSHCommand: "ssh -o UserKnownHostsFile=/foo/bar",
		},
		"set env var without options": {
			envValue:              "ssh",
			knownHostsPath:        "/foo/bar",
			expectedGitSSHCommand: "ssh -o UserKnownHostsFile=/foo/bar",
		},
		"set env var with other options": {
			envValue:              "ssh -o stricthostkeychecking=yes",
			knownHostsPath:        "/foo/bar",
			expectedGitSSHCommand: "ssh -o stricthostkeychecking=yes -o UserKnownHostsFile=/foo/bar",
		},
		"set env var with other options and known hosts file option": {
			envValue:              "ssh -o stricthostkeychecking=yes -o userknownhostsFile=/another/file",
			knownHostsPath:        "/foo/bar",
			expectedGitSSHCommand: "ssh -o stricthostkeychecking=yes -o UserKnownHostsFile=/foo/bar",
		},
		"set env var with other options and known hosts file option specified multiple times": {
			envValue:              "ssh -o userknownhostsFile=/another/file -o UserKnownHostsFile=/yet/another/file -o stricthostkeychecking=yes",
			knownHostsPath:        "/foo/bar",
			expectedGitSSHCommand: "ssh -o stricthostkeychecking=yes -o UserKnownHostsFile=/foo/bar",
		},
	}

	bkpEnv := os.Getenv("GIT_SSH_COMMAND")
	defer os.Setenv("GIT_SSH_COMMAND", bkpEnv)

	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			defer os.Unsetenv("GIT_SSH_COMMAND")

			if test.envValue != "" {
				os.Setenv("GIT_SSH_COMMAND", test.envValue)
			} else {
				os.Unsetenv("GIT_SSH_COMMAND")
			}

			restore, err := setEnv(test.knownHostsPath)
			if !errors.Is(err, test.expectedErr) {
				t.Errorf("expected err %v, got %v", test.expectedErr, err)
			}

			if gitSSHCommand := os.Getenv("GIT_SSH_COMMAND"); gitSSHCommand != test.expectedGitSSHCommand {
				t.Errorf("expected GIT_SSH_COMMAND %q, got %q", test.expectedGitSSHCommand, gitSSHCommand)
			}

			if restoreErr := restore(); restoreErr != nil {
				t.Errorf("expected nil restore error, got %v", restoreErr)
			}

			restoredEnvValue, isSet := os.LookupEnv("GIT_SSH_COMMAND")
			if restoredEnvValue != test.envValue {
				t.Errorf(
					"expected restored GIT_SSH_COMMAND value to be %q, got %t/%q",
					test.envValue,
					isSet,
					restoredEnvValue,
				)
			}
		})
	}
}

func TestWriteTmpKnownHosts(t *testing.T) {
	tests := map[string]struct {
		knownHosts       string
		isSet            bool
		expectFileExists bool
	}{
		"does not write to known hosts file if FLEET_KNOWN_HOSTS is unset": {},
		"does not write to known hosts file if FLEET_KNOWN_HOSTS is empty": {isSet: true},
		"writes FLEET_KNOWN_HOSTS to custom known hosts file if set": {
			knownHosts:       "foo",
			isSet:            true,
			expectFileExists: true,
		},
	}

	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			if test.isSet {
				if err := os.Setenv("FLEET_KNOWN_HOSTS", test.knownHosts); err != nil {
					t.Errorf("failed to set FLEET_KNOWN_HOSTS env var: %v", err)
				}

				defer os.Unsetenv("FLEET_KNOWN_HOSTS")
			}

			khPath, err := writeTmpKnownHosts()
			if err != nil {
				t.Errorf("expected nil error from writeTmpKnownHosts, got: %v", err)
			}

			if !test.expectFileExists {
				return
			}

			gotKnownHosts, err := os.ReadFile(khPath)
			if err != nil {
				t.Errorf("failed to read known_hosts file: %v", err)
			}

			defer os.RemoveAll(khPath)

			if test.knownHosts != "" {
				if string(gotKnownHosts) != test.knownHosts {
					t.Errorf("known_hosts mismatch: expected\n\t%s\ngot:\n\t%s", test.knownHosts, gotKnownHosts)
				}
			}
		})
	}
}

func TestAddAuthToOpts(t *testing.T) {
	tests := map[string]struct {
		name         string
		apply        Apply
		knownHosts   string
		expectedOpts *apply.Options
		expectedErr  error
	}{
		"Auth is empty if no arguments are provided": {
			apply:        Apply{},
			expectedOpts: &apply.Options{},
			expectedErr:  nil,
		},
		"known_hosts file is populated if the env var is set": {
			apply:        Apply{},
			expectedOpts: &apply.Options{},
			expectedErr:  nil,
			knownHosts:   "foo",
		},
		"Auth contains values from username, password, caCerts and sshPrivatey when helmSecretsNameByPath not provided": {
			apply:        Apply{PasswordFile: password_file, Username: username, CACertsFile: caCerts_file, SSHPrivateKeyFile: sshPrivateKey_file},
			expectedOpts: &apply.Options{Auth: bundlereader.Auth{Username: username, Password: password_content, CABundle: []byte(caCerts_content), SSHPrivateKey: []byte(sshPrivateKey_content)}},
			expectedErr:  nil,
		},
		"AuthByPath contains values from HelmCredentialsByPathFile if provided": {
			apply:        Apply{HelmCredentialsByPathFile: helmSecretsNameByPath_file},
			expectedOpts: &apply.Options{AuthByPath: helmSecretsNameByPath_content},
			expectedErr:  nil,
		},
		"HelmCredentialsByPathFile has priority over username and password for a generic helm secret if both are provided": {
			apply:        Apply{HelmCredentialsByPathFile: helmSecretsNameByPath_file, PasswordFile: password_file, Username: username, CACertsFile: caCerts_file, SSHPrivateKeyFile: sshPrivateKey_file},
			expectedOpts: &apply.Options{AuthByPath: helmSecretsNameByPath_content},
			expectedErr:  nil,
		},
		"Error if file doesn't exist": {
			apply:        Apply{HelmCredentialsByPathFile: "notfound"},
			expectedOpts: &apply.Options{},
			expectedErr:  errorNotFound,
		},
	}

	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			opts := &apply.Options{}
			err := test.apply.addAuthToOpts(opts, mockReadFile, false, false)
			if !cmp.Equal(opts, test.expectedOpts) {
				t.Errorf("opts don't match: expected %v, got %v", test.expectedOpts, opts)
			}
			if !errors.Is(err, test.expectedErr) {
				t.Errorf("errors don't match: expected %v, got %v", test.expectedErr, err)
			}
		})
	}
}

var errorNotFound = errors.New("not found")

func mockReadFile(name string) ([]byte, error) {
	switch name {
	case helmSecretsNameByPath_file:
		b, err := yaml.Marshal(helmSecretsNameByPath_content)
		if err != nil {
			return nil, err
		}
		return b, nil
	case password_file:
		return []byte(password_content), nil
	case caCerts_file:
		return []byte(caCerts_content), nil
	case sshPrivateKey_file:
		return []byte(sshPrivateKey_content), nil
	}

	return nil, errorNotFound
}



================================================
FILE: internal/cmd/cli/cleanup.go
================================================
package cli

import (
	"errors"
	"fmt"
	"math"
	"strconv"
	"time"

	"github.com/spf13/cobra"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"

	command "github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/cmd/cli/cleanup"
)

// NewCleanup returns a subcommand to `cleanup` cluster registrations
func NewCleanUp() *cobra.Command {
	cleanup := command.Command(&Cleanup{}, cobra.Command{
		Short:         "Clean up outdated resources",
		SilenceUsage:  true,
		SilenceErrors: true,
	})
	cleanup.AddCommand(
		NewClusterRegistration(),
		NewGitjob(),
	)
	return cleanup
}

func NewClusterRegistration() *cobra.Command {
	return command.Command(&ClusterRegistration{}, cobra.Command{
		Use:           "clusterregistration [flags]",
		Short:         "Clean up outdated cluster registrations",
		SilenceUsage:  true,
		SilenceErrors: true,
	})
}

func NewGitjob() *cobra.Command {
	return command.Command(&Gitjob{}, cobra.Command{
		Use:           "gitjob [flags]",
		Short:         "Clean up outdated git jobs",
		SilenceUsage:  true,
		SilenceErrors: true,
	})
}

type Cleanup struct {
}

func (c *Cleanup) Run(cmd *cobra.Command, args []string) error {
	return cmd.Help()
}

type ClusterRegistration struct {
	FleetClient
	Min    string `usage:"Minimum delay between deletes (default: 10ms)" name:"min"`
	Max    string `usage:"Maximum delay between deletes (default: 5s)" name:"max"`
	Factor string `usage:"Factor to increase delay between deletes (default: 1.1)" name:"factor"`
}

func (r *ClusterRegistration) PersistentPre(_ *cobra.Command, _ []string) error {
	if err := r.SetupDebug(); err != nil {
		return fmt.Errorf("failed to set up debug logging: %w", err)
	}
	return nil
}

func (a *ClusterRegistration) Run(cmd *cobra.Command, args []string) error {
	var err error

	min := 10 * time.Millisecond
	if a.Min != "" {
		min, err = time.ParseDuration(a.Min)
		if err != nil {
			return err
		}
		if min <= 0 {
			return errors.New("min cannot be zero or less")
		}
	}

	max := 3 * time.Second
	if a.Max != "" {
		max, err = time.ParseDuration(a.Max)
		if err != nil {
			return err
		}
		if max <= 0 {
			return errors.New("max cannot be zero or less")
		}
	}

	if max < min {
		return errors.New("max cannot be less than min")
	}

	factor := 1.05
	if a.Factor != "" {
		factor, err = strconv.ParseFloat(a.Factor, 64)
		if err != nil {
			return err
		}
		if factor <= 1 || math.IsInf(factor, 0) || math.IsNaN(factor) {
			return errors.New("factor must be greater than 1 and finite")
		}
	}

	opts := cleanup.Options{
		Min:    min,
		Max:    max,
		Factor: factor,
	}

	ctrl.SetLogger(zap.New(zap.UseFlagOptions(&zopts)))
	ctx := log.IntoContext(cmd.Context(), ctrl.Log)

	cfg := ctrl.GetConfigOrDie()
	client, err := client.New(cfg, client.Options{Scheme: scheme})
	if err != nil {
		return err
	}

	fmt.Printf("Cleaning up outdated cluster registrations: %#v\n", opts)

	return cleanup.ClusterRegistrations(ctx, client, opts)
}

type Gitjob struct {
	FleetClient
	BatchSize int `usage:"Number of git jobs to retrieve at once" name:"batch-size" default:"5000"`
}

func (r *Gitjob) PersistentPre(_ *cobra.Command, _ []string) error {
	if err := r.SetupDebug(); err != nil {
		return fmt.Errorf("failed to set up debug logging: %w", err)
	}
	return nil
}

func (r *Gitjob) Run(cmd *cobra.Command, args []string) error {
	bs := r.BatchSize
	if bs <= 1 {
		return errors.New("factor must be greater than 1")
	}

	ctrl.SetLogger(zap.New(zap.UseFlagOptions(&zopts)))
	ctx := log.IntoContext(cmd.Context(), ctrl.Log)

	cfg := ctrl.GetConfigOrDie()
	client, err := client.New(cfg, client.Options{Scheme: scheme})
	if err != nil {
		return err
	}

	fmt.Printf("Cleaning up outdated git jobs, batch size at %d\n", bs)

	return cleanup.GitJobs(ctx, client, bs)
}



================================================
FILE: internal/cmd/cli/deploy.go
================================================
package cli

import (
	"bytes"
	"flag"
	"fmt"
	"os"
	"reflect"

	"github.com/spf13/cobra"
	"helm.sh/helm/v4/pkg/cli"
	releasev1 "helm.sh/helm/v4/pkg/release/v1"
	"k8s.io/apimachinery/pkg/runtime"

	command "github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/content"
	"github.com/rancher/fleet/internal/helmdeployer"
	"github.com/rancher/fleet/internal/manifest"
	"github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	wyaml "github.com/rancher/wrangler/v3/pkg/yaml"

	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"
	"sigs.k8s.io/yaml"
)

const defaultNamespace = "default"

// NewDeploy returns a subcommand to deploy a bundledeployment/content resource to a cluster.
func NewDeploy() *cobra.Command {
	cmd := command.Command(&Deploy{}, cobra.Command{
		Short: "Deploy a bundledeployment/content resource to a cluster, by creating a Helm release. This will not deploy the bundledeployment/content resources directly to the cluster.",
	})
	cmd.SetOut(os.Stdout)

	// add command line flags from zap and controller-runtime, which use
	// goflags and convert them to pflags
	fs := flag.NewFlagSet("", flag.ExitOnError)
	zopts.BindFlags(fs)
	ctrl.RegisterFlags(fs)
	cmd.Flags().AddGoFlagSet(fs)
	return cmd
}

type Deploy struct {
	InputFile   string `usage:"Location of the YAML file containing the content and the bundledeployment resource" short:"i"`
	DryRun      bool   `usage:"Print the resources that would be deployed, but do not actually deploy them" short:"d"`
	Namespace   string `usage:"Set the default namespace. Deploy helm chart into this namespace." short:"n"`
	KubeVersion string `usage:"For dry runs, sets the Kubernetes version to assume when validating Chart Kubernetes version constraints."`

	// AgentNamespace is set as an annotation on the chart.yaml in the helm release. Fleet-agent will manage charts with a matching label.
	AgentNamespace string `usage:"Set the agent namespace, normally cattle-fleet-system. If set, fleet agent will garbage collect the helm release, i.e. delete it if the bundledeployment is missing." short:"a"`
}

func (d *Deploy) Run(cmd *cobra.Command, args []string) error {
	ctrl.SetLogger(zap.New(zap.UseFlagOptions(&zopts)))
	ctx := log.IntoContext(cmd.Context(), ctrl.Log)

	if d.InputFile == "" {
		return cmd.Help()
	}

	b, err := os.ReadFile(d.InputFile)
	if err != nil {
		return err
	}

	c := &v1alpha1.Content{}
	bd := &v1alpha1.BundleDeployment{}
	objs, err := wyaml.ToObjects(bytes.NewBuffer(b))
	if err != nil {
		return err
	}

	// position of the content and bundledeployment resources in the file is not guaranteed
	for _, obj := range objs {
		switch obj.GetObjectKind().GroupVersionKind().Kind {
		case "Content":
			un, err := runtime.DefaultUnstructuredConverter.ToUnstructured(obj)
			if err != nil {
				return err
			}
			err = runtime.DefaultUnstructuredConverter.FromUnstructured(un, c)
			if err != nil {
				return err
			}
		case "BundleDeployment":
			un, err := runtime.DefaultUnstructuredConverter.ToUnstructured(obj)
			if err != nil {
				return err
			}
			err = runtime.DefaultUnstructuredConverter.FromUnstructured(un, bd)
			if err != nil {
				return err
			}
		}
	}

	emptyContent := &v1alpha1.Content{}
	if reflect.DeepEqual(c, emptyContent) {
		return fmt.Errorf("failed to read content resource from file")
	}

	emptyBD := &v1alpha1.BundleDeployment{}
	if reflect.DeepEqual(bd, emptyBD) {
		return fmt.Errorf("failed to read bundledeployment resource from file")
	}

	data, err := content.GUnzip(c.Content)
	if err != nil {
		return err
	}
	manifest, err := manifest.FromJSON(data, c.SHA256Sum)
	if err != nil {
		return err
	}

	if d.DryRun {
		rel, err := helmdeployer.Template(ctx, bd.Name, manifest, bd.Spec.Options, d.KubeVersion)
		if err != nil {
			return err
		}

		return printRelease(cmd, rel)
	}

	cfg := ctrl.GetConfigOrDie()
	client, err := client.New(cfg, client.Options{Scheme: scheme})
	if err != nil {
		return err
	}

	namespace := defaultNamespace
	if d.Namespace != "" {
		namespace = d.Namespace
	}

	deployer := helmdeployer.New(
		d.AgentNamespace,
		namespace,
		defaultNamespace,
		d.AgentNamespace,
	)

	if kubeconfig := flag.Lookup("kubeconfig").Value.String(); kubeconfig != "" {
		// set KUBECONFIG env var so helm can find it
		os.Setenv("KUBECONFIG", kubeconfig)
	}

	// Note: deployer does not check the bundles dependencies
	err = deployer.Setup(ctx, client, cli.New().RESTClientGetter())
	if err != nil {
		return err
	}

	rel, err := deployer.Deploy(ctx, bd.Name, manifest, bd.Spec.Options)
	if err != nil {
		return err
	}

	return printRelease(cmd, rel)
}

func printRelease(cmd *cobra.Command, rel *releasev1.Release) error {
	resources, err := wyaml.ToObjects(bytes.NewBufferString(rel.Manifest))
	if err != nil {
		return err
	}

	for _, h := range rel.Hooks {
		hookResources, err := wyaml.ToObjects(bytes.NewBufferString(h.Manifest))
		if err != nil {
			return err
		}
		resources = append(resources, hookResources...)
	}

	b, err := yaml.Marshal(resources)
	if err != nil {
		return err
	}
	cmd.Println(string(b))

	return nil
}



================================================
FILE: internal/cmd/cli/root.go
================================================
// Package cli sets up the CLI commands for the fleet apply binary.
package cli

import (
	"github.com/spf13/cobra"

	command "github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/cmd/cli/gitcloner"
	"github.com/rancher/fleet/pkg/version"
)

const (
	JSONOutputEnvVar = "FLEET_JSON_OUTPUT"
	JobNameEnvVar    = "JOB_NAME"
)

func App() *cobra.Command {
	root := command.Command(&Fleet{}, cobra.Command{
		Version:       version.FriendlyVersion(),
		SilenceUsage:  true,
		SilenceErrors: true,
	})

	root.AddCommand(
		NewApply(),
		NewTest(),
		NewCleanUp(),

		NewTarget(),
		NewDeploy(),
		gitcloner.NewCmd(gitcloner.New()),
	)

	return root
}

type Fleet struct {
}

func (r *Fleet) Run(cmd *cobra.Command, _ []string) error {
	return cmd.Help()
}

type FleetClient struct {
	command.DebugConfig
	Namespace  string `usage:"namespace" env:"NAMESPACE" default:"fleet-local" short:"n"`
	Kubeconfig string `usage:"kubeconfig for authentication" short:"k"`
	Context    string `usage:"kubeconfig context for authentication"`
}

type BundleInputArgs struct {
	File       string `usage:"Location of the fleet.yaml" short:"f"`
	BundleFile string `usage:"Location of the raw Bundle resource yaml" short:"b"`
}

type OutputArgsNoDefault struct {
	Output string `usage:"Output contents to file or - for stdout"  short:"o"`
}



================================================
FILE: internal/cmd/cli/target.go
================================================
package cli

import (
	"flag"
	"fmt"
	"os"
	"reflect"

	"github.com/spf13/cobra"

	command "github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/cmd/controller/target"
	"github.com/rancher/fleet/internal/content"
	"github.com/rancher/fleet/internal/manifest"
	"github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	batchv1 "k8s.io/api/batch/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	clientgoscheme "k8s.io/client-go/kubernetes/scheme"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"
	"sigs.k8s.io/yaml"
)

var (
	zopts  = zap.Options{Development: true}
	scheme = runtime.NewScheme()
)

func init() {
	utilruntime.Must(clientgoscheme.AddToScheme(scheme))
	utilruntime.Must(v1alpha1.AddToScheme(scheme))
	utilruntime.Must(batchv1.AddToScheme(scheme))
	//+kubebuilder:scaffold:scheme
}

// NewTarget returns a subcommand to print available targets for a bundle
func NewTarget() *cobra.Command {
	cmd := command.Command(&Target{}, cobra.Command{
		Short: "Print available targets for a bundle",
	})
	cmd.SetOut(os.Stdout)

	// add command line flags from zap and controller-runtime, which use
	// goflags and convert them to pflags
	fs := flag.NewFlagSet("", flag.ExitOnError)
	zopts.BindFlags(fs)
	ctrl.RegisterFlags(fs)
	cmd.Flags().AddGoFlagSet(fs)
	return cmd
}

type Target struct {
	BundleFile    string `usage:"Location of the Bundle resource yaml" short:"b"`
	DumpInputList bool   `usage:"Dump the live resources, which impact targeting, like clusters, as YAML" short:"l"`

	Namespace string `usage:"Override the namespace of the bundle. Targeting searches this namespace for clusters." short:"n"`
}

func (t *Target) Run(cmd *cobra.Command, args []string) error {
	ctrl.SetLogger(zap.New(zap.UseFlagOptions(&zopts)))
	ctx := log.IntoContext(cmd.Context(), ctrl.Log)

	if t.BundleFile == "" {
		return cmd.Help()
	}

	b, err := os.ReadFile(t.BundleFile)
	if err != nil {
		return err
	}
	bundle := &v1alpha1.Bundle{}
	err = yaml.Unmarshal(b, bundle)
	if err != nil {
		return err
	}

	empty := &v1alpha1.Bundle{TypeMeta: metav1.TypeMeta{APIVersion: "v1"}}
	if reflect.DeepEqual(bundle, empty) {
		return fmt.Errorf("failed to read bundle from file, bundle is empty")
	}

	if t.Namespace != "" {
		bundle.Namespace = t.Namespace
	}

	manifest := manifest.FromBundle(bundle)
	manifestID, err := manifest.ID()
	if err != nil {
		return err
	}

	cfg := ctrl.GetConfigOrDie()
	client, err := client.New(cfg, client.Options{Scheme: scheme})
	if err != nil {
		return err
	}

	builder := target.New(client, client)
	matchedTargets, err := builder.Targets(ctx, bundle, manifestID)
	if err != nil {
		return err
	}

	if t.DumpInputList {
		// remove managed fields
		for _, target := range matchedTargets {
			target.Cluster.SetManagedFields(nil)
			for _, cg := range target.ClusterGroups {
				cg.SetManagedFields(nil)
			}
		}
		b, err := yaml.Marshal(matchedTargets)
		if err != nil {
			return err
		}
		cmd.PrintErrln(string(b))
	}

	// output manifest/content resource
	data, err := manifest.Content()
	if err != nil {
		return err
	}
	digest, err := manifest.SHASum()
	if err != nil {
		return err
	}

	compressed, err := content.Gzip(data)
	if err != nil {
		return err
	}

	content := v1alpha1.Content{
		ObjectMeta: metav1.ObjectMeta{
			Name: manifestID,
		},
		Content:   compressed,
		SHA256Sum: digest,
	}
	content.SetGroupVersionKind(v1alpha1.SchemeGroupVersion.WithKind("Content"))

	b, err = yaml.Marshal(content)
	if err != nil {
		return err
	}
	cmd.Println("---")
	cmd.Println(string(b))

	// Needs to be set to print all targets. UpdatePartitions will only
	// create this many deployments if the bundle is new.
	bundle.Status.MaxNew = len(matchedTargets)

	if err := target.UpdatePartitions(&bundle.Status, matchedTargets); err != nil {
		return err
	}
	for _, target := range matchedTargets {
		if target.Deployment == nil {
			continue
		}
		bd := target.BundleDeployment()
		bd.SetGroupVersionKind(v1alpha1.SchemeGroupVersion.WithKind("BundleDeployment"))

		b, err := yaml.Marshal(bd)
		if err != nil {
			return err
		}

		cmd.Println("---")
		cmd.Println(string(b))
	}

	return nil
}



================================================
FILE: internal/cmd/cli/test.go
================================================
package cli

import (
	"os"

	"github.com/spf13/cobra"

	command "github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/cmd/cli/match"
)

// NewTest returns a subcommand to test bundle matching, deprecated
func NewTest() *cobra.Command {
	return command.Command(&Test{}, cobra.Command{
		Args:       cobra.MaximumNArgs(1),
		Deprecated: "use target and deploy sub-commands instead.",
		Short:      "Match a bundle to a target and render the output (deprecated)",
	})
}

type Test struct {
	BundleInputArgs
	Quiet      bool              `usage:"Just print the match and don't print the resources" short:"q"`
	Group      string            `usage:"Cluster group to match against" short:"g"`
	Name       string            `usage:"Cluster name to match against" short:"N"`
	Label      map[string]string `usage:"Cluster labels to match against" short:"l"`
	GroupLabel map[string]string `usage:"Cluster group labels to match against" short:"L"`
	Target     string            `usage:"Explicit target to match" short:"t"`
}

func (m *Test) Run(cmd *cobra.Command, args []string) error {
	baseDir := "."
	if len(args) > 0 {
		baseDir = args[0]
	}

	opts := &match.Options{
		Output:             os.Stdout,
		BaseDir:            baseDir,
		BundleSpec:         m.File,
		BundleFile:         m.BundleFile,
		ClusterName:        m.Name,
		ClusterGroup:       m.Group,
		ClusterLabels:      m.Label,
		ClusterGroupLabels: m.GroupLabel,
		Target:             m.Target,
	}

	if m.Quiet {
		opts.Output = nil
	}

	if opts.ClusterGroup == "" &&
		len(opts.ClusterLabels) == 0 &&
		len(opts.ClusterGroupLabels) == 0 &&
		opts.Target == "" {
		opts.ClusterGroup = "default"
	}

	return match.Match(cmd.Context(), opts)
}



================================================
FILE: internal/cmd/cli/apply/apply.go
================================================
// Package apply creates bundle resources from gitrepo resources.
package apply

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"io/fs"
	"os"
	"path/filepath"
	"regexp"
	"slices"
	"strconv"
	"strings"

	"github.com/rancher/fleet/internal/bundlereader"
	"github.com/rancher/fleet/internal/content"
	"github.com/rancher/fleet/internal/fleetyaml"
	"github.com/rancher/fleet/internal/helmvalues"
	"github.com/rancher/fleet/internal/manifest"
	"github.com/rancher/fleet/internal/names"
	"github.com/rancher/fleet/internal/ocistorage"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	fleetevent "github.com/rancher/fleet/pkg/event"

	"github.com/rancher/wrangler/v3/pkg/yaml"
	"github.com/sirupsen/logrus"
	"golang.org/x/sync/errgroup"
	k8syaml "sigs.k8s.io/yaml"

	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/client-go/tools/record"
	"k8s.io/utils/ptr"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
)

var (
	ErrNoResources = errors.New("no resources found to deploy")
)

const (
	JSONOutputEnvVar                    = "FLEET_JSON_OUTPUT"
	JobNameEnvVar                       = "JOB_NAME"
	FleetApplyConflictRetriesEnv        = "FLEET_APPLY_CONFLICT_RETRIES"
	BundleCreationMaxConcurrencyEnv     = "FLEET_BUNDLE_CREATION_MAX_CONCURRENCY"
	defaultApplyConflictRetries         = 1
	defaultBundleCreationMaxConcurrency = 4
)

type Getter interface {
	Get() (*client.Client, error)
	GetNamespace() string
}

type OCIRegistrySpec struct {
	Reference       string
	Username        string
	Password        string
	BasicHTTP       bool
	InsecureSkipTLS bool
}

type Options struct {
	Namespace                    string
	BundleFile                   string
	TargetsFile                  string
	Compress                     bool
	BundleReader                 io.Reader
	Output                       io.Writer
	ServiceAccount               string
	TargetNamespace              string
	Paused                       bool
	Labels                       map[string]string
	SyncGeneration               int64
	Auth                         bundlereader.Auth
	HelmRepoURLRegex             string
	KeepResources                bool
	DeleteNamespace              bool
	AuthByPath                   map[string]bundlereader.Auth
	CorrectDrift                 bool
	CorrectDriftForce            bool
	CorrectDriftKeepFailHistory  bool
	OCIRegistry                  OCIRegistrySpec
	OCIRegistrySecret            string
	DrivenScan                   bool
	DrivenScanSeparator          string
	JobNameEnvVar                string
	BundleCreationMaxConcurrency int
}

type bundleWithOpts struct {
	bundle *fleet.Bundle
	scans  []*fleet.ImageScan
	opts   *Options
}

func globDirs(baseDir string) (result []string, err error) {
	for strings.HasPrefix(baseDir, "/") {
		baseDir = baseDir[1:]
	}
	paths, err := filepath.Glob(baseDir)
	if err != nil {
		return nil, err
	}
	for _, path := range paths {
		if s, err := os.Stat(path); err == nil && s.IsDir() {
			result = append(result, path)
		}
	}
	return
}

func getEffectiveMaxConcurrency(configured int) int {
	if configured <= 0 {
		return defaultBundleCreationMaxConcurrency
	}
	return configured
}

// CreateBundles creates bundles from the baseDirs, their names are prefixed with
// repoName. Depending on opts.Output the bundles are created in the cluster or
// printed to stdout, ...
func CreateBundles(pctx context.Context, client client.Client, r record.EventRecorder, repoName string, baseDirs []string, opts Options) error {
	if len(baseDirs) == 0 {
		baseDirs = []string{"."}
	}

	maxConcurrency := getEffectiveMaxConcurrency(opts.BundleCreationMaxConcurrency)

	// Using an errgroup to manage concurrency
	// 1. Goroutines will be launched, honouring the concurrency limit, and eventually block trying to write to `bundlesChan`.
	// 2. The main function will read from `bundlesChan`, hence unblocking the goroutines. This will continue to read from `bundlesChan` until it is closed.
	// 3. We use another goroutine to wait for all goroutines to finish, then close `bundlesChan`, finally unblocking the main function.

	bundlesChan := make(chan *bundleWithOpts)
	eg, ctx := errgroup.WithContext(pctx)
	eg.SetLimit(maxConcurrency + 1) // extra goroutine for WalkDir loop
	eg.Go(func() error {
		for _, baseDir := range baseDirs {
			matches, err := globDirs(baseDir)
			if err != nil {
				return fmt.Errorf("invalid path glob %s: %w", baseDir, err)
			}
			for _, baseDir := range matches {
				if err := filepath.WalkDir(baseDir, func(path string, entry fs.DirEntry, err error) error {
					if err != nil {
						return fmt.Errorf("failed walking path %q: %w", path, err)
					}
					if entry.IsDir() && entry.Name() == ".git" {
						return filepath.SkipDir
					}
					createBundle, e := shouldCreateBundleForThisPath(baseDir, path, entry)
					if e != nil {
						return fmt.Errorf("checking for bundle in path %q: %w", path, err)
					}
					if !createBundle {
						return nil
					}

					// needed as opts are mutated in this loop
					opts := opts
					eg.Go(func() error {
						if err := setAuthByPath(&opts, path); err != nil {
							return err
						}

						bundle, scans, err := bundleFromDir(ctx, repoName, path, opts)
						if err != nil {
							if errors.Is(err, ErrNoResources) {
								logrus.Warnf("%s: %v", path, err)
								return nil
							}
							return err
						}
						select {
						case <-ctx.Done():
							return ctx.Err()
						case bundlesChan <- &bundleWithOpts{bundle: bundle, scans: scans, opts: &opts}:
						}
						return nil
					})
					return nil
				}); err != nil {
					return err
				}
			}
		}
		return nil
	})
	go func() {
		_ = eg.Wait()
		close(bundlesChan)
	}()

	gitRepoBundlesMap := make(map[string]*fleet.Bundle)
	var bundlesToWrite []*bundleWithOpts
	for b := range bundlesChan {
		gitRepoBundlesMap[b.bundle.Name] = b.bundle
		bundlesToWrite = append(bundlesToWrite, b)
	}
	// Recovers any error that could happen in the errgroup, won't actually wait
	if err := eg.Wait(); err != nil {
		return err
	}
	ctx = pctx // context from ErrorGroup is canceled after the first Wait() returns

	if opts.Output == nil {
		err := pruneBundlesNotFoundInRepo(ctx, client, repoName, opts.Namespace, gitRepoBundlesMap)
		if err != nil {
			return err
		}
	}

	if len(gitRepoBundlesMap) == 0 {
		return fmt.Errorf("no resource found at the following paths to deploy: %v", baseDirs)
	}

	egWrite, ctx := errgroup.WithContext(pctx)
	egWrite.SetLimit(maxConcurrency)
	for _, b := range bundlesToWrite {
		egWrite.Go(func() error {
			return writeBundle(ctx, client, r, b.bundle, b.scans, *b.opts)
		})
	}

	return egWrite.Wait()
}

// CreateBundlesDriven creates bundles from the given baseDirs. Those bundles' names will be prefixed with
// repoName. Depending on opts.Output the bundles are created in the cluster or
// printed to stdout, ...
// CreateBundlesDriven does not scan the given dirs recursively, it simply considers each of them
// to be the base path for a bundle.
// The given baseDirs may describe a simple path or a path and a fleet file,
// separated by a character set in opts.
// If no fleet file is provided it tries to load a fleet.yaml in the root of the dir, or will consider
// the directory as a raw content folder.
func CreateBundlesDriven(pctx context.Context, client client.Client, r record.EventRecorder, repoName string, baseDirs []string, opts Options) error {
	if len(baseDirs) == 0 {
		baseDirs = []string{"."}
	}

	maxConcurrency := getEffectiveMaxConcurrency(opts.BundleCreationMaxConcurrency)

	// Using an errgroup to manage concurrency
	// 1. Goroutines will be launched, honouring the concurrency limit, and eventually block trying to write to `bundlesChan`.
	// 2. The main function will read from `bundlesChan`, hence unblocking the goroutines. This will continue to read from `bundlesChan` until it is closed.
	// 3. We use another goroutine to wait for all goroutines to finish, then close `bundlesChan`, finally unblocking the main function.
	bundlesChan := make(chan *bundleWithOpts)
	eg, ctx := errgroup.WithContext(pctx)
	eg.SetLimit(maxConcurrency + 1) // extra goroutine for scanning loop
	eg.Go(func() error {
		for _, baseDir := range baseDirs {
			opts := opts
			eg.Go(func() error {
				var err error
				baseDir, opts.BundleFile, err = getPathAndFleetYaml(baseDir, opts.DrivenScanSeparator)
				if err != nil {
					return err
				}

				if err := setAuthByPath(&opts, baseDir); err != nil {
					return err
				}

				bundle, scans, err := bundleFromDir(ctx, repoName, baseDir, opts)
				if err != nil {
					if errors.Is(err, ErrNoResources) {
						logrus.Warnf("%s: %v", baseDir, err)
						return nil
					}
					return err
				}
				select {
				case <-ctx.Done():
					return ctx.Err()
				case bundlesChan <- &bundleWithOpts{bundle: bundle, scans: scans, opts: &opts}:
				}
				return nil
			})
		}
		return nil
	})
	go func() {
		_ = eg.Wait()
		close(bundlesChan)
	}()

	gitRepoBundlesMap := make(map[string]*fleet.Bundle)
	var bundlesToWrite []*bundleWithOpts
	for b := range bundlesChan {
		gitRepoBundlesMap[b.bundle.Name] = b.bundle
		bundlesToWrite = append(bundlesToWrite, b)
	}
	// Recovers any error that could happen in the errgroup, won't actually wait
	if err := eg.Wait(); err != nil {
		return err
	}
	ctx = pctx // context from ErrorGroup is canceled after the first Wait() returns

	if opts.Output == nil {
		err := pruneBundlesNotFoundInRepo(ctx, client, repoName, opts.Namespace, gitRepoBundlesMap)
		if err != nil {
			return err
		}
	}

	if len(gitRepoBundlesMap) == 0 {
		return fmt.Errorf("no resource found at the following paths to deploy: %v", baseDirs)
	}

	egWrite, ctx := errgroup.WithContext(pctx)
	egWrite.SetLimit(maxConcurrency)
	for _, b := range bundlesToWrite {
		egWrite.Go(func() error {
			return writeBundle(ctx, client, r, b.bundle, b.scans, *b.opts)
		})
	}

	return egWrite.Wait()
}

// getPathAndFleetYaml returns the path and options file from a given path.
// The path and options file should be separated by the given separator
func getPathAndFleetYaml(path, separator string) (string, string, error) {
	baseDirFleetFile := strings.Split(path, separator)
	if len(baseDirFleetFile) == 2 {
		return baseDirFleetFile[0], baseDirFleetFile[1], nil
	}

	if len(baseDirFleetFile) > 2 {
		return "", "", fmt.Errorf("invalid bundle path: %q", path)
	}

	return path, "", nil
}

// pruneBundlesNotFoundInRepo lists all bundles for this gitrepo and prunes those not found in the repo
func pruneBundlesNotFoundInRepo(
	ctx context.Context,
	c client.Client,
	repoName,
	ns string,
	gitRepoBundlesMap map[string]*fleet.Bundle,
) error {
	filter := labels.SelectorFromSet(labels.Set{fleet.RepoLabel: repoName})
	bundleList := &fleet.BundleList{}
	err := c.List(ctx, bundleList, &client.ListOptions{LabelSelector: filter, Namespace: ns})

	for _, bundle := range bundleList.Items {
		if _, ok := gitRepoBundlesMap[bundle.Name]; !ok {
			logrus.Debugf("Bundle to be deleted since it is not found in gitrepo %v anymore %v %v", repoName, bundle.Namespace, bundle.Name)

			// Populate new bundles' `Overwrites` field with possible overlaps between the in-cluster bundle, to be deleted,
			// and bundles which will be created in the cluster.
			// Knowing about these overlaps, if any, the Fleet agent will then be able to:
			// 1. match them against possible missing resources in a bundle deployment's status
			// 2. trigger a new deployment, re-creating missing resources if those are overwritten by the
			// bundle deployment
			// See fleet#3770 for more context.
			for _, inClusterRsc := range bundle.Spec.Resources {
				for _, grb := range gitRepoBundlesMap {
					logrus.Debugf("gitRepo bundle: %v", grb)
					for _, grRsc := range grb.Spec.Resources {
						if inClusterRsc.Name != grRsc.Name {
							continue
						}

						logrus.Debugf("resources: [in cluster] %v\n, [in gitrepo] %v", inClusterRsc, grRsc)

						ow1, err := getKindNS(grRsc, grb.Name)
						if err != nil {
							logrus.Debugf("for bundle from git repo, failed to get kind and namespace for resource %v", grRsc)
							continue
						}
						if ow1.Kind == "" {
							// Skipping non-manifest resources, e.g. Chart.yaml and values
							// files.
							continue
						}

						ow2, err := getKindNS(inClusterRsc, bundle.Name)
						if err != nil {
							logrus.Debugf("for in-cluster bundle, failed to get kind and namespace for resource %v", grRsc)
							continue
						}
						if ow2.Kind == "" {
							continue
						}

						if ow1.Kind == ow2.Kind && ow1.Name == ow2.Name && ow1.Namespace == ow2.Namespace {
							// Warning: this will not work with bundlenamespacemappings
							grb.Spec.Overwrites = append(grb.Spec.Overwrites, ow1)
						}
					}
				}
			}
			err = c.Delete(ctx, &bundle)
			if err != nil {
				return err
			}
		}
	}
	return err
}

// newBundle reads bundle data from a source and returns a bundle with the
// given name, or the name from the raw source file
func newBundle(ctx context.Context, name, baseDir string, opts Options) (*fleet.Bundle, []*fleet.ImageScan, error) {
	var bundle *fleet.Bundle
	var scans []*fleet.ImageScan
	if opts.BundleReader != nil {
		if err := json.NewDecoder(opts.BundleReader).Decode(bundle); err != nil {
			return nil, nil, fmt.Errorf("decoding bundle %s: %w", name, err)
		}
	} else {
		var err error
		bundle, scans, err = bundlereader.NewBundle(ctx, name, baseDir, opts.BundleFile, &bundlereader.Options{
			BundleFile:       opts.BundleFile,
			Compress:         opts.Compress,
			Labels:           opts.Labels,
			ServiceAccount:   opts.ServiceAccount,
			TargetsFile:      opts.TargetsFile,
			TargetNamespace:  opts.TargetNamespace,
			Paused:           opts.Paused,
			SyncGeneration:   opts.SyncGeneration,
			Auth:             opts.Auth,
			HelmRepoURLRegex: opts.HelmRepoURLRegex,
			KeepResources:    opts.KeepResources,
			DeleteNamespace:  opts.DeleteNamespace,
			CorrectDrift: &fleet.CorrectDrift{
				Enabled:         opts.CorrectDrift,
				Force:           opts.CorrectDriftForce,
				KeepFailHistory: opts.CorrectDriftKeepFailHistory,
			},
		})
		if err != nil {
			return nil, nil, err
		}
	}
	bundle.Namespace = opts.Namespace
	return bundle, scans, nil
}

// bundleFromDir reads a specific directory and produces a bundle and image scans.
//
// name: the gitrepo name, passed to 'fleet apply' on the cli
// basedir: a directory containing a Bundle, as observed by CreateBundles or CreateBundlesDriven
func bundleFromDir(ctx context.Context, name, baseDir string, opts Options) (*fleet.Bundle, []*fleet.ImageScan, error) {
	// The bundleID is a valid helm release name, it's used as a default if a release name is not specified in helm options.
	// It's also used to create the bundle name.
	bundleID := filepath.Join(name, baseDir)
	if opts.BundleFile != "" {
		bundleID = filepath.Join(bundleID, strings.TrimSuffix(opts.BundleFile, filepath.Ext(opts.BundleFile)))
	}
	bundleID = names.HelmReleaseName(bundleID)

	bundle, scans, err := newBundle(ctx, bundleID, baseDir, opts)
	if err != nil {
		return nil, nil, err
	} else if len(bundle.Spec.Resources) == 0 {
		return nil, nil, ErrNoResources
	}
	return bundle, scans, nil
}

func writeBundle(ctx context.Context, c client.Client, r record.EventRecorder, bundle *fleet.Bundle, scans []*fleet.ImageScan, opts Options) error {
	// Early return for "offline" mode, only printing the result to stdout/file
	if opts.Output != nil {
		return printToOutput(opts.Output, bundle, scans)
	}

	// We need to exit early if the bundle is being deleted
	tmp := &fleet.Bundle{}
	if err := c.Get(ctx, client.ObjectKey{Name: bundle.Name, Namespace: bundle.Namespace}, tmp); err == nil {
		if tmp.DeletionTimestamp != nil {
			return fmt.Errorf("the bundle %q is being deleted, cannot create during a delete operation", bundle.Name)
		}
	}

	h, data, err := helmvalues.ExtractValues(bundle)
	if err != nil {
		return err
	}

	// If values were found in the bundle the hash is not empty, we
	// remove the values from the bundle. Also, delete any old
	// secret if the values are empty.
	if h != "" {
		helmvalues.ClearValues(bundle)
	} else if err := deleteSecretIfExists(ctx, c, bundle.Name, bundle.Namespace); err != nil {
		return err
	}
	bundle.Spec.ValuesHash = h

	var ociOpts ocistorage.OCIOpts
	secretOCIRegistryID := client.ObjectKey{Name: opts.OCIRegistrySecret, Namespace: bundle.Namespace}
	useOCIRegistry, err := shouldStoreInOCIRegistry(ctx, c, secretOCIRegistryID, &ociOpts)
	if err != nil {
		return err
	}
	if useOCIRegistry {
		if bundle, err = saveOCIBundle(ctx, c, r, bundle, ociOpts); err != nil {
			return err
		}
	} else {
		if bundle, err = save(ctx, c, bundle); err != nil {
			return err
		}
	}

	// Saves the Helm values as a secret. The secret is owned by
	// the bundle. It will not create a secret if the values are
	// empty.
	if len(data) > 0 {
		valuesSecret := newValuesSecret(bundle, data)
		updated := valuesSecret.DeepCopy()
		_, err = controllerutil.CreateOrUpdate(ctx, c, valuesSecret, func() error {
			valuesSecret.OwnerReferences = updated.OwnerReferences
			valuesSecret.Labels = updated.Labels
			valuesSecret.Data = updated.Data
			valuesSecret.Type = updated.Type
			return nil
		})
		if err != nil {
			return err
		}
	}

	return saveImageScans(ctx, c, bundle, scans)
}

func printToOutput(w io.Writer, bundle *fleet.Bundle, scans []*fleet.ImageScan) error {
	objects := []runtime.Object{bundle}
	for _, scan := range scans {
		objects = append(objects, scan)
	}

	b, err := yaml.Export(objects...)
	if err != nil {
		return err
	}

	_, err = w.Write(b)
	return err
}

func shouldStoreInOCIRegistry(ctx context.Context, c client.Reader, ociSecretKey client.ObjectKey, ociOpts *ocistorage.OCIOpts) (bool, error) {
	if !ocistorage.OCIIsEnabled() {
		return false, nil
	}

	opts, err := ocistorage.ReadOptsFromSecret(ctx, c, ociSecretKey)
	if err != nil {
		if apierrors.IsNotFound(err) && ociSecretKey.Name == "" {
			// don't return not found errors when no secret name was specified by the user
			return false, nil
		}
		return false, err
	}
	ociOpts.Reference = opts.Reference
	ociOpts.Username = opts.Username
	ociOpts.Password = opts.Password
	ociOpts.AgentUsername = opts.AgentUsername
	ociOpts.AgentPassword = opts.AgentPassword
	ociOpts.BasicHTTP = opts.BasicHTTP
	ociOpts.InsecureSkipTLS = opts.InsecureSkipTLS

	return true, nil
}

func pushOCIManifest(ctx context.Context, bundle *fleet.Bundle, opts ocistorage.OCIOpts) (string, error) {
	manifest := manifest.FromBundle(bundle)
	manifestID, err := manifest.ID()
	if err != nil {
		return "", err
	}
	oci := ocistorage.NewOCIWrapper()
	err = oci.PushManifest(ctx, opts, manifestID, manifest)
	if err != nil {
		return "", err
	}
	return manifestID, nil
}

func save(ctx context.Context, c client.Client, bundle *fleet.Bundle) (*fleet.Bundle, error) {
	updated := bundle.DeepCopy()
	result, err := controllerutil.CreateOrUpdate(ctx, c, bundle, func() error {
		if bundle.Spec.HelmOpOptions != nil {
			return fmt.Errorf("a helmOps bundle with name %q already exists", bundle.Name)
		}
		// We cannot update a bundle that is going to be deleted, our update would be lost
		if bundle.DeletionTimestamp != nil {
			return fmt.Errorf("the bundle %q is being deleted", bundle.Name)
		}

		if bundle.Spec.ContentsID != "" {
			// this bundle was previously deployed to an OCI registry.
			// Delete the OCI artifact as it's no longer required.
			if err := deleteOCIManifest(ctx, c, bundle, ocistorage.OCIOpts{}); err != nil {
				// we log the error and continue, since the OCI registry is an external entity to the the cluster
				// we may encounter various types of transient errors (such as connection or access issues).
				logrus.Warnf("deleting OCI artifact: %v", err)
				return err

			}
		}

		bundle.Spec = updated.Spec
		bundle.Annotations = updated.Annotations
		bundle.Labels = updated.Labels
		return nil
	})
	if err != nil {
		return nil, err
	}
	logrus.Infof("%s (bundle): %s/%s", result, bundle.Namespace, bundle.Name)

	return bundle, nil
}

func saveImageScans(ctx context.Context, c client.Client, bundle *fleet.Bundle, scans []*fleet.ImageScan) error {
	for _, scan := range scans {
		scan.Namespace = bundle.Namespace
		scan.Spec.GitRepoName = bundle.Labels[fleet.RepoLabel]
		updated := scan.DeepCopy()
		result, err := controllerutil.CreateOrUpdate(ctx, c, scan, func() error {
			scan.Spec = updated.Spec
			scan.Annotations = bundle.Annotations
			scan.Labels = bundle.Labels
			return nil
		})
		if err != nil {
			return err
		}
		logrus.Infof("%s (scan): %s/%s", result, scan.Namespace, scan.Name)
	}
	return nil
}

func saveOCIBundle(ctx context.Context, c client.Client, r record.EventRecorder, bundle *fleet.Bundle, opts ocistorage.OCIOpts) (*fleet.Bundle, error) {
	manifestID, err := pushOCIManifest(ctx, bundle, opts)
	if err != nil {
		return bundle, err
	}
	logrus.Infof("OCI artifact stored successfully: %s %s", bundle.Name, manifestID)

	updated := bundle.DeepCopy()
	_, err = controllerutil.CreateOrUpdate(ctx, c, bundle, func() error {
		if bundle.DeletionTimestamp != nil {
			return fmt.Errorf("the bundle %q is being deleted", bundle.Name)
		}

		if bundle.Spec.HelmOpOptions != nil {
			return fmt.Errorf("a helmOps bundle with name %q already exists", bundle.Name)
		}

		// If the current manifestID is different from the previous one,
		// delete the previous OCI artifact
		if bundle.Spec.ContentsID != "" && bundle.Spec.ContentsID != manifestID {
			if err := deleteOCIManifest(ctx, c, bundle, opts); err != nil {
				// we log the error and continue, since the OCI registry is an external entity to the the cluster
				// we may encounter various types of transient errors (such as connection or access issues).
				logrus.Warnf("deleting OCI artifact: %v", err)
				sendWarningEvent(r, bundle.Namespace, bundle.Spec.ContentsID, err)
			}
		}

		bundle.Spec = updated.Spec
		bundle.Annotations = updated.Annotations
		bundle.Labels = updated.Labels

		// We don't store the resources in the bundle. Just keep the manifestID for
		// being able to access the bundle's contents later.
		bundle.Spec.Resources = nil
		bundle.Spec.ContentsID = manifestID
		return nil
	})
	if err != nil {
		return nil, err
	}

	secret := newOCISecret(manifestID, bundle, opts)
	data := secret.Data
	result, err := controllerutil.CreateOrUpdate(ctx, c, secret, func() error {
		secret.Data = data
		return nil
	})
	if err != nil {
		return nil, err
	}
	logrus.Infof("%s (oci secret): %s/%s", result, bundle.Namespace, bundle.Name)

	return bundle, nil
}

func deleteOCIManifest(ctx context.Context, c client.Client, bundle *fleet.Bundle, opts ocistorage.OCIOpts) error {
	if bundle.Spec.ContentsID == "" {
		return nil
	}
	secretID := client.ObjectKey{Name: bundle.Spec.ContentsID, Namespace: bundle.Namespace}
	if opts.Reference == "" {
		// we don't have the reference details, get them from the bundle's secret
		var err error
		opts, err = ocistorage.ReadOptsFromSecret(ctx, c, secretID)
		if err != nil {
			return err
		}
	}
	if err := ocistorage.NewOCIWrapper().DeleteManifest(ctx, opts, bundle.Spec.ContentsID); err != nil {
		return err
	}

	// also delete the bundle secret as it's no longer needed
	secretToDelete := &corev1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:      bundle.Spec.ContentsID,
			Namespace: bundle.Namespace,
		},
	}
	if err := c.Delete(ctx, secretToDelete); err != nil {
		return err
	}

	return nil
}

// when using the OCI registry manifestID won't be empty
// In this case we need to create a secret to store the
// OCI registry reference and credentials so the fleet controller is
// able to access.
func newOCISecret(manifestID string, bundle *fleet.Bundle, opts ocistorage.OCIOpts) *corev1.Secret {
	return &corev1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:      manifestID,
			Namespace: bundle.Namespace,
			Labels:    map[string]string{fleet.InternalSecretLabel: "true"},
			OwnerReferences: []metav1.OwnerReference{
				{
					APIVersion:         fleet.SchemeGroupVersion.String(),
					Kind:               "Bundle",
					Name:               bundle.GetName(),
					UID:                bundle.GetUID(),
					BlockOwnerDeletion: ptr.To(true),
					Controller:         ptr.To(true),
				},
			},
		},
		Data: map[string][]byte{
			ocistorage.OCISecretReference:     []byte(opts.Reference),
			ocistorage.OCISecretUsername:      []byte(opts.Username),
			ocistorage.OCISecretPassword:      []byte(opts.Password),
			ocistorage.OCISecretAgentUsername: []byte(opts.AgentUsername),
			ocistorage.OCISecretAgentPassword: []byte(opts.AgentPassword),
			ocistorage.OCISecretBasicHTTP:     []byte(strconv.FormatBool(opts.BasicHTTP)),
			ocistorage.OCISecretInsecure:      []byte(strconv.FormatBool(opts.InsecureSkipTLS)),
		},
		Type: fleet.SecretTypeOCIStorage,
	}
}

func newValuesSecret(bundle *fleet.Bundle, data map[string][]byte) *corev1.Secret {
	return &corev1.Secret{
		Type: fleet.SecretTypeBundleValues,
		ObjectMeta: metav1.ObjectMeta{
			Name:      bundle.Name,
			Namespace: bundle.Namespace,
			OwnerReferences: []metav1.OwnerReference{
				{
					APIVersion:         fleet.SchemeGroupVersion.String(),
					Kind:               "Bundle",
					Name:               bundle.Name,
					UID:                bundle.GetUID(),
					BlockOwnerDeletion: ptr.To(true),
					Controller:         ptr.To(true),
				},
			},
			Labels: bundle.Labels,
		},

		Data: data,
	}
}

// shouldCreateBundleForThisPath returns true if a bundle should be created for this path. This happens when:
// 1) Root path contains resources in the root directory or any subdirectory without a fleet.yaml.
// 2) Or it is a subdirectory with a fleet.yaml
func shouldCreateBundleForThisPath(baseDir, path string, entry fs.DirEntry) (bool, error) {
	isRootPath := baseDir == path
	if isRootPath {
		// always create a Bundle if fleet.yaml is found in the root path
		if !fleetyaml.FoundFleetYamlInDirectory(path) {
			// don't create a Bundle if any subdirectory with resources and without a fleet.yaml is found
			createBundleForRoot, err := hasSubDirectoryWithResourcesAndWithoutFleetYaml(path)
			if err != nil {
				return false, err
			}
			return createBundleForRoot, nil
		}
	} else {
		if !entry.IsDir() {
			return false, nil
		}
		if !fleetyaml.FoundFleetYamlInDirectory(path) {
			return false, nil
		}
	}

	return true, nil
}

// hasSubDirectoryWithResourcesAndWithoutFleetYaml returns true if this path or any of its subdirectories contains any
// resource, and it doesn't contain a fleet.yaml.
func hasSubDirectoryWithResourcesAndWithoutFleetYaml(path string) (bool, error) {
	if fleetyaml.FoundFleetYamlInDirectory(path) {
		return false, nil
	}
	files, err := os.ReadDir(path)
	if err != nil {
		return false, err
	}

	for _, file := range files {
		if !file.IsDir() {
			if ext := filepath.Ext(file.Name()); ext == ".yaml" || ext == ".yml" {
				return true, nil
			}
		} else {
			// check if this subdirectory contains resources without a fleet.yaml. If it contains a fleet.yaml a new
			// Bundle for this subdirectory will be created
			containsResources, err := hasSubDirectoryWithResourcesAndWithoutFleetYaml(filepath.Join(path, file.Name()))
			if err != nil {
				return false, err
			}
			if containsResources {
				return true, nil
			}
		}
	}

	return false, nil
}

func deleteSecretIfExists(ctx context.Context, c client.Client, name, ns string) error {
	secret := &corev1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:      name,
			Namespace: ns,
		},
	}
	if err := c.Delete(ctx, secret); err != nil && !apierrors.IsNotFound(err) {
		return err
	}

	return nil
}

func sendWarningEvent(r record.EventRecorder, namespace, artifactID string, errorToLog error) {
	jobName := os.Getenv(JobNameEnvVar)
	if jobName == "" {
		logrus.Warnf("%q environment variable not set", JobNameEnvVar)
		return
	}
	job := &batchv1.Job{
		ObjectMeta: metav1.ObjectMeta{
			Name:      jobName,
			Namespace: namespace,
		},
	}
	r.Event(job, fleetevent.Warning, "FailedToDeleteOCIArtifact", fmt.Sprintf("deleting OCI artifact %q: %v", artifactID, errorToLog.Error()))
}

func setAuthByPath(opts *Options, path string) error {
	if auth, ok := opts.AuthByPath[path]; ok {
		opts.Auth = auth

		return nil
	}

	// No direct match; check for globs instead.
	var patternKeys []string
	for k := range opts.AuthByPath {
		patternKeys = append(patternKeys, k)
	}
	// Sort patterns in lexical order to work around
	// non-deterministic iteration order for Go maps.
	slices.Sort(patternKeys)

	for _, pattern := range patternKeys {
		isMatch, err := filepath.Match(pattern, path)
		if err != nil {
			return fmt.Errorf("failed to check for matches in auth paths: %w", err)
		}

		if isMatch {
			opts.Auth = opts.AuthByPath[pattern]
			break
		}
	}

	return nil
}

// getIntEnvVar reads an integer from an environment variable, returning the default if unset or invalid.
func getIntEnvVar(envVarName string, defaultValue int) (int, error) {
	s := os.Getenv(envVarName)
	if s == "" {
		return defaultValue, nil
	}

	val, err := strconv.Atoi(s)
	if err != nil {
		return defaultValue, err
	}
	return val, nil
}

func GetOnConflictRetries() (int, error) {
	return getIntEnvVar(FleetApplyConflictRetriesEnv, defaultApplyConflictRetries)
}

func GetBundleCreationMaxConcurrency() (int, error) {
	return getIntEnvVar(BundleCreationMaxConcurrencyEnv, defaultBundleCreationMaxConcurrency)
}

type k8sWithNS struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
}

func getKindNS(br fleet.BundleResource, bundleName string) (fleet.OverwrittenResource, error) {
	var contents []byte
	var err error
	if br.Encoding == "base64+gz" {
		contents, err = content.GUnzip([]byte(br.Content))
		if err != nil {
			logrus.Debugf("could not uncompress contents of resource %s in bundle %s;"+
				" skipping overlap detection for this resource", br.Name, bundleName)
			return fleet.OverwrittenResource{}, nil //nolint:nilerr // intentionally skipping this resource
		}
	} else {
		// encoding should be empty
		contents = []byte(br.Content)
	}

	// Replace templating tags to prevent unmarshalling errors. We are not interested in the resource contents
	// beyond its kind, name and namespace.
	placeholder := "TEMPLATED"
	templating := regexp.MustCompile("{{[^}]+}}")
	c := templating.ReplaceAll(contents, []byte(placeholder))

	var rsc k8sWithNS
	err = k8syaml.Unmarshal(c, &rsc)
	if err != nil {
		return fleet.OverwrittenResource{}, fmt.Errorf("could not convert resource contents into object: %w", err)
	}

	logrus.Debugf("contents from bundle resource: %v", string(contents))

	or := fleet.OverwrittenResource{
		Kind:      rsc.Kind,
		Name:      rsc.Name,
		Namespace: rsc.Namespace,
	}
	logrus.Debugf("returning overwritten resource: %v", or)
	return or, nil
}



================================================
FILE: internal/cmd/cli/apply/apply_concurrency_test.go
================================================
package apply

import (
	"os"
	"testing"
)

func TestGetEffectiveMaxConcurrency(t *testing.T) {
	tests := map[string]struct {
		input    int
		expected int
	}{
		"zero defaults to 4":     {0, 4},
		"negative defaults to 4": {-1, 4},
		"custom value 8":         {8, 8},
		"custom value 16":        {16, 16},
		"custom value 12":        {12, 12},
	}

	for name, tt := range tests {
		t.Run(name, func(t *testing.T) {
			if got := getEffectiveMaxConcurrency(tt.input); got != tt.expected {
				t.Errorf("expected %d, got %d", tt.expected, got)
			}
		})
	}
}

func TestGetBundleCreationMaxConcurrency(t *testing.T) {
	tests := []struct {
		name          string
		envValue      string
		expectedValue int
		expectedError bool
	}{
		{
			name:          "default when env var not set",
			envValue:      "",
			expectedValue: 4,
			expectedError: false,
		},
		{
			name:          "custom value 8",
			envValue:      "8",
			expectedValue: 8,
			expectedError: false,
		},
		{
			name:          "custom value 16",
			envValue:      "16",
			expectedValue: 16,
			expectedError: false,
		},
		{
			name:          "invalid value returns error",
			envValue:      "not_a_number",
			expectedValue: 4,
			expectedError: true,
		},
		{
			name:          "zero is valid but caller handles default",
			envValue:      "0",
			expectedValue: 0,
			expectedError: false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Save and restore the environment variable
			oldVal, wasSet := os.LookupEnv(BundleCreationMaxConcurrencyEnv)
			defer func() {
				if wasSet {
					os.Setenv(BundleCreationMaxConcurrencyEnv, oldVal)
				} else {
					os.Unsetenv(BundleCreationMaxConcurrencyEnv)
				}
			}()

			if tt.envValue != "" {
				os.Setenv(BundleCreationMaxConcurrencyEnv, tt.envValue)
			} else {
				os.Unsetenv(BundleCreationMaxConcurrencyEnv)
			}

			got, err := GetBundleCreationMaxConcurrency()
			if (err != nil) != tt.expectedError {
				t.Errorf("expected error %v, got %v", tt.expectedError, err != nil)
			}
			if got != tt.expectedValue {
				t.Errorf("expected %d, got %d", tt.expectedValue, got)
			}
		})
	}
}



================================================
FILE: internal/cmd/cli/apply/apply_test.go
================================================
package apply

import (
	"testing"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

func Test_getKindNS(t *testing.T) {
	// TODO add test cases covering templating in name and namespace (less likely in kind)
	// → we should return an empty overwrite in such cases
	br := fleet.BundleResource{
		Name: "folder/my-cm.yaml",
		Content: `apiVersion: v1
kind: ConfigMap
metadata:
  name: Foo
  namespace: my-namespace
data:
  bar: baz
  name: {{ .Values.name }}`,
		// No encoding
	}

	or, err := getKindNS(br, "my-bundle")
	if err != nil {
		t.Fatalf("expected no error, got %v", err)
	}

	if or.Kind != "ConfigMap" {
		t.Fatalf("expected ConfigMap kind, got %v", or.Kind)
	}

	if or.Name != "Foo" {
		t.Fatalf("expected name Foo, got %v", or.Name)
	}

	if or.Namespace != "my-namespace" {
		t.Fatalf("expected namespace my-namespace, got %v", or.Namespace)
	}

}



================================================
FILE: internal/cmd/cli/cleanup/cleanup.go
================================================
package cleanup

import (
	"context"
	"time"

	"github.com/go-logr/logr"
	"github.com/jpillora/backoff"

	fleetv1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	rbacv1 "k8s.io/api/rbac/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

type Options struct {
	Min    time.Duration
	Max    time.Duration
	Factor float64
}

func ClusterRegistrations(ctx context.Context, cl client.Client, opts Options) error {
	logger := log.FromContext(ctx)

	// lookup for all existing clusters
	seen := map[types.NamespacedName]struct{}{}
	clusterList := &fleetv1.ClusterList{}
	_ = cl.List(ctx, clusterList)
	for _, c := range clusterList.Items {
		clusterKey := types.NamespacedName{Namespace: c.Namespace, Name: c.Name}
		seen[clusterKey] = struct{}{}
	}

	crList := &fleetv1.ClusterRegistrationList{}
	_ = cl.List(ctx, crList)

	logger.Info("Listing resources", "clusters", len(clusterList.Items), "clusterRegistrations", len(crList.Items))

	// figure out the latest granted registration request per cluster
	latestGranted := map[types.NamespacedName]metav1.Time{}
	for _, cr := range crList.Items {
		if cr.Status.ClusterName == "" {
			continue
		}

		logger.Info("Mapping cluster registration")
		clusterKey := types.NamespacedName{Namespace: cr.Namespace, Name: cr.Status.ClusterName}
		ts := latestGranted[clusterKey]
		if cr.Status.Granted && ts.Before(&cr.CreationTimestamp) {
			latestGranted[clusterKey] = cr.CreationTimestamp
		}
	}

	// sleep to not overload the API server
	b := backoff.Backoff{
		Min:    opts.Min,
		Max:    opts.Max,
		Factor: opts.Factor,
		Jitter: true,
	}

	// it should be safe to delete up to, but not including, the latest
	// granted registration
	// * requests after that might be in flight
	// * requests before that are outdated
	for _, cr := range crList.Items {
		logger := logger.WithValues("namespace", cr.Namespace, "name", cr.Name)
		logger.Info("Inspecting cluster registration")
		if cr.Status.ClusterName == "" {
			continue
		}

		clusterKey := types.NamespacedName{Namespace: cr.Namespace, Name: cr.Status.ClusterName}
		latest, found := latestGranted[clusterKey]
		if found && cr.CreationTimestamp.Before(&latest) {
			t := b.Duration()
			logger.Info("Deleting outdated, granted cluster registration, waiting", "duration", t)
			time.Sleep(t)
			if err := cl.Delete(ctx, &cr); err != nil && !apierrors.IsNotFound(err) {
				logger.Error(err, "Failed to delete clusterregistration")
			}
			// also try to delete orphan resources
			_ = cl.Delete(ctx, &rbacv1.Role{ObjectMeta: metav1.ObjectMeta{Namespace: cr.Namespace, Name: cr.Name}})
			_ = cl.Delete(ctx, &rbacv1.RoleBinding{ObjectMeta: metav1.ObjectMeta{Namespace: cr.Namespace, Name: cr.Name}})

			saList := &corev1.ServiceAccountList{}
			_ = cl.List(ctx, saList, client.MatchingLabels{
				"fleet.cattle.io/cluster-registration":           cr.Name,
				"fleet.cattle.io/cluster-registration-namespace": cr.Namespace})
			for _, sa := range saList.Items {
				_ = cl.Delete(ctx, &sa)
			}

			owner := client.MatchingLabels{
				"objectset.rio.cattle.io/owner-name":      cr.Name,
				"objectset.rio.cattle.io/owner-namespace": cr.Namespace,
			}
			rbList := &rbacv1.RoleBindingList{}
			_ = cl.List(ctx, rbList, owner)
			for _, rb := range rbList.Items {
				_ = cl.Delete(ctx, &rb)
			}

			crbList := &rbacv1.ClusterRoleBindingList{}
			_ = cl.List(ctx, crbList, owner)
			for _, crb := range crbList.Items {
				_ = cl.Delete(ctx, &crb)
			}
		}
	}

	// delete cluster registrations that have no cluster
	for _, cr := range crList.Items {
		if cr.Status.ClusterName == "" {
			continue
		}

		clusterKey := types.NamespacedName{Namespace: cr.Namespace, Name: cr.Status.ClusterName}
		if _, found := seen[clusterKey]; !found {
			logger := logger.WithValues("namespace", cr.Namespace, "name", cr.Name)
			logger.Info("Deleting granted cluster registration without cluster")
			if err := cl.Delete(ctx, &cr); err != nil && !apierrors.IsNotFound(err) {
				logger.Error(err, "Failed to delete cluster registration")
			}
			continue
		}
	}

	return nil
}

func GitJobs(ctx context.Context, cl client.Client, bs int) error {
	logger := log.FromContext(ctx).WithName("cleanup-jobs").WithValues("batchSize", bs)

	list := &batchv1.JobList{}
	if err := cl.List(ctx, list, client.Limit(bs)); err != nil {
		return err
	}
	logger.Info("Listing resources", "jobs", len(list.Items))

	jobs := list.Items

	for list.Continue != "" {
		if err := cl.List(ctx, list, client.Limit(bs), client.Continue(list.Continue)); err != nil {
			return err
		}
		logger.Info("Listing more resources", "jobs", len(list.Items))

		jobs = append(jobs, list.Items...)
	}

	if err := cleanupGitJobs(ctx, logger, cl, jobs); err != nil {
		return err
	}

	return nil
}

func cleanupGitJobs(ctx context.Context, logger logr.Logger, cl client.Client, jobs []batchv1.Job) error {
	for _, job := range jobs {
		if job.OwnerReferences == nil {
			continue
		}
		for _, or := range job.OwnerReferences {
			if or.Kind != "GitRepo" || or.APIVersion != "fleet.cattle.io/v1alpha1" {
				continue
			}
			if job.Status.Succeeded == 1 && job.Status.CompletionTime != nil {
				logger.V(1).Info("Deleting job", "namespace", job.Namespace, "name", job.Name, "gitrepo", or.Name)
				err := cl.Delete(ctx, &job, client.PropagationPolicy(metav1.DeletePropagationBackground))
				if err != nil && !apierrors.IsNotFound(err) {
					logger.Error(err, "Failed to delete job", "namespace", job.Namespace, "name", job.Name)
				}
			}
			break
		}
	}

	return nil
}



================================================
FILE: internal/cmd/cli/gitcloner/cloner.go
================================================
package gitcloner

import (
	"fmt"
	"os"
	"strings"

	"github.com/go-git/go-git/v5"
	"github.com/go-git/go-git/v5/plumbing"
	"github.com/go-git/go-git/v5/plumbing/protocol/packp/capability"
	"github.com/go-git/go-git/v5/plumbing/transport"
	httpgit "github.com/go-git/go-git/v5/plumbing/transport/http"
	gossh "github.com/go-git/go-git/v5/plumbing/transport/ssh"
	"github.com/sirupsen/logrus"
	"golang.org/x/crypto/ssh"

	fleetgithub "github.com/rancher/fleet/internal/github"
	fleetssh "github.com/rancher/fleet/internal/ssh"
	giturls "github.com/rancher/fleet/pkg/git-urls"
)

const defaultBranch = "master"

var (
	plainClone                              = git.PlainClone
	readFile                                = os.ReadFile
	fileStat                                = os.Stat
	appAuthGetter fleetgithub.AppAuthGetter = fleetgithub.DefaultAppAuthGetter{}
)

type Cloner struct{}

func New() *Cloner {
	return &Cloner{}
}

func (c *Cloner) CloneRepo(opts *GitCloner) error {
	url, err := giturls.Parse(opts.Repo)
	if err != nil {
		return fmt.Errorf("failed to parse git URL: %w", err)
	}
	if strings.HasPrefix(url.String(), "ssh://") {
		if opts.SSHPrivateKeyFile == "" {
			return fmt.Errorf("SSH private key file is required for SSH/SCP-style URLs: %s", url)
		}
	}

	// Azure DevOps requires capabilities multi_ack / multi_ack_detailed,
	// which are not fully implemented and by default are included in
	// transport.UnsupportedCapabilities.
	// Public repos in Azure can't be cloned.
	// This can be removed once go-git implements the git v2 protocol.
	// https://github.com/go-git/go-git/issues/64
	transport.UnsupportedCapabilities = []capability.Capability{
		capability.ThinPack,
	}
	auth, err := createAuthFromOpts(opts)
	if err != nil {
		return fmt.Errorf("failed to create auth from options for %s: %w", repo(opts), err)
	}
	caBundle, err := getCABundleFromFile(opts.CABundleFile)
	if err != nil {
		return fmt.Errorf("failed to read CA bundle from file for %s: %w", repo(opts), err)
	}

	if opts.Branch == "" && opts.Revision == "" {
		opts.Branch = defaultBranch
		return cloneBranch(opts, auth, caBundle)
	}

	if opts.Branch != "" {
		if opts.Revision != "" {
			logrus.Warn("Using branch for cloning the repo. Revision will be skipped.")
		}
		return cloneBranch(opts, auth, caBundle)
	}

	return cloneRevision(opts, auth, caBundle)
}

func cloneBranch(opts *GitCloner, auth transport.AuthMethod, caBundle []byte) error {
	_, err := plainClone(opts.Path, false, &git.CloneOptions{
		URL:               opts.Repo,
		Auth:              auth,
		InsecureSkipTLS:   opts.InsecureSkipTLS,
		CABundle:          caBundle,
		SingleBranch:      true,
		ReferenceName:     plumbing.ReferenceName(opts.Branch),
		RecurseSubmodules: git.DefaultSubmoduleRecursionDepth,
	})

	if err != nil {
		return fmt.Errorf("failed to clone repo from branch %s: %w", repo(opts), err)
	}
	return nil
}

func cloneRevision(opts *GitCloner, auth transport.AuthMethod, caBundle []byte) error {
	r, err := plainClone(opts.Path, false, &git.CloneOptions{
		URL:               opts.Repo,
		Auth:              auth,
		InsecureSkipTLS:   opts.InsecureSkipTLS,
		CABundle:          caBundle,
		RecurseSubmodules: git.DefaultSubmoduleRecursionDepth,
	})
	if err != nil {
		return fmt.Errorf("failed to clone repo from revision %s: %w", repo(opts), err)
	}
	h, err := r.ResolveRevision(plumbing.Revision(opts.Revision))
	if err != nil {
		return fmt.Errorf("failed to resolve revision %s: %w", repo(opts), err)
	}
	w, err := r.Worktree()
	if err != nil {
		return fmt.Errorf("failed to get filesystem worktree for %s: %w", repo(opts), err)
	}

	if err := w.Checkout(&git.CheckoutOptions{Hash: *h}); err != nil {
		return fmt.Errorf("failed to checkout in worktree %s: %w", repo(opts), err)
	}

	return nil
}

func getCABundleFromFile(path string) ([]byte, error) {
	if path == "" {
		return nil, nil
	}
	return readFile(path)
}

// createAuthFromOpts adds auth for cloning git repos based on the parameters provided in opts.
func createAuthFromOpts(opts *GitCloner) (transport.AuthMethod, error) {
	knownHosts, isKnownHostsSet := os.LookupEnv(fleetssh.KnownHostsEnvVar)
	if knownHosts == "" {
		isKnownHostsSet = false
	}

	if opts.SSHPrivateKeyFile != "" {
		privateKey, err := readFile(opts.SSHPrivateKeyFile)
		if err != nil {
			return nil, err
		}
		gitURL, err := giturls.Parse(opts.Repo)
		if err != nil {
			return nil, err
		}
		auth, err := gossh.NewPublicKeys(gitURL.User.Username(), privateKey, "")
		if err != nil {
			return nil, err
		}
		if isKnownHostsSet {
			knownHostsCallBack, err := fleetssh.CreateKnownHostsCallBack([]byte(knownHosts))
			if err != nil {
				return nil, fmt.Errorf("could not create known_hosts callback: %w", err)
			}

			auth.HostKeyCallback = knownHostsCallBack
		} else {
			//nolint:gosec // G106: Use of ssh InsecureIgnoreHostKey should be audited - this will run in an init-container, so there is no persistence
			auth.HostKeyCallback = ssh.InsecureIgnoreHostKey()
		}
		return auth, nil
	}

	if opts.GitHubAppID != 0 && opts.GitHubAppInstallation != 0 && opts.GitHubAppKeyFile != "" {
		if _, err := fileStat(opts.GitHubAppKeyFile); err != nil {
			return nil, fmt.Errorf("failed to resolve GitHub app private key from path: %w", err)
		}

		key, err := readFile(opts.GitHubAppKeyFile)
		if err != nil {
			return nil, fmt.Errorf("failed to read GitHub app private key from file: %w", err)
		}

		auth, err := appAuthGetter.Get(opts.GitHubAppID, opts.GitHubAppInstallation, key)
		if err != nil {
			return nil, err
		}
		return auth, nil
	}

	if opts.PasswordFile != "" {
		password, err := readFile(opts.PasswordFile)
		if err != nil {
			return nil, err
		}

		if len(opts.Username) == 0 {
			return &httpgit.BasicAuth{
				Username: string(password),
			}, nil
		}

		return &httpgit.BasicAuth{
			Username: opts.Username,
			Password: string(password),
		}, nil
	}

	return nil, nil
}

func repo(opts *GitCloner) string {
	return fmt.Sprintf("repo=%q branch=%q revision=%q path=%q", opts.Repo, opts.Branch, opts.Revision, opts.Path)
}



================================================
FILE: internal/cmd/cli/gitcloner/cloner_test.go
================================================
package gitcloner

import (
	"errors"
	"os"
	"testing"

	"github.com/go-git/go-git/v5"
	"github.com/go-git/go-git/v5/plumbing/protocol/packp/capability"
	"github.com/go-git/go-git/v5/plumbing/transport"
	httpgit "github.com/go-git/go-git/v5/plumbing/transport/http"
	gossh "github.com/go-git/go-git/v5/plumbing/transport/ssh"
	"github.com/google/go-cmp/cmp"
)

type fakeGetter struct{}

func (fakeGetter) Get(appID, instID int64, key []byte) (*httpgit.BasicAuth, error) {
	return &httpgit.BasicAuth{
		Username: "x-access-token",
		Password: "token",
	}, nil
}

func TestCloneRepo(t *testing.T) {
	const (
		passwordFile        = "passFile"
		passwordFileContent = "1234"
		sshPrivateKeyFile   = "sshFile"

		sshPrivateKeyFileContent = `-----BEGIN RSA PRIVATE KEY-----
MIICXQIBAAKBgQC1ZuFGlFeAFqeS6p04QsliOXG3NH1/lQC4UMXdQ0F73ciYBPKq
iQZcoyOu8a2Hsi5HvxDqR1rreTAkJ37C3ErrmKcE1CUJwxBVqkgE17Fzw63QBu0X
0OVtaUarG8Pd9HuKbXPK8HXFTEh6F5hoqmzCmG7cRHmagBeh1SqZm1awzQIDAQAB
AoGAChHZ84cMjGm1h6xKafMbJr61l0vso4Zr8c9aDHxNSEj5d6beqaTNm5rawj1c
Oqojc4whrj+jxmqFx5wBp2N/LRi7GhpPco4wy8gg2t/OjmcR+jTRJgT1x1Co9W58
U+O5c001YFTNoa1UUUBweqye/sX/k5GBCUt0V2G839Cn+8ECQQD2K2eZcyUeeBHT
/YhGAq++mmfVEkzMY7U+G59oeF038zXX+wtMwoKmC9/LHwVPWpnzL/oMu3zZqv4a
jzCOAdZpAkEAvKVas8KUctHUBvDoU6hq9bVyIZMZZnlBfysuFEeJLU8efp/n4KRO
93EyhcXe2FmOC/VzGbkiQobmAqVvIwTixQJBAIKYZE20GG0hpdOhHTqHElU79PnE
y5ljDDP204rI0Ctui5IZTNVcG5ObmQ5ZVqfSmPm66hz3GjUf0c6lSE0ODIECQHB0
silO6We5JggtPJICaCCpVawmIJIx3pWMjB+StXfJHoilknkb+ecQF+ofFsUqPb9r
Rn4jGwVFnYAeVq4tj3ECQQCyeMeCprz5AQ8HSd16Asd3zhv7N7olpb4XMIP6YZXy
udiSlDctMM/X3ZM2JN5M1rtAJ2WR3ZQtmWbOjZAbG2Eq
-----END RSA PRIVATE KEY-----`
		githubAppKeyFile = "githubAppKeyFile"
	)
	var (
		pathCalled      string
		isBareCalled    bool
		cloneOptsCalled *git.CloneOptions
	)

	sshAuth, _ := gossh.NewPublicKeys("git", []byte(sshPrivateKeyFileContent), "")
	sshKeyComparer := cmp.Comparer(func(x, y gossh.PublicKeys) bool {
		return x.User == y.User &&
			x.Signer.PublicKey().Type() == y.Signer.PublicKey().Type() &&
			cmp.Equal(x.Signer.PublicKey().Marshal(), y.Signer.PublicKey().Marshal())
	})
	plainClone = func(path string, isBare bool, o *git.CloneOptions) (*git.Repository, error) {
		pathCalled = path
		isBareCalled = isBare
		cloneOptsCalled = o

		return &git.Repository{}, nil
	}
	readFile = func(name string) ([]byte, error) {
		if name == passwordFile {
			return []byte(passwordFileContent), nil
		}
		if name == sshPrivateKeyFile {
			return []byte(sshPrivateKeyFileContent), nil
		}
		if name == githubAppKeyFile {
			return []byte(sshPrivateKeyFileContent), nil
		}
		return nil, errors.New("file not found")
	}
	fileStat = func(name string) (os.FileInfo, error) {
		if name == githubAppKeyFile {
			return nil, nil
		}
		return nil, errors.New("file not found")
	}
	origGetter := appAuthGetter
	appAuthGetter = fakeGetter{}
	defer func() {
		plainClone = git.PlainClone
		readFile = os.ReadFile
		fileStat = os.Stat
		appAuthGetter = origGetter
	}()

	tests := map[string]struct {
		opts              *GitCloner
		expectedCloneOpts *git.CloneOptions
		expectedErr       error
	}{
		"branch no auth": {
			opts: &GitCloner{
				Repo:   "https://repo",
				Path:   "path",
				Branch: "master",
			},
			expectedCloneOpts: &git.CloneOptions{
				URL:               "https://repo",
				SingleBranch:      true,
				ReferenceName:     "master",
				RecurseSubmodules: git.DefaultSubmoduleRecursionDepth,
			},
		},
		"branch basic auth": {
			opts: &GitCloner{
				Repo:         "https://repo",
				Path:         "path",
				Branch:       "master",
				Username:     "user",
				PasswordFile: passwordFile,
			},
			expectedCloneOpts: &git.CloneOptions{
				URL:           "https://repo",
				SingleBranch:  true,
				ReferenceName: "master",
				Auth: &httpgit.BasicAuth{
					Username: "user",
					Password: passwordFileContent,
				},
				RecurseSubmodules: git.DefaultSubmoduleRecursionDepth,
			},
		},
		"branch ssh auth": {
			opts: &GitCloner{
				Repo:              "ssh://git@localhost/test/test-repo",
				Path:              "path",
				Branch:            "master",
				SSHPrivateKeyFile: sshPrivateKeyFile,
			},
			expectedCloneOpts: &git.CloneOptions{
				URL:               "ssh://git@localhost/test/test-repo",
				SingleBranch:      true,
				ReferenceName:     "master",
				Auth:              sshAuth,
				RecurseSubmodules: git.DefaultSubmoduleRecursionDepth,
			},
		},
		"branch github app auth": {
			opts: &GitCloner{
				Repo:                  "https://repo",
				Path:                  "path",
				Branch:                "master",
				GitHubAppID:           123,
				GitHubAppInstallation: 456,
				GitHubAppKeyFile:      githubAppKeyFile,
			},
			expectedCloneOpts: &git.CloneOptions{
				URL:           "https://repo",
				SingleBranch:  true,
				ReferenceName: "master",
				Auth: &httpgit.BasicAuth{
					Username: "x-access-token",
					Password: "token",
				},
				RecurseSubmodules: git.DefaultSubmoduleRecursionDepth,
			},
		},
		"password file does not exist": {
			opts: &GitCloner{
				Repo:         "https://repo",
				Branch:       "master",
				PasswordFile: "doesntexist",
				Username:     "user",
			},
			expectedCloneOpts: nil,
			expectedErr:       errors.New(`failed to create auth from options for repo="https://repo" branch="master" revision="" path="": file not found`),
		},
		"ca file does not exist": {
			opts: &GitCloner{
				Repo:         "https://repo",
				Branch:       "master",
				CABundleFile: "doesntexist",
			},
			expectedCloneOpts: nil,
			expectedErr:       errors.New(`failed to read CA bundle from file for repo="https://repo" branch="master" revision="" path="": file not found`),
		},
		"ssh private key file does not exist": {
			opts: &GitCloner{
				Repo:              "https://repo",
				Branch:            "master",
				SSHPrivateKeyFile: "doesntexist",
			},
			expectedCloneOpts: nil,
			expectedErr:       errors.New(`failed to create auth from options for repo="https://repo" branch="master" revision="" path="": file not found`),
		},
		"github app key file does not exist": {
			opts: &GitCloner{
				Repo:                  "https://repo",
				Branch:                "master",
				GitHubAppID:           123,
				GitHubAppInstallation: 456,
				GitHubAppKeyFile:      "doesntexist",
			},
			expectedCloneOpts: nil,
			expectedErr:       errors.New(`failed to create auth from options for repo="https://repo" branch="master" revision="" path="": failed to resolve GitHub app private key from path: file not found`),
		},
	}

	for name, test := range tests {
		// clear mock vars
		pathCalled = ""
		cloneOptsCalled = nil

		t.Run(name, func(t *testing.T) {
			c := Cloner{}
			err := c.CloneRepo(test.opts)
			if test.expectedErr == nil && err != nil {
				t.Fatalf("err unexpected: %v", err)
			}
			if test.expectedErr != nil {
				if err == nil {
					t.Fatalf("err expected to be [%v], got [%v]", test.expectedErr, err)
				}
				if !cmp.Equal(test.expectedErr.Error(), err.Error()) {
					t.Fatalf("err expected to be [%s], got [%s]", test.expectedErr.Error(), err.Error())
				}
			}

			if pathCalled != test.opts.Path {
				t.Fatalf("path expected to be %v, got %v", test.opts.Path, pathCalled)
			}

			if isBareCalled {
				t.Fatalf("isBareCalled expected to be false, got %v", isBareCalled)
			}

			if !cmp.Equal(cloneOptsCalled, test.expectedCloneOpts, sshKeyComparer) {
				t.Fatalf("expected options %v, got %v", test.expectedCloneOpts, cloneOptsCalled)
			}

			if !cmp.Equal(transport.UnsupportedCapabilities, []capability.Capability{capability.ThinPack}) {
				t.Errorf("expected transport.UnsupportedCapabilities []capability.Capability{capability.ThinPack}, got %v", transport.UnsupportedCapabilities)
			}
		})
	}
}



================================================
FILE: internal/cmd/cli/gitcloner/cmd.go
================================================
package gitcloner

import (
	"github.com/spf13/cobra"
)

type CloneGit interface {
	CloneRepo(opts *GitCloner) error
}

type GitCloner struct {
	Repo                  string
	Path                  string
	Branch                string
	Revision              string
	CABundleFile          string
	Username              string
	PasswordFile          string
	SSHPrivateKeyFile     string
	InsecureSkipTLS       bool
	GitHubAppID           int64
	GitHubAppInstallation int64
	GitHubAppKeyFile      string
}

var opts *GitCloner

func NewCmd(gitCloner CloneGit) *cobra.Command {
	cmd := &cobra.Command{
		Use:   "gitcloner [REPO] [PATH]",
		Short: "Clones a git repository",
		Args:  cobra.ExactArgs(2),
		RunE: func(cmd *cobra.Command, args []string) error {
			opts.Repo = args[0]
			opts.Path = args[1]

			return gitCloner.CloneRepo(opts)
		},
	}
	opts = &GitCloner{}
	cmd.Flags().StringVarP(&opts.Branch, "branch", "b", "", "git branch")
	cmd.Flags().StringVar(&opts.Revision, "revision", "", "git revision")
	cmd.Flags().StringVar(&opts.CABundleFile, "ca-bundle-file", "", "CA bundle file")
	cmd.Flags().StringVarP(&opts.Username, "username", "u", "", "user name for basic auth")
	cmd.Flags().StringVar(&opts.PasswordFile, "password-file", "", "password file for basic auth")
	cmd.Flags().StringVar(&opts.SSHPrivateKeyFile, "ssh-private-key-file", "", "ssh private key file path")
	cmd.Flags().BoolVar(&opts.InsecureSkipTLS, "insecure-skip-tls", false, "do not verify tls certificates")
	cmd.Flags().Int64Var(&opts.GitHubAppID, "github-app-id", 0, "GitHub App ID")
	cmd.Flags().Int64Var(&opts.GitHubAppInstallation, "github-app-installation-id", 0, "GitHub App installation ID")
	cmd.Flags().StringVar(&opts.GitHubAppKeyFile, "github-app-key-file", "", "path to GitHub App private-key PEM")

	return cmd
}



================================================
FILE: internal/cmd/cli/gitcloner/cmd_test.go
================================================
package gitcloner

import (
	"testing"
)

func TestArgsAreSet(t *testing.T) {
	mock := &clonerMock{}
	cmd := NewCmd(mock)
	cmd.SetArgs([]string{"test-repo", "test-path", "--branch", "master", "--revision", "v0.1.0", "--ca-bundle-file", "caFile", "--username", "user",
		"--password-file", "passwordFile", "--ssh-private-key-file", "sshFile", "--insecure-skip-tls", "--github-app-id", "123",
		"--github-app-installation-id", "456", "--github-app-key-file", "gitHubAppKeyFile"})
	err := cmd.Execute()
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}
	if mock.opts.Repo != "test-repo" {
		t.Fatalf("expected repo test-repo, got %v", mock.opts.Repo)
	}
	if mock.opts.Path != "test-path" {
		t.Fatalf("expected path test-path, got %v", mock.opts.Path)
	}
	if mock.opts.Branch != "master" {
		t.Fatalf("expected branch master, got %v", mock.opts.Branch)
	}
	if mock.opts.Revision != "v0.1.0" {
		t.Fatalf("expected revision v0.1.0, got %v", mock.opts.Revision)
	}
	if mock.opts.CABundleFile != "caFile" {
		t.Fatalf("expected CABundleFile caFile, got %v", mock.opts.CABundleFile)
	}
	if mock.opts.Username != "user" {
		t.Fatalf("expected Username user, got %v", mock.opts.Username)
	}
	if mock.opts.PasswordFile != "passwordFile" {
		t.Fatalf("expected PasswordFile passwordFile, got %v", mock.opts.PasswordFile)
	}
	if !mock.opts.InsecureSkipTLS {
		t.Fatalf("expected InsecureSkipTLS to be true")
	}
	if mock.opts.GitHubAppID != 123 {
		t.Fatalf("expected GitHubAppID 123, got %v", mock.opts.GitHubAppID)
	}
	if mock.opts.GitHubAppInstallation != 456 {
		t.Fatalf("expected GitHubAppInstallation 456, got %v", mock.opts.GitHubAppInstallation)
	}
	if mock.opts.GitHubAppKeyFile != "gitHubAppKeyFile" {
		t.Fatalf("expected GitHubAppKeyFile gitHubAppKeyFile, got %v", mock.opts.GitHubAppKeyFile)
	}
}

type clonerMock struct {
	opts *GitCloner
}

func (m *clonerMock) CloneRepo(opts *GitCloner) error {
	m.opts = opts

	return nil
}



================================================
FILE: internal/cmd/cli/match/match.go
================================================
// Package match is used to test matching a bundles to a target on the command line.
//
// It's not used by fleet, but it is available in the fleet CLI as "test" sub
// command. The tests in fleet-examples use it.
package match

import (
	"bytes"
	"context"
	"errors"
	"fmt"
	"io"
	"os"

	"github.com/rancher/fleet/internal/bundlereader"
	"github.com/rancher/fleet/internal/cmd/controller/options"
	"github.com/rancher/fleet/internal/cmd/controller/target/matcher"
	"github.com/rancher/fleet/internal/helmdeployer"
	"github.com/rancher/fleet/internal/manifest"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"github.com/rancher/wrangler/v3/pkg/yaml"
)

type Options struct {
	Output             io.Writer
	BaseDir            string
	BundleSpec         string
	BundleFile         string
	ClusterName        string
	ClusterGroup       string
	ClusterLabels      map[string]string
	ClusterGroupLabels map[string]string
	Target             string
}

func Match(ctx context.Context, opts *Options) error {
	if opts == nil {
		opts = &Options{}
	}

	var (
		bundle *fleet.Bundle
		err    error
	)

	if opts.BundleFile == "" {
		bundle, _, err = bundlereader.NewBundle(ctx, "test", opts.BaseDir, opts.BundleSpec, nil)
		if err != nil {
			return err
		}
	} else {
		data, err := os.ReadFile(opts.BundleFile)
		if err != nil {
			return err
		}

		bundle = &fleet.Bundle{}
		if err := yaml.Unmarshal(data, bundle); err != nil {
			return err
		}
	}

	bm, err := matcher.New(bundle)
	if err != nil {
		return err
	}

	if opts.Target == "" {
		m := bm.Match(opts.ClusterName, map[string]map[string]string{
			opts.ClusterGroup: opts.ClusterGroupLabels,
		}, opts.ClusterLabels)
		return printMatch(ctx, bundle, m, opts.Output)
	}

	return printMatch(ctx, bundle, bm.MatchForTarget(opts.Target), opts.Output)
}

func printMatch(ctx context.Context, bundle *fleet.Bundle, target *fleet.BundleTarget, output io.Writer) error {
	if target == nil {
		return errors.New("no match found")
	}
	fmt.Fprintf(os.Stderr, "# Matched: %s\n", target.Name)
	if output == nil {
		return nil
	}

	opts := options.Merge(bundle.Spec.BundleDeploymentOptions, target.BundleDeploymentOptions)

	manifest := manifest.New(bundle.Spec.Resources)

	rel, err := helmdeployer.Template(ctx, bundle.Name, manifest, opts, "")
	if err != nil {
		return err
	}

	objs, err := yaml.ToObjects(bytes.NewBufferString(rel.Manifest))
	if err != nil {
		return err
	}

	data, err := yaml.Export(objs...)
	if err != nil {
		return err
	}

	_, err = io.Copy(output, bytes.NewBuffer(data))
	return err
}



================================================
FILE: internal/cmd/cli/writer/writer.go
================================================
// Package writer provides a writer that can be used to write to a file or stdout.
package writer

import (
	"io"
	"os"
	"path/filepath"
)

type nopCloser struct {
	io.Writer
}

func (nopCloser) Close() error { return nil }

func NewDefaultNone(output string) io.WriteCloser {
	if output == "" {
		return nil
	}
	return New(output)
}

func New(output string) io.WriteCloser {
	switch output {
	case "":
		return nopCloser{Writer: io.Discard}
	case "-":
		return os.Stdout
	default:
		return &lazyFileWriter{
			path: output,
		}
	}
}

type lazyFileWriter struct {
	path string
	file *os.File
}

func (l *lazyFileWriter) Write(data []byte) (int, error) {
	if l.file == nil {
		dir := filepath.Dir(l.path)
		if err := os.MkdirAll(dir, 0755); err != nil {
			return 0, err
		}
		f, err := os.Create(l.path)
		if err != nil {
			return 0, err
		}
		l.file = f
	}
	return l.file.Write(data)
}

func (l *lazyFileWriter) Close() error {
	if l.file == nil {
		return nil
	}
	return l.file.Close()
}



================================================
FILE: internal/cmd/controller/operator.go
================================================
package controller

import (
	"context"
	"fmt"

	"github.com/reugn/go-quartz/quartz"

	"github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/cmd/controller/reconciler"
	"github.com/rancher/fleet/internal/cmd/controller/target"
	"github.com/rancher/fleet/internal/experimental"
	"github.com/rancher/fleet/internal/manifest"
	"github.com/rancher/fleet/internal/metrics"
	"github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"k8s.io/apimachinery/pkg/runtime"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	clientgoscheme "k8s.io/client-go/kubernetes/scheme"
	"k8s.io/client-go/rest"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/healthz"
	metricsserver "sigs.k8s.io/controller-runtime/pkg/metrics/server"
)

var (
	scheme = runtime.NewScheme()
)

func init() {
	utilruntime.Must(clientgoscheme.AddToScheme(scheme))
	utilruntime.Must(v1alpha1.AddToScheme(scheme))
	//+kubebuilder:scaffold:scheme
}

func start(
	ctx context.Context,
	systemNamespace string,
	config *rest.Config,
	leaderElection bool,
	leaderOpts cmd.LeaderElectionOptions,
	workersOpts ControllerReconcilerWorkers,
	bindAddresses BindAddresses,
	disableMetrics bool,
	shardID string,
) error {
	setupLog.Info("listening for changes on local cluster",
		"disableMetrics", disableMetrics,
	)

	var metricServerOptions metricsserver.Options
	if disableMetrics {
		metricServerOptions = metricsserver.Options{BindAddress: "0"}
	} else {
		metricServerOptions = metricsserver.Options{BindAddress: bindAddresses.Metrics}
		metrics.RegisterMetrics()
	}

	var leaderElectionSuffix string
	if shardID != "" {
		leaderElectionSuffix = fmt.Sprintf("-%s", shardID)
	}

	mgr, err := ctrl.NewManager(config, ctrl.Options{
		Scheme:                 scheme,
		Metrics:                metricServerOptions,
		HealthProbeBindAddress: bindAddresses.HealthProbe,

		LeaderElection:          leaderElection,
		LeaderElectionID:        fmt.Sprintf("fleet-controller-leader-election-shard%s", leaderElectionSuffix),
		LeaderElectionNamespace: systemNamespace,
		LeaseDuration:           &leaderOpts.LeaseDuration,
		RenewDeadline:           &leaderOpts.RenewDeadline,
		RetryPeriod:             &leaderOpts.RetryPeriod,
	})
	if err != nil {
		setupLog.Error(err, "unable to start manager")
		return err
	}

	// Set up the config reconciler
	if err = (&reconciler.ConfigReconciler{
		Client: mgr.GetClient(),
		Scheme: mgr.GetScheme(),

		SystemNamespace: systemNamespace,
		ShardID:         shardID,
	}).SetupWithManager(mgr); err != nil {
		setupLog.Error(err, "unable to create controller", "controller", "ConfigMap")
		return err
	}

	// bundle related controllers
	store := manifest.NewStore(mgr.GetClient())
	builder := target.New(mgr.GetClient(), mgr.GetAPIReader())

	if err = (&reconciler.ClusterReconciler{
		Client: mgr.GetClient(),
		Scheme: mgr.GetScheme(),

		Query:   builder,
		ShardID: shardID,

		Workers: workersOpts.Cluster,
	}).SetupWithManager(mgr); err != nil {
		setupLog.Error(err, "unable to create controller", "controller", "Cluster")
		return err
	}

	var shardIDSuffix string
	if shardID != "" {
		shardIDSuffix = fmt.Sprintf("-%s", shardID)
	}
	if err = (&reconciler.BundleReconciler{
		Client:   mgr.GetClient(),
		Scheme:   mgr.GetScheme(),
		Recorder: mgr.GetEventRecorderFor(fmt.Sprintf("fleet-bundle-ctrl%s", shardIDSuffix)),

		Builder: builder,
		Store:   store,
		Query:   builder,
		ShardID: shardID,

		Workers: workersOpts.Bundle,
	}).SetupWithManager(mgr); err != nil {
		setupLog.Error(err, "unable to create controller", "controller", "Bundle")
		return err
	}

	sched, err := quartz.NewStdScheduler()
	if err != nil {
		return fmt.Errorf("failed to create scheduler: %w", err)
	}

	// controllers that update status.display
	if err = (&reconciler.ClusterGroupReconciler{
		Client:  mgr.GetClient(),
		Scheme:  mgr.GetScheme(),
		ShardID: shardID,

		Workers: workersOpts.ClusterGroup,
	}).SetupWithManager(mgr); err != nil {
		setupLog.Error(err, "unable to create controller", "controller", "ClusterGroup")
		return err
	}

	if err = (&reconciler.BundleDeploymentReconciler{
		Client:  mgr.GetClient(),
		Scheme:  mgr.GetScheme(),
		ShardID: shardID,

		Workers: workersOpts.BundleDeployment,
	}).SetupWithManager(mgr); err != nil {
		setupLog.Error(err, "unable to create controller", "controller", "BundleDeployment")
		return err
	}

	// imagescan controller
	if err = (&reconciler.ImageScanReconciler{
		Client: mgr.GetClient(),
		Scheme: mgr.GetScheme(),

		Scheduler: sched,
		ShardID:   shardID,

		Workers: workersOpts.ImageScan,
	}).SetupWithManager(mgr); err != nil {
		setupLog.Error(err, "unable to create controller", "controller", "ImageScan")
		return err
	}

	if experimental.SchedulesEnabled() {
		if err = (&reconciler.ScheduleReconciler{
			Client:   mgr.GetClient(),
			Scheme:   mgr.GetScheme(),
			Recorder: mgr.GetEventRecorderFor(fmt.Sprintf("fleet-schedule-ctrl%s", shardIDSuffix)),
			ShardID:  shardID,

			Workers:   workersOpts.Schedule,
			Scheduler: sched,
		}).SetupWithManager(mgr); err != nil {
			setupLog.Error(err, "unable to create controller", "controller", "Schedule")
			return err
		}
	}

	//+kubebuilder:scaffold:builder

	if err := reconciler.Load(ctx, mgr.GetAPIReader(), systemNamespace); err != nil {
		setupLog.Error(err, "failed to load config")
		return err
	}

	if err := mgr.AddHealthzCheck("healthz", healthz.Ping); err != nil {
		setupLog.Error(err, "unable to set up health check")
		return err
	}
	if err := mgr.AddReadyzCheck("readyz", healthz.Ping); err != nil {
		setupLog.Error(err, "unable to set up ready check")
		return err
	}

	setupLog.Info("starting job scheduler")
	jobCtx, cancel := context.WithCancel(ctx)
	defer cancel()
	sched.Start(jobCtx)

	setupLog.Info("starting manager")
	if err := mgr.Start(ctx); err != nil {
		setupLog.Error(err, "problem running manager")
		return err

	}

	sched.Stop()

	return nil
}



================================================
FILE: internal/cmd/controller/root.go
================================================
// Package controller starts the fleet controller.
package controller

import (
	"flag"
	"fmt"
	"log"
	"net/http"
	"os"
	"strconv"

	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement"
	"github.com/rancher/fleet/internal/cmd/controller/gitops"
	"github.com/rancher/fleet/internal/cmd/controller/helmops"

	"github.com/spf13/cobra"

	ctrl "sigs.k8s.io/controller-runtime"
	clog "sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"

	command "github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/cmd/controller/cleanup"
	"github.com/rancher/fleet/pkg/version"
)

type FleetController struct {
	command.DebugConfig
	Kubeconfig           string `usage:"Kubeconfig file"`
	Namespace            string `usage:"namespace to watch" default:"cattle-fleet-system" env:"NAMESPACE"`
	DisableMetrics       bool   `usage:"disable metrics" name:"disable-metrics"`
	ShardID              string `usage:"only manage resources labeled with a specific shard ID" name:"shard-id"`
	EnableLeaderElection bool   `name:"leader-elect" default:"true" usage:"Enable leader election for controller manager. Enabling this will ensure there is only one active controller manager."`
}

type ControllerReconcilerWorkers struct {
	Bundle           int
	BundleDeployment int
	Cluster          int
	ClusterGroup     int
	ImageScan        int
	Schedule         int
}

type BindAddresses struct {
	Metrics     string
	HealthProbe string
}

var (
	setupLog = ctrl.Log.WithName("setup")
	zopts    = &zap.Options{
		Development: true,
	}
)

func (f *FleetController) PersistentPre(_ *cobra.Command, _ []string) error {
	if err := f.SetupDebug(); err != nil {
		return fmt.Errorf("failed to setup debug logging: %w", err)
	}
	zopts = f.OverrideZapOpts(zopts)

	return nil
}

func (f *FleetController) Run(cmd *cobra.Command, args []string) error {
	ctrl.SetLogger(zap.New(zap.UseFlagOptions(zopts)))
	ctx := clog.IntoContext(cmd.Context(), ctrl.Log)

	kubeconfig := ctrl.GetConfigOrDie()
	workersOpts := ControllerReconcilerWorkers{}

	leaderOpts, err := command.NewLeaderElectionOptions()
	if err != nil {
		return err
	}

	bindAddresses := BindAddresses{
		Metrics:     ":8080",
		HealthProbe: ":8081",
	}
	if d := os.Getenv("FLEET_METRICS_BIND_ADDRESS"); d != "" {
		bindAddresses.Metrics = d
	}
	if d := os.Getenv("FLEET_HEALTHPROBE_BIND_ADDRESS"); d != "" {
		bindAddresses.HealthProbe = d
	}

	if d := os.Getenv("BUNDLE_RECONCILER_WORKERS"); d != "" {
		w, err := strconv.Atoi(d)
		if err != nil {
			setupLog.Error(err, "failed to parse BUNDLE_RECONCILER_WORKERS", "value", d)
		}
		workersOpts.Bundle = w
	}

	if d := os.Getenv("BUNDLEDEPLOYMENT_RECONCILER_WORKERS"); d != "" {
		w, err := strconv.Atoi(d)
		if err != nil {
			setupLog.Error(err, "failed to parse BUNDLEDEPLOYMENT_RECONCILER_WORKERS", "value", d)
		}
		workersOpts.BundleDeployment = w
	}

	if d := os.Getenv("CLUSTER_RECONCILER_WORKERS"); d != "" {
		w, err := strconv.Atoi(d)
		if err != nil {
			setupLog.Error(err, "failed to parse CLUSTER_RECONCILER_WORKERS", "value", d)
		}
		workersOpts.Cluster = w
	}

	if d := os.Getenv("CLUSTERGROUP_RECONCILER_WORKERS"); d != "" {
		w, err := strconv.Atoi(d)
		if err != nil {
			setupLog.Error(err, "failed to parse CLUSTERGROUP_RECONCILER_WORKERS", "value", d)
		}
		workersOpts.ClusterGroup = w
	}

	if d := os.Getenv("IMAGESCAN_RECONCILER_WORKERS"); d != "" {
		w, err := strconv.Atoi(d)
		if err != nil {
			setupLog.Error(err, "failed to parse IMAGESCAN_RECONCILER_WORKERS", "value", d)
		}
		workersOpts.ImageScan = w
	}

	if d := os.Getenv("SCHEDULE_RECONCILER_WORKERS"); d != "" {
		w, err := strconv.Atoi(d)
		if err != nil {
			setupLog.Error(err, "failed to parse SCHEDULE_RECONCILER_WORKERS", "value", d)
		}
		workersOpts.Schedule = w
	}

	go func() {
		log.Println(http.ListenAndServe("localhost:6060", nil)) //nolint:gosec // Debugging only
	}()
	if err := start(
		ctx,
		f.Namespace,
		kubeconfig,
		f.EnableLeaderElection,
		leaderOpts,
		workersOpts,
		bindAddresses,
		f.DisableMetrics,
		f.ShardID,
	); err != nil {
		return err
	}

	<-cmd.Context().Done()
	return nil
}

func App() *cobra.Command {
	root := command.Command(&FleetController{}, cobra.Command{
		Version: version.FriendlyVersion(),
	})
	fs := flag.NewFlagSet("", flag.ExitOnError)
	zopts.BindFlags(fs)
	ctrl.RegisterFlags(fs)
	root.Flags().AddGoFlagSet(fs)

	root.AddCommand(
		cleanup.App(),
		agentmanagement.App(),
		gitops.App(zopts),
		helmops.App(zopts),
	)
	return root
}



================================================
FILE: internal/cmd/controller/agentmanagement/root.go
================================================
package agentmanagement

import (
	"fmt"
	"os"
	"strconv"

	command "github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/agent"
	"github.com/rancher/fleet/pkg/version"
	"github.com/spf13/cobra"
)

type AgentManagement struct {
	command.DebugConfig
	Kubeconfig       string `usage:"kubeconfig file"`
	Namespace        string `usage:"namespace to watch" env:"NAMESPACE"`
	DisableBootstrap bool   `usage:"disable local cluster components" name:"disable-bootstrap"`
}

// HelpFunc hides the global flag from the help output
func (a *AgentManagement) HelpFunc(cmd *cobra.Command, strings []string) {
	_ = cmd.Flags().MarkHidden("disable-metrics")
	_ = cmd.Flags().MarkHidden("shard-id")
	cmd.Parent().HelpFunc()(cmd, strings)
}

func (a *AgentManagement) PersistentPre(_ *cobra.Command, _ []string) error {
	// if debug is enabled in controller, enable in agents too (unless otherwise specified)
	propagateDebug, _ := strconv.ParseBool(os.Getenv("FLEET_PROPAGATE_DEBUG_SETTINGS_TO_AGENTS"))
	if propagateDebug && a.Debug {
		agent.DebugEnabled = true
		agent.DebugLevel = a.DebugLevel
	}

	disableSecurityContext, _ := strconv.ParseBool(os.Getenv("FLEET_DEBUG_DISABLE_SECURITY_CONTEXT"))
	if propagateDebug && disableSecurityContext {
		agent.DisableSecurityContext = true
	}

	return nil
}

func (a *AgentManagement) Run(cmd *cobra.Command, args []string) error {
	if a.Namespace == "" {
		return fmt.Errorf("--namespace or env NAMESPACE is required to be set")
	}
	return start(cmd.Context(), a.Kubeconfig, a.Namespace, a.DisableBootstrap)
}

func App() *cobra.Command {
	return command.Command(&AgentManagement{}, cobra.Command{
		Version: version.FriendlyVersion(),
		Use:     "agentmanagement",
	})
}



================================================
FILE: internal/cmd/controller/agentmanagement/start.go
================================================
package agentmanagement

import (
	"context"

	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/controllers"

	"github.com/rancher/wrangler/v3/pkg/kubeconfig"
	"github.com/rancher/wrangler/v3/pkg/leader"
	"github.com/rancher/wrangler/v3/pkg/schemes"

	"github.com/sirupsen/logrus"

	v1 "k8s.io/api/apps/v1"
	policyv1 "k8s.io/api/policy/v1"
	schedulingv1 "k8s.io/api/scheduling/v1"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
)

func start(ctx context.Context, kubeConfig, namespace string, disableBootstrap bool) error {
	clientConfig := kubeconfig.GetNonInteractiveClientConfig(kubeConfig)
	kc, err := clientConfig.ClientConfig()
	if err != nil {
		return err
	}

	// try to claim leadership lease without rate limiting
	localConfig := rest.CopyConfig(kc)
	localConfig.QPS = -1
	localConfig.RateLimiter = nil
	k8s, err := kubernetes.NewForConfig(localConfig)
	if err != nil {
		return err
	}

	// Register all Kinds we apply dynamically so their GVKs resolve
	err = schemes.Register(v1.AddToScheme)
	if err != nil {
		return err
	}
	if err = schemes.Register(policyv1.AddToScheme); err != nil {
		return err
	}
	if err = schemes.Register(schedulingv1.AddToScheme); err != nil {
		return err
	}

	leader.RunOrDie(ctx, namespace, "fleet-agentmanagement-lock", k8s, func(ctx context.Context) {
		appCtx, err := controllers.NewAppContext(clientConfig)
		if err != nil {
			logrus.Fatal(err)
		}
		if err := controllers.Register(ctx, appCtx, namespace, disableBootstrap); err != nil {
			logrus.Fatal(err)
		}
		if err := appCtx.Start(ctx); err != nil {
			logrus.Fatal(err)
		}
	})

	return nil
}



================================================
FILE: internal/cmd/controller/agentmanagement/agent/agent.go
================================================
// Package agent builds manifests for creating a managed fleet-agent.
package agent

import (
	"bytes"
	"context"
	"fmt"
	"time"

	"github.com/rancher/fleet/internal/client"
	"github.com/rancher/fleet/internal/cmd"
	fleetns "github.com/rancher/fleet/internal/cmd/controller/namespace"
	"github.com/rancher/fleet/internal/config"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"
	fleetcontrollers "github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io/v1alpha1"

	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/util/yaml"
	"k8s.io/apimachinery/pkg/watch"
)

type Options struct {
	ManifestOptions
	ConfigOptions
	APIServerCA  []byte
	APIServerURL string
	NoCA         bool // unused
}

// AgentWithConfig returns the agent manifest. It includes an updated agent
// token secret from the cluster. It finds or creates the agent config inside a
// configmap.
//
// This is used when importing a cluster.
func AgentWithConfig(ctx context.Context, agentNamespace, controllerNamespace, agentScope string, cg *client.Getter, tokenName string, opts *Options) ([]runtime.Object, error) {
	if opts == nil {
		opts = &Options{}
	}

	objs := []runtime.Object{
		&v1.Namespace{ObjectMeta: metav1.ObjectMeta{Name: agentNamespace}},
	}

	client, err := cg.Get()
	if err != nil {
		return objs, err
	}

	secret, err := agentBootstrapSecret(ctx, agentNamespace, controllerNamespace, client, tokenName, opts)
	if err != nil {
		return objs, err
	}

	objs = append(objs, secret)

	agentConfig, err := agentConfig(ctx, agentNamespace, controllerNamespace, cg, &opts.ConfigOptions)
	if err != nil {
		return objs, err
	}

	objs = append(objs, agentConfig...)

	// get a fresh config from the API
	cfg, err := config.Lookup(ctx, controllerNamespace, config.ManagerConfigName, client.Core.ConfigMap())
	if err != nil {
		return objs, err
	}

	// keep in sync with manageagent.go
	mo := opts.ManifestOptions
	mo.AgentImage = cfg.AgentImage
	mo.AgentImagePullPolicy = cfg.AgentImagePullPolicy
	mo.CheckinInterval = cfg.AgentCheckinInterval.Duration.String()
	mo.SystemDefaultRegistry = cfg.SystemDefaultRegistry
	mo.BundleDeploymentWorkers = cfg.AgentWorkers.BundleDeployment
	mo.DriftWorkers = cfg.AgentWorkers.Drift

	mo.AgentReplicas = cmd.ParseEnvAgentReplicaCount()
	mo.LeaderElectionOptions, err = cmd.NewLeaderElectionOptionsWithPrefix("FLEET_AGENT")
	if err != nil {
		return objs, err
	}

	objs = append(objs, Manifest(agentNamespace, agentScope, mo)...)

	return objs, err
}

// agentBootstrapSecret creates the fleet-agent-bootstrap secret from the
// import-token-<clusterName> secret and adds the APIServer options.
func agentBootstrapSecret(ctx context.Context, agentNamespace, controllerNamespace string, client *client.Client, tokenName string, opts *Options) (*v1.Secret, error) {
	data, err := getToken(ctx, controllerNamespace, tokenName, client)
	if err != nil {
		return nil, err
	}

	if opts.APIServerURL != "" {
		data[config.APIServerURLKey] = []byte(opts.APIServerURL)
	}
	if len(opts.APIServerCA) > 0 {
		data[config.APIServerCAKey] = opts.APIServerCA
	}

	return &v1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:      config.AgentBootstrapConfigName,
			Namespace: agentNamespace,
		},
		Data: data,
	}, nil
}

// getToken load the import-token-local secret and
// check system registration namespace is expected
func getToken(ctx context.Context, controllerNamespace, tokenName string, client *client.Client) (map[string][]byte, error) {
	secretName, err := waitForSecretName(ctx, tokenName, client)
	if err != nil {
		return nil, err
	}

	secret, err := client.Core.Secret().Get(client.Namespace, secretName, metav1.GetOptions{})
	if err != nil {
		return nil, err
	}

	// unmarshal kubeconfig yaml from values key
	values := secret.Data[config.ImportTokenSecretValuesKey]
	if len(values) == 0 {
		return nil, fmt.Errorf("failed to find \"values\" on secret %s/%s", client.Namespace, secretName)
	}

	data := map[string]interface{}{}
	if err := yaml.NewYAMLToJSONDecoder(bytes.NewBuffer(values)).Decode(&data); err != nil {
		return nil, err
	}

	if _, ok := data["token"]; !ok {
		return nil, fmt.Errorf("failed to find token in values")
	}

	expectedNamespace := fleetns.SystemRegistrationNamespace(controllerNamespace)
	actualNamespace := data["systemRegistrationNamespace"]
	if actualNamespace != expectedNamespace {
		return nil, fmt.Errorf("registration namespace (%s) from secret (%s/%s) does not match expected: %s", actualNamespace, secret.Namespace, secret.Name, expectedNamespace)
	}

	byteData := map[string][]byte{}
	for k, v := range data {
		if s, ok := v.(string); ok {
			byteData[k] = []byte(s)
		}
	}

	return byteData, nil
}

func waitForSecretName(ctx context.Context, tokenName string, client *client.Client) (string, error) {
	watcher, err := startWatch(client.Namespace, client.Fleet.ClusterRegistrationToken())
	if err != nil {
		return "", err
	}
	defer func() {
		watcher.Stop()
		for range watcher.ResultChan() {
			// drain the channel
		}
	}()

	crt, err := client.Fleet.ClusterRegistrationToken().Get(client.Namespace, tokenName, metav1.GetOptions{})
	if err != nil {
		return "", fmt.Errorf("failed to lookup token %s: %w", tokenName, err)
	}
	if crt.Status.SecretName != "" {
		return crt.Status.SecretName, nil
	}

	timeout := time.After(durations.AgentSecretTimeout)
	for {
		var event watch.Event
		select {
		case <-timeout:
			return "", fmt.Errorf("timeout getting credential for cluster group")
		case <-ctx.Done():
			return "", ctx.Err()
		case event = <-watcher.ResultChan():
		}

		if newCGT, ok := event.Object.(*fleet.ClusterRegistrationToken); ok {
			if newCGT.UID != crt.UID || newCGT.Status.SecretName == "" {
				continue
			}
			return newCGT.Status.SecretName, nil
		}
	}
}

func startWatch(namespace string, sa fleetcontrollers.ClusterRegistrationTokenClient) (watch.Interface, error) {
	secrets, err := sa.List(namespace, metav1.ListOptions{
		Limit: 1,
	})
	if err != nil {
		return nil, err
	}
	return sa.Watch(namespace, metav1.ListOptions{ResourceVersion: secrets.ResourceVersion})
}



================================================
FILE: internal/cmd/controller/agentmanagement/agent/config.go
================================================
package agent

import (
	"context"

	"github.com/rancher/fleet/internal/client"
	"github.com/rancher/fleet/internal/config"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
)

type ConfigOptions struct {
	Labels                    map[string]string
	ClientID                  string
	AgentTLSMode              string
	GarbageCollectionInterval metav1.Duration
}

func agentConfig(ctx context.Context, agentNamespace, controllerNamespace string, cg *client.Getter, opts *ConfigOptions) ([]runtime.Object, error) {
	if opts == nil {
		opts = &ConfigOptions{}
	}

	client, err := cg.Get()
	if err != nil {
		return nil, err
	}

	// sanity test the controllerNamespace is correct
	_, err = config.Lookup(ctx, controllerNamespace, config.ManagerConfigName, client.Core.ConfigMap())
	if err != nil {
		return nil, err
	}

	return configObjects(agentNamespace, opts)
}

func configObjects(controllerNamespace string, co *ConfigOptions) ([]runtime.Object, error) {
	cm, err := config.ToConfigMap(controllerNamespace, config.AgentConfigName, &config.Config{
		Labels:                    co.Labels,
		ClientID:                  co.ClientID,
		AgentTLSMode:              co.AgentTLSMode,
		GarbageCollectionInterval: co.GarbageCollectionInterval,
	})
	if err != nil {
		return nil, err
	}
	cm.Name = "fleet-agent"
	return []runtime.Object{
		&corev1.Namespace{
			ObjectMeta: metav1.ObjectMeta{
				Name: controllerNamespace,
			},
		},
		cm,
	}, nil
}



================================================
FILE: internal/cmd/controller/agentmanagement/agent/manifest.go
================================================
package agent

import (
	"fmt"
	"path"
	"strconv"
	"strings"

	"github.com/sirupsen/logrus"

	"github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/config"
	"github.com/rancher/fleet/internal/experimental"
	"github.com/rancher/fleet/internal/names"

	appsv1 "k8s.io/api/apps/v1"
	corev1 "k8s.io/api/core/v1"
	networkv1 "k8s.io/api/networking/v1"
	rbacv1 "k8s.io/api/rbac/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
)

var (
	DebugEnabled           bool
	DebugLevel             = 0
	DisableSecurityContext bool
)

const (
	DefaultName = "fleet-agent"
)

type ManifestOptions struct {
	AgentEnvVars            []corev1.EnvVar
	AgentImage              string // DefaultAgentImage = "rancher/fleet-agent" + ":" + version.Version
	AgentImagePullPolicy    string
	AgentTolerations        []corev1.Toleration
	AgentReplicas           int32
	CheckinInterval         string
	PrivateRepoURL          string // PrivateRepoURL = registry.yourdomain.com:5000
	SystemDefaultRegistry   string
	AgentAffinity           *corev1.Affinity
	AgentResources          *corev1.ResourceRequirements
	HostNetwork             bool
	BundleDeploymentWorkers string
	DriftWorkers            string
	cmd.LeaderElectionOptions
	PriorityClassName string
}

// Manifest builds and returns a deployment manifest for the fleet-agent with a
// cluster role, two service accounts and a network policy
//
// It allows the downstream agent to create any resource on its cluster.
//
// This is called by both, import and manageagent.
func Manifest(namespace string, agentScope string, opts ManifestOptions) []runtime.Object {
	if opts.AgentImage == "" {
		opts.AgentImage = config.DefaultAgentImage
	}

	admin := serviceAccount(namespace, DefaultName)

	logrus.Debugf("Building manifest for fleet-agent in namespace %s (sa: %s)", namespace, admin.Name)

	defaultSa := serviceAccount(namespace, "default")
	defaultSa.AutomountServiceAccountToken = new(bool)

	clusterRole := []runtime.Object{
		&rbacv1.ClusterRole{
			ObjectMeta: metav1.ObjectMeta{
				Name: names.SafeConcatName(admin.Namespace, admin.Name, "role"),
			},
			Rules: []rbacv1.PolicyRule{
				{
					Verbs:     []string{rbacv1.VerbAll},
					APIGroups: []string{rbacv1.APIGroupAll},
					Resources: []string{rbacv1.ResourceAll},
				},
				{
					Verbs:           []string{rbacv1.VerbAll},
					NonResourceURLs: []string{rbacv1.ResourceAll},
				},
			},
		},
		&rbacv1.ClusterRoleBinding{
			ObjectMeta: metav1.ObjectMeta{
				Name: names.SafeConcatName(admin.Namespace, admin.Name, "role", "binding"),
			},
			Subjects: []rbacv1.Subject{
				{
					Kind:      "ServiceAccount",
					Name:      admin.Name,
					Namespace: admin.Namespace,
				},
			},
			RoleRef: rbacv1.RoleRef{
				APIGroup: rbacv1.GroupName,
				Kind:     "ClusterRole",
				Name:     names.SafeConcatName(admin.Namespace, admin.Name, "role"),
			},
		},
	}

	agent := agentApp(namespace, agentScope, opts)

	networkPolicy := &networkv1.NetworkPolicy{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "default-allow-all",
			Namespace: namespace,
		},
		Spec: networkv1.NetworkPolicySpec{
			PolicyTypes: []networkv1.PolicyType{
				networkv1.PolicyTypeIngress,
				networkv1.PolicyTypeEgress,
			},
			Ingress: []networkv1.NetworkPolicyIngressRule{
				{},
			},
			Egress: []networkv1.NetworkPolicyEgressRule{
				{},
			},
			PodSelector: metav1.LabelSelector{},
		},
	}

	var objs []runtime.Object
	objs = append(objs, clusterRole...)
	objs = append(objs, admin, defaultSa, agent, networkPolicy)

	return objs
}

func Resolve(global, prefix, image string) string {
	if global != "" && prefix != "" {
		image = strings.TrimPrefix(image, global)
	}
	if prefix != "" && !strings.HasPrefix(image, prefix) {
		return path.Join(prefix, image)
	}

	return image
}

func agentApp(namespace string, agentScope string, opts ManifestOptions) *appsv1.Deployment {
	name := DefaultName
	serviceAccount := DefaultName
	image := Resolve(opts.SystemDefaultRegistry, opts.PrivateRepoURL, opts.AgentImage)
	replicas := opts.AgentReplicas

	app := &appsv1.Deployment{
		ObjectMeta: metav1.ObjectMeta{
			Namespace: namespace,
			Name:      name,
		},
		Spec: appsv1.DeploymentSpec{
			Replicas: &replicas,
			Selector: &metav1.LabelSelector{
				MatchLabels: map[string]string{
					"app": name,
				},
			},
			Template: corev1.PodTemplateSpec{
				ObjectMeta: metav1.ObjectMeta{
					Labels: map[string]string{
						"app": name,
					},
				},
				Spec: corev1.PodSpec{
					PriorityClassName:  opts.PriorityClassName,
					ServiceAccountName: serviceAccount,
					Containers: []corev1.Container{
						{
							Name:            name,
							Image:           image,
							ImagePullPolicy: corev1.PullPolicy(opts.AgentImagePullPolicy),
							Env: []corev1.EnvVar{
								{
									Name:  "BUNDLEDEPLOYMENT_RECONCILER_WORKERS",
									Value: opts.BundleDeploymentWorkers,
								},
								{
									Name:  "DRIFT_RECONCILER_WORKERS",
									Value: opts.DriftWorkers,
								},
								{
									Name: "NAMESPACE",
									ValueFrom: &corev1.EnvVarSource{
										FieldRef: &corev1.ObjectFieldSelector{
											FieldPath: "metadata.namespace",
										},
									},
								},
								{Name: "AGENT_SCOPE", Value: agentScope},
								{Name: "CHECKIN_INTERVAL", Value: opts.CheckinInterval},
								{Name: "CATTLE_ELECTION_LEASE_DURATION", Value: opts.LeaseDuration.String()},
								{Name: "CATTLE_ELECTION_RETRY_PERIOD", Value: opts.RetryPeriod.String()},
								{Name: "CATTLE_ELECTION_RENEW_DEADLINE", Value: opts.RenewDeadline.String()},
								{Name: "EXPERIMENTAL_COPY_RESOURCES_DOWNSTREAM", Value: fmt.Sprintf("%t", experimental.CopyResourcesDownstreamEnabled())},
							},
							Command: []string{
								"fleetagent",
							},
							VolumeMounts: []corev1.VolumeMount{
								{
									Name:      "kube",
									MountPath: "/.kube",
								},
								{
									Name:      "tmp",
									MountPath: "/tmp",
								},
							},
						},
					},
					Volumes: []corev1.Volume{
						{
							Name: "kube",
							VolumeSource: corev1.VolumeSource{
								EmptyDir: &corev1.EmptyDirVolumeSource{},
							},
						},
						{
							Name: "tmp",
							VolumeSource: corev1.VolumeSource{
								EmptyDir: &corev1.EmptyDirVolumeSource{},
							},
						},
					},
					NodeSelector: map[string]string{"kubernetes.io/os": "linux"},
					Affinity: &corev1.Affinity{
						NodeAffinity: &corev1.NodeAffinity{
							PreferredDuringSchedulingIgnoredDuringExecution: []corev1.PreferredSchedulingTerm{
								{
									Weight: 1,
									Preference: corev1.NodeSelectorTerm{
										MatchExpressions: []corev1.NodeSelectorRequirement{
											{
												Key:      "fleet.cattle.io/agent",
												Operator: corev1.NodeSelectorOpIn,
												Values:   []string{"true"},
											},
										},
									},
								},
							},
						},
					},
					Tolerations: []corev1.Toleration{
						{
							Key:      "node.cloudprovider.kubernetes.io/uninitialized",
							Operator: corev1.TolerationOpEqual,
							Value:    "true",
							Effect:   corev1.TaintEffectNoSchedule,
						},
						{
							Key:      "cattle.io/os",
							Operator: corev1.TolerationOpEqual,
							Value:    "linux",
							Effect:   corev1.TaintEffectNoSchedule,
						},
					},
				},
			},
		},
	}

	if !DisableSecurityContext {
		app.Spec.Template.Spec.SecurityContext = &corev1.PodSecurityContext{
			RunAsNonRoot: &[]bool{true}[0],
			RunAsUser:    &[]int64{1000}[0],
			RunAsGroup:   &[]int64{1000}[0],
		}
	}

	// additional tolerations from cluster
	app.Spec.Template.Spec.Tolerations = append(app.Spec.Template.Spec.Tolerations, opts.AgentTolerations...)

	// Set hostNetwork
	app.Spec.Template.Spec.HostNetwork = opts.HostNetwork

	// overwrite affinity if present on cluster
	if opts.AgentAffinity != nil {
		app.Spec.Template.Spec.Affinity = opts.AgentAffinity
	}

	// modify containers via pointers to the containers
	var containers []*corev1.Container
	for i := range app.Spec.Template.Spec.Containers {
		containers = append(containers, &app.Spec.Template.Spec.Containers[i])
	}
	for i := range app.Spec.Template.Spec.InitContainers {
		containers = append(containers, &app.Spec.Template.Spec.InitContainers[i])
	}
	for _, container := range containers {
		// set resources if present on cluster
		if opts.AgentResources != nil {
			container.Resources = *opts.AgentResources
		}

		// additional env vars from cluster
		if opts.AgentEnvVars != nil {
			container.Env = append(container.Env, opts.AgentEnvVars...)
		}

		if DebugEnabled {
			container.Command = append(container.Command, "--debug", "--debug-level", strconv.Itoa(DebugLevel))
		}
		if !DisableSecurityContext {
			container.SecurityContext = &corev1.SecurityContext{
				AllowPrivilegeEscalation: &[]bool{false}[0],
				ReadOnlyRootFilesystem:   &[]bool{true}[0],
				Privileged:               &[]bool{false}[0],
				Capabilities:             &corev1.Capabilities{Drop: []corev1.Capability{"ALL"}},
			}
		}
	}

	return app
}

func serviceAccount(namespace, name string) *corev1.ServiceAccount {
	return &corev1.ServiceAccount{
		ObjectMeta: metav1.ObjectMeta{
			Name:      name,
			Namespace: namespace,
		},
	}
}



================================================
FILE: internal/cmd/controller/agentmanagement/agent/manifest_test.go
================================================
package agent_test

import (
	"reflect"
	"testing"
	"time"

	appsv1 "k8s.io/api/apps/v1"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"

	"github.com/google/go-cmp/cmp"
	"github.com/google/go-cmp/cmp/cmpopts"

	"github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/agent"
)

const namespace = "fleet-system"

var (
	second     = time.Second
	leaderOpts = cmd.LeaderElectionOptions{
		LeaseDuration: second,
		RenewDeadline: second,
		RetryPeriod:   second,
	}
)

func TestImageResolve(t *testing.T) {
	tests := []struct {
		systemDefaultRegistry string
		privateRepoURL        string
		image                 string
		expected              string
	}{
		{"", "", "rancher/fleet:dev", "rancher/fleet:dev"},
		{"mirror.example/", "", "mirror.example/rancher/fleet:dev", "mirror.example/rancher/fleet:dev"},
		{"mirror.example/", "local.example", "mirror.example/rancher/fleet:dev", "local.example/rancher/fleet:dev"},
	}

	for _, d := range tests {
		image := agent.Resolve(d.systemDefaultRegistry, d.privateRepoURL, d.image)
		if image != d.expected {
			t.Errorf("expected %s, got %s", d.expected, image)
		}
	}
}

func getAgentFromManifests(scope string, opts agent.ManifestOptions) *appsv1.Deployment {
	objects := agent.Manifest(namespace, scope, opts)
	for _, obj := range objects {
		dep, ok := obj.(*appsv1.Deployment)
		if ok {
			return dep
		}
	}
	return nil
}

func TestManifestAgentTolerations(t *testing.T) {
	const scope = "test-scope"
	baseOpts := agent.ManifestOptions{
		AgentEnvVars:          []corev1.EnvVar{},
		AgentImage:            "rancher/fleet:1.2.3",
		AgentImagePullPolicy:  "Always",
		AgentTolerations:      []corev1.Toleration{},
		CheckinInterval:       "1s",
		PrivateRepoURL:        "private.rancher.com:5000",
		SystemDefaultRegistry: "default.rancher.com",
		LeaderElectionOptions: leaderOpts,
	}

	// these tolerations should exist regardless of what user sent
	baseTolerations := []corev1.Toleration{
		{Key: "cattle.io/os", Operator: "Equal", Value: "linux", Effect: "NoSchedule"},
		{Key: "node.cloudprovider.kubernetes.io/uninitialized", Operator: "Equal", Value: "true", Effect: "NoSchedule"},
	}

	less := func(a, b corev1.Toleration) bool { return a.Key < b.Key }
	cmpOpt := cmpopts.SortSlices(less)

	for _, testCase := range []struct {
		name                string
		getOpts             func() agent.ManifestOptions
		expectedTolerations []corev1.Toleration
	}{
		{
			name: "BaseOpts",
			getOpts: func() agent.ManifestOptions {
				return baseOpts
			},
			expectedTolerations: baseTolerations,
		},
		{
			name: "ExtraToleration",
			getOpts: func() agent.ManifestOptions {
				withTolerationsOpts := baseOpts
				withTolerationsOpts.AgentTolerations = []corev1.Toleration{
					{Key: "fleet-agent", Operator: "Equals", Value: "true", Effect: "NoSchedule"},
				}
				return withTolerationsOpts
			},
			expectedTolerations: append(baseTolerations,
				corev1.Toleration{Key: "fleet-agent", Operator: "Equals", Value: "true", Effect: "NoSchedule"},
			),
		},
	} {
		t.Run(testCase.name, func(t *testing.T) {
			agent := getAgentFromManifests(scope, testCase.getOpts())
			if agent == nil {
				t.Fatal("there were no deployments returned from the manifests")
			}

			//nolint:SA5011 // agent is checked for nil above; t.Fatal prevents execution if nil
			if !cmp.Equal(agent.Spec.Template.Spec.Tolerations, testCase.expectedTolerations, cmpOpt) {
				t.Fatalf("tolerations were not as expected: %v", agent.Spec.Template.Spec.Tolerations)
			}
		})
	}
}

func TestManifestAgentHostNetwork(t *testing.T) {
	const scope = "test-scope"
	baseOpts := agent.ManifestOptions{
		AgentEnvVars:          []corev1.EnvVar{},
		AgentImage:            "rancher/fleet:1.2.3",
		AgentImagePullPolicy:  "Always",
		AgentTolerations:      []corev1.Toleration{},
		CheckinInterval:       "1s",
		PrivateRepoURL:        "private.rancher.com:5000",
		SystemDefaultRegistry: "default.rancher.com",
		LeaderElectionOptions: leaderOpts,
	}

	for _, testCase := range []struct {
		name            string
		getOpts         func() agent.ManifestOptions
		expectedNetwork bool
	}{
		{
			name: "DefaultSetting",
			getOpts: func() agent.ManifestOptions {
				return baseOpts
			},
			expectedNetwork: false,
		},
		{
			name: "With hostNetwork",
			getOpts: func() agent.ManifestOptions {
				withHostNetwork := baseOpts
				withHostNetwork.HostNetwork = true
				return withHostNetwork
			},
			expectedNetwork: true,
		},
	} {
		t.Run(testCase.name, func(t *testing.T) {
			agent := getAgentFromManifests(scope, testCase.getOpts())
			if agent == nil {
				t.Fatal("there were no deployments returned from the manifests")
			}

			//nolint:SA5011 // agent is checked for nil above; t.Fatal prevents execution if nil
			if !cmp.Equal(agent.Spec.Template.Spec.HostNetwork, testCase.expectedNetwork) {
				t.Fatalf("hostNetwork is not as expected: %v", agent.Spec.Template.Spec.HostNetwork)
			}
		})
	}
}

func TestManifestAgentAffinity(t *testing.T) {
	// this is the value from manifest.go
	builtinAffinity := &corev1.Affinity{NodeAffinity: &corev1.NodeAffinity{
		PreferredDuringSchedulingIgnoredDuringExecution: []corev1.PreferredSchedulingTerm{{
			Weight: 1,
			Preference: corev1.NodeSelectorTerm{
				MatchExpressions: []corev1.NodeSelectorRequirement{
					{Key: "fleet.cattle.io/agent", Operator: corev1.NodeSelectorOpIn, Values: []string{"true"}},
				},
			},
		}},
	}}

	customAffinity := &corev1.Affinity{NodeAffinity: &corev1.NodeAffinity{
		PreferredDuringSchedulingIgnoredDuringExecution: []corev1.PreferredSchedulingTerm{{
			Weight: 1,
			Preference: corev1.NodeSelectorTerm{
				MatchExpressions: []corev1.NodeSelectorRequirement{
					{Key: "custom/label", Operator: corev1.NodeSelectorOpIn, Values: []string{"true"}},
				},
			},
		}},
	}}

	baseOpts := agent.ManifestOptions{
		LeaderElectionOptions: leaderOpts,
	}

	for _, testCase := range []struct {
		name             string
		getOpts          func() agent.ManifestOptions
		expectedAffinity *corev1.Affinity
	}{
		{
			name: "Builtin Affinity",
			getOpts: func() agent.ManifestOptions {
				return baseOpts
			},
			expectedAffinity: builtinAffinity,
		},
		{
			name: "Remove Affinity",
			getOpts: func() agent.ManifestOptions {
				opts := baseOpts
				opts.AgentAffinity = &corev1.Affinity{}
				return opts
			},
			expectedAffinity: &corev1.Affinity{},
		},
		{
			name: "Override Affinity",
			getOpts: func() agent.ManifestOptions {
				opts := baseOpts
				opts.AgentAffinity = customAffinity
				return opts
			},
			expectedAffinity: customAffinity,
		},
	} {
		t.Run(testCase.name, func(t *testing.T) {
			agent := getAgentFromManifests("", testCase.getOpts())
			if agent == nil {
				t.Fatal("there were no deployments returned from the manifests")
			}

			//nolint:SA5011 // agent is checked for nil above; t.Fatal prevents execution if nil
			if !cmp.Equal(agent.Spec.Template.Spec.Affinity, testCase.expectedAffinity) {
				t.Fatalf("affinity was not as expected: %v %v", testCase.expectedAffinity, agent.Spec.Template.Spec.Affinity)
			}
		})
	}
}

func TestManifestAgentResources(t *testing.T) {
	// this is the value from manifest.go
	builtinResources := corev1.ResourceRequirements{}

	customResources := corev1.ResourceRequirements{
		Limits: corev1.ResourceList{
			corev1.ResourceCPU:    resource.MustParse("100m"),
			corev1.ResourceMemory: resource.MustParse("100Mi"),
		},

		Requests: corev1.ResourceList{
			corev1.ResourceCPU:    resource.MustParse("50m"),
			corev1.ResourceMemory: resource.MustParse("50Mi"),
		},
	}

	baseOpts := agent.ManifestOptions{
		LeaderElectionOptions: leaderOpts,
	}

	for _, testCase := range []struct {
		name              string
		getOpts           func() agent.ManifestOptions
		expectedResources corev1.ResourceRequirements
	}{
		{
			name: "Builtin Resources",
			getOpts: func() agent.ManifestOptions {
				return baseOpts
			},
			expectedResources: builtinResources,
		},
		{
			name: "Remove Resources",
			getOpts: func() agent.ManifestOptions {
				opts := baseOpts
				opts.AgentResources = &corev1.ResourceRequirements{}
				return opts
			},
			expectedResources: corev1.ResourceRequirements{},
		},
		{
			name: "Override Resources",
			getOpts: func() agent.ManifestOptions {
				opts := baseOpts
				opts.AgentResources = &customResources
				return opts
			},
			expectedResources: customResources,
		},
	} {
		t.Run(testCase.name, func(t *testing.T) {
			agent := getAgentFromManifests("", testCase.getOpts())
			if agent == nil {
				t.Fatal("there were no deployments returned from the manifests")
			}

			if !reflect.DeepEqual(agent.Spec.Template.Spec.Containers[0].Resources, testCase.expectedResources) {
				t.Fatalf("resources was not as expected: %v %v", testCase.expectedResources, agent.Spec.Template.Spec.Containers[0].Resources)
			}
		})
	}
}

func TestPriorityClassName(t *testing.T) {
	tests := []struct {
		name              string
		priorityClassName string
	}{
		{
			name:              "empty priorityClassName",
			priorityClassName: "",
		},
		{
			name:              "priorityClassName specified",
			priorityClassName: "foo",
		},
	}

	for _, test := range tests {
		t.Run(test.name, func(t *testing.T) {
			d := getAgentFromManifests("test-scope", agent.ManifestOptions{
				PriorityClassName: test.priorityClassName,
			})
			if d.Spec.Template.Spec.PriorityClassName != test.priorityClassName {
				t.Fatalf("expected PriorityClassName to be %s, got %s", test.priorityClassName, d.Spec.Template.Spec.PriorityClassName)
			}
		})
	}
}



================================================
FILE: internal/cmd/controller/agentmanagement/connection/connection.go
================================================
// Package connection provides a connection to a Kubernetes cluster, used when importing a cluster.
package connection

import (
	"k8s.io/client-go/kubernetes"
)

func SmokeTestKubeClientConnection(client *kubernetes.Clientset) error {
	_, err := client.Discovery().ServerVersion()
	return err
}



================================================
FILE: internal/cmd/controller/agentmanagement/controllers/controllers.go
================================================
package controllers

import (
	"context"

	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/controllers/bootstrap"
	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/controllers/cluster"
	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/controllers/clusterregistration"
	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/controllers/clusterregistrationtoken"
	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/controllers/config"
	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/controllers/manageagent"
	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/controllers/resources"
	fleetns "github.com/rancher/fleet/internal/cmd/controller/namespace"
	"github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"
	"github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io"
	fleetcontrollers "github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io/v1alpha1"

	"github.com/rancher/lasso/pkg/cache"
	"github.com/rancher/lasso/pkg/client"
	"github.com/rancher/lasso/pkg/controller"
	"github.com/rancher/wrangler/v3/pkg/apply"
	"github.com/rancher/wrangler/v3/pkg/generated/controllers/apps"
	appscontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/apps/v1"
	"github.com/rancher/wrangler/v3/pkg/generated/controllers/core"
	corecontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"
	"github.com/rancher/wrangler/v3/pkg/generated/controllers/rbac"
	rbaccontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/rbac/v1"
	"github.com/rancher/wrangler/v3/pkg/start"

	"github.com/sirupsen/logrus"

	"k8s.io/apimachinery/pkg/api/meta"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	"k8s.io/client-go/util/workqueue"
)

type AppContext struct {
	fleetcontrollers.Interface

	K8s          kubernetes.Interface
	Core         corecontrollers.Interface
	Apps         appscontrollers.Interface
	RBAC         rbaccontrollers.Interface
	RESTMapper   meta.RESTMapper
	Apply        apply.Apply
	ClientConfig clientcmd.ClientConfig
	starters     []start.Starter
}

func (a *AppContext) Start(ctx context.Context) error {
	return start.All(ctx, 50, a.starters...)
}

func Register(ctx context.Context, appCtx *AppContext, systemNamespace string, disableBootstrap bool) error {
	systemRegistrationNamespace := fleetns.SystemRegistrationNamespace(systemNamespace)

	// config should be registered first to ensure the global
	// config is available to all components
	if err := config.Register(ctx,
		systemNamespace,
		appCtx.Core.ConfigMap()); err != nil {
		return err
	}

	if err := resources.ApplyBootstrapResources(
		systemNamespace,
		systemRegistrationNamespace,
		appCtx.Apply.
			WithSetID("fleet-bootstrap-data").
			WithDynamicLookup().
			WithNoDeleteGVK(fleetns.GVK()),
	); err != nil {
		return err
	}
	if !disableBootstrap {
		bootstrap.Register(ctx,
			systemNamespace,
			appCtx.Apply.WithCacheTypes(
				appCtx.GitRepo(),
				appCtx.Cluster(),
				appCtx.ClusterGroup(),
				appCtx.Core.Namespace(),
				appCtx.Core.Secret()),
			appCtx.ClientConfig,
			appCtx.Core.ServiceAccount().Cache(),
			appCtx.Core.Secret(),
			appCtx.Core.Secret().Cache(),
			appCtx.Apps.Deployment().Cache())
	}

	cluster.Register(ctx,
		appCtx.BundleDeployment(),
		appCtx.ClusterGroup().Cache(),
		appCtx.Cluster(),
		appCtx.GitRepo().Cache(),
		appCtx.Core.Namespace(),
		appCtx.ClusterRegistration())

	cluster.RegisterImport(ctx,
		systemNamespace,
		appCtx.Core.Secret(),
		appCtx.Cluster(),
		appCtx.ClusterRegistrationToken(),
		appCtx.Bundle(),
		appCtx.Core.Namespace())

	clusterregistration.Register(ctx,
		appCtx.Apply.WithCacheTypes(
			appCtx.RBAC.ClusterRole(),
			appCtx.RBAC.ClusterRoleBinding(),
		),
		systemNamespace,
		systemRegistrationNamespace,
		appCtx.Core.ServiceAccount(),
		appCtx.Core.Secret(),
		appCtx.RBAC.Role(),
		appCtx.RBAC.RoleBinding(),
		appCtx.ClusterRegistration(),
		appCtx.Cluster())

	clusterregistrationtoken.Register(ctx,
		systemNamespace,
		systemRegistrationNamespace,
		appCtx.Apply.WithCacheTypes(
			appCtx.Core.Secret(),
			appCtx.Core.ServiceAccount(),
			appCtx.RBAC.Role(),
			appCtx.RBAC.RoleBinding()),
		appCtx.ClusterRegistrationToken(),
		appCtx.Core.ServiceAccount(),
		appCtx.Core.Secret().Cache(),
		appCtx.Core.Secret())

	manageagent.Register(ctx,
		systemNamespace,
		appCtx.Apply,
		appCtx.Core.Namespace(),
		appCtx.Cluster(),
		appCtx.Bundle())

	if err := appCtx.Start(ctx); err != nil {
		logrus.Fatal(err)
	}

	return nil
}

func NewAppContext(cfg clientcmd.ClientConfig) (*AppContext, error) {
	client, err := cfg.ClientConfig()
	if err != nil {
		return nil, err
	}
	client.QPS = -1
	client.RateLimiter = nil

	scf, err := controllerFactory(client)
	if err != nil {
		return nil, err
	}

	core, err := core.NewFactoryFromConfigWithOptions(client, &core.FactoryOptions{
		SharedControllerFactory: scf,
	})
	if err != nil {
		return nil, err
	}
	corev := core.Core().V1()

	fleet, err := fleet.NewFactoryFromConfigWithOptions(client, &fleet.FactoryOptions{
		SharedControllerFactory: scf,
	})
	if err != nil {
		return nil, err
	}
	fleetv := fleet.Fleet().V1alpha1()

	rbac, err := rbac.NewFactoryFromConfigWithOptions(client, &rbac.FactoryOptions{
		SharedControllerFactory: scf,
	})
	if err != nil {
		return nil, err
	}
	rbacv := rbac.Rbac().V1()

	apps, err := apps.NewFactoryFromConfigWithOptions(client, &apps.FactoryOptions{
		SharedControllerFactory: scf,
	})
	if err != nil {
		return nil, err
	}
	appsv := apps.Apps().V1()

	apply, err := apply.NewForConfig(client)
	if err != nil {
		return nil, err
	}
	apply = apply.WithSetOwnerReference(false, false)

	k8s, err := kubernetes.NewForConfig(client)
	if err != nil {
		return nil, err
	}

	return &AppContext{
		K8s:          k8s,
		Interface:    fleetv,
		Core:         corev,
		Apps:         appsv,
		RBAC:         rbacv,
		Apply:        apply,
		ClientConfig: cfg,
		starters: []start.Starter{
			core,
			fleet,
			rbac,
		},
	}, nil
}

func controllerFactory(rest *rest.Config) (controller.SharedControllerFactory, error) {
	rateLimit := workqueue.NewTypedItemExponentialFailureRateLimiter[any](
		durations.FailureRateLimiterBase,
		durations.FailureRateLimiterMax,
	)
	clusterRateLimiter := workqueue.NewTypedItemExponentialFailureRateLimiter[any](
		durations.SlowFailureRateLimiterBase,
		durations.SlowFailureRateLimiterMax,
	)
	clientFactory, err := client.NewSharedClientFactory(rest, nil)
	if err != nil {
		return nil, err
	}

	cacheFactory := cache.NewSharedCachedFactory(clientFactory, nil)
	return controller.NewSharedControllerFactory(cacheFactory, &controller.SharedControllerFactoryOptions{
		DefaultRateLimiter:     rateLimit,
		DefaultWorkers:         50,
		SyncOnlyChangedObjects: true,
		KindRateLimiter: map[schema.GroupVersionKind]workqueue.RateLimiter{
			v1alpha1.SchemeGroupVersion.WithKind("Cluster"): clusterRateLimiter,
		},
	}), nil
}



================================================
FILE: internal/cmd/controller/agentmanagement/controllers/bootstrap/bootstrap.go
================================================
package bootstrap

import (
	"context"
	"os"
	"regexp"

	"github.com/pkg/errors"
	"github.com/sirupsen/logrus"

	secretutil "github.com/rancher/fleet/internal/cmd/controller/agentmanagement/secret"
	fleetns "github.com/rancher/fleet/internal/cmd/controller/namespace"
	fleetconfig "github.com/rancher/fleet/internal/config"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/wrangler/v3/pkg/apply"
	appscontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/apps/v1"
	corecontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"

	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"

	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	clientcmdapi "k8s.io/client-go/tools/clientcmd/api"
)

const (
	FleetBootstrap = "fleet-controller-bootstrap"
)

var (
	splitter          = regexp.MustCompile(`\s*,\s*`)
	ErrNoHostInConfig = errors.New("failed to find cluster server parameter")
)

type handler struct {
	apply               apply.Apply
	systemNamespace     string
	serviceAccountCache corecontrollers.ServiceAccountCache
	secretsCache        corecontrollers.SecretCache
	secretsController   corecontrollers.SecretController
	deploymentsCache    appscontrollers.DeploymentCache
	cfg                 clientcmd.ClientConfig
}

func Register(ctx context.Context,
	systemNamespace string,
	apply apply.Apply,
	cfg clientcmd.ClientConfig,
	serviceAccountCache corecontrollers.ServiceAccountCache,
	secretsController corecontrollers.SecretController,
	secretsCache corecontrollers.SecretCache,
	deploymentCache appscontrollers.DeploymentCache,
) {
	h := handler{
		systemNamespace:     systemNamespace,
		serviceAccountCache: serviceAccountCache,
		secretsCache:        secretsCache,
		secretsController:   secretsController,
		deploymentsCache:    deploymentCache,
		apply:               apply.WithSetID("fleet-bootstrap"),
		cfg:                 cfg,
	}
	fleetconfig.OnChange(ctx, h.OnConfig)
}

func (h *handler) OnConfig(config *fleetconfig.Config) error {
	logrus.Debugf("Bootstrap config set, building namespace '%s', secret, local cluster, cluster group, ...", config.Bootstrap.Namespace)

	var objs []runtime.Object

	if config.Bootstrap.Namespace == "" || config.Bootstrap.Namespace == "-" {
		return nil
	}

	secret, err := h.buildSecret(config.Bootstrap.Namespace, h.cfg)
	if err != nil {
		return err
	}
	fleetControllerDeployment, err := h.deploymentsCache.Get(h.systemNamespace, fleetconfig.ManagerConfigName)
	if err != nil {
		return err
	}
	objs = append(objs, &corev1.Namespace{
		ObjectMeta: metav1.ObjectMeta{
			Name: config.Bootstrap.Namespace,
		},
	}, secret, &fleet.Cluster{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "local",
			Namespace: config.Bootstrap.Namespace,
			Labels: map[string]string{
				"name": "local",
			},
		},
		Spec: fleet.ClusterSpec{
			KubeConfigSecret: secret.Name,
			AgentNamespace:   config.Bootstrap.AgentNamespace,
			// copy tolerations from fleet-controller
			AgentTolerations: fleetControllerDeployment.Spec.Template.Spec.Tolerations,
		},
	}, &fleet.ClusterGroup{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "default",
			Namespace: config.Bootstrap.Namespace,
		},
		Spec: fleet.ClusterGroupSpec{
			Selector: &metav1.LabelSelector{
				MatchLabels: map[string]string{
					"name": "local",
				},
			},
		},
	})

	// A repo to add at install time that will deploy to the local cluster. This allows
	// one to fully bootstrap fleet, its configuration and all its downstream clusters
	// in one shot.
	if config.Bootstrap.Repo != "" {
		var paths []string
		if len(config.Bootstrap.Paths) > 0 {
			paths = splitter.Split(config.Bootstrap.Paths, -1)
		}
		objs = append(objs, &fleet.GitRepo{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "bootstrap",
				Namespace: config.Bootstrap.Namespace,
			},
			Spec: fleet.GitRepoSpec{
				Repo:             config.Bootstrap.Repo,
				Branch:           config.Bootstrap.Branch,
				ClientSecretName: config.Bootstrap.Secret,
				Paths:            paths,
			},
		})
	}

	return h.apply.WithNoDeleteGVK(fleetns.GVK()).ApplyObjects(objs...)
}

func (h *handler) buildSecret(bootstrapNamespace string, cfg clientcmd.ClientConfig) (*corev1.Secret, error) {
	rawConfig, err := cfg.RawConfig()
	if err != nil {
		return nil, err
	}

	host, err := getHost(rawConfig)
	if err != nil {
		return nil, err
	}

	ca, err := getCA(rawConfig)
	if err != nil {
		return nil, err
	}

	token, err := h.getToken()
	if err != nil {
		return nil, err
	}

	value, err := buildKubeConfig(host, ca, token, rawConfig)
	if err != nil {
		return nil, err
	}

	return &corev1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "local-cluster",
			Namespace: bootstrapNamespace,
			Labels: map[string]string{
				fleet.ManagedLabel: "true",
			},
		},
		Data: map[string][]byte{
			fleetconfig.KubeConfigSecretValueKey: value,
			fleetconfig.APIServerURLKey:          []byte(host),
			fleetconfig.APIServerCAKey:           ca,
		},
	}, nil
}

func getHost(rawConfig clientcmdapi.Config) (string, error) {
	icc, err := rest.InClusterConfig()
	if err == nil {
		return icc.Host, nil
	}

	cluster, ok := rawConfig.Clusters[rawConfig.CurrentContext]
	if ok {
		return cluster.Server, nil
	}

	for _, v := range rawConfig.Clusters {
		return v.Server, nil
	}

	return "", ErrNoHostInConfig
}

func getCA(rawConfig clientcmdapi.Config) ([]byte, error) {
	icc, err := rest.InClusterConfig()
	if err == nil {
		return os.ReadFile(icc.CAFile)
	}

	cluster, ok := rawConfig.Clusters[rawConfig.CurrentContext]
	if !ok {
		for _, v := range rawConfig.Clusters {
			cluster = v
			break
		}
	}

	if cluster != nil {
		if len(cluster.CertificateAuthorityData) > 0 {
			return cluster.CertificateAuthorityData, nil
		}
		return os.ReadFile(cluster.CertificateAuthority)
	}

	return nil, nil
}

func (h *handler) getToken() (string, error) {
	sa, err := h.serviceAccountCache.Get(h.systemNamespace, FleetBootstrap)
	if apierrors.IsNotFound(err) {
		icc, err := rest.InClusterConfig()
		if err == nil {
			return icc.BearerToken, nil
		}
		return "", nil
	} else if err != nil {
		return "", err
	}

	// kubernetes 1.24 doesn't populate sa.Secrets
	if len(sa.Secrets) == 0 {
		logrus.Infof("waiting on secret for service account %s/%s", h.systemNamespace, FleetBootstrap)
		secret, err := secretutil.GetServiceAccountTokenSecret(sa, h.secretsController)
		if err != nil {
			return "", err
		}
		return string(secret.Data[corev1.ServiceAccountTokenKey]), nil
	}

	secret, err := h.secretsCache.Get(h.systemNamespace, sa.Secrets[0].Name)
	if err != nil {
		return "", err
	}

	return string(secret.Data[corev1.ServiceAccountTokenKey]), nil
}

func buildKubeConfig(host string, ca []byte, token string, rawConfig clientcmdapi.Config) ([]byte, error) {
	if token == "" {
		return clientcmd.Write(rawConfig)
	}
	return clientcmd.Write(clientcmdapi.Config{
		Clusters: map[string]*clientcmdapi.Cluster{
			"cluster": {
				Server:                   host,
				CertificateAuthorityData: ca,
			},
		},
		AuthInfos: map[string]*clientcmdapi.AuthInfo{
			"user": {
				Token: token,
			},
		},
		Contexts: map[string]*clientcmdapi.Context{
			"default": {
				Cluster:  "cluster",
				AuthInfo: "user",
			},
		},
		CurrentContext: "default",
	})

}



================================================
FILE: internal/cmd/controller/agentmanagement/controllers/cluster/controller.go
================================================
// Package cluster provides controllers for managing clusters: status changes, importing, bootstrapping.
package cluster

import (
	"context"

	"github.com/sirupsen/logrus"

	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/controllers/manageagent"
	"github.com/rancher/fleet/internal/names"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	fleetcontrollers "github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io/v1alpha1"

	corecontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"
	"github.com/rancher/wrangler/v3/pkg/kv"
	"github.com/rancher/wrangler/v3/pkg/relatedresource"

	v1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
)

type handler struct {
	clusters             fleetcontrollers.ClusterController
	clusterCache         fleetcontrollers.ClusterCache
	clusterGroups        fleetcontrollers.ClusterGroupCache
	bundleDeployment     fleetcontrollers.BundleDeploymentCache
	namespaceCache       corecontrollers.NamespaceCache
	namespaces           corecontrollers.NamespaceController
	gitRepos             fleetcontrollers.GitRepoCache
	clusterRegistrations fleetcontrollers.ClusterRegistrationController
}

func Register(ctx context.Context,
	bundleDeployment fleetcontrollers.BundleDeploymentController,
	clusterGroups fleetcontrollers.ClusterGroupCache,
	clusters fleetcontrollers.ClusterController,
	gitRepos fleetcontrollers.GitRepoCache,
	namespaces corecontrollers.NamespaceController,
	clusterRegistrations fleetcontrollers.ClusterRegistrationController) {

	h := &handler{
		clusterGroups:        clusterGroups,
		clusterCache:         clusters.Cache(),
		clusters:             clusters,
		bundleDeployment:     bundleDeployment.Cache(),
		namespaceCache:       namespaces.Cache(),
		namespaces:           namespaces,
		gitRepos:             gitRepos,
		clusterRegistrations: clusterRegistrations,
	}

	clusters.OnChange(ctx, "managed-cluster-trigger", h.ensureNSDeleted)
	fleetcontrollers.RegisterClusterStatusHandler(ctx,
		clusters,
		"Processed",
		"managed-cluster",
		h.OnClusterChanged)

	// enqueue cluster event for bundledeployment changes
	relatedresource.Watch(ctx, "managed-cluster", h.findClusters(namespaces.Cache()), clusters, bundleDeployment)
}

// ensureNSDeleted is a handler that enqueues the cluster registration namespace, when a cluster is deleted.
func (h *handler) ensureNSDeleted(key string, obj *fleet.Cluster) (*fleet.Cluster, error) {
	if obj == nil {
		logrus.Debugf("Cluster %s deleted, enqueue cluster namespace deletion", key)
		h.namespaces.Enqueue(clusterNamespace(kv.Split(key, "/")))
	}
	return obj, nil
}

// findClusters enqueues the cluster when the bundledeployment changes. It uses the cluster namespace to determine the cluster.
func (h *handler) findClusters(namespaces corecontrollers.NamespaceCache) relatedresource.Resolver {
	return func(namespace, _ string, obj runtime.Object) ([]relatedresource.Key, error) {
		if _, ok := obj.(*fleet.BundleDeployment); !ok {
			return nil, nil
		}

		ns, err := namespaces.Get(namespace)
		if err != nil {
			// Namespace not found (e.g., during deletion) - no clusters to enqueue
			//nolint:nilerr // Intentionally ignore error to handle deletion race condition gracefully
			return nil, nil
		}

		clusterNS := ns.Annotations[fleet.ClusterNamespaceAnnotation]
		clusterName := ns.Annotations[fleet.ClusterAnnotation]
		if clusterNS == "" || clusterName == "" {
			return nil, nil
		}
		logrus.Debugf("Enqueueing cluster %s/%s for bundledeployment %s/%s", clusterNS, clusterName, namespace, obj.(*fleet.BundleDeployment).Name)
		return []relatedresource.Key{
			{
				Namespace: clusterNS,
				Name:      clusterName,
			},
		}, nil
	}
}

// clusterNamespace returns the cluster namespace name
// for a given cluster name, e.g.:
// "cluster-fleet-local-cluster-294db1acfa77-d9ccf852678f"
func clusterNamespace(clusterNamespace, clusterName string) string {
	return names.SafeConcatName("cluster",
		clusterNamespace,
		clusterName,
		names.KeyHash(clusterNamespace+"::"+clusterName))
}

// OnClusterChanged is a handler that creates the clusters namespace
func (h *handler) OnClusterChanged(cluster *fleet.Cluster, status fleet.ClusterStatus) (fleet.ClusterStatus, error) {
	if manageagent.SkipCluster(cluster) {
		return status, nil
	}

	logrus.Debugf("OnClusterChanged for cluster status %s, updating namespace in status", cluster.Name)
	if status.Namespace == "" {
		status.Namespace = clusterNamespace(cluster.Namespace, cluster.Name)
	}
	return status, h.createNamespace(cluster, status)
}

func (h *handler) createNamespace(cluster *fleet.Cluster, status fleet.ClusterStatus) error {
	_, err := h.namespaceCache.Get(status.Namespace)
	if apierrors.IsNotFound(err) {
		_, err = h.namespaces.Create(&v1.Namespace{
			ObjectMeta: metav1.ObjectMeta{
				Name: status.Namespace,
				Labels: map[string]string{
					fleet.ManagedLabel: "true",
				},
				Annotations: map[string]string{
					fleet.ClusterNamespaceAnnotation: cluster.Namespace,
					fleet.ClusterAnnotation:          cluster.Name,
				},
			},
		})
	}

	if apierrors.IsAlreadyExists(err) {
		return nil
	}
	return err
}



================================================
FILE: internal/cmd/controller/agentmanagement/controllers/cluster/import.go
================================================
package cluster

import (
	"cmp"
	"context"
	"crypto/sha256"
	"encoding/json"
	"errors"
	"fmt"
	"net/http"
	"os"

	"github.com/sirupsen/logrus"
	"k8s.io/apimachinery/pkg/labels"

	"github.com/rancher/fleet/internal/client"
	"github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/desiredset"
	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/agent"
	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/connection"
	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/controllers/manageagent"
	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/scheduling"
	fleetns "github.com/rancher/fleet/internal/cmd/controller/namespace"
	"github.com/rancher/fleet/internal/config"
	"github.com/rancher/fleet/internal/names"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"
	fleetcontrollers "github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io/v1alpha1"

	"github.com/rancher/wrangler/v3/pkg/apply"
	corecontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"
	"github.com/rancher/wrangler/v3/pkg/randomtoken"
	"github.com/rancher/wrangler/v3/pkg/yaml"

	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	"k8s.io/utils/ptr"
)

const (
	// clusterForKubeconfigSecretIndexer indexes Clusters by the key of the kubeconfig secret they reference in their spec
	clusterForKubeconfigSecretIndexer = "agentmanagement.fleet.cattle.io/cluster-for-kubeconfig"
)

var (
	ImportTokenPrefix = "import-token-"

	errUnavailableAPIServerURL = errors.New("missing apiServerURL in fleet config for cluster auto registration")
)

type importHandler struct {
	ctx                 context.Context
	systemNamespace     string
	clusters            fleetcontrollers.ClusterController
	clustersCache       fleetcontrollers.ClusterCache
	secretsCache        corecontrollers.SecretCache
	tokens              fleetcontrollers.ClusterRegistrationTokenCache
	tokenClient         fleetcontrollers.ClusterRegistrationTokenClient
	bundleClient        fleetcontrollers.BundleClient
	namespaceController corecontrollers.NamespaceController
}

func RegisterImport(
	ctx context.Context,
	systemNamespace string,
	secrets corecontrollers.SecretController,
	clusters fleetcontrollers.ClusterController,
	tokens fleetcontrollers.ClusterRegistrationTokenController,
	bundles fleetcontrollers.BundleClient,
	namespaceController corecontrollers.NamespaceController,
) {
	h := importHandler{
		ctx:                 ctx,
		systemNamespace:     systemNamespace,
		clusters:            clusters,
		clustersCache:       clusters.Cache(),
		secretsCache:        secrets.Cache(),
		tokens:              tokens.Cache(),
		tokenClient:         tokens,
		namespaceController: namespaceController,
		bundleClient:        bundles,
	}

	clusters.OnChange(ctx, "import-cluster", h.OnChange)
	fleetcontrollers.RegisterClusterStatusHandler(ctx, clusters, "Imported", "import-cluster", h.importCluster)
	config.OnChange(ctx, h.onConfig)

	clustersCache := clusters.Cache()
	clustersCache.AddIndexer(clusterForKubeconfigSecretIndexer, func(cluster *fleet.Cluster) ([]string, error) {
		if cluster == nil || len(cluster.Spec.KubeConfigSecret) == 0 {
			return []string{}, nil
		}
		secretKey := getKubeConfigSecretNS(cluster) + "/" + cluster.Spec.KubeConfigSecret
		return []string{secretKey}, nil
	})
	secrets.OnChange(ctx, "kubeconfig-secrets-watch", func(key string, secret *corev1.Secret) (*corev1.Secret, error) {
		clusters, err := clustersCache.GetByIndex(clusterForKubeconfigSecretIndexer, key)
		if err != nil {
			return nil, err
		}
		cfg := config.Get()
		for _, cluster := range clusters {
			if err := h.checkForConfigChange(cfg, cluster, secret); err != nil {
				logrus.WithError(err).Errorf("cluster %s/%s: could not check for config changes", cluster.Namespace, cluster.Name)
			}
		}
		// Successfully checked all clusters for config changes. No secret modification needed,
		// and no error occurred. The secret watcher processed the event successfully.
		return nil, nil
	})
}

// onConfig triggers clusters which rely on the fallback config in the
// fleet-controller config map. This is important for changes to apiServerURL
// and apiServerCA, as they are needed e.g. to update the fleet-agent-bootstrap
// secret.
func (i *importHandler) onConfig(cfg *config.Config) error {
	if cfg == nil {
		return errors.New("config is nil: this should never happen")
	}

	clusters, err := i.clustersCache.List(metav1.NamespaceAll, labels.Everything())
	if err != nil {
		return err
	}

	if len(clusters) == 0 {
		return nil
	}

	for _, cluster := range clusters {
		if cluster.Spec.KubeConfigSecret == "" {
			continue
		}

		secret, err := i.secretsCache.Get(getKubeConfigSecretNS(cluster), cluster.Spec.KubeConfigSecret)
		if err != nil {
			return fmt.Errorf("cluster %s/%s: could not check for config changes: %w", cluster.Namespace, cluster.Name, err)
		}
		if err := i.checkForConfigChange(cfg, cluster, secret); err != nil {
			logrus.WithError(err).Warnf("cluster %s/%s: could not check for config changes", cluster.Namespace, cluster.Name)
			continue
		}
	}
	return nil
}

// hasAPIServerConfigChanged checks for changes in API server URL or CA configuration, comparing the current state of
// the cluster with cfg. However, if the cluster references a secret through its `KubeConfigSecret` field, then API
// server URL and CA are understood to be sourced from there, hence config changes for those fields will be skipped.
// Returns a boolean indicating whether URL or CA config has changed, and any error that may have occurred (such as the
// referenced secret not being found).
func (i *importHandler) hasAPIServerConfigChanged(cfg *config.Config, secret *corev1.Secret, cluster *fleet.Cluster) (bool, error) {
	var secretAPIServerCA, secretAPIServerURL []byte
	if secret != nil {
		secretAPIServerURL = secret.Data[config.APIServerURLKey]
		secretAPIServerCA = secret.Data[config.APIServerCAKey]
	}

	if len(secretAPIServerURL) == 0 && len(cfg.APIServerURL) == 0 {
		return false, errUnavailableAPIServerURL
	}

	// if the API server URL is non-empty in the secret, then it is sourced from there; config changes for that field
	// are irrelevant.
	// The same applies to the CA.
	hasURLChanged := len(secretAPIServerURL) == 0 && cfg.APIServerURL != cluster.Status.APIServerURL
	hasCAChanged := len(secretAPIServerCA) == 0 && hashStatusField(cfg.APIServerCA) != cluster.Status.APIServerCAHash

	return hasURLChanged || hasCAChanged, nil
}

func hashStatusField(field any) string {
	hasher := sha256.New224()
	b, err := json.Marshal(field)
	if err != nil {
		return ""
	}
	hasher.Write(b)
	return fmt.Sprintf("%x", hasher.Sum(nil))
}

func agentDeployed(cluster *fleet.Cluster) bool {
	if cluster.Status.AgentConfigChanged {
		return false
	}

	if !cluster.Status.AgentMigrated {
		return false
	}

	if !cluster.Status.CattleNamespaceMigrated {
		return false
	}

	if cluster.Status.AgentDeployedGeneration == nil {
		return false
	}

	if !cluster.Status.AgentNamespaceMigrated {
		return false
	}

	if cluster.Spec.AgentNamespace != "" && cluster.Status.Agent.Namespace != cluster.Spec.AgentNamespace {
		return false
	}

	return *cluster.Status.AgentDeployedGeneration == cluster.Spec.RedeployAgentGeneration
}

// OnChange is triggered when a cluster changes, for manager initiated
// deployments and the local agent. It updates the client ID, only when
// KubeConfigSecret is configured or the agent is not already deployed.
func (i *importHandler) OnChange(key string, cluster *fleet.Cluster) (_ *fleet.Cluster, err error) {
	if cluster == nil {
		return cluster, nil
	}

	if manageagent.SkipCluster(cluster) {
		return cluster, nil
	}

	// cluster.spec.KubeConfigSecret is empty when agent-initiated registration is used
	if cluster.Spec.KubeConfigSecret == "" || agentDeployed(cluster) {
		return cluster, nil
	}

	// NOTE(mm): why is this not done in importCluster?
	if cluster.Spec.ClientID == "" {
		logrus.Debugf("Cluster import for '%s/%s'. Agent found, updating ClientID", cluster.Namespace, cluster.Name)

		cluster = cluster.DeepCopy()
		cluster.Spec.ClientID, err = randomtoken.Generate()
		if err != nil {
			return nil, err
		}
		return i.clusters.Update(cluster)
	}

	return cluster, nil
}

func (i *importHandler) deleteOldAgentBundle(cluster *fleet.Cluster) error {
	if err := i.bundleClient.Delete(cluster.Namespace, names.SafeConcatName(manageagent.AgentBundleName, cluster.Name), nil); err != nil {
		return err
	}
	i.namespaceController.Enqueue(cluster.Namespace)
	return nil
}

func (i *importHandler) deleteOldAgent(cluster *fleet.Cluster, kc kubernetes.Interface, namespace string) error {
	err := kc.CoreV1().Secrets(namespace).Delete(i.ctx, config.AgentConfigName, metav1.DeleteOptions{})
	if err != nil && !apierrors.IsNotFound(err) {
		return err
	}

	err = kc.CoreV1().Secrets(namespace).Delete(i.ctx, config.AgentBootstrapConfigName, metav1.DeleteOptions{})
	if err != nil && !apierrors.IsNotFound(err) {
		return err
	}

	if err := kc.AppsV1().StatefulSets(namespace).Delete(i.ctx, config.AgentConfigName, metav1.DeleteOptions{}); err != nil && !apierrors.IsNotFound(err) {
		return err
	}
	if err := kc.AppsV1().Deployments(namespace).Delete(i.ctx, config.AgentConfigName, metav1.DeleteOptions{}); err != nil && !apierrors.IsNotFound(err) {
		return err
	}
	if err := kc.SchedulingV1().PriorityClasses().Delete(i.ctx, scheduling.FleetAgentPriorityClassName, metav1.DeleteOptions{}); err != nil && !apierrors.IsNotFound(err) {
		return err
	}
	if err := kc.PolicyV1().PodDisruptionBudgets(namespace).Delete(i.ctx, scheduling.FleetAgentPodDisruptionBudgetName, metav1.DeleteOptions{}); err != nil && !apierrors.IsNotFound(err) {
		return err
	}

	logrus.Infof("Deleted old agent for cluster (%s/%s) in namespace %s", cluster.Namespace, cluster.Name, namespace)

	return nil
}

// importCluster is triggered for manager initiated deployments and the local agent, It re-deploys the agent on the downstream cluster.
// Since it re-creates the fleet-agent-bootstrap secret, it will also re-register the agent.
//
//nolint:gocyclo
func (i *importHandler) importCluster(cluster *fleet.Cluster, status fleet.ClusterStatus) (fleet.ClusterStatus, error) {
	if manageagent.SkipCluster(cluster) {
		return status, nil
	}

	if shouldMigrateFromLegacyNamespace(cluster.Status.Agent.Namespace) {
		cluster.Status.CattleNamespaceMigrated = false
	}

	// cluster.spec.KubeConfigSecret is empty when agent-initiated registration is used
	if cluster.Spec.KubeConfigSecret == "" ||
		agentDeployed(cluster) ||
		cluster.Spec.ClientID == "" {
		return status, nil
	}

	kubeConfigSecretNamespace := getKubeConfigSecretNS(cluster)

	logrus.Debugf("Cluster import for '%s/%s'. Getting kubeconfig from secret in namespace %s", cluster.Namespace, cluster.Name, kubeConfigSecretNamespace)

	secret, err := i.secretsCache.Get(kubeConfigSecretNamespace, cluster.Spec.KubeConfigSecret)
	if err != nil {
		return status, err
	}

	logrus.Debugf("Cluster import for '%s/%s'. Setting up agent with kubeconfig from secret '%s/%s'", cluster.Namespace, cluster.Name, kubeConfigSecretNamespace, cluster.Spec.KubeConfigSecret)
	var (
		cfg          = config.Get()
		apiServerURL = string(secret.Data[config.APIServerURLKey])
		apiServerCA  = secret.Data[config.APIServerCAKey]
	)

	if apiServerURL == "" {
		if len(cfg.APIServerURL) == 0 {
			// Current config cannot be deployed, so remove the "config changed" mark
			logrus.Warnf("cannot import cluster '%s/%s', missing apiServerURL in fleet config for cluster auto registration", cluster.Namespace, cluster.Name)
			status.AgentConfigChanged = false
			return status, nil
		}
		logrus.Debugf("Cluster import for '%s/%s'. Using apiServerURL from fleet-controller config", cluster.Namespace, cluster.Name)
		apiServerURL = cfg.APIServerURL
	}

	if len(apiServerCA) == 0 {
		apiServerCA = cfg.APIServerCA
	}

	restConfig, err := i.restConfigFromKubeConfig(secret.Data[config.KubeConfigSecretValueKey], cfg.AgentTLSMode)
	if err != nil {
		return status, err
	}
	restConfig.Timeout = durations.RestConfigTimeout

	kc, err := kubernetes.NewForConfig(restConfig)
	if err != nil {
		return status, err
	}

	if err := connection.SmokeTestKubeClientConnection(kc); err != nil {
		logrus.Errorf("Cluster import for '%s/%s'. Smoke test failed: %v", cluster.Namespace, cluster.Name, err)
		return status, err
	}

	apply, err := apply.NewForConfig(restConfig)
	if err != nil {
		return status, err
	}
	setID := desiredset.GetSetID(config.AgentBootstrapConfigName, "", cluster.Spec.AgentNamespace)
	apply = apply.WithDynamicLookup().WithSetID(setID).WithNoDeleteGVK(fleetns.GVK())

	tokenName := names.SafeConcatName(ImportTokenPrefix + cluster.Name)
	token, err := i.tokens.Get(cluster.Namespace, tokenName)
	if err != nil {
		// If token doesn't exist, try to create it
		_, err = i.tokenClient.Create(&fleet.ClusterRegistrationToken{
			ObjectMeta: metav1.ObjectMeta{
				Namespace: cluster.Namespace,
				Name:      tokenName,
				OwnerReferences: []metav1.OwnerReference{
					{
						APIVersion: fleet.SchemeGroupVersion.String(),
						Kind:       "Cluster",
						Name:       cluster.Name,
						UID:        cluster.UID,
					},
				},
			},
			Spec: fleet.ClusterRegistrationTokenSpec{
				TTL: &metav1.Duration{Duration: durations.ClusterImportTokenTTL},
			},
		})
		// Ignore AlreadyExists errors (race condition with another reconcile)
		if err != nil && !apierrors.IsAlreadyExists(err) {
			logrus.Debugf("Failed to create ClusterRegistrationToken for cluster %s/%s: %v (requeuing)", cluster.Namespace, cluster.Name, err)
			i.clusters.EnqueueAfter(cluster.Namespace, cluster.Name, durations.TokenClusterEnqueueDelay)
			return status, err
		}
	}

	agentNamespace := i.systemNamespace
	if cluster.Spec.AgentNamespace != "" {
		agentNamespace = cluster.Spec.AgentNamespace
	}

	clusterLabels := yaml.CleanAnnotationsForExport(cluster.Labels)
	agentReplicas := cmd.ParseEnvAgentReplicaCount()

	var (
		objs              []runtime.Object
		priorityClassName string
	)
	if sc := cluster.Spec.AgentSchedulingCustomization; sc != nil {
		if sc.PriorityClass != nil {
			priorityClassName = scheduling.FleetAgentPriorityClassName
			objs = append(objs, scheduling.PriorityClass(sc.PriorityClass))
		}

		if sc.PodDisruptionBudget != nil {
			pdb, err := scheduling.PodDisruptionBudget(agentNamespace, sc.PodDisruptionBudget)
			if err != nil {
				return status, err
			}
			objs = append(objs, pdb)
		}
	}

	// Notice we only set the agentScope when it's a non-default agentNamespace. This is for backwards compatibility
	// for when we didn't have agent scope before
	agentObjs, err := agent.AgentWithConfig(
		i.ctx, agentNamespace, i.systemNamespace,
		cluster.Spec.AgentNamespace,
		&client.Getter{Namespace: cluster.Namespace},
		token.Name,
		&agent.Options{
			APIServerCA:  apiServerCA,
			APIServerURL: apiServerURL,
			ConfigOptions: agent.ConfigOptions{
				ClientID:                  cluster.Spec.ClientID,
				Labels:                    clusterLabels,
				AgentTLSMode:              cfg.AgentTLSMode,
				GarbageCollectionInterval: cfg.GarbageCollectionInterval,
			},
			// keep in sync with manageagent.go
			ManifestOptions: agent.ManifestOptions{
				AgentEnvVars:      cluster.Spec.AgentEnvVars,
				AgentTolerations:  cluster.Spec.AgentTolerations,
				PrivateRepoURL:    cluster.Spec.PrivateRepoURL,
				AgentAffinity:     cluster.Spec.AgentAffinity,
				AgentResources:    cluster.Spec.AgentResources,
				HostNetwork:       *cmp.Or(cluster.Spec.HostNetwork, ptr.To(false)),
				AgentReplicas:     agentReplicas,
				PriorityClassName: priorityClassName,
			},
		})
	objs = append(objs, agentObjs...)
	if err != nil {
		return status, err
	}

	if cluster.Status.Agent.Namespace != agentNamespace || !cluster.Status.AgentNamespaceMigrated {
		// delete old agent if moving namespaces for agent
		if err := i.deleteOldAgentBundle(cluster); err != nil {
			return status, err
		}
		if cluster.Status.Agent.Namespace != "" {
			if err := i.deleteOldAgent(cluster, kc, cluster.Status.Agent.Namespace); err != nil {
				return status, err
			}
		}
	}

	if err := i.deleteOldAgent(cluster, kc, agentNamespace); err != nil {
		return status, err
	}

	if err := apply.ApplyObjects(objs...); err != nil {
		logrus.Errorf("Failed cluster import for '%s/%s'. Cannot create agent deployment", cluster.Namespace, cluster.Name)
		return status, err
	}
	logrus.Infof("Cluster import for '%s/%s'. Deployed new agent", cluster.Namespace, cluster.Name)

	if i.systemNamespace != config.DefaultNamespace {
		// Clean up the leftover agent if it exists.
		_, err := kc.CoreV1().Namespaces().Get(i.ctx, config.DefaultNamespace, metav1.GetOptions{})
		if err == nil {
			logrus.Infof("System namespace (%s) does not equal default namespace (%s), checking for leftover objects...", i.systemNamespace, config.DefaultNamespace)
			if err := i.deleteOldAgent(cluster, kc, config.DefaultNamespace); err != nil {
				return status, err
			}
		} else if !apierrors.IsNotFound(err) {
			return status, err
		}

		// Clean up the leftover clusters namespace if it exists.
		// We want to keep the DefaultNamespace alive, but not the clusters namespace.
		err = kc.CoreV1().Namespaces().Delete(i.ctx, fleetns.SystemRegistrationNamespace(config.DefaultNamespace), metav1.DeleteOptions{})
		if err != nil && !apierrors.IsNotFound(err) {
			return status, err
		}
	}

	status.AgentDeployedGeneration = &cluster.Spec.RedeployAgentGeneration
	status.AgentMigrated = true
	status.CattleNamespaceMigrated = true
	status.Agent = fleet.AgentStatus{
		Namespace: cluster.Spec.AgentNamespace,
	}
	status.AgentNamespaceMigrated = true
	status.AgentConfigChanged = false
	status.APIServerURL = apiServerURL
	status.APIServerCAHash = hashStatusField(apiServerCA)
	status.AgentTLSMode = cfg.AgentTLSMode
	status.GarbageCollectionInterval = &cfg.GarbageCollectionInterval

	return status, nil
}

func shouldMigrateFromLegacyNamespace(agentStatusNs string) bool {
	return !isLegacyAgentNamespaceSelectedByUser() && agentStatusNs == config.LegacyDefaultNamespace
}

func isLegacyAgentNamespaceSelectedByUser() bool {
	cfg := config.Get()

	return os.Getenv("NAMESPACE") == config.LegacyDefaultNamespace ||
		cfg.Bootstrap.AgentNamespace == config.LegacyDefaultNamespace
}

// restConfigFromKubeConfig checks kubeconfig data and tries to connect to server. If server is behind public CA, remove
// CertificateAuthorityData in kubeconfig file unless strict TLS mode is enabled.
func (i *importHandler) restConfigFromKubeConfig(data []byte, agentTLSMode string) (*rest.Config, error) {
	if agentTLSMode != config.AgentTLSModeStrict && agentTLSMode != config.AgentTLSModeSystemStore {
		return nil, fmt.Errorf(
			"provided config value for agentTLSMode is none of [%q,%q]",
			config.AgentTLSModeStrict,
			config.AgentTLSModeSystemStore,
		)
	}

	clientConfig, err := clientcmd.NewClientConfigFromBytes(data)
	if err != nil {
		return nil, err
	}

	raw, err := clientConfig.RawConfig()
	if err != nil {
		return nil, err
	}

	if agentTLSMode == config.AgentTLSModeSystemStore && raw.Contexts[raw.CurrentContext] != nil {
		cluster := raw.Contexts[raw.CurrentContext].Cluster
		if raw.Clusters[cluster] != nil {
			req, err := http.NewRequestWithContext(context.Background(), http.MethodGet, raw.Clusters[cluster].Server, nil)
			if err == nil {
				if resp, err := http.DefaultClient.Do(req); err == nil {
					resp.Body.Close()
					raw.Clusters[cluster].CertificateAuthorityData = nil
				}
			}
		}
	}

	return clientcmd.NewDefaultClientConfig(raw, &clientcmd.ConfigOverrides{}).ClientConfig()
}

func (i *importHandler) checkForConfigChange(cfg *config.Config, cluster *fleet.Cluster, secret *corev1.Secret) error {
	// Already marked for attempting to import
	if cluster.Status.AgentConfigChanged {
		return nil
	}

	apiServerConfigChanged, err := i.hasAPIServerConfigChanged(cfg, secret, cluster)
	if err != nil {
		if errors.Is(err, errUnavailableAPIServerURL) {
			// skip the rest of checks
			logrus.WithError(err).Warnf("cluster %s/%s: could not check for config changes", cluster.Namespace, cluster.Name)
			return nil
		}
		return err
	}
	hasConfigChanged := apiServerConfigChanged ||
		cfg.AgentTLSMode != cluster.Status.AgentTLSMode ||
		hasGarbageCollectionIntervalChanged(cfg, cluster)

	if !hasConfigChanged {
		return nil
	}

	logrus.Infof("API server config changed, trigger cluster import for cluster %s/%s", cluster.Namespace, cluster.Name)
	c := cluster.DeepCopy()
	c.Status.AgentConfigChanged = true
	_, err = i.clusters.UpdateStatus(c)
	return err
}

func getKubeConfigSecretNS(cluster *fleet.Cluster) string {
	if cluster.Spec.KubeConfigSecretNamespace == "" {
		return cluster.Namespace
	}

	return cluster.Spec.KubeConfigSecretNamespace
}

func hasGarbageCollectionIntervalChanged(config *config.Config, cluster *fleet.Cluster) bool {
	return (config.GarbageCollectionInterval.Duration != 0 && cluster.Status.GarbageCollectionInterval == nil) ||
		(cluster.Status.GarbageCollectionInterval != nil &&
			config.GarbageCollectionInterval.Duration != cluster.Status.GarbageCollectionInterval.Duration)
}



================================================
FILE: internal/cmd/controller/agentmanagement/controllers/cluster/import_test.go
================================================
package cluster

import (
	"testing"
	"time"

	"github.com/rancher/wrangler/v3/pkg/generic/fake"
	"go.uber.org/mock/gomock"
	"k8s.io/apimachinery/pkg/labels"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"

	"github.com/rancher/fleet/internal/config"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

func TestOnConfig(t *testing.T) {
	cases := map[string]struct {
		cfg              config.Config
		handlerWithMocks func(t *testing.T) importHandler
	}{
		"no clusters, no import": {
			cfg: config.Config{
				APIServerCA:  []byte("foo"),
				APIServerURL: "https://hello.world",
				AgentTLSMode: "system-store",
			},
			handlerWithMocks: func(t *testing.T) importHandler {
				t.Helper()
				ctrl := gomock.NewController(t)

				secretsCache := fake.NewMockCacheInterface[*corev1.Secret](ctrl)

				clustersCache := fake.NewMockCacheInterface[*fleet.Cluster](ctrl)
				clustersCache.EXPECT().List("", gomock.Eq(labels.Everything())).Return(nil, nil)

				return importHandler{
					clustersCache: clustersCache,
					secretsCache:  secretsCache,
				}
			},
		},
		"no URL or CA in secret, do not trigger import when URL changes": {
			cfg: config.Config{
				APIServerCA:               []byte("foo"),
				APIServerURL:              "https://hello.new.world",
				AgentTLSMode:              "system-store",
				GarbageCollectionInterval: metav1.Duration{Duration: 10 * time.Minute},
			},
			handlerWithMocks: func(t *testing.T) importHandler {
				t.Helper()
				ctrl := gomock.NewController(t)

				secretsCache := fake.NewMockCacheInterface[*corev1.Secret](ctrl)
				secretsCache.EXPECT().Get(gomock.Any(), "my-kubeconfig-secret").Return(&corev1.Secret{}, nil)

				clustersCache := fake.NewMockCacheInterface[*fleet.Cluster](ctrl)
				clustersCache.EXPECT().List("", gomock.Eq(labels.Everything())).
					Return([]*fleet.Cluster{
						{
							ObjectMeta: metav1.ObjectMeta{
								Name:      "cluster",
								Namespace: "fleet-default",
							},
							Spec: fleet.ClusterSpec{
								KubeConfigSecret: "my-kubeconfig-secret",
							},
							Status: fleet.ClusterStatus{
								APIServerURL:              "https://hello.world",
								APIServerCAHash:           hashStatusField("foo"),
								AgentTLSMode:              "system-store",
								GarbageCollectionInterval: &metav1.Duration{Duration: 10 * time.Minute},
							},
						},
					}, nil)
				clustersController := fake.NewMockControllerInterface[*fleet.Cluster, *fleet.ClusterList](ctrl)
				clustersController.EXPECT().UpdateStatus(gomock.Any()) // import triggered

				return importHandler{
					clusters:      clustersController,
					clustersCache: clustersCache,
					secretsCache:  secretsCache,
				}
			},
		},
		"no URL or CA in secret, do not trigger import when CA changes": {
			cfg: config.Config{
				APIServerCA:               []byte("new-foo"),
				APIServerURL:              "https://hello.world",
				AgentTLSMode:              "system-store",
				GarbageCollectionInterval: metav1.Duration{Duration: 10 * time.Minute},
			},
			handlerWithMocks: func(t *testing.T) importHandler {
				t.Helper()
				ctrl := gomock.NewController(t)

				secretsCache := fake.NewMockCacheInterface[*corev1.Secret](ctrl)
				secretsCache.EXPECT().Get(gomock.Any(), "my-kubeconfig-secret").Return(&corev1.Secret{}, nil)

				clustersCache := fake.NewMockCacheInterface[*fleet.Cluster](ctrl)
				clustersCache.EXPECT().List("", gomock.Eq(labels.Everything())).
					Return([]*fleet.Cluster{
						{
							ObjectMeta: metav1.ObjectMeta{
								Name:      "cluster",
								Namespace: "fleet-default",
							},
							Spec: fleet.ClusterSpec{
								KubeConfigSecret: "my-kubeconfig-secret",
							},
							Status: fleet.ClusterStatus{
								APIServerURL:              "https://hello.world",
								APIServerCAHash:           hashStatusField("foo"),
								AgentTLSMode:              "system-store",
								GarbageCollectionInterval: &metav1.Duration{Duration: 10 * time.Minute},
							},
						},
					}, nil)
				clustersController := fake.NewMockControllerInterface[*fleet.Cluster, *fleet.ClusterList](ctrl)
				clustersController.EXPECT().UpdateStatus(gomock.Any()) // import triggered

				return importHandler{
					clusters:      clustersController,
					clustersCache: clustersCache,
					secretsCache:  secretsCache,
				}
			},
		},
		"non-ready config and no URL or CA in secret, do not trigger import when CA changes": {
			cfg: config.Config{
				APIServerURL:              "",
				AgentTLSMode:              "strict",
				GarbageCollectionInterval: metav1.Duration{Duration: 10 * time.Minute},
			},
			handlerWithMocks: func(t *testing.T) importHandler {
				t.Helper()
				ctrl := gomock.NewController(t)

				secretsCache := fake.NewMockCacheInterface[*corev1.Secret](ctrl)
				secretsCache.EXPECT().Get(gomock.Any(), "my-kubeconfig-secret").Return(&corev1.Secret{}, nil)

				clustersCache := fake.NewMockCacheInterface[*fleet.Cluster](ctrl)
				clustersCache.EXPECT().List("", gomock.Eq(labels.Everything())).
					Return([]*fleet.Cluster{
						{
							ObjectMeta: metav1.ObjectMeta{
								Name:      "cluster",
								Namespace: "fleet-default",
							},
							Spec: fleet.ClusterSpec{
								KubeConfigSecret: "my-kubeconfig-secret",
							},
							Status: fleet.ClusterStatus{
								APIServerURL:              "",
								APIServerCAHash:           "",
								AgentTLSMode:              "system-store",
								GarbageCollectionInterval: &metav1.Duration{Duration: 10 * time.Minute},
							},
						},
					}, nil)
				clustersController := fake.NewMockControllerInterface[*fleet.Cluster, *fleet.ClusterList](ctrl)
				clustersController.EXPECT().UpdateStatus(gomock.Any()).Times(0) // import not triggered

				return importHandler{
					clusters:      clustersController,
					clustersCache: clustersCache,
					secretsCache:  secretsCache,
				}
			},
		},
		"no URL or CA in secret, trigger import when agent TLS mode changes": {
			cfg: config.Config{
				APIServerCA:               []byte("foo"),
				APIServerURL:              "https://hello.world",
				AgentTLSMode:              "strict",
				GarbageCollectionInterval: metav1.Duration{Duration: 10 * time.Minute},
			},
			handlerWithMocks: func(t *testing.T) importHandler {
				t.Helper()
				ctrl := gomock.NewController(t)

				secretsCache := fake.NewMockCacheInterface[*corev1.Secret](ctrl)
				secretsCache.EXPECT().Get(gomock.Any(), "my-kubeconfig-secret").Return(&corev1.Secret{}, nil)

				clustersCache := fake.NewMockCacheInterface[*fleet.Cluster](ctrl)
				clustersCache.EXPECT().List("", gomock.Eq(labels.Everything())).
					Return([]*fleet.Cluster{
						{
							ObjectMeta: metav1.ObjectMeta{
								Name:      "cluster",
								Namespace: "fleet-default",
							},
							Spec: fleet.ClusterSpec{
								KubeConfigSecret: "my-kubeconfig-secret",
							},
							Status: fleet.ClusterStatus{
								APIServerURL:              "https://hello.world",
								APIServerCAHash:           hashStatusField("foo"),
								AgentTLSMode:              "system-store",
								GarbageCollectionInterval: &metav1.Duration{Duration: 10 * time.Minute},
							},
						},
					}, nil)
				clustersController := fake.NewMockControllerInterface[*fleet.Cluster, *fleet.ClusterList](ctrl)
				clustersController.EXPECT().UpdateStatus(gomock.Any()) // import triggered

				return importHandler{
					clusters:      clustersController,
					clustersCache: clustersCache,
					secretsCache:  secretsCache,
				}
			},
		},
		"no URL or CA in secret, trigger import when agent garbage collection interval changes": {
			cfg: config.Config{
				APIServerCA:               []byte("foo"),
				APIServerURL:              "https://hello.world",
				AgentTLSMode:              "system-store",
				GarbageCollectionInterval: metav1.Duration{Duration: 5 * time.Minute},
			},
			handlerWithMocks: func(t *testing.T) importHandler {
				t.Helper()
				ctrl := gomock.NewController(t)

				secretsCache := fake.NewMockCacheInterface[*corev1.Secret](ctrl)
				secretsCache.EXPECT().Get(gomock.Any(), "my-kubeconfig-secret").Return(&corev1.Secret{}, nil)

				clustersCache := fake.NewMockCacheInterface[*fleet.Cluster](ctrl)
				clustersCache.EXPECT().List("", gomock.Eq(labels.Everything())).
					Return([]*fleet.Cluster{
						{
							ObjectMeta: metav1.ObjectMeta{
								Name:      "cluster",
								Namespace: "fleet-default",
							},
							Spec: fleet.ClusterSpec{
								KubeConfigSecret: "my-kubeconfig-secret",
							},
							Status: fleet.ClusterStatus{
								APIServerURL:              "https://hello.world",
								APIServerCAHash:           hashStatusField("foo"),
								AgentTLSMode:              "system-store",
								GarbageCollectionInterval: &metav1.Duration{Duration: 10 * time.Minute},
							},
						},
					}, nil)
				clustersController := fake.NewMockControllerInterface[*fleet.Cluster, *fleet.ClusterList](ctrl)
				clustersController.EXPECT().UpdateStatus(gomock.Any()) // import triggered

				return importHandler{
					clusters:      clustersController,
					clustersCache: clustersCache,
					secretsCache:  secretsCache,
				}
			},
		},
		"URL and CA in secret, do not trigger import when only URL or CA changes": {
			cfg: config.Config{
				APIServerCA:               []byte("new-foo"),
				APIServerURL:              "https://hello.new.world",
				AgentTLSMode:              "system-store",
				GarbageCollectionInterval: metav1.Duration{Duration: 10 * time.Minute},
			},
			handlerWithMocks: func(t *testing.T) importHandler {
				t.Helper()
				ctrl := gomock.NewController(t)

				clustersCache := fake.NewMockCacheInterface[*fleet.Cluster](ctrl)
				clustersCache.EXPECT().List("", gomock.Eq(labels.Everything())).
					Return([]*fleet.Cluster{
						{
							ObjectMeta: metav1.ObjectMeta{
								Name:      "cluster",
								Namespace: "fleet-default",
							},
							Spec: fleet.ClusterSpec{
								KubeConfigSecret: "my-kubeconfig-secret",
							},
							Status: fleet.ClusterStatus{
								APIServerURL:              "https://hello.secret.world",
								APIServerCAHash:           hashStatusField("secret-foo"),
								AgentTLSMode:              "system-store",
								GarbageCollectionInterval: &metav1.Duration{Duration: 10 * time.Minute},
							},
						},
					}, nil)
				secretsCache := fake.NewMockCacheInterface[*corev1.Secret](ctrl)
				secretsCache.EXPECT().Get(gomock.Any(), "my-kubeconfig-secret").Return(&corev1.Secret{
					Data: map[string][]byte{
						"apiServerURL": []byte("https://hello.secret.world"),
						"apiServerCA":  []byte(hashStatusField("secret-foo")),
					},
				}, nil)

				// No UpdateStatus expected

				return importHandler{
					clustersCache: clustersCache,
					secretsCache:  secretsCache,
				}
			},
		},
		"URL and CA in secret, trigger import when agent TLS mode changes": {
			cfg: config.Config{
				APIServerCA:               []byte("foo"),
				APIServerURL:              "https://hello.world",
				AgentTLSMode:              "strict",
				GarbageCollectionInterval: metav1.Duration{Duration: 10 * time.Minute},
			},
			handlerWithMocks: func(t *testing.T) importHandler {
				t.Helper()
				ctrl := gomock.NewController(t)

				clustersCache := fake.NewMockCacheInterface[*fleet.Cluster](ctrl)
				clustersCache.EXPECT().List("", gomock.Eq(labels.Everything())).
					Return([]*fleet.Cluster{
						{
							ObjectMeta: metav1.ObjectMeta{
								Name:      "cluster",
								Namespace: "fleet-default",
							},
							Spec: fleet.ClusterSpec{
								KubeConfigSecret: "my-kubeconfig-secret",
							},
							Status: fleet.ClusterStatus{
								APIServerURL:              "https://hello.secret.world",
								APIServerCAHash:           hashStatusField("secret-foo"),
								AgentTLSMode:              "system-store",
								GarbageCollectionInterval: &metav1.Duration{Duration: 10 * time.Minute},
							},
						},
					}, nil)
				secretsCache := fake.NewMockCacheInterface[*corev1.Secret](ctrl)
				secretsCache.EXPECT().Get(gomock.Any(), "my-kubeconfig-secret").Return(&corev1.Secret{
					Data: map[string][]byte{
						"apiServerURL": []byte("https://hello.secret.world"),
						"apiServerCA":  []byte(hashStatusField("secret-foo")),
					},
				}, nil)

				clustersController := fake.NewMockControllerInterface[*fleet.Cluster, *fleet.ClusterList](ctrl)
				clustersController.EXPECT().UpdateStatus(gomock.Any()) // import triggered

				return importHandler{
					clusters:      clustersController,
					clustersCache: clustersCache,
					secretsCache:  secretsCache,
				}
			},
		},
	}

	for name, c := range cases {
		t.Run(name, func(t *testing.T) {
			ih := c.handlerWithMocks(t)

			err := ih.onConfig(&c.cfg)
			if err != nil {
				t.Errorf("unexpected error: expected nil, got %v", err)
			}

		})
	}
}



================================================
FILE: internal/cmd/controller/agentmanagement/controllers/clusterregistration/controller.go
================================================
// Package clusterregistration implements manager-initiated and agent-initiated registration.
//
// Add or import downstream clusters / agents to Fleet and keep information
// from their registration (e.g. local cluster kubeconfig) up-to-date.
package clusterregistration

import (
	"context"
	"fmt"
	"time"

	"github.com/sirupsen/logrus"

	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/controllers/resources"
	secretutil "github.com/rancher/fleet/internal/cmd/controller/agentmanagement/secret"
	"github.com/rancher/fleet/internal/config"
	"github.com/rancher/fleet/internal/names"
	"github.com/rancher/fleet/internal/registration"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"
	fleetcontrollers "github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io/v1alpha1"

	"github.com/rancher/wrangler/v3/pkg/apply"
	corecontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"
	rbaccontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/rbac/v1"
	"github.com/rancher/wrangler/v3/pkg/generic"
	"github.com/rancher/wrangler/v3/pkg/relatedresource"

	v1 "k8s.io/api/core/v1"
	rbacv1 "k8s.io/api/rbac/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
)

const (
	AgentCredentialSecretType     = "fleet.cattle.io/agent-credential" //nolint:gosec // not a credential
	clusterByClientID             = "clusterByClientID"
	clusterRegistrationByClientID = "clusterRegistrationByClientID"
	deleteSecretAfter             = durations.ClusterRegistrationDeleteDelay
)

type handler struct {
	systemNamespace             string
	systemRegistrationNamespace string
	clusterRegistration         fleetcontrollers.ClusterRegistrationController
	clusterCache                fleetcontrollers.ClusterCache
	clusters                    fleetcontrollers.ClusterClient
	serviceAccountCache         corecontrollers.ServiceAccountCache
	secretsCache                corecontrollers.SecretCache
	secrets                     corecontrollers.SecretController
}

func Register(ctx context.Context,
	apply apply.Apply,
	systemNamespace string,
	systemRegistrationNamespace string,
	serviceAccount corecontrollers.ServiceAccountController,
	secret corecontrollers.SecretController,
	role rbaccontrollers.RoleController,
	roleBinding rbaccontrollers.RoleBindingController,
	clusterRegistration fleetcontrollers.ClusterRegistrationController,
	clusters fleetcontrollers.ClusterController) {
	h := &handler{
		systemNamespace:             systemNamespace,
		systemRegistrationNamespace: systemRegistrationNamespace,
		clusterRegistration:         clusterRegistration,
		clusterCache:                clusters.Cache(),
		clusters:                    clusters,
		serviceAccountCache:         serviceAccount.Cache(),
		secrets:                     secret,
		secretsCache:                secret.Cache(),
	}

	fleetcontrollers.RegisterClusterRegistrationGeneratingHandler(ctx,
		clusterRegistration,
		apply.WithCacheTypes(serviceAccount,
			secret,
			role,
			roleBinding,
		),
		"",
		"cluster-registration",
		h.OnChange,
		&generic.GeneratingHandlerOptions{
			AllowClusterScoped: true,
		})

	secret.OnChange(ctx, "registration-expire", h.OnSecretChange)
	clusters.OnChange(ctx, "cluster-to-clusterregistration", h.OnCluster)
	clusters.Cache().AddIndexer(clusterByClientID, func(obj *fleet.Cluster) ([]string, error) {
		return []string{
			fmt.Sprintf("%s/%s", obj.Namespace, obj.Spec.ClientID),
		}, nil
	})
	clusterRegistration.Cache().AddIndexer(clusterRegistrationByClientID, func(obj *fleet.ClusterRegistration) ([]string, error) {
		return []string{
			fmt.Sprintf("%s/%s", obj.Namespace, obj.Spec.ClientID),
		}, nil
	})
	relatedresource.Watch(ctx, "sa-to-cluster-registration", saToClusterRegistration, clusterRegistration, serviceAccount)
}

func saToClusterRegistration(namespace, name string, obj runtime.Object) ([]relatedresource.Key, error) {
	if sa, ok := obj.(*v1.ServiceAccount); ok {
		ns := sa.Annotations[fleet.ClusterRegistrationNamespaceAnnotation]
		name := sa.Annotations[fleet.ClusterRegistrationAnnotation]
		if ns != "" && name != "" {
			return []relatedresource.Key{{
				Namespace: ns,
				Name:      name,
			}}, nil
		}
	}
	return nil, nil
}

func (h *handler) OnCluster(key string, cluster *fleet.Cluster) (*fleet.Cluster, error) {
	if cluster == nil || cluster.Status.Namespace == "" {
		return cluster, nil
	}

	crs, err := h.clusterRegistration.Cache().GetByIndex(clusterRegistrationByClientID,
		fmt.Sprintf("%s/%s", cluster.Namespace, cluster.Spec.ClientID))
	if err != nil {
		return nil, err
	}
	for _, cr := range crs {
		if !cr.Status.Granted {
			logrus.Infof("Namespace assigned to cluster '%s/%s' enqueues cluster registration '%s/%s'", cluster.Namespace, cluster.Name,
				cr.Namespace, cr.Name)
			h.clusterRegistration.Enqueue(cr.Namespace, cr.Name)
		}
	}

	return cluster, nil
}

func (h *handler) OnSecretChange(key string, secret *v1.Secret) (*v1.Secret, error) {
	if secret == nil || secret.Namespace != h.systemRegistrationNamespace ||
		secret.Labels[fleet.ClusterAnnotation] == "" {
		return secret, nil
	}

	if time.Since(secret.CreationTimestamp.Time) > deleteSecretAfter {
		logrus.Infof("Deleting expired registration secret %s/%s", secret.Namespace, secret.Name)
		return secret, h.secrets.Delete(secret.Namespace, secret.Name, nil)
	}

	h.secrets.EnqueueAfter(secret.Namespace, secret.Name, deleteSecretAfter/2)
	return secret, nil
}

func skipClusterRegistration(cr *fleet.ClusterRegistration) bool {
	if cr == nil {
		return true
	}
	if cr.Labels == nil {
		return false
	}
	if cr.Labels[fleet.ClusterManagementLabel] != "" {
		return true
	}
	return false
}

// OnChange creates the service account and roles for a cluster registration.
// The service account's token is deployed to the downstream cluster, via the
// fleet-secret. It allows the downstream fleet-agent to list
// bundledeployments and update their status in its own cluster namespace on upstream.
// It can also get content resources, but not list them. The name of content
// resources is random.
func (h *handler) OnChange(request *fleet.ClusterRegistration, status fleet.ClusterRegistrationStatus) ([]runtime.Object, fleet.ClusterRegistrationStatus, error) {
	if status.Granted {
		// only create the cluster for the request once
		return nil, status, generic.ErrSkip
	}
	if skipClusterRegistration(request) {
		return nil, status, generic.ErrSkip
	}

	cluster, err := h.createOrGetCluster(request)
	if err != nil || cluster == nil {
		return nil, status, err
	}

	if cluster.Status.Namespace == "" {
		status.ClusterName = cluster.Name
		return nil, status, nil
	}

	// set the Cluster as owner of the cluster registration request
	// ownerFound is used to avoid calling update on request whenever OnChange is called
	ownerFound := false
	for _, owner := range request.OwnerReferences {
		if owner.Kind == "Cluster" && owner.Name == cluster.Name && owner.UID == cluster.UID {
			ownerFound = true
			break
		}
	}
	if !ownerFound {
		request.SetOwnerReferences([]metav1.OwnerReference{
			{
				APIVersion: fleet.SchemeGroupVersion.String(),
				Kind:       "Cluster",
				Name:       cluster.Name,
				UID:        cluster.UID,
			},
		})
		request, err = h.clusterRegistration.Update(request)
		if err != nil {
			return nil, status, err
		}
	}

	saName := names.SafeConcatName(request.Name, string(request.UID))
	sa, err := h.serviceAccountCache.Get(cluster.Status.Namespace, saName)
	if err != nil && apierrors.IsNotFound(err) {
		// create request service account if missing
		status.ClusterName = cluster.Name
		return []runtime.Object{requestSA(saName, cluster, request)}, status, nil
	} else if err != nil {
		return nil, status, fmt.Errorf("failed to retrieve service account from cache: %w", err)
	}

	// try to get request service account's token
	var secret *v1.Secret
	if secret, err = h.authorizeCluster(sa, cluster, request); err != nil {
		return nil, status, fmt.Errorf("failed to authorize cluster, cannot get service account token: %w", err)
	} else if secret == nil {
		status.ClusterName = cluster.Name
		logrus.Infof("Cluster registration request '%s/%s', cluster '%s/%s' not granted, waiting for service account token",
			request.Namespace, request.Name, cluster.Namespace, cluster.Name)
		return nil, status, nil
	}

	// delete old cluster registrations
	crlist, _ := h.clusterRegistration.List(request.Namespace, metav1.ListOptions{})
	for _, creg := range crlist.Items {
		if shouldDelete(creg, *request) {
			logrus.Debugf("Deleting old clusterregistration '%s/%s', now at '%s'", creg.Namespace, creg.Name, request.Name)
			if err := h.clusterRegistration.Delete(creg.Namespace, creg.Name, nil); err != nil && !apierrors.IsNotFound(err) {
				return nil, status, err
			}
		}
	}

	// request is granted, create the registration secret and roles
	status.ClusterName = cluster.Name
	status.Granted = true

	logrus.Infof("Cluster registration request '%s/%s' granted, creating cluster, request service account, registration secret", request.Namespace, request.Name)

	return []runtime.Object{
		// the registration secret c-clientID-clientRandom
		secret,
		// Update the existing service account 'request-UID' in the
		// cluster namespace, e.g. 'cluster-fleet-default-NAME-ID'
		requestSA(saName, cluster, request),
		// Add role bindings to manage bundledeployments and contents,
		// the agent could previously only access secrets in
		// 'cattle-fleet-clusters-system' and clusterregistrations in
		// the cluster registration namespace (e.g. 'fleet-default'). See
		// clusterregistrationtoken controller for details.
		&rbacv1.Role{
			ObjectMeta: metav1.ObjectMeta{
				Name:      request.Name,
				Namespace: request.Namespace,
				Labels: map[string]string{
					fleet.ManagedLabel: "true",
				},
			},
			Rules: []rbacv1.PolicyRule{
				{
					Verbs:         []string{"patch"},
					APIGroups:     []string{fleet.SchemeGroupVersion.Group},
					Resources:     []string{fleet.ClusterResourceNamePlural + "/status"},
					ResourceNames: []string{cluster.Name},
				},
			},
		},
		&rbacv1.RoleBinding{
			ObjectMeta: metav1.ObjectMeta{
				Name:      request.Name,
				Namespace: request.Namespace,
				Labels: map[string]string{
					fleet.ManagedLabel: "true",
				},
			},
			Subjects: []rbacv1.Subject{
				{
					Kind:      "ServiceAccount",
					Name:      saName,
					Namespace: cluster.Status.Namespace,
				},
			},
			RoleRef: rbacv1.RoleRef{
				APIGroup: rbacv1.GroupName,
				Kind:     "Role",
				Name:     request.Name,
			},
		},
		// cluster role "fleet-bundle-deployment" created when
		// fleet-controller starts
		&rbacv1.RoleBinding{
			ObjectMeta: metav1.ObjectMeta{
				Name:      request.Name,
				Namespace: cluster.Status.Namespace,
				Labels: map[string]string{
					fleet.ManagedLabel: "true",
				},
			},
			Subjects: []rbacv1.Subject{
				{
					Kind:      "ServiceAccount",
					Name:      saName,
					Namespace: cluster.Status.Namespace,
				},
			},
			RoleRef: rbacv1.RoleRef{
				APIGroup: rbacv1.GroupName,
				Kind:     "ClusterRole",
				Name:     resources.BundleDeploymentClusterRole,
			},
		},
		// cluster role "fleet-content" created when fleet-controller
		// starts
		&rbacv1.ClusterRoleBinding{
			ObjectMeta: metav1.ObjectMeta{
				Name: names.SafeConcatName(request.Name, "content"),
				Labels: map[string]string{
					fleet.ManagedLabel: "true",
				},
			},
			Subjects: []rbacv1.Subject{
				{
					Kind:      "ServiceAccount",
					Name:      saName,
					Namespace: cluster.Status.Namespace,
				},
			},
			RoleRef: rbacv1.RoleRef{
				APIGroup: rbacv1.GroupName,
				Kind:     "ClusterRole",
				Name:     resources.ContentClusterRole,
			},
		},
	}, status, nil
}

// shouldDelete returns true for any other cluster registration with the same clientID, but different random and older creation timestamp
func shouldDelete(creg fleet.ClusterRegistration, request fleet.ClusterRegistration) bool {
	return creg.Spec.ClientID == request.Spec.ClientID &&
		creg.Spec.ClientRandom != request.Spec.ClientRandom &&
		creg.Name != request.Name &&
		creg.CreationTimestamp.Time.Before(request.CreationTimestamp.Time)
}

func (h *handler) createOrGetCluster(request *fleet.ClusterRegistration) (*fleet.Cluster, error) {
	clusters, err := h.clusterCache.GetByIndex(clusterByClientID, fmt.Sprintf("%s/%s", request.Namespace, request.Spec.ClientID))
	if err == nil && len(clusters) > 0 {
		return clusters[0], nil
	} else if err != nil && !apierrors.IsNotFound(err) {
		return nil, err
	}

	clusterName := names.SafeConcatName("cluster", names.KeyHash(request.Spec.ClientID))
	if cluster, err := h.clusterCache.Get(request.Namespace, clusterName); !apierrors.IsNotFound(err) {
		if cluster.Spec.ClientID != request.Spec.ClientID {
			// This would happen with a hash collision
			return nil, fmt.Errorf("non-matching ClientID on cluster %s/%s got %s expected %s",
				request.Namespace, clusterName, cluster.Spec.ClientID, request.Spec.ClientID)
		}
		return cluster, err
	}

	// need to create the cluster for agent initiated registration, local
	// and managed clusters would already exist
	labels := map[string]string{}
	if !config.Get().IgnoreClusterRegistrationLabels {
		for k, v := range request.Spec.ClusterLabels {
			labels[k] = v
		}
	}
	labels[fleet.ClusterAnnotation] = clusterName

	cluster, err := h.clusters.Create(&fleet.Cluster{
		ObjectMeta: metav1.ObjectMeta{
			Name:      clusterName,
			Namespace: request.Namespace,
			Labels:    labels,
		},
		Spec: fleet.ClusterSpec{
			ClientID: request.Spec.ClientID,
		},
	})
	if apierrors.IsAlreadyExists(err) {
		return h.clusters.Get(request.Namespace, clusterName, metav1.GetOptions{})
	}
	if err == nil {
		logrus.Infof("Created cluster %s/%s", request.Namespace, clusterName)
	}
	return cluster, err
}

func (h *handler) authorizeCluster(sa *v1.ServiceAccount, cluster *fleet.Cluster, req *fleet.ClusterRegistration) (*v1.Secret, error) {
	var secret *v1.Secret
	var err error
	if len(sa.Secrets) != 0 {
		secret, err = h.secretsCache.Get(sa.Namespace, sa.Secrets[0].Name)
		if apierrors.IsNotFound(err) {
			// secrets can be slow to propagate to the cache
			secret, err = h.secrets.Get(sa.Namespace, sa.Secrets[0].Name, metav1.GetOptions{})
		}
	} else {
		secret, err = secretutil.GetServiceAccountTokenSecret(sa, h.secrets)
	}
	if err != nil || secret == nil {
		return nil, err
	}
	return &v1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:      registration.SecretName(req.Spec.ClientID, req.Spec.ClientRandom),
			Namespace: h.systemRegistrationNamespace,
			Labels: map[string]string{
				fleet.ClusterAnnotation: cluster.Name,
				fleet.ManagedLabel:      "true",
			},
		},
		Type: AgentCredentialSecretType,
		Data: map[string][]byte{
			"token":               secret.Data["token"],
			"deploymentNamespace": []byte(cluster.Status.Namespace),
			"clusterNamespace":    []byte(cluster.Namespace),
			"clusterName":         []byte(cluster.Name),
			"systemNamespace":     []byte(h.systemNamespace),
		},
	}, nil
}

func requestSA(saName string, cluster *fleet.Cluster, request *fleet.ClusterRegistration) *v1.ServiceAccount {
	return &v1.ServiceAccount{
		ObjectMeta: metav1.ObjectMeta{
			Name:      saName,
			Namespace: cluster.Status.Namespace,
			Labels: map[string]string{
				fleet.ManagedLabel: "true",
			},
			Annotations: map[string]string{
				fleet.ClusterAnnotation:                      cluster.Name,
				fleet.ClusterRegistrationAnnotation:          request.Name,
				fleet.ClusterRegistrationNamespaceAnnotation: request.Namespace,
			},
		},
	}
}



================================================
FILE: internal/cmd/controller/agentmanagement/controllers/clusterregistration/controller_test.go
================================================
package clusterregistration

import (
	"fmt"

	"github.com/rancher/wrangler/v3/pkg/generic"
	"github.com/rancher/wrangler/v3/pkg/generic/fake"
	"go.uber.org/mock/gomock"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

var _ = Describe("ClusterRegistration OnChange", func() {
	var (
		request *fleet.ClusterRegistration
		status  fleet.ClusterRegistrationStatus
		cluster *fleet.Cluster
		sa      *corev1.ServiceAccount

		saCache                       *fake.MockCacheInterface[*corev1.ServiceAccount]
		secretCache                   *fake.MockCacheInterface[*corev1.Secret]
		secretController              *fake.MockControllerInterface[*corev1.Secret, *corev1.SecretList]
		clusterClient                 *fake.MockClientInterface[*fleet.Cluster, *fleet.ClusterList]
		clusterRegistrationController *fake.MockControllerInterface[*fleet.ClusterRegistration, *fleet.ClusterRegistrationList]
		clusterCache                  *fake.MockCacheInterface[*fleet.Cluster]
		h                             *handler
		notFound                      = errors.NewNotFound(schema.GroupResource{}, "")
		anError                       = fmt.Errorf("an error occurred")
	)

	BeforeEach(func() {
		ctrl := gomock.NewController(GinkgoT())
		saCache = fake.NewMockCacheInterface[*corev1.ServiceAccount](ctrl)
		secretCache = fake.NewMockCacheInterface[*corev1.Secret](ctrl)
		secretController = fake.NewMockControllerInterface[*corev1.Secret, *corev1.SecretList](ctrl)
		clusterClient = fake.NewMockClientInterface[*fleet.Cluster, *fleet.ClusterList](ctrl)
		clusterRegistrationController = fake.NewMockControllerInterface[*fleet.ClusterRegistration, *fleet.ClusterRegistrationList](ctrl)
		clusterCache = fake.NewMockCacheInterface[*fleet.Cluster](ctrl)

		h = &handler{
			systemNamespace:             "fleet-system",
			systemRegistrationNamespace: "fleet-clusters-system",
			clusterRegistration:         clusterRegistrationController,
			clusterCache:                clusterCache,
			clusters:                    clusterClient,
			secretsCache:                secretCache,
			secrets:                     secretController,
			serviceAccountCache:         saCache,
		}

	})

	Context("ClusterRegistration already granted", func() {
		BeforeEach(func() {
			status = fleet.ClusterRegistrationStatus{
				Granted: true,
			}
		})

		It("does nothing", func() {
			objs, newStatus, err := h.OnChange(request, status)
			Expect(err).To(Equal(generic.ErrSkip))
			Expect(objs).To(BeEmpty())
			Expect(newStatus.Granted).To(BeTrue())
		})
	})

	Context("Cluster is missing", func() {
		BeforeEach(func() {
			request = &fleet.ClusterRegistration{
				Spec: fleet.ClusterRegistrationSpec{
					ClientID: "client-id",
				},
			}
			status = fleet.ClusterRegistrationStatus{}

			clusterCache.EXPECT().GetByIndex(gomock.Any(), gomock.Any()).Return(nil, nil)
			// code panics if cache.Get returns an error or nil
			clusterCache.EXPECT().Get(gomock.Any(), gomock.Any()).Return(nil, nil).Return(nil, notFound)
		})

		When("cluster creation works", func() {
			BeforeEach(func() {
				clusterClient.EXPECT().Create(gomock.Any()).Return(nil, nil).Do(func(obj interface{}) {
					switch cluster := obj.(type) {
					case *fleet.Cluster:
						Expect(cluster.Spec.ClientID).To(Equal("client-id"))
					default:
						Fail("unexpected type")
					}
				})
			})

			It("creates the missing cluster", func() {
				objs, newStatus, err := h.OnChange(request, status)
				Expect(err).ToNot(HaveOccurred())
				Expect(objs).To(BeEmpty())
				Expect(newStatus.Granted).To(BeFalse())
			})
		})

		When("cluster creation fails", func() {
			BeforeEach(func() {
				clusterClient.EXPECT().Create(gomock.Any()).Return(nil, anError)
			})

			It("returns an error", func() {
				objs, newStatus, err := h.OnChange(request, status)
				Expect(err).To(HaveOccurred())
				Expect(objs).To(BeEmpty())
				Expect(newStatus.Granted).To(BeFalse())
			})
		})
	})

	Context("Cluster exists", func() {
		BeforeEach(func() {
			request = &fleet.ClusterRegistration{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "request-1",
					Namespace: "fleet-default",
				},
				Spec: fleet.ClusterRegistrationSpec{
					ClientID: "client-id",
				},
			}

			cluster = &fleet.Cluster{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "cluster",
					Namespace: "fleet-default",
				},
				Spec: fleet.ClusterSpec{
					ClientID: "client-id",
				},
			}
			status = fleet.ClusterRegistrationStatus{}

			clusterCache.EXPECT().GetByIndex(gomock.Any(), gomock.Any()).Return(nil, nil)
			clusterCache.EXPECT().Get(gomock.Any(), gomock.Any()).Return(nil, nil).Return(cluster, nil)
		})

		When("cluster status has no namespace", func() {
			It("sets the cluster name into the registrations status", func() {
				objs, newStatus, err := h.OnChange(request, status)
				Expect(err).ToNot(HaveOccurred())
				Expect(objs).To(BeEmpty())
				Expect(newStatus.Granted).To(BeFalse())
				Expect(newStatus.ClusterName).To(Equal("cluster"))
			})
		})

		When("service account does not exist", func() {
			BeforeEach(func() {
				cluster.Status = fleet.ClusterStatus{Namespace: "fleet-default"}
				saCache.EXPECT().Get(gomock.Any(), gomock.Any()).Return(nil, notFound)
				clusterRegistrationController.EXPECT().Update(gomock.Any()).Return(&fleet.ClusterRegistration{}, nil)
			})

			It("creates a new service account", func() {
				objs, newStatus, err := h.OnChange(request, status)
				Expect(err).ToNot(HaveOccurred())
				Expect(objs).To(HaveLen(1))
				Expect(newStatus.Granted).To(BeFalse())
				Expect(newStatus.ClusterName).To(Equal("cluster"))
			})
		})

		When("service account secret is missing", func() {
			BeforeEach(func() {
				cluster.Status = fleet.ClusterStatus{Namespace: "fleet-default"}
				// post k8s 1.24 service account without sa.Secrets list
				sa = &corev1.ServiceAccount{}
				saCache.EXPECT().Get(gomock.Any(), gomock.Any()).Return(sa, nil)
				clusterRegistrationController.EXPECT().Update(gomock.Any()).Return(&fleet.ClusterRegistration{}, nil)
			})

			Context("cannot create secret", func() {
				BeforeEach(func() {
					secretController.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any()).Return(nil, notFound)
					secretController.EXPECT().Create(gomock.Any()).Return(nil, anError)
				})

				It("creates a new service account and errors", func() {
					objs, _, err := h.OnChange(request, status)
					Expect(err).To(HaveOccurred())
					Expect(err.Error()).To(ContainSubstring("failed to authorize cluster"))
					Expect(objs).To(BeEmpty())
				})
			})

			Context("authorizeCluster returns nil,nil", func() {
				BeforeEach(func() {
					// pre k8s 1.24 service account has sa.Secrets list
					sa.Secrets = []corev1.ObjectReference{{Name: "tokensecret"}}
					secretCache.EXPECT().Get(gomock.Any(), gomock.Any()).Return(nil, notFound)
					secretController.EXPECT().Get(gomock.Any(), "tokensecret", gomock.Any()).Return(nil, nil)
				})

				It("returns early", func() {
					objs, newStatus, err := h.OnChange(request, status)
					Expect(err).ToNot(HaveOccurred())
					Expect(objs).To(BeEmpty())
					Expect(newStatus.ClusterName).To(Equal("cluster"))
					Expect(newStatus.Granted).To(BeFalse())
				})
			})
		})

		When("service account secret exists", func() {
			BeforeEach(func() {
				cluster.Status = fleet.ClusterStatus{Namespace: "fleet-default"}

				sa = &corev1.ServiceAccount{}
				saCache.EXPECT().Get(gomock.Any(), gomock.Any()).Return(sa, nil)

				// needs token here, otherwise controller will sleep to wait for it
				secret := &corev1.Secret{
					Data: map[string][]byte{"token": []byte("secrettoken")},
				}
				secretController.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any()).Return(secret, nil)

				clusterRegistrationController.EXPECT().List(gomock.Any(), gomock.Any()).Return(&fleet.ClusterRegistrationList{}, nil)

				clusterRegistrationController.EXPECT().Update(gomock.Any()).Return(&fleet.ClusterRegistration{}, nil)
			})

			Context("grants registration, cleans up and creates objects", func() {
				BeforeEach(func() {
				})

				It("creates a new secret", func() {
					objs, newStatus, err := h.OnChange(request, status)
					Expect(err).ToNot(HaveOccurred())
					Expect(objs).To(HaveLen(6))
					Expect(newStatus.Granted).To(BeTrue())
				})
			})
		})
	})
})



================================================
FILE: internal/cmd/controller/agentmanagement/controllers/clusterregistration/suite_test.go
================================================
package clusterregistration_test

import (
	"testing"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
	"github.com/rancher/fleet/internal/config"
)

func TestFleet(t *testing.T) {
	RegisterFailHandler(Fail)
	RunSpecs(t, "ClusterRegistration Controller Suite")
}

var _ = BeforeSuite(func() {
	_ = config.SetAndTrigger(&config.Config{IgnoreClusterRegistrationLabels: false})
})



================================================
FILE: internal/cmd/controller/agentmanagement/controllers/clusterregistrationtoken/handler.go
================================================
// Package clusterregistrationtoken provides a controller for ClusterRegistrationToken.
//
// It creates a service account and role binding for the token.
package clusterregistrationtoken

import (
	"context"
	"time"

	"github.com/sirupsen/logrus"

	secretutil "github.com/rancher/fleet/internal/cmd/controller/agentmanagement/secret"
	"github.com/rancher/fleet/internal/config"
	"github.com/rancher/fleet/internal/names"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	fleetcontrollers "github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io/v1alpha1"

	"github.com/rancher/wrangler/v3/pkg/apply"
	corecontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"
	"github.com/rancher/wrangler/v3/pkg/relatedresource"

	corev1 "k8s.io/api/core/v1"
	rbacv1 "k8s.io/api/rbac/v1"
	apierror "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	yaml "sigs.k8s.io/yaml"
)

type handler struct {
	systemNamespace             string
	systemRegistrationNamespace string
	clusterRegistrationTokens   fleetcontrollers.ClusterRegistrationTokenController
	serviceAccountCache         corecontrollers.ServiceAccountCache
	secretsCache                corecontrollers.SecretCache
	secretsController           corecontrollers.SecretController
}

func Register(ctx context.Context,
	systemNamespace string,
	systemRegistrationNamespace string,
	apply apply.Apply,
	clusterGroupToken fleetcontrollers.ClusterRegistrationTokenController,
	serviceAccounts corecontrollers.ServiceAccountController,
	secretsCache corecontrollers.SecretCache,
	secretsController corecontrollers.SecretController,
) {
	h := &handler{
		systemNamespace:             systemNamespace,
		systemRegistrationNamespace: systemRegistrationNamespace,
		clusterRegistrationTokens:   clusterGroupToken,
		serviceAccountCache:         serviceAccounts.Cache(),
		secretsCache:                secretsCache,
		secretsController:           secretsController,
	}

	fleetcontrollers.RegisterClusterRegistrationTokenGeneratingHandler(ctx,
		clusterGroupToken,
		apply,
		"",
		"cluster-group-token",
		h.OnChange,
		nil)

	relatedresource.Watch(ctx, "sa-to-cgt",
		relatedresource.OwnerResolver(true, fleet.SchemeGroupVersion.String(), "ClusterRegistrationToken"),
		clusterGroupToken, serviceAccounts)
}

func (h *handler) OnChange(token *fleet.ClusterRegistrationToken, status fleet.ClusterRegistrationTokenStatus) ([]runtime.Object, fleet.ClusterRegistrationTokenStatus, error) {
	if gone, err := h.deleteExpired(token); err != nil || gone {
		return nil, status, nil //nolint:nilerr // intentionally ignoring error to avoid retry loops on deletion failures
	}

	logrus.Debugf("Cluster registration token '%s/%s', creating import service account, roles and secret", token.Namespace, token.Name)

	var (
		saName  = names.SafeConcatName(token.Name, string(token.UID))
		secrets []runtime.Object
	)
	status.SecretName = ""
	sa, err := h.serviceAccountCache.Get(token.Namespace, saName)
	switch {
	case apierror.IsNotFound(err):
		logrus.Infof("ClusterRegistrationToken SA does not exist %v", saName)
		// secret doesn't exist
	case err != nil:
		return nil, status, err
	case len(sa.Secrets) > 0:
		status.SecretName = token.Name
		secrets, err = h.clusterRegistrationSecret(token, sa.Secrets[0].Name)
		if err != nil {
			return nil, status, err
		}
	case len(sa.Secrets) == 0:
		// Kubernetes 1.24 doesn't populate serviceAccount.Secrets:
		// "This field should not be used to find auto-generated
		// service account token secrets for use outside of pods."
		secretCreated, err := secretutil.GetServiceAccountTokenSecret(sa, h.secretsController)
		if err != nil {
			return nil, status, err
		}

		if string(secretCreated.Data["token"]) == "" {
			logrus.Debugf("ClusterRegistrationToken SA does not have a secret %s/%s", token.Namespace, saName)

			secretReloaded, err := h.secretsCache.Get(token.Namespace, secretCreated.Name)
			if err != nil {
				return nil, status, err
			}

			if string(secretReloaded.Data["token"]) == "" {
				// it can take some time for the secret to be populated, try later
				h.clusterRegistrationTokens.Enqueue(token.Namespace, token.Name)
				return nil, status, err
			}
		}

		status.SecretName = token.Name
		secrets, err = h.clusterRegistrationSecret(token, secretCreated.Name)
		if err != nil {
			return nil, status, err
		}
	}

	status.Expires = nil
	if token.Spec.TTL != nil {
		status.Expires = &metav1.Time{Time: token.CreationTimestamp.Add(token.Spec.TTL.Duration)}
	}

	// Add service account, e.g.: import-token-local in the system
	// registration namespace. This account is used during registration to
	// access secrets in the system registration namespace
	// 'cattle-fleet-clusters-system' and clusterregistrations in the
	// cluster registration namespace (e.g. 'fleet-default').
	return append([]runtime.Object{
		&corev1.ServiceAccount{
			ObjectMeta: metav1.ObjectMeta{
				Name:      saName,
				Namespace: token.Namespace,
				Labels: map[string]string{
					fleet.ManagedLabel: "true",
				},
			},
		},
		&rbacv1.Role{
			ObjectMeta: metav1.ObjectMeta{
				Name:      names.SafeConcatName(saName, "role"),
				Namespace: token.Namespace,
				Labels: map[string]string{
					fleet.ManagedLabel: "true",
				},
			},
			Rules: []rbacv1.PolicyRule{
				{
					Verbs:     []string{"create"},
					APIGroups: []string{fleet.SchemeGroupVersion.Group},
					Resources: []string{fleet.ClusterRegistrationResourceNamePlural},
				},
			},
		},
		&rbacv1.RoleBinding{
			ObjectMeta: metav1.ObjectMeta{
				Name:      names.SafeConcatName(saName, "to", "role"),
				Namespace: token.Namespace,
				Labels: map[string]string{
					fleet.ManagedLabel: "true",
				},
			},
			Subjects: []rbacv1.Subject{
				{
					Kind:      "ServiceAccount",
					Name:      saName,
					Namespace: token.Namespace,
				},
			},
			RoleRef: rbacv1.RoleRef{
				APIGroup: rbacv1.GroupName,
				Kind:     "Role",
				Name:     names.SafeConcatName(saName, "role"),
			},
		},
		&rbacv1.Role{
			ObjectMeta: metav1.ObjectMeta{
				Name:      names.SafeConcatName(saName, "creds"),
				Namespace: h.systemRegistrationNamespace,
			},
			Rules: []rbacv1.PolicyRule{
				{
					Verbs:     []string{"get"},
					APIGroups: []string{""},
					Resources: []string{"secrets"},
				},
			},
		},
		&rbacv1.RoleBinding{
			ObjectMeta: metav1.ObjectMeta{
				Name:      names.SafeConcatName(saName, "creds"),
				Namespace: h.systemRegistrationNamespace,
			},
			Subjects: []rbacv1.Subject{
				{
					Kind:      "ServiceAccount",
					Name:      saName,
					Namespace: token.Namespace,
				},
			},
			RoleRef: rbacv1.RoleRef{
				APIGroup: rbacv1.GroupName,
				Kind:     "Role",
				Name:     names.SafeConcatName(saName, "creds"),
			},
		},
	}, secrets...), status, nil
}

// clusterRegistrationSecret fetches the "clusterregistrationtoken" service
// account's secret and returns the cluster registration values secret, e.g.
// 'import-token-NAME'. The returned secret is of the type
// "fleet.cattle.io/cluster-registration-values".
func (h *handler) clusterRegistrationSecret(token *fleet.ClusterRegistrationToken, secretName string) ([]runtime.Object, error) {
	if secretName == "" {
		return nil, nil
	}

	secret, err := h.secretsCache.Get(token.Namespace, secretName)
	if err != nil {
		return nil, err
	}

	values := map[string]interface{}{
		"clusterNamespace":            token.Namespace,
		config.APIServerURLKey:        config.Get().APIServerURL,
		config.APIServerCAKey:         string(config.Get().APIServerCA),
		"token":                       string(secret.Data["token"]), // from service account
		"systemRegistrationNamespace": h.systemRegistrationNamespace,
	}

	if h.systemNamespace != config.DefaultNamespace {
		values["internal"] = map[string]interface{}{
			"systemNamespace": h.systemNamespace,
		}
	}

	data, err := yaml.Marshal(values)
	if err != nil {
		return nil, err
	}

	return []runtime.Object{
		&corev1.Secret{
			ObjectMeta: metav1.ObjectMeta{
				Name:      token.Name,
				Namespace: token.Namespace,
				Labels: map[string]string{
					fleet.ManagedLabel: "true",
				},
			},
			Immutable: nil,
			Data: map[string][]byte{
				config.ImportTokenSecretValuesKey: data,
			},
			Type: "fleet.cattle.io/cluster-registration-values",
		},
	}, nil

}

func (h *handler) deleteExpired(token *fleet.ClusterRegistrationToken) (bool, error) {
	ttl := token.Spec.TTL
	if ttl == nil || ttl.Duration <= 0 {
		return false, nil
	}
	expire := token.CreationTimestamp.Add(ttl.Duration)
	if time.Now().After(expire) {
		return true, h.clusterRegistrationTokens.Delete(token.Namespace, token.Name, nil)
	}

	h.clusterRegistrationTokens.EnqueueAfter(token.Namespace, token.Name, time.Until(expire))
	return false, nil
}



================================================
FILE: internal/cmd/controller/agentmanagement/controllers/config/controller.go
================================================
// Package config reads the initial global configuration.
package config

import (
	"context"

	"github.com/rancher/fleet/internal/config"

	corecontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"

	v1 "k8s.io/api/core/v1"
)

// Register watches for changes in the config. Both fleetcontroller and agentmanagement needs to register this since it
// is used in both programs.
func Register(ctx context.Context,
	namespace string,
	cm corecontrollers.ConfigMapController) error {

	cm.OnChange(ctx, "global-config", func(_ string, configMap *v1.ConfigMap) (*v1.ConfigMap, error) {
		return reloadConfig(namespace, configMap)
	})

	cfg, err := config.Lookup(ctx, namespace, config.ManagerConfigName, cm)
	if err != nil {
		return err
	}

	return config.SetAndTrigger(cfg)
}

func reloadConfig(namespace string, configMap *v1.ConfigMap) (*v1.ConfigMap, error) {
	if configMap == nil {
		// ConfigMap was deleted, nothing to reload. This is a normal deletion event.
		return nil, nil
	}

	if configMap.Name != config.ManagerConfigName ||
		configMap.Namespace != namespace {
		return configMap, nil
	}

	cfg, err := config.ReadConfig(configMap)
	if err != nil {
		return configMap, err
	}

	return configMap, config.SetAndTrigger(cfg)
}



================================================
FILE: internal/cmd/controller/agentmanagement/controllers/manageagent/manageagent.go
================================================
// Package manageagent provides a controller for managing the agent bundle.
//
// Allows Fleet to deploy the Fleet Agent itself as a Bundle, which ensures
// changes to Fleet’s configuration are reflected in the Agent.
// The agent is deployed into the namespace, that contains a cluster resource.
package manageagent

import (
	"cmp"
	"context"
	"crypto/sha256"
	"encoding/json"
	"fmt"

	"github.com/sirupsen/logrus"

	"github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/agent"
	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/scheduling"
	"github.com/rancher/fleet/internal/config"
	"github.com/rancher/fleet/internal/names"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	fleetcontrollers "github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io/v1alpha1"

	"github.com/rancher/wrangler/v3/pkg/apply"
	corecontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"
	"github.com/rancher/wrangler/v3/pkg/relatedresource"
	"github.com/rancher/wrangler/v3/pkg/yaml"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/utils/ptr"
)

const (
	AgentBundleName = "fleet-agent"
)

type handler struct {
	apply           apply.Apply
	systemNamespace string
	clusterCache    fleetcontrollers.ClusterCache
	bundleCache     fleetcontrollers.BundleCache
	namespaces      corecontrollers.NamespaceController
}

func Register(ctx context.Context,
	systemNamespace string,
	apply apply.Apply,
	namespaces corecontrollers.NamespaceController,
	clusters fleetcontrollers.ClusterController,
	bundle fleetcontrollers.BundleController,
) {
	h := handler{
		systemNamespace: systemNamespace,
		clusterCache:    clusters.Cache(),
		bundleCache:     bundle.Cache(),
		namespaces:      namespaces,
		apply: apply.
			WithSetID("fleet-manage-agent").
			WithCacheTypes(bundle),
	}

	// update the agent bundles for all clusters in the triggered namespace
	namespaces.OnChange(ctx, "manage-agent", h.OnNamespace)
	// enqueue events for the agent bundle's namespace when clusters change
	relatedresource.WatchClusterScoped(ctx, "manage-agent-resolver", h.resolveNS, namespaces, clusters)
	fleetcontrollers.RegisterClusterStatusHandler(ctx,
		clusters,
		"Reconciled",
		"agent-env-vars",
		h.onClusterStatusChange)
}

func (h *handler) onClusterStatusChange(cluster *fleet.Cluster, status fleet.ClusterStatus) (fleet.ClusterStatus, error) {
	if SkipCluster(cluster) {
		return status, nil
	}

	logrus.Debugf("Reconciling agent settings for cluster %s/%s", cluster.Namespace, cluster.Name)

	status, vars, err := h.reconcileAgentEnvVars(cluster, status)
	if err != nil {
		return status, err
	}

	status, changed, err := h.updateClusterStatus(cluster, status)
	if err != nil {
		return status, err
	}

	if vars || changed {
		// trigger importCluster to re-create the deployment, in case
		// the agent cannot update itself from the bundle
		status.AgentConfigChanged = true
		h.namespaces.Enqueue(cluster.Namespace)
	}

	return status, nil
}

func hashStatusField(field any) (string, error) {
	hasher := sha256.New224()
	b, err := json.Marshal(field)
	if err != nil {
		return "", err
	}
	hasher.Write(b)
	return fmt.Sprintf("%x", hasher.Sum(nil)), nil
}

func hashChanged(field any, statusHash string) (bool, string, error) {
	isNil := func(field any) bool {
		switch field := field.(type) {
		case *fleet.AgentSchedulingCustomization:
			return field == nil
		case *corev1.Affinity:
			return field == nil
		case *corev1.ResourceRequirements:
			return field == nil
		case []corev1.Toleration:
			return len(field) == 0
		default:
			return false
		}
	}

	if isNil(field) {
		if statusHash != "" {
			return true, "", nil
		}
		return false, "", nil
	}

	hash, err := hashStatusField(field)
	if err != nil {
		return false, "", err
	}

	return statusHash != hash, hash, nil
}

// reconcileAgentEnvVars checks if the agent environment variables field was
// updated by hashing its contents into a status field.
func (h *handler) reconcileAgentEnvVars(cluster *fleet.Cluster, status fleet.ClusterStatus) (fleet.ClusterStatus, bool, error) {
	enqueue := false

	if len(cluster.Spec.AgentEnvVars) < 1 {
		// Remove the existing hash if the environment variables have been deleted.
		if status.AgentEnvVarsHash != "" {
			// We enqueue to ensure that we edit the status after other controllers.
			enqueue = true
			status.AgentEnvVarsHash = ""
		}
		return status, enqueue, nil
	}

	hash, err := hashStatusField(cluster.Spec.AgentEnvVars)
	if err != nil {
		return status, enqueue, err
	}

	if status.AgentEnvVarsHash != hash {
		// We enqueue to ensure that we edit the status after other controllers.
		enqueue = true
		status.AgentEnvVarsHash = hash
	}

	return status, enqueue, nil
}

func (h *handler) updateClusterStatus(cluster *fleet.Cluster, status fleet.ClusterStatus) (fleet.ClusterStatus, bool, error) {
	changed := false

	if status.AgentPrivateRepoURL != cluster.Spec.PrivateRepoURL {
		status.AgentPrivateRepoURL = cluster.Spec.PrivateRepoURL
		changed = true
	}

	if hostNetwork := *cmp.Or(cluster.Spec.HostNetwork, ptr.To(false)); status.AgentHostNetwork != hostNetwork {
		status.AgentHostNetwork = hostNetwork
		changed = true
	}

	if c, hash, err := hashChanged(cluster.Spec.AgentSchedulingCustomization, status.AgentSchedulingCustomizationHash); err != nil {
		return status, changed, err
	} else if c {
		status.AgentSchedulingCustomizationHash = hash
		changed = c
	}

	if c, hash, err := hashChanged(cluster.Spec.AgentAffinity, status.AgentAffinityHash); err != nil {
		return status, changed, err
	} else if c {
		status.AgentAffinityHash = hash
		changed = c
	}

	if c, hash, err := hashChanged(cluster.Spec.AgentResources, status.AgentResourcesHash); err != nil {
		return status, changed, err
	} else if c {
		status.AgentResourcesHash = hash
		changed = c
	}

	if c, hash, err := hashChanged(cluster.Spec.AgentTolerations, status.AgentTolerationsHash); err != nil {
		return status, changed, err
	} else if c {
		status.AgentTolerationsHash = hash
		changed = c
	}

	return status, changed, nil
}

// resolveNS is a handler that enqueues the cluster registration namespace (e.g. fleet-default) for a changed cluster
func (h *handler) resolveNS(namespace, _ string, obj runtime.Object) ([]relatedresource.Key, error) {
	if cluster, ok := obj.(*fleet.Cluster); ok {
		if _, err := h.bundleCache.Get(namespace, names.SafeConcatName(AgentBundleName, cluster.Name)); err != nil {
			// Bundle doesn't exist (or can't be retrieved), enqueue the namespace to create/reconcile it
			//nolint:nilerr // Intentionally ignoring error - "not found" is expected and should trigger reconciliation
			return []relatedresource.Key{{Name: namespace}}, nil
		}
	}
	return nil, nil
}

// OnNamespace updates agent bundles for all clusters in the namespace
func (h *handler) OnNamespace(key string, namespace *corev1.Namespace) (*corev1.Namespace, error) {
	if namespace == nil {
		// Namespace was deleted, nothing to process. This is a normal deletion event.
		return nil, nil
	}

	cfg := config.Get()
	// managed agents are disabled, so we don't need to create the bundle
	if cfg.ManageAgent != nil && !*cfg.ManageAgent {
		// Agent management is disabled via configuration. Nothing to do.
		return nil, nil
	}

	clusters, err := h.clusterCache.List(namespace.Name, labels.Everything())
	if err != nil {
		return nil, err
	}

	if len(clusters) == 0 {
		return namespace, nil
	}

	var objs []runtime.Object

	for _, cluster := range clusters {
		if SkipCluster(cluster) {
			continue
		}
		logrus.Infof("Update agent bundle for cluster %s/%s", cluster.Namespace, cluster.Name)
		bundle, err := h.newAgentBundle(namespace.Name, cluster)
		if err != nil {
			logrus.Errorf("Failed to update agent bundle for cluster %s/%s", cluster.Namespace, cluster.Name)
			return nil, err
		}
		objs = append(objs, bundle)
	}

	return namespace, h.apply.
		WithOwner(namespace).
		WithDefaultNamespace(namespace.Name).
		WithListerNamespace(namespace.Name).
		ApplyObjects(objs...)
}

func (h *handler) newAgentBundle(ns string, cluster *fleet.Cluster) (runtime.Object, error) {
	cfg := config.Get()
	agentNamespace := h.systemNamespace
	if cluster.Spec.AgentNamespace != "" {
		agentNamespace = cluster.Spec.AgentNamespace
	}

	agentReplicas := cmd.ParseEnvAgentReplicaCount()
	leaderElectionOptions, err := cmd.NewLeaderElectionOptionsWithPrefix("FLEET_AGENT")
	if err != nil {
		return nil, err
	}

	priorityClassName := ""
	if cluster.Spec.AgentSchedulingCustomization != nil && cluster.Spec.AgentSchedulingCustomization.PriorityClass != nil {
		priorityClassName = scheduling.FleetAgentPriorityClassName
	}

	// Notice we only set the agentScope when it's a non-default agentNamespace. This is for backwards compatibility
	// for when we didn't have agent scope before
	objs := agent.Manifest(
		agentNamespace, cluster.Spec.AgentNamespace,
		agent.ManifestOptions{
			// keep in sync with cluster/import.go
			AgentEnvVars:     cluster.Spec.AgentEnvVars,
			AgentTolerations: cluster.Spec.AgentTolerations,
			PrivateRepoURL:   cluster.Spec.PrivateRepoURL,
			AgentAffinity:    cluster.Spec.AgentAffinity,
			AgentResources:   cluster.Spec.AgentResources,
			HostNetwork:      *cmp.Or(cluster.Spec.HostNetwork, ptr.To(false)),

			// keep in sync with agent/agent.go
			AgentImage:              cfg.AgentImage,
			AgentImagePullPolicy:    cfg.AgentImagePullPolicy,
			CheckinInterval:         cfg.AgentCheckinInterval.Duration.String(),
			SystemDefaultRegistry:   cfg.SystemDefaultRegistry,
			BundleDeploymentWorkers: cfg.AgentWorkers.BundleDeployment,
			DriftWorkers:            cfg.AgentWorkers.Drift,
			AgentReplicas:           agentReplicas,
			LeaderElectionOptions:   leaderElectionOptions,
			PriorityClassName:       priorityClassName,
		},
	)
	agentYAML, err := yaml.Export(objs...)
	if err != nil {
		return nil, err
	}

	return &fleet.Bundle{
		ObjectMeta: metav1.ObjectMeta{
			Name:      names.SafeConcatName(AgentBundleName, cluster.Name),
			Namespace: ns,
		},
		Spec: fleet.BundleSpec{
			BundleDeploymentOptions: fleet.BundleDeploymentOptions{
				DefaultNamespace: agentNamespace,
				Helm: &fleet.HelmOptions{
					TakeOwnership: true,
				},
			},
			Resources: []fleet.BundleResource{
				{
					Name:    "agent.yaml",
					Content: string(agentYAML),
				},
			},
			Targets: []fleet.BundleTarget{
				{
					ClusterSelector: &metav1.LabelSelector{
						MatchExpressions: []metav1.LabelSelectorRequirement{
							{
								Key:      "fleet.cattle.io/non-managed-agent",
								Operator: metav1.LabelSelectorOpDoesNotExist,
							},
						},
					},
					ClusterName: cluster.Name,
				},
			},
		},
	}, nil
}

// SkipCluster checks if the cluster, according to its label, should be skipped
// for classic agent management.
func SkipCluster(cluster *fleet.Cluster) bool {
	if cluster == nil {
		return true
	}
	if cluster.Labels == nil {
		return false
	}
	if cluster.Labels[fleet.ClusterManagementLabel] != "" {
		return true
	}
	return false
}



================================================
FILE: internal/cmd/controller/agentmanagement/controllers/manageagent/manageagent_test.go
================================================
package manageagent

import (
	"testing"

	"github.com/rancher/wrangler/v3/pkg/generic/fake"
	"go.uber.org/mock/gomock"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	"k8s.io/utils/ptr"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

func TestOnClusterChangeAffinity(t *testing.T) {
	ctrl := gomock.NewController(t)
	namespaces := fake.NewMockNonNamespacedControllerInterface[*corev1.Namespace, *corev1.NamespaceList](ctrl)
	h := &handler{namespaces: namespaces}

	// defaultAffinity from the manifest in manifest.go
	defaultAffinity := &corev1.Affinity{NodeAffinity: &corev1.NodeAffinity{
		RequiredDuringSchedulingIgnoredDuringExecution: &corev1.NodeSelector{
			NodeSelectorTerms: []corev1.NodeSelectorTerm{{
				MatchExpressions: []corev1.NodeSelectorRequirement{
					{Key: "fleet.cattle.io/agent", Operator: corev1.NodeSelectorOpIn, Values: []string{"true"}},
				},
			}},
		}},
	}
	hash, _ := hashStatusField(defaultAffinity)

	customAffinity := &corev1.Affinity{NodeAffinity: &corev1.NodeAffinity{
		RequiredDuringSchedulingIgnoredDuringExecution: &corev1.NodeSelector{
			NodeSelectorTerms: []corev1.NodeSelectorTerm{{
				MatchExpressions: []corev1.NodeSelectorRequirement{
					{Key: "foo", Operator: corev1.NodeSelectorOpIn, Values: []string{"bar"}},
				},
			}},
		}},
	}
	customHash, _ := hashStatusField(customAffinity)

	emptyHash, _ := hashStatusField(&corev1.Affinity{})

	for _, tt := range []struct {
		name           string
		cluster        *fleet.Cluster
		status         fleet.ClusterStatus
		expectedStatus fleet.ClusterStatus
		enqueues       int
	}{
		{
			name:           "Empty Affinity",
			cluster:        &fleet.Cluster{},
			status:         fleet.ClusterStatus{},
			expectedStatus: fleet.ClusterStatus{},
			enqueues:       0,
		},
		{
			name:           "Equal Affinity",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{AgentAffinity: defaultAffinity}},
			status:         fleet.ClusterStatus{AgentAffinityHash: hash},
			expectedStatus: fleet.ClusterStatus{AgentAffinityHash: hash},
			enqueues:       0,
		},
		{
			name:           "Equal Custom Affinity",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{AgentAffinity: customAffinity}},
			status:         fleet.ClusterStatus{AgentAffinityHash: customHash},
			expectedStatus: fleet.ClusterStatus{AgentAffinityHash: customHash},
			enqueues:       0,
		},
		{
			name:           "Changed Affinity",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{AgentAffinity: customAffinity}},
			status:         fleet.ClusterStatus{AgentAffinityHash: hash},
			expectedStatus: fleet.ClusterStatus{AgentAffinityHash: customHash},
			enqueues:       1,
		},
		{
			name:           "Changed to Empty Affinity",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{}},
			status:         fleet.ClusterStatus{AgentAffinityHash: customHash},
			expectedStatus: fleet.ClusterStatus{AgentAffinityHash: ""},
			enqueues:       1,
		},
		{
			name:           "Removed Affinity",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{AgentAffinity: &corev1.Affinity{}}},
			status:         fleet.ClusterStatus{AgentAffinityHash: customHash},
			expectedStatus: fleet.ClusterStatus{AgentAffinityHash: emptyHash},
			enqueues:       1,
		},
	} {
		t.Run(tt.name, func(t *testing.T) {
			namespaces.EXPECT().Enqueue(gomock.Any()).Times(tt.enqueues)

			status, err := h.onClusterStatusChange(tt.cluster, tt.status)
			if err != nil {
				t.Error(err)
			}

			if status.AgentAffinityHash != tt.expectedStatus.AgentAffinityHash {
				t.Fatalf("agent affinity hash is not equal: %v vs %v", status.AgentAffinityHash, tt.expectedStatus.AgentAffinityHash)
			}
		})
	}
}

func TestOnClusterChangeResources(t *testing.T) {
	ctrl := gomock.NewController(t)
	namespaces := fake.NewMockNonNamespacedControllerInterface[*corev1.Namespace, *corev1.NamespaceList](ctrl)
	h := &handler{namespaces: namespaces}

	customResources := corev1.ResourceRequirements{
		Limits: corev1.ResourceList{
			corev1.ResourceCPU:    resource.MustParse("100m"),
			corev1.ResourceMemory: resource.MustParse("100Mi"),
		},

		Requests: corev1.ResourceList{
			corev1.ResourceCPU:    resource.MustParse("50m"),
			corev1.ResourceMemory: resource.MustParse("50Mi"),
		},
	}
	customHash, _ := hashStatusField(customResources)

	for _, tt := range []struct {
		name           string
		cluster        *fleet.Cluster
		status         fleet.ClusterStatus
		expectedStatus fleet.ClusterStatus
		enqueues       int
	}{
		{
			name:           "Empty Resources",
			cluster:        &fleet.Cluster{},
			status:         fleet.ClusterStatus{},
			expectedStatus: fleet.ClusterStatus{},
			enqueues:       0,
		},
		{
			name:           "Equal Resources",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{AgentResources: &customResources}},
			status:         fleet.ClusterStatus{AgentResourcesHash: customHash},
			expectedStatus: fleet.ClusterStatus{AgentResourcesHash: customHash},
			enqueues:       0,
		},
		{
			name:           "Changed Resources",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{AgentResources: &customResources}},
			status:         fleet.ClusterStatus{AgentResourcesHash: ""},
			expectedStatus: fleet.ClusterStatus{AgentResourcesHash: customHash},
			enqueues:       1,
		},
		{
			name:           "Removed Resources",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{}},
			status:         fleet.ClusterStatus{AgentTolerationsHash: customHash},
			expectedStatus: fleet.ClusterStatus{AgentTolerationsHash: ""},
			enqueues:       1,
		},
	} {
		t.Run(tt.name, func(t *testing.T) {
			namespaces.EXPECT().Enqueue(gomock.Any()).Times(tt.enqueues)

			status, err := h.onClusterStatusChange(tt.cluster, tt.status)
			if err != nil {
				t.Error(err)
			}

			if status.AgentResourcesHash != tt.expectedStatus.AgentResourcesHash {
				t.Fatalf("agent resources hash is not equal: %v vs %v", status.AgentResourcesHash, tt.expectedStatus.AgentResourcesHash)
			}
		})
	}
}

func TestOnClusterChangeTolerations(t *testing.T) {
	ctrl := gomock.NewController(t)
	namespaces := fake.NewMockNonNamespacedControllerInterface[*corev1.Namespace, *corev1.NamespaceList](ctrl)
	h := &handler{namespaces: namespaces}

	// defaultTolerations from the manifest in manifest.go
	defaultTolerations := []corev1.Toleration{
		{
			Key:      "node.cloudprovider.kubernetes.io/uninitialized",
			Operator: corev1.TolerationOpEqual,
			Value:    "true",
			Effect:   corev1.TaintEffectNoSchedule,
		},
		{
			Key:      "cattle.io/os",
			Operator: corev1.TolerationOpEqual,
			Value:    "linux",
			Effect:   corev1.TaintEffectNoSchedule,
		},
	}
	hash, _ := hashStatusField(defaultTolerations)

	customTolerations := []corev1.Toleration{
		{
			Key:      "node.cloudprovider.kubernetes.io/windows",
			Operator: corev1.TolerationOpEqual,
			Value:    "false",
			Effect:   corev1.TaintEffectNoSchedule,
		},
	}
	customHash, _ := hashStatusField(customTolerations)

	for _, tt := range []struct {
		name           string
		cluster        *fleet.Cluster
		status         fleet.ClusterStatus
		expectedStatus fleet.ClusterStatus
		enqueues       int
	}{
		{
			name:           "Empty Resources",
			cluster:        &fleet.Cluster{},
			status:         fleet.ClusterStatus{},
			expectedStatus: fleet.ClusterStatus{},
			enqueues:       0,
		},
		{
			name:           "Equal Tolerations",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{AgentTolerations: defaultTolerations}},
			status:         fleet.ClusterStatus{AgentTolerationsHash: hash},
			expectedStatus: fleet.ClusterStatus{AgentTolerationsHash: hash},
			enqueues:       0,
		},
		{
			name:           "Equal Custom Tolerations",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{AgentTolerations: customTolerations}},
			status:         fleet.ClusterStatus{AgentTolerationsHash: customHash},
			expectedStatus: fleet.ClusterStatus{AgentTolerationsHash: customHash},
			enqueues:       0,
		},
		{
			name:           "Changed Tolerations",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{AgentTolerations: customTolerations}},
			status:         fleet.ClusterStatus{AgentTolerationsHash: hash},
			expectedStatus: fleet.ClusterStatus{AgentTolerationsHash: customHash},
			enqueues:       1,
		},
		{
			name:           "Removed Tolerations, omitted",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{}},
			status:         fleet.ClusterStatus{AgentTolerationsHash: customHash},
			expectedStatus: fleet.ClusterStatus{AgentTolerationsHash: ""},
			enqueues:       1,
		},
		{
			name:           "Removed Tolerations, empty list",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{AgentTolerations: []corev1.Toleration{}}},
			status:         fleet.ClusterStatus{AgentTolerationsHash: customHash},
			expectedStatus: fleet.ClusterStatus{AgentTolerationsHash: ""},
			enqueues:       1,
		},
	} {
		t.Run(tt.name, func(t *testing.T) {
			namespaces.EXPECT().Enqueue(gomock.Any()).Times(tt.enqueues)

			status, err := h.onClusterStatusChange(tt.cluster, tt.status)
			if err != nil {
				t.Error(err)
			}

			if status.AgentTolerationsHash != tt.expectedStatus.AgentTolerationsHash {
				t.Fatalf("agent tolerations hash is not equal: %v vs %v", status.AgentTolerationsHash, tt.expectedStatus.AgentTolerationsHash)
			}
		})
	}
}

func TestOnClusterChangeHostNetwork(t *testing.T) {
	ctrl := gomock.NewController(t)
	namespaces := fake.NewMockNonNamespacedControllerInterface[*corev1.Namespace, *corev1.NamespaceList](ctrl)
	h := &handler{namespaces: namespaces}

	for _, tt := range []struct {
		name           string
		cluster        *fleet.Cluster
		status         fleet.ClusterStatus
		expectedStatus fleet.ClusterStatus
		enqueues       int
	}{
		{
			name:           "Empty",
			cluster:        &fleet.Cluster{},
			status:         fleet.ClusterStatus{},
			expectedStatus: fleet.ClusterStatus{},
			enqueues:       0,
		},
		{
			name:           "Equal HostNetwork",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{HostNetwork: ptr.To(true)}},
			status:         fleet.ClusterStatus{AgentHostNetwork: true},
			expectedStatus: fleet.ClusterStatus{AgentHostNetwork: true},
			enqueues:       0,
		},
		{
			name:           "Changed HostNetwork",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{HostNetwork: ptr.To(true)}},
			status:         fleet.ClusterStatus{AgentHostNetwork: false},
			expectedStatus: fleet.ClusterStatus{AgentHostNetwork: true},
			enqueues:       1,
		},
		{
			name:           "Removed Resources",
			cluster:        &fleet.Cluster{Spec: fleet.ClusterSpec{}},
			status:         fleet.ClusterStatus{AgentHostNetwork: true},
			expectedStatus: fleet.ClusterStatus{AgentHostNetwork: false},
			enqueues:       1,
		},
	} {
		t.Run(tt.name, func(t *testing.T) {
			namespaces.EXPECT().Enqueue(gomock.Any()).Times(tt.enqueues)

			status, err := h.onClusterStatusChange(tt.cluster, tt.status)
			if err != nil {
				t.Error(err)
			}

			if status.AgentHostNetwork != tt.expectedStatus.AgentHostNetwork {
				t.Fatalf("agent hostStatus is not equal: %v vs %v", status.AgentHostNetwork, tt.expectedStatus.AgentHostNetwork)
			}
		})
	}
}



================================================
FILE: internal/cmd/controller/agentmanagement/controllers/resources/data.go
================================================
package resources

import (
	"github.com/rancher/fleet/internal/experimental"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"github.com/rancher/wrangler/v3/pkg/apply"

	corev1 "k8s.io/api/core/v1"
	rbacv1 "k8s.io/api/rbac/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

const (
	BundleDeploymentClusterRole = "fleet-bundle-deployment"
	ContentClusterRole          = "fleet-content"
)

// ApplyBootstrapResources creates the cluster roles, system namespace and system registration namespace
func ApplyBootstrapResources(systemNamespace, systemRegistrationNamespace string, apply apply.Apply) error {
	rules := []rbacv1.PolicyRule{
		{
			Verbs:     []string{"get", "list", "watch"},
			APIGroups: []string{fleet.SchemeGroupVersion.Group},
			Resources: []string{fleet.BundleDeploymentResourceNamePlural},
		},
		{
			Verbs:     []string{"update", "patch"},
			APIGroups: []string{fleet.SchemeGroupVersion.Group},
			Resources: []string{fleet.BundleDeploymentResourceNamePlural + "/status"},
		},
		{
			Verbs:     []string{"get"},
			APIGroups: []string{""},
			Resources: []string{"secrets"},
		},
	}

	if experimental.CopyResourcesDownstreamEnabled() {
		rules = append(rules, rbacv1.PolicyRule{
			Verbs:     []string{"get"},
			APIGroups: []string{""},
			Resources: []string{"configmaps"},
		})
	}

	return apply.ApplyObjects(
		// used by request-* service accounts from agents
		&rbacv1.ClusterRole{
			ObjectMeta: metav1.ObjectMeta{
				Name: BundleDeploymentClusterRole,
			},
			Rules: rules,
		},
		// used by request-* service accounts from agents
		&rbacv1.ClusterRole{
			ObjectMeta: metav1.ObjectMeta{
				Name: ContentClusterRole,
			},
			Rules: []rbacv1.PolicyRule{
				{
					Verbs:     []string{"get"},
					APIGroups: []string{fleet.SchemeGroupVersion.Group},
					Resources: []string{fleet.ContentResourceNamePlural},
				},
			},
		},
		&corev1.Namespace{
			ObjectMeta: metav1.ObjectMeta{
				Name: systemNamespace,
			},
		},
		&corev1.Namespace{
			ObjectMeta: metav1.ObjectMeta{
				Name: systemRegistrationNamespace,
			},
		},
	)
}



================================================
FILE: internal/cmd/controller/agentmanagement/scheduling/scheduling.go
================================================
package scheduling

import (
	"fmt"

	"github.com/sirupsen/logrus"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	policyv1 "k8s.io/api/policy/v1"
	schedulingv1 "k8s.io/api/scheduling/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/intstr"
)

const (
	FleetAgentPriorityClassName       = "fleet-agent-priority-class"
	FleetAgentPodDisruptionBudgetName = "fleet-agent-pod-disruption-budget"
)

func PriorityClass(priorityClass *fleet.PriorityClassSpec) *schedulingv1.PriorityClass {
	return &schedulingv1.PriorityClass{
		ObjectMeta: metav1.ObjectMeta{
			Name: FleetAgentPriorityClassName,
		},
		//nolint:gosec // TODO: PriorityClassSpec should use int32 as its Value type in a future minor release.
		Value:            int32(priorityClass.Value),
		Description:      "Priority class for Fleet Agent",
		PreemptionPolicy: priorityClass.PreemptionPolicy,
	}
}

func PodDisruptionBudget(agentNamespace string, pdbs *fleet.PodDisruptionBudgetSpec) (*policyv1.PodDisruptionBudget, error) {
	pdbSpec := policyv1.PodDisruptionBudgetSpec{
		Selector: &metav1.LabelSelector{
			MatchLabels: map[string]string{"app": "fleet-agent"},
		},
	}

	switch {
	case pdbs.MaxUnavailable == "" && pdbs.MinAvailable == "":
		logrus.Warnf("Neither MaxUnavailable nor MinAvailable is set, defaulting to 0 for MaxUnavailable")
		pdbSpec.MaxUnavailable = &intstr.IntOrString{IntVal: 0}
	case pdbs.MaxUnavailable != "" && (pdbs.MinAvailable == "" || pdbs.MinAvailable == "0"):
		mu := intstr.Parse(pdbs.MaxUnavailable)
		pdbSpec.MaxUnavailable = &mu
	case pdbs.MinAvailable != "" && (pdbs.MaxUnavailable == "" || pdbs.MaxUnavailable == "0"):
		ma := intstr.Parse(pdbs.MinAvailable)
		pdbSpec.MinAvailable = &ma
	case pdbs.MaxUnavailable != "" && pdbs.MinAvailable != "":
		return &policyv1.PodDisruptionBudget{},
			fmt.Errorf("both MaxUnavailable (%s) and MinAvailable (%s) are set, not creating PDB", pdbs.MaxUnavailable, pdbs.MinAvailable)
	}

	return &policyv1.PodDisruptionBudget{
		ObjectMeta: metav1.ObjectMeta{
			Name:      FleetAgentPodDisruptionBudgetName,
			Namespace: agentNamespace,
		},
		Spec: pdbSpec,
	}, nil
}



================================================
FILE: internal/cmd/controller/agentmanagement/scheduling/scheduling_test.go
================================================
package scheduling_test

import (
	"math"
	"testing"

	"github.com/rancher/fleet/internal/cmd/controller/agentmanagement/scheduling"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/intstr"

	corev1 "k8s.io/api/core/v1"
	schedulingv1 "k8s.io/api/scheduling/v1"
)

func ptr[T any](v T) *T {
	return &v
}

func TestPodDisruptionBudget(t *testing.T) {
	tests := []struct {
		name    string
		spec    *fleet.PodDisruptionBudgetSpec
		wantMU  *intstr.IntOrString
		wantMA  *intstr.IntOrString
		wantErr bool
	}{
		{
			name:   "neither value set should default to maxUnavailable 0",
			spec:   &fleet.PodDisruptionBudgetSpec{},
			wantMU: &intstr.IntOrString{Type: intstr.Int, IntVal: 0},
		},
		{
			name:   "maxUnavailable set as int",
			spec:   &fleet.PodDisruptionBudgetSpec{MaxUnavailable: "1"},
			wantMU: ptr(intstr.FromInt(1)),
		},
		{
			name:   "maxUnavailable set as percent string",
			spec:   &fleet.PodDisruptionBudgetSpec{MaxUnavailable: "50%"},
			wantMU: ptr(intstr.FromString("50%")),
		},
		{
			name:   "minAvailable set as int",
			spec:   &fleet.PodDisruptionBudgetSpec{MinAvailable: "2"},
			wantMA: ptr(intstr.FromInt(2)),
		},
		{
			name:    "both values set should result in an error",
			spec:    &fleet.PodDisruptionBudgetSpec{MinAvailable: "1", MaxUnavailable: "1"},
			wantErr: true,
		},
		{
			name:   "having both values, MinAvailable as zero, should disable the zero value",
			spec:   &fleet.PodDisruptionBudgetSpec{MinAvailable: "0", MaxUnavailable: "1"},
			wantMU: ptr(intstr.FromInt(1)),
		},
		{
			name:   "having both values, MaxUnavailable as zero, should disable the zero value",
			spec:   &fleet.PodDisruptionBudgetSpec{MinAvailable: "1", MaxUnavailable: "0"},
			wantMA: ptr(intstr.FromInt(1)),
		},
		{
			name:   "having a percent value while MinAvailable is zero should disable the zero value",
			spec:   &fleet.PodDisruptionBudgetSpec{MinAvailable: "0", MaxUnavailable: "10%"},
			wantMU: ptr(intstr.FromString("10%")),
		},
		{
			name:   "having a percent value while MaxUnavailable is zero should disable the zero value",
			spec:   &fleet.PodDisruptionBudgetSpec{MinAvailable: "10%", MaxUnavailable: "0"},
			wantMA: ptr(intstr.FromString("10%")),
		},
		{
			name:    "two non-zero values shouldn't work",
			spec:    &fleet.PodDisruptionBudgetSpec{MinAvailable: "1", MaxUnavailable: "1"},
			wantErr: true,
		},
	}

	for _, test := range tests {
		t.Run(test.name, func(t *testing.T) {
			pdb, err := scheduling.PodDisruptionBudget("agent-ns", test.spec)
			if test.wantErr {
				if err == nil {
					t.Fatalf("expected error, got nil")
				}
				return
			}
			if err != nil {
				t.Fatalf("unexpected error: %v", err)
			}
			if pdb.Namespace != "agent-ns" {
				t.Fatalf("namespace = %s, want agent-ns", pdb.Namespace)
			}
			if pdb.Spec.Selector == nil || pdb.Spec.Selector.MatchLabels["app"] != "fleet-agent" {
				t.Fatalf("selector mismatch: %#v", pdb.Spec.Selector)
			}
			if pdb.Name != "fleet-agent-pod-disruption-budget" {
				t.Fatalf("name = %s, want fleet-agent-pod-disruption-budget", pdb.Name)
			}
			if test.wantMU != nil {
				if pdb.Spec.MaxUnavailable == nil || *pdb.Spec.MaxUnavailable != *test.wantMU {
					t.Fatalf("MaxUnavailable = %#v, want %#v", pdb.Spec.MaxUnavailable, test.wantMU)
				}
				if pdb.Spec.MinAvailable != nil {
					t.Fatalf("MinAvailable should be nil when MaxUnavailable set")
				}
			}
			if test.wantMA != nil {
				if pdb.Spec.MinAvailable == nil || *pdb.Spec.MinAvailable != *test.wantMA {
					t.Fatalf("MinAvailable = %#v, want %#v", pdb.Spec.MinAvailable, test.wantMA)
				}
				if pdb.Spec.MaxUnavailable != nil {
					t.Fatalf("MaxUnavailable should be nil when MinAvailable set")
				}
			}
		})
	}
}

func TestPriorityClass(t *testing.T) {
	tests := []struct {
		name     string
		spec     *fleet.PriorityClassSpec
		expected *schedulingv1.PriorityClass
	}{
		{
			name: "priority 1000 with preemption",
			spec: &fleet.PriorityClassSpec{
				Value:            1000,
				PreemptionPolicy: ptr(corev1.PreemptLowerPriority),
			},
			expected: &schedulingv1.PriorityClass{
				ObjectMeta: metav1.ObjectMeta{
					Name: "fleet-agent-priority-class",
				},
				Value:            1000,
				Description:      "Priority class for Fleet Agent",
				PreemptionPolicy: ptr(corev1.PreemptLowerPriority),
			},
		},
		{
			name: "priority 100 without preemption",
			spec: &fleet.PriorityClassSpec{
				Value:            1000,
				PreemptionPolicy: ptr(corev1.PreemptNever),
			},
			expected: &schedulingv1.PriorityClass{
				ObjectMeta: metav1.ObjectMeta{
					Name: "fleet-agent-priority-class",
				},
				Value:            1000,
				Description:      "Priority class for Fleet Agent",
				PreemptionPolicy: ptr(corev1.PreemptNever),
			},
		},
		{
			name: "max value allowed",
			spec: &fleet.PriorityClassSpec{
				Value:            math.MaxInt32,
				PreemptionPolicy: ptr(corev1.PreemptNever),
			},
			expected: &schedulingv1.PriorityClass{
				ObjectMeta: metav1.ObjectMeta{
					Name: "fleet-agent-priority-class",
				},
				Value:            math.MaxInt32,
				Description:      "Priority class for Fleet Agent",
				PreemptionPolicy: ptr(corev1.PreemptNever),
			},
		},
		{
			name: "least possible value allowed",
			spec: &fleet.PriorityClassSpec{
				Value:            math.MinInt32,
				PreemptionPolicy: ptr(corev1.PreemptNever),
			},
			expected: &schedulingv1.PriorityClass{
				ObjectMeta: metav1.ObjectMeta{
					Name: "fleet-agent-priority-class",
				},
				Value:            math.MinInt32,
				Description:      "Priority class for Fleet Agent",
				PreemptionPolicy: ptr(corev1.PreemptNever),
			},
		},
	}

	for _, test := range tests {
		t.Run(test.name, func(t *testing.T) {
			result := scheduling.PriorityClass(test.spec)
			if result.Name != test.expected.Name {
				t.Fatalf("PriorityClass name mismatch: expected %s, got %s", test.expected.Name, result.Name)
			}
			if result.Value != test.expected.Value {
				t.Fatalf("PriorityClass value mismatch: expected %d, got %d", test.expected.Value, result.Value)
			}
			if result.Description != test.expected.Description {
				t.Fatalf("PriorityClass description mismatch: expected %s, got %s", test.expected.Description, result.Description)
			}
			if result.PreemptionPolicy == nil && test.expected.PreemptionPolicy != nil {
				t.Fatalf("PriorityClass preemption policy mismatch: expected %s, got nil", *test.expected.PreemptionPolicy)
			}
		})
	}
}



================================================
FILE: internal/cmd/controller/agentmanagement/secret/util.go
================================================
// Package secret gets or creates service account secrets for cluster registration.
package secret

import (
	"fmt"
	"time"

	"github.com/sirupsen/logrus"

	"github.com/rancher/fleet/pkg/durations"
	corecontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"

	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// GetServiceAccountTokenSecret gets or creates a secret for the service
// account. It waits 2 seconds for the data to be populated with a token.
func GetServiceAccountTokenSecret(sa *corev1.ServiceAccount, secretsController corecontrollers.SecretController) (*corev1.Secret, error) {
	name := sa.Name + "-token"
	secret, err := secretsController.Get(sa.Namespace, name, metav1.GetOptions{})
	if err != nil {
		if !apierrors.IsNotFound(err) {
			return nil, fmt.Errorf("error getting secret: %w", err)
		}
		return createServiceAccountTokenSecret(sa, secretsController)
	}
	return secret, nil
}

func createServiceAccountTokenSecret(sa *corev1.ServiceAccount, secretsController corecontrollers.SecretController) (*corev1.Secret, error) {
	// create the secret for the serviceAccount
	logrus.Debugf("creating ServiceAccountTokenSecret for sa %v", sa.Name)
	name := sa.Name + "-token"
	sc := corev1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:      name,
			Namespace: sa.Namespace,
			OwnerReferences: []metav1.OwnerReference{
				{
					APIVersion: "v1",
					Kind:       "ServiceAccount",
					Name:       sa.Name,
					UID:        sa.UID,
				},
			},
			Annotations: map[string]string{
				"kubernetes.io/service-account.name": sa.Name,
			},
		},
		Type: corev1.SecretTypeServiceAccountToken,
	}
	secret, err := secretsController.Create(&sc)
	if err != nil {
		if !apierrors.IsAlreadyExists(err) {
			return nil, fmt.Errorf("error creating secret: %w", err)
		}
		secret, err = secretsController.Get(sa.Namespace, name, metav1.GetOptions{})
		if err != nil {
			logrus.Debugf("secret %v already exists, error getting it", name)
			return nil, fmt.Errorf("error getting secret: %w", err)
		}
	}
	// Kubernetes auto populates the secret token after it is created, for which we should wait
	logrus.Infof("Waiting for service account token key to be populated for secret %s/%s", secret.Namespace, secret.Name)
	if _, ok := secret.Data[corev1.ServiceAccountTokenKey]; !ok {
		for {
			logrus.Debugf("wait for svc account secret to be populated with token %s", secret.Name)
			time.Sleep(durations.ServiceTokenSleep)
			secret, err = secretsController.Get(sa.Namespace, name, metav1.GetOptions{})
			if err != nil {
				return nil, err
			}
			if _, ok := secret.Data[corev1.ServiceAccountTokenKey]; ok {
				break
			}
		}
	}
	return secret, nil
}



================================================
FILE: internal/cmd/controller/cleanup/root.go
================================================
package cleanup

import (
	"fmt"

	command "github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/pkg/version"
	"github.com/spf13/cobra"
)

type CleanUp struct {
	Kubeconfig string `usage:"kubeconfig file"`
	Namespace  string `usage:"namespace to watch" env:"NAMESPACE"`
}

// HelpFunc hides the global flags from the help output
func (c *CleanUp) HelpFunc(cmd *cobra.Command, strings []string) {
	_ = cmd.Flags().MarkHidden("disable-metrics")
	_ = cmd.Flags().MarkHidden("shard-id")
	cmd.Parent().HelpFunc()(cmd, strings)
}

func (c *CleanUp) Run(cmd *cobra.Command, args []string) error {
	if c.Namespace == "" {
		return fmt.Errorf("--namespace or env NAMESPACE is required to be set")
	}
	return start(cmd.Context(), c.Kubeconfig, c.Namespace)
}

func App() *cobra.Command {
	return command.Command(&CleanUp{}, cobra.Command{
		Version: version.FriendlyVersion(),
		Use:     "cleanup",
	})
}



================================================
FILE: internal/cmd/controller/cleanup/start.go
================================================
package cleanup

import (
	"context"

	"github.com/sirupsen/logrus"

	"github.com/rancher/fleet/internal/cmd/controller/cleanup/controllers"
	"github.com/rancher/wrangler/v3/pkg/kubeconfig"
	"github.com/rancher/wrangler/v3/pkg/leader"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
)

func start(ctx context.Context, kubeConfig, namespace string) error {
	clientConfig := kubeconfig.GetNonInteractiveClientConfig(kubeConfig)
	kc, err := clientConfig.ClientConfig()
	if err != nil {
		return err
	}

	// try to claim leadership lease without rate limiting
	localConfig := rest.CopyConfig(kc)
	localConfig.QPS = -1
	localConfig.RateLimiter = nil
	k8s, err := kubernetes.NewForConfig(localConfig)
	if err != nil {
		return err
	}

	leader.RunOrDie(ctx, namespace, "fleet-cleanup-lock", k8s, func(ctx context.Context) {
		appCtx, err := controllers.NewAppContext(clientConfig)
		if err != nil {
			logrus.Fatal(err)
		}
		if err := controllers.Register(ctx, appCtx); err != nil {
			logrus.Fatal(err)
		}
	})

	return nil
}



================================================
FILE: internal/cmd/controller/cleanup/controllers/controllers.go
================================================
package controllers

import (
	"context"

	"github.com/rancher/fleet/internal/cmd/controller/cleanup/controllers/cleanup"
	"github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"
	"github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io"
	fleetcontrollers "github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io/v1alpha1"

	"github.com/rancher/lasso/pkg/cache"
	"github.com/rancher/lasso/pkg/client"
	"github.com/rancher/lasso/pkg/controller"
	"github.com/rancher/wrangler/v3/pkg/apply"
	"github.com/rancher/wrangler/v3/pkg/generated/controllers/core"
	corecontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"
	"github.com/rancher/wrangler/v3/pkg/generated/controllers/rbac"
	rbaccontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/rbac/v1"
	"github.com/rancher/wrangler/v3/pkg/start"

	"github.com/sirupsen/logrus"

	"k8s.io/apimachinery/pkg/api/meta"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	"k8s.io/client-go/util/workqueue"
)

type AppContext struct {
	fleetcontrollers.Interface

	K8s          kubernetes.Interface
	Core         corecontrollers.Interface
	RBAC         rbaccontrollers.Interface
	RESTMapper   meta.RESTMapper
	Apply        apply.Apply
	ClientConfig clientcmd.ClientConfig
	starters     []start.Starter
}

func (a *AppContext) Start(ctx context.Context) error {
	return start.All(ctx, 50, a.starters...)
}

func Register(ctx context.Context, appCtx *AppContext) error {
	cleanup.Register(ctx,
		appCtx.Apply.WithCacheTypes(
			appCtx.Core.Secret(),
			appCtx.Core.ServiceAccount(),
			appCtx.RBAC.Role(),
			appCtx.RBAC.RoleBinding(),
			appCtx.RBAC.ClusterRole(),
			appCtx.RBAC.ClusterRoleBinding(),
			appCtx.ClusterRegistrationToken(),
			appCtx.ClusterRegistration(),
			appCtx.ClusterGroup(),
			appCtx.Cluster(),
			appCtx.Core.Namespace()),
		appCtx.Core.Secret(),
		appCtx.Core.ServiceAccount(),
		appCtx.RBAC.Role(),
		appCtx.RBAC.RoleBinding(),
		appCtx.RBAC.ClusterRole(),
		appCtx.RBAC.ClusterRoleBinding(),
		appCtx.Core.Namespace(),
		appCtx.Cluster().Cache(),
	)

	if err := appCtx.Start(ctx); err != nil {
		logrus.Fatal(err)
	}

	return nil
}

func NewAppContext(cfg clientcmd.ClientConfig) (*AppContext, error) {
	client, err := cfg.ClientConfig()
	if err != nil {
		return nil, err
	}
	client.QPS = -1
	client.RateLimiter = nil

	scf, err := controllerFactory(client)
	if err != nil {
		return nil, err
	}

	core, err := core.NewFactoryFromConfigWithOptions(client, &core.FactoryOptions{
		SharedControllerFactory: scf,
	})
	if err != nil {
		return nil, err
	}
	corev := core.Core().V1()

	fleet, err := fleet.NewFactoryFromConfigWithOptions(client, &fleet.FactoryOptions{
		SharedControllerFactory: scf,
	})
	if err != nil {
		return nil, err
	}
	fleetv := fleet.Fleet().V1alpha1()

	rbac, err := rbac.NewFactoryFromConfigWithOptions(client, &rbac.FactoryOptions{
		SharedControllerFactory: scf,
	})
	if err != nil {
		return nil, err
	}
	rbacv := rbac.Rbac().V1()

	apply, err := apply.NewForConfig(client)
	if err != nil {
		return nil, err
	}
	apply = apply.WithSetOwnerReference(false, false)

	k8s, err := kubernetes.NewForConfig(client)
	if err != nil {
		return nil, err
	}

	return &AppContext{
		K8s:          k8s,
		Interface:    fleetv,
		Core:         corev,
		RBAC:         rbacv,
		Apply:        apply,
		ClientConfig: cfg,
		starters: []start.Starter{
			core,
			fleet,
			rbac,
		},
	}, nil
}

func controllerFactory(rest *rest.Config) (controller.SharedControllerFactory, error) {
	rateLimit := workqueue.NewTypedItemExponentialFailureRateLimiter[any](
		durations.FailureRateLimiterBase,
		durations.FailureRateLimiterMax,
	)
	clusterRateLimiter := workqueue.NewTypedItemExponentialFailureRateLimiter[any](
		durations.SlowFailureRateLimiterBase,
		durations.SlowFailureRateLimiterMax,
	)
	clientFactory, err := client.NewSharedClientFactory(rest, nil)
	if err != nil {
		return nil, err
	}

	cacheFactory := cache.NewSharedCachedFactory(clientFactory, nil)
	return controller.NewSharedControllerFactory(cacheFactory, &controller.SharedControllerFactoryOptions{
		DefaultRateLimiter:     rateLimit,
		DefaultWorkers:         50,
		SyncOnlyChangedObjects: true,
		KindRateLimiter: map[schema.GroupVersionKind]workqueue.RateLimiter{
			v1alpha1.SchemeGroupVersion.WithKind("Cluster"): clusterRateLimiter,
		},
	}), nil
}



================================================
FILE: internal/cmd/controller/cleanup/controllers/cleanup/controller.go
================================================
// Package cleanup provides a controller that cleans up resources that are no longer needed.
package cleanup

import (
	"context"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	fleetcontrollers "github.com/rancher/fleet/pkg/generated/controllers/fleet.cattle.io/v1alpha1"
	"github.com/sirupsen/logrus"

	"github.com/rancher/wrangler/v3/pkg/apply"
	corecontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"
	rbaccontrollers "github.com/rancher/wrangler/v3/pkg/generated/controllers/rbac/v1"

	corev1 "k8s.io/api/core/v1"
	rbacv1 "k8s.io/api/rbac/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/api/meta"
	"k8s.io/apimachinery/pkg/runtime"
)

type handler struct {
	apply      apply.Apply
	clusters   fleetcontrollers.ClusterCache
	namespaces corecontrollers.NamespaceClient
}

func Register(ctx context.Context, apply apply.Apply,
	secrets corecontrollers.SecretController,
	serviceAccount corecontrollers.ServiceAccountController,
	role rbaccontrollers.RoleController,
	roleBinding rbaccontrollers.RoleBindingController,
	clusterRole rbaccontrollers.ClusterRoleController,
	clusterRoleBinding rbaccontrollers.ClusterRoleBindingController,
	namespaces corecontrollers.NamespaceController,
	clusterCache fleetcontrollers.ClusterCache) {
	h := &handler{
		apply:      apply,
		clusters:   clusterCache,
		namespaces: namespaces,
	}

	clusterRole.OnChange(ctx, "managed-cleanup", func(_ string, obj *rbacv1.ClusterRole) (*rbacv1.ClusterRole, error) {
		if obj == nil {
			return nil, nil
		}
		return obj, h.cleanup(obj)
	})

	clusterRoleBinding.OnChange(ctx, "managed-cleanup", func(_ string, obj *rbacv1.ClusterRoleBinding) (*rbacv1.ClusterRoleBinding, error) {
		if obj == nil {
			return nil, nil
		}
		return obj, h.cleanup(obj)
	})

	role.OnChange(ctx, "managed-cleanup", func(_ string, obj *rbacv1.Role) (*rbacv1.Role, error) {
		if obj == nil {
			return nil, nil
		}
		return obj, h.cleanup(obj)
	})

	roleBinding.OnChange(ctx, "managed-cleanup", func(_ string, obj *rbacv1.RoleBinding) (*rbacv1.RoleBinding, error) {
		if obj == nil {
			return nil, nil
		}
		return obj, h.cleanup(obj)
	})

	serviceAccount.OnChange(ctx, "managed-cleanup", func(_ string, obj *corev1.ServiceAccount) (*corev1.ServiceAccount, error) {
		if obj == nil {
			return nil, nil
		}
		return obj, h.cleanup(obj)
	})

	secrets.OnChange(ctx, "managed-cleanup", func(_ string, obj *corev1.Secret) (*corev1.Secret, error) {
		if obj == nil {
			return nil, nil
		}
		return obj, h.cleanup(obj)
	})

	namespaces.OnChange(ctx, "managed-namespace-cleanup", h.cleanupNamespace)
}

func (h *handler) cleanupNamespace(key string, obj *corev1.Namespace) (*corev1.Namespace, error) {
	if obj == nil || obj.Labels[fleet.ManagedLabel] != "true" {
		return obj, nil
	}

	// check if the cluster for this cluster namespace still exists, otherwise clean up the namespace
	_, err := h.clusters.Get(obj.Annotations[fleet.ClusterNamespaceAnnotation], obj.Annotations[fleet.ClusterAnnotation])
	if apierrors.IsNotFound(err) {
		logrus.Infof("Cleaning up fleet-managed namespace %q, cluster not found", obj.Name)

		err = h.namespaces.Delete(key, nil)
		return obj, err
	}
	return obj, err
}

func (h *handler) cleanup(obj runtime.Object) error {
	meta, err := meta.Accessor(obj)
	if err != nil {
		return err
	}
	if meta.GetLabels()[fleet.ManagedLabel] != "true" {
		return nil
	}

	// If orphaned, purge the fleet-managed resource, this is often a no-op
	err = h.apply.PurgeOrphan(obj)
	if apierrors.IsNotFound(err) {
		return nil
	}
	return err
}



================================================
FILE: internal/cmd/controller/errorutil/errorutil.go
================================================
package errorutil

import (
	"errors"

	apierrors "k8s.io/apimachinery/pkg/api/errors"
)

var ErrRetryable = errors.New("requeue event")

func IgnoreConflict(err error) error {
	if apierrors.IsConflict(err) {
		return nil
	}
	return err
}



================================================
FILE: internal/cmd/controller/finalize/finalize.go
================================================
package finalize

import (
	"context"
	"slices"
	"strings"

	"github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/wrangler/v3/pkg/kv"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/util/retry"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

const (
	HelmOpFinalizer           = "fleet.cattle.io/helmop-finalizer"
	GitRepoFinalizer          = "fleet.cattle.io/gitrepo-finalizer"
	BundleFinalizer           = "fleet.cattle.io/bundle-finalizer"
	BundleDeploymentFinalizer = "fleet.cattle.io/bundle-deployment-finalizer"
	ClusterFinalizer          = "fleet.cattle.io/cluster-finalizer"
	ScheduleFinalizer         = "fleet.cattle.io/schedule-finalizer"
)

// PurgeBundles deletes all bundles related to the given resource namespaced name
// It deletes resources in cascade. Deleting Bundles, its BundleDeployments, and
// the related namespace if Bundle.Spec.DeleteNamespace is set to true.
func PurgeBundles(ctx context.Context, c client.Client, gitrepo types.NamespacedName, resourceLabel string) error {
	bundles := &v1alpha1.BundleList{}
	err := c.List(ctx, bundles, client.MatchingLabels{resourceLabel: gitrepo.Name}, client.InNamespace(gitrepo.Namespace))
	if err != nil {
		return err
	}

	// At this point, access to the GitRepo is unavailable as it has been deleted and cannot be found within the cluster.
	// Nevertheless, `deleteNamespace` can be found within all bundles generated from that GitRepo. Checking any bundle to get this value would be enough.
	namespace := ""
	deleteNamespace := false
	sampleBundle := v1alpha1.Bundle{}
	if len(bundles.Items) > 0 {
		sampleBundle = bundles.Items[0]
		deleteNamespace = sampleBundle.Spec.DeleteNamespace
		namespace = sampleBundle.Spec.TargetNamespace

		if sampleBundle.Spec.KeepResources {
			deleteNamespace = false
		}
	}

	if err = PurgeNamespace(ctx, c, deleteNamespace, namespace); err != nil {
		return err
	}

	for _, bundle := range bundles.Items {
		// Just delete the bundle and let the Bundle reconciler purge its BundleDeployments
		err := c.Delete(ctx, &bundle)
		if client.IgnoreNotFound(err) != nil {
			return err
		}
	}

	return nil
}

// PurgeContent tries to delete the content resource related with the given bundle deployment.
func PurgeContent(ctx context.Context, c client.Client, name, deplID string) error {
	contentID, _ := kv.Split(deplID, ":")
	content := &v1alpha1.Content{}
	if err := c.Get(ctx, types.NamespacedName{Name: contentID}, content); err != nil {
		return client.IgnoreNotFound(err)
	}

	logger := log.FromContext(ctx).WithName("purge-content").WithValues("contentID", contentID, "finalizerName", name)

	nn := types.NamespacedName{Name: content.Name}
	if controllerutil.ContainsFinalizer(content, name) {
		err := retry.RetryOnConflict(retry.DefaultRetry, func() error {
			if err := c.Get(ctx, nn, content); err != nil {
				return client.IgnoreNotFound(err)
			}

			controllerutil.RemoveFinalizer(content, name)

			return c.Update(ctx, content)
		})
		if err != nil {
			return err
		}

		logger.V(1).Info("Removed finalizer from content resource")
	}

	if len(content.Finalizers) == 0 {
		if err := c.Delete(ctx, content); err != nil {
			return err
		}
		logger.V(1).Info("Deleted content resource")
	}

	return nil
}

// PurgeImageScans deletes all ImageScan resources related with the given GitRepo namespaces name.
func PurgeImageScans(ctx context.Context, c client.Client, gitrepo types.NamespacedName) error {
	images := &v1alpha1.ImageScanList{}
	err := c.List(ctx, images, client.InNamespace(gitrepo.Namespace))
	if err != nil {
		return err
	}

	for _, image := range images.Items {
		if image.Spec.GitRepoName == gitrepo.Name {
			err := c.Delete(ctx, &image)
			if err != nil {
				return err
			}
		}

	}
	return nil
}

// PurgeNamespace deletes the given namespace if deleteNamespace is set to true.
// It ignores the following namespaces, that are considered as default by fleet or kubernetes:
// fleet-local, cattle-fleet-system, fleet-default, cattle-fleet-clusters-system, default
func PurgeNamespace(ctx context.Context, c client.Client, deleteNamespace bool, ns string) error {
	if !deleteNamespace {
		return nil
	}

	if ns == "" {
		return nil
	}

	// Ignore default namespaces
	defaultNamespaces := []string{"fleet-local", "cattle-fleet-system", "fleet-default", "cattle-fleet-clusters-system", "default"}
	if slices.Contains(defaultNamespaces, ns) {
		return nil
	}

	// Ignore system namespaces
	if _, isKubeNamespace := strings.CutPrefix(ns, "kube-"); isKubeNamespace {
		return nil
	}

	namespace := &corev1.Namespace{
		ObjectMeta: metav1.ObjectMeta{
			Name: ns,
		},
	}
	if err := c.Delete(ctx, namespace); err != nil {
		return err
	}

	return nil
}

// EnsureFinalizer adds a finalizer to the given object if it doesn't exist.
func EnsureFinalizer(ctx context.Context, c client.Client, obj client.Object, finalizer string) error {
	if controllerutil.ContainsFinalizer(obj, finalizer) {
		return nil
	}

	controllerutil.AddFinalizer(obj, finalizer)
	return c.Update(ctx, obj)
}



================================================
FILE: internal/cmd/controller/gitops/operator.go
================================================
package gitops

import (
	"context"
	"fmt"
	"net/http"
	"os"
	"strconv"
	"time"

	"github.com/reugn/go-quartz/quartz"
	"github.com/spf13/cobra"
	"golang.org/x/sync/errgroup"

	"k8s.io/apimachinery/pkg/runtime"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	clientgoscheme "k8s.io/client-go/kubernetes/scheme"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/cache"
	"sigs.k8s.io/controller-runtime/pkg/client"
	clog "sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"
	metricsserver "sigs.k8s.io/controller-runtime/pkg/metrics/server"

	command "github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/cmd/controller/gitops/reconciler"
	fcreconciler "github.com/rancher/fleet/internal/cmd/controller/reconciler"
	"github.com/rancher/fleet/internal/metrics"
	"github.com/rancher/fleet/internal/ssh"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/git"
	"github.com/rancher/fleet/pkg/version"
	"github.com/rancher/fleet/pkg/webhook"
)

var (
	scheme            = runtime.NewScheme()
	setupLog          = ctrl.Log.WithName("setup")
	zopts             *zap.Options
	defaultSyncPeriod = 10 * time.Hour
)

func init() {
	utilruntime.Must(clientgoscheme.AddToScheme(scheme))
	utilruntime.Must(fleet.AddToScheme(scheme))
}

type GitOperator struct {
	command.DebugConfig
	Kubeconfig           string `usage:"Kubeconfig file"`
	Namespace            string `usage:"namespace to watch" default:"cattle-fleet-system" env:"NAMESPACE"`
	MetricsAddr          string `name:"metrics-bind-address" default:":8081" usage:"The address the metric endpoint binds to."`
	DisableMetrics       bool   `name:"disable-metrics" usage:"Disable the metrics server."`
	EnableLeaderElection bool   `name:"leader-elect" default:"true" usage:"Enable leader election for controller manager. Enabling this will ensure there is only one active controller manager."`
	Image                string `name:"gitjob-image" default:"rancher/fleet:dev" usage:"The gitjob image that will be used in the generated job."`
	Listen               string `default:":8080" usage:"The port the webhook listens."`
	ShardID              string `usage:"only manage resources labeled with a specific shard ID" name:"shard-id"`
	ShardNodeSelector    string `usage:"node selector to apply to jobs based on the shard ID, if any" name:"shard-node-selector"`
	SkipHostKeyChecks    bool   `name:"insecure-skip-host-key-checks" usage:"Enable SSH connections to succeed even without matching known_hosts entries. Enabling this will expose SSH operations to man-in-the-middle attacks."`
}

func App(zo *zap.Options) *cobra.Command {
	zopts = zo
	return command.Command(&GitOperator{}, cobra.Command{
		Version: version.FriendlyVersion(),
		Use:     "gitjob",
	})
}

// HelpFunc hides the global flag from the help output
func (c *GitOperator) HelpFunc(cmd *cobra.Command, strings []string) {
	cmd.Parent().HelpFunc()(cmd, strings)
}

func (g *GitOperator) PersistentPre(_ *cobra.Command, _ []string) error {
	if err := g.SetupDebug(); err != nil {
		return fmt.Errorf("failed to setup debug logging: %w", err)
	}
	zopts = g.OverrideZapOpts(zopts)

	return nil
}

func (g *GitOperator) Run(cmd *cobra.Command, args []string) error {
	ctrl.SetLogger(zap.New(zap.UseFlagOptions(zopts)))
	ctx := clog.IntoContext(cmd.Context(), ctrl.Log.WithName("gitjob-reconciler"))

	namespace := g.Namespace

	leaderOpts, err := command.NewLeaderElectionOptions()
	if err != nil {
		return err
	}

	var shardIDSuffix string
	if g.ShardID != "" {
		shardIDSuffix = fmt.Sprintf("-%s", g.ShardID)
	}

	syncPeriod := defaultSyncPeriod
	if d := os.Getenv("GITREPO_SYNC_PERIOD"); d != "" {
		syncPeriod, err = time.ParseDuration(d)
		if err != nil {
			return err
		}
	}

	mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{
		Scheme:                  scheme,
		Metrics:                 g.setupMetrics(),
		LeaderElection:          g.EnableLeaderElection,
		LeaderElectionID:        fmt.Sprintf("fleet-gitops-leader-election-shard%s", shardIDSuffix),
		LeaderElectionNamespace: namespace,
		LeaseDuration:           &leaderOpts.LeaseDuration,
		RenewDeadline:           &leaderOpts.RenewDeadline,
		RetryPeriod:             &leaderOpts.RetryPeriod,
		// resync to pick up lost gitrepos
		Cache: cache.Options{
			SyncPeriod: &syncPeriod,
		},
	})

	if err != nil {
		return err
	}

	sched, err := quartz.NewStdScheduler()
	if err != nil {
		return fmt.Errorf("failed to create scheduler: %w", err)
	}

	var workers int
	if d := os.Getenv("GITREPO_RECONCILER_WORKERS"); d != "" {
		w, err := strconv.Atoi(d)
		if err != nil {
			setupLog.Error(err, "failed to parse GITREPO_RECONCILER_WORKERS", "value", d)
		}
		workers = w
	}

	kh := ssh.KnownHosts{EnforceHostKeyChecks: !g.SkipHostKeyChecks}

	gitJobReconciler := &reconciler.GitJobReconciler{
		Client:          mgr.GetClient(),
		Scheme:          mgr.GetScheme(),
		Image:           g.Image,
		Scheduler:       sched,
		Workers:         workers,
		ShardID:         g.ShardID,
		JobNodeSelector: g.ShardNodeSelector,
		GitFetcher:      &git.Fetch{KnownHosts: kh},
		Clock:           reconciler.RealClock{},
		Recorder:        mgr.GetEventRecorderFor(fmt.Sprintf("fleet-gitops%s", shardIDSuffix)),
		SystemNamespace: namespace,
		KnownHosts:      kh,
	}

	statusReconciler := &reconciler.StatusReconciler{
		Client:  mgr.GetClient(),
		Scheme:  mgr.GetScheme(),
		ShardID: g.ShardID,
		Workers: workers,
	}

	configReconciler := &fcreconciler.ConfigReconciler{
		Client:          mgr.GetClient(),
		Scheme:          mgr.GetScheme(),
		SystemNamespace: namespace,
		ShardID:         g.ShardID,
	}

	if err := fcreconciler.Load(ctx, mgr.GetAPIReader(), namespace); err != nil {
		setupLog.Error(err, "failed to load config")
		return err
	}

	group, ctx := errgroup.WithContext(ctx)
	group.Go(func() error {
		return startWebhook(ctx, namespace, g.Listen, mgr.GetClient(), mgr.GetCache())
	})
	group.Go(func() error {
		setupLog.Info("starting config controller")
		if err = configReconciler.SetupWithManager(mgr); err != nil {
			return err
		}

		setupLog.Info("starting gitops controller")
		if err = gitJobReconciler.SetupWithManager(mgr); err != nil {
			return err
		}

		setupLog.Info("starting gitops status controller")
		if err = statusReconciler.SetupWithManager(mgr); err != nil {
			return err
		}

		return mgr.Start(ctx)
	})

	gitJobReconciler.Scheduler.Start(ctx)

	return group.Wait()
}

func (g *GitOperator) setupMetrics() metricsserver.Options {
	if g.DisableMetrics {
		return metricsserver.Options{BindAddress: "0"}
	}

	metricsAddr := g.MetricsAddr
	if d := os.Getenv("GITOPS_METRICS_BIND_ADDRESS"); d != "" {
		metricsAddr = d
	}

	metricServerOpts := metricsserver.Options{BindAddress: metricsAddr}
	metrics.RegisterGitOptsMetrics() // enable gitops related metrics

	return metricServerOpts
}

func startWebhook(ctx context.Context, namespace string, addr string, client client.Client, cacheClient cache.Cache) error {
	setupLog.Info("Setting up webhook listener")
	handler, err := webhook.HandleHooks(ctx, namespace, client, cacheClient)
	if err != nil {
		return fmt.Errorf("webhook handler can't be created: %w", err)
	}
	server := &http.Server{
		Addr:    addr,
		Handler: handler,
		// According to https://blog.cloudflare.com/the-complete-guide-to-golang-net-http-timeouts/
		ReadHeaderTimeout: 5 * time.Second,
		WriteTimeout:      10 * time.Second,
	}

	go func() {
		<-ctx.Done()
		shutdownCtx, cancel := context.WithTimeout(context.WithoutCancel(ctx), 2*time.Second)
		defer cancel()
		_ = server.Shutdown(shutdownCtx)
	}()

	if err := server.ListenAndServe(); err != nil && err != http.ErrServerClosed {
		return fmt.Errorf("failed to listen on %s: %w", addr, err)
	}

	return nil
}



================================================
FILE: internal/cmd/controller/gitops/reconciler/gitjob.go
================================================
package reconciler

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"slices"
	"strconv"
	"strings"
	"time"

	"github.com/go-logr/logr"
	fleetapply "github.com/rancher/fleet/internal/cmd/cli/apply"
	"github.com/rancher/fleet/internal/config"
	fleetgithub "github.com/rancher/fleet/internal/github"
	"github.com/rancher/fleet/internal/names"
	"github.com/rancher/fleet/internal/ocistorage"
	ssh "github.com/rancher/fleet/internal/ssh"
	v1alpha1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/cert"
	fleetevent "github.com/rancher/fleet/pkg/event"
	"github.com/rancher/fleet/pkg/sharding"

	appsv1 "k8s.io/api/apps/v1"
	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/util/retry"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

const (
	bundleCAVolumeName        = "additional-ca"
	bundleCAFile              = "additional-ca.crt"
	gitCredentialVolumeName   = "git-credential" // #nosec G101 this is not a credential
	ociRegistryAuthVolumeName = "oci-auth"
	gitClonerVolumeName       = "git-cloner"
	emptyDirVolumeName        = "git-cloner-empty-dir"

	fleetHomeDir = "/fleet-home"

	bundleOptionsSeparatorChars = ":,|?<>"
)

type helmSecretOptions struct {
	HasCACerts      bool
	InsecureSkipTLS bool
	BasicHTTP       bool
}

func (r *GitJobReconciler) createJobAndResources(ctx context.Context, gitrepo *v1alpha1.GitRepo, logger logr.Logger) error {
	logger.V(1).Info("Creating Git job resources")

	if err := r.createJobRBAC(ctx, gitrepo); err != nil {
		return fmt.Errorf("failed to create RBAC resources for git job: %w", err)
	}
	if err := r.createTargetsConfigMap(ctx, gitrepo); err != nil {
		return fmt.Errorf("failed to create targets config map for git job: %w", err)
	}
	if _, err := r.createCABundleSecret(ctx, gitrepo, caBundleName(gitrepo)); err != nil {
		return fmt.Errorf("failed to create cabundle secret for git job: %w", err)
	}
	if err := r.createJob(ctx, gitrepo); err != nil {
		return fmt.Errorf("error creating git job: %w", err)
	}

	r.Recorder.Event(gitrepo, fleetevent.Normal, "Created", "GitJob was created")
	return nil
}

func (r *GitJobReconciler) createTargetsConfigMap(ctx context.Context, gitrepo *v1alpha1.GitRepo) error {
	configMap, err := newTargetsConfigMap(gitrepo)
	if err != nil {
		return err
	}
	if err := controllerutil.SetControllerReference(gitrepo, configMap, r.Scheme); err != nil {
		return err
	}
	data := configMap.BinaryData
	_, err = controllerutil.CreateOrUpdate(ctx, r.Client, configMap, func() error {
		configMap.BinaryData = data
		return nil
	})

	return err
}

// createCABundleSecret creates a CA bundle secret, if the provided source contains data.
// That provided source may be the CABundle field of the provided gitrepo (if the provided name matches the CA bundle
// name expected for the gitrepo, and that CABundle field is non-empty), or Rancher-configured secrets in all other cases.
// This returns a boolean indicating whether the secret has been successfully created (or updated, in case it already
// existed), and an error.
func (r *GitJobReconciler) createCABundleSecret(ctx context.Context, gitrepo *v1alpha1.GitRepo, name string) (bool, error) {
	var caBundle []byte
	fieldName := "cacerts"

	if name == caBundleName(gitrepo) {
		caBundle = gitrepo.Spec.CABundle
		fieldName = bundleCAFile
	}

	if len(caBundle) == 0 {
		cab, err := cert.GetRancherCABundle(ctx, r.Client)
		if err != nil {
			return false, err
		}

		if len(cab) == 0 {
			return false, nil
		}

		caBundle = cab
	}

	secret := &corev1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Namespace: gitrepo.Namespace,
			Name:      name,
		},
	}
	_, err := controllerutil.CreateOrUpdate(ctx, r.Client, secret, func() error {
		if secret.Annotations == nil {
			secret.Annotations = map[string]string{}
		}
		secret.Annotations["revision"] = strconv.FormatInt(time.Now().Unix(), 10)
		secret.Data = map[string][]byte{
			fieldName: caBundle,
		}
		if err := controllerutil.SetControllerReference(gitrepo, secret, r.Scheme); err != nil {
			return err
		}
		return nil
	})
	if err != nil {
		return false, err
	}

	updatedSecret := &corev1.Secret{}
	err = retry.OnError(retry.DefaultRetry, func(_ error) bool {
		return true
	}, func() error {
		if err := r.Get(ctx, types.NamespacedName{
			Namespace: gitrepo.Namespace,
			Name:      name,
		}, updatedSecret); err != nil {
			return err
		}
		if !strings.EqualFold(updatedSecret.Annotations["revision"], secret.Annotations["revision"]) {
			return fmt.Errorf("CA bundle secret %s/%s has not been synced in time before git job creation", gitrepo.Namespace, name)
		}
		return nil
	})
	if err != nil {
		return false, err
	}

	return true, nil
}

func (r *GitJobReconciler) createJob(ctx context.Context, gitRepo *v1alpha1.GitRepo) error {
	job, err := r.newGitJob(ctx, gitRepo)
	if err != nil {
		return err
	}
	if err := controllerutil.SetControllerReference(gitRepo, job, r.Scheme); err != nil {
		return err
	}
	return r.Create(ctx, job)
}

func (r *GitJobReconciler) newGitJob(ctx context.Context, obj *v1alpha1.GitRepo) (*batchv1.Job, error) {
	jobSpec, err := r.newJobSpec(ctx, obj)
	if err != nil {
		return nil, err
	}
	var fleetControllerDeployment appsv1.Deployment
	if err := r.Get(ctx, types.NamespacedName{
		Namespace: r.SystemNamespace,
		Name:      config.ManagerConfigName,
	}, &fleetControllerDeployment); err != nil {
		return nil, err
	}

	// add tolerations from the fleet-controller deployment
	jobSpec.Template.Spec.Tolerations = append(
		jobSpec.Template.Spec.Tolerations,
		fleetControllerDeployment.Spec.Template.Spec.Tolerations...,
	)
	job := &batchv1.Job{
		ObjectMeta: metav1.ObjectMeta{
			Annotations: map[string]string{
				"generation": strconv.Itoa(int(obj.Generation)),
				"commit":     obj.Status.Commit,
			},
			Labels: map[string]string{
				forceSyncGenerationLabel: fmt.Sprintf("%d", obj.Spec.ForceSyncGeneration),
				generationLabel:          fmt.Sprintf("%d", obj.Generation),
			},
			Namespace: obj.Namespace,
			Name:      jobName(obj),
		},
		Spec: *jobSpec,
	}
	// if the repo references a shard, add the same label to the job
	// this avoids a call to Reconcile for controllers that do not match
	// the shard-id
	label, hasLabel := obj.GetLabels()[sharding.ShardingRefLabel]
	if hasLabel {
		job.Labels = labels.Merge(job.Labels, map[string]string{
			sharding.ShardingRefLabel: label,
		})
	}

	knownHostsData, err := r.KnownHosts.Get(ctx, r.Client, obj.Namespace, obj.Spec.ClientSecretName)
	if err != nil {
		return nil, err
	}

	initContainer, err := r.newGitCloner(ctx, obj, knownHostsData)
	if err != nil {
		return nil, err
	}

	job.Spec.Template.Spec.InitContainers = []corev1.Container{initContainer}
	job.Spec.Template.Spec.Volumes = append(job.Spec.Template.Spec.Volumes,
		corev1.Volume{
			Name: gitClonerVolumeName,
			VolumeSource: corev1.VolumeSource{
				EmptyDir: &corev1.EmptyDirVolumeSource{},
			},
		}, corev1.Volume{
			Name: emptyDirVolumeName,
			VolumeSource: corev1.VolumeSource{
				EmptyDir: &corev1.EmptyDirVolumeSource{},
			},
		},
	)

	// Look for a `--ca-bundle-file` arg to the git cloner. This applies to cases where the GitRepo's `Spec.CABundle` is
	// specified, but also to cases where a CA bundle secret has been created instead, with data from Rancher
	// secrets.
	if slices.Contains(initContainer.Args, "--ca-bundle-file") {
		job.Spec.Template.Spec.Volumes = append(job.Spec.Template.Spec.Volumes, corev1.Volume{
			Name: bundleCAVolumeName,
			VolumeSource: corev1.VolumeSource{
				Secret: &corev1.SecretVolumeSource{
					SecretName: caBundleName(obj),
				},
			},
		})
	}

	if obj.Spec.ClientSecretName != "" {
		job.Spec.Template.Spec.Volumes = append(job.Spec.Template.Spec.Volumes,
			corev1.Volume{
				Name: gitCredentialVolumeName,
				VolumeSource: corev1.VolumeSource{
					Secret: &corev1.SecretVolumeSource{
						SecretName: obj.Spec.ClientSecretName,
					},
				},
			},
		)
	} else {
		// Create a volume for the default credentials secret if it exists
		var secret corev1.Secret
		err := r.Get(ctx, types.NamespacedName{
			Namespace: obj.Namespace,
			Name:      config.DefaultGitCredentialsSecretName,
		}, &secret)

		if err != nil && !apierrors.IsNotFound(err) {
			return nil, err
		}

		if err == nil {
			job.Spec.Template.Spec.Volumes = append(job.Spec.Template.Spec.Volumes,
				corev1.Volume{
					Name: gitCredentialVolumeName,
					VolumeSource: corev1.VolumeSource{
						Secret: &corev1.SecretVolumeSource{
							SecretName: config.DefaultGitCredentialsSecretName,
						},
					},
				},
			)
		}
	}

	for i := range job.Spec.Template.Spec.Containers {
		job.Spec.Template.Spec.Containers[i].VolumeMounts = append(job.Spec.Template.Spec.Containers[i].VolumeMounts, corev1.VolumeMount{
			MountPath: "/workspace/source",
			Name:      gitClonerVolumeName,
		})

		if knownHostsData != "" {
			job.Spec.Template.Spec.Containers[i].Env = append(
				job.Spec.Template.Spec.Containers[i].Env,
				corev1.EnvVar{Name: ssh.KnownHostsEnvVar, Value: knownHostsData},
			)
		}

		job.Spec.Template.Spec.Containers[i].Env = append(job.Spec.Template.Spec.Containers[i].Env,
			corev1.EnvVar{
				Name:  "COMMIT",
				Value: obj.Status.Commit,
			},
		)
		job.Spec.Template.Spec.Containers[i].Env = append(job.Spec.Template.Spec.Containers[i].Env, proxyEnvVars()...)
	}

	return job, nil
}

func (r *GitJobReconciler) newJobSpec(ctx context.Context, gitrepo *v1alpha1.GitRepo) (*batchv1.JobSpec, error) {
	var CACertsFilePathOverride string

	paths := gitrepo.Spec.Paths
	if len(paths) == 0 {
		paths = []string{"."}
	}

	drivenScanSeparator := ""
	var err error
	if len(gitrepo.Spec.Bundles) > 0 {
		paths = []string{}
		// use driven scan instead
		// We calculate a separator because we will continue using the
		// same call format for "fleet apply."
		// The bundle definitions + options file (fleet.yaml)
		// will be passed at the end, in the same way we pass the bundle
		// directories for the classic fleet scan, but since we need to
		// pass 2 strings, we will separate them with
		// the calculated separator.
		drivenScanSeparator, err = getDrivenScanSeparator(*gitrepo)
		if err != nil {
			return nil, err
		}
		for _, b := range gitrepo.Spec.Bundles {
			path := b.Base
			if b.Options != "" {
				path = path + drivenScanSeparator + b.Options
			}
			paths = append(paths, path)
		}
	}

	// compute configmap, needed because its name contains a hash
	configMap, err := newTargetsConfigMap(gitrepo)
	if err != nil {
		return nil, err
	}

	volumes, volumeMounts := volumes(configMap.Name)
	var certVolCreated bool
	var helmInsecure bool
	var helmBasicHTTP bool

	if gitrepo.Spec.HelmSecretNameForPaths != "" {
		vols, volMnts, helmSecretOpts := volumesFromSecret(ctx, r.Client,
			gitrepo.Namespace,
			gitrepo.Spec.HelmSecretNameForPaths,
			"helm-secret-by-path",
			"",
		)

		certVolCreated = helmSecretOpts.HasCACerts

		volumes = append(volumes, vols...)
		volumeMounts = append(volumeMounts, volMnts...)

	} else if gitrepo.Spec.HelmSecretName != "" {
		vols, volMnts, helmSecretOpts := volumesFromSecret(ctx, r.Client,
			gitrepo.Namespace,
			gitrepo.Spec.HelmSecretName,
			"helm-secret",
			"",
		)

		certVolCreated = helmSecretOpts.HasCACerts
		helmInsecure = helmSecretOpts.InsecureSkipTLS
		helmBasicHTTP = helmSecretOpts.BasicHTTP

		volumes = append(volumes, vols...)
		volumeMounts = append(volumeMounts, volMnts...)
	}

	// In case no Helm secret volume has been created, because Helm secrets don't exist or don't contain a CA
	// bundle, mount a volume with a Rancher CA bundle.
	if !certVolCreated {
		// Fall back to Rancher-configured secrets
		// We need to copy secret data from Rancher, because Rancher secrets live in a different namespace and
		// can therefore not be used as sources for a volume.
		secretName := rancherCABundleName(gitrepo)
		res, err := r.createCABundleSecret(ctx, gitrepo, secretName)
		if err != nil {
			return nil, err
		}

		if res {
			CACertsDirOverride := "/etc/rancher/certs"

			// Override the volume name and mount path to prevent any conflict with an existing Helm secret
			// providing username and password.
			vols, volMnts, _ := volumesFromSecret(ctx, r.Client,
				gitrepo.Namespace,
				secretName,
				"rancher-helm-secret",
				CACertsDirOverride,
			)

			volumes = append(volumes, vols...)
			volumeMounts = append(volumeMounts, volMnts...)

			CACertsFilePathOverride = CACertsDirOverride + "/cacerts"
		}
	}

	shardID := gitrepo.Labels[sharding.ShardingRefLabel]

	nodeSelector := map[string]string{"kubernetes.io/os": "linux"}
	if shardID != "" && len(strings.TrimSpace(r.JobNodeSelector)) > 0 {
		var shardNodeSelector map[string]string
		if err := json.Unmarshal([]byte(r.JobNodeSelector), &shardNodeSelector); err != nil {
			return nil, fmt.Errorf("could not decode shard node selector: %w", err)
		}

		for k, v := range shardNodeSelector {
			nodeSelector[k] = v
		}
	}

	saName := names.SafeConcatName("git", gitrepo.Name)
	logger := log.FromContext(ctx)
	args, envs := argsAndEnvs(gitrepo, logger, CACertsFilePathOverride, r.KnownHosts, drivenScanSeparator, helmInsecure, helmBasicHTTP)

	zero := int32(0)

	return &batchv1.JobSpec{
		BackoffLimit: &zero,
		Template: corev1.PodTemplateSpec{
			ObjectMeta: metav1.ObjectMeta{
				CreationTimestamp: metav1.Time{Time: time.Unix(0, 0)},
			},
			Spec: corev1.PodSpec{
				Volumes: volumes,
				SecurityContext: &corev1.PodSecurityContext{
					RunAsUser: &[]int64{1000}[0],
				},
				ServiceAccountName: saName,
				RestartPolicy:      corev1.RestartPolicyNever,
				Containers: []corev1.Container{
					{
						Name:         "fleet",
						Image:        r.Image,
						Command:      []string{"log.sh"},
						Args:         append(args, paths...),
						WorkingDir:   "/workspace/source",
						VolumeMounts: volumeMounts,
						Env:          envs,
						SecurityContext: &corev1.SecurityContext{
							AllowPrivilegeEscalation: &[]bool{false}[0],
							ReadOnlyRootFilesystem:   &[]bool{true}[0],
							Privileged:               &[]bool{false}[0],
							RunAsNonRoot:             &[]bool{true}[0],
							SeccompProfile: &corev1.SeccompProfile{
								Type: corev1.SeccompProfileTypeRuntimeDefault,
							},
							Capabilities: &corev1.Capabilities{Drop: []corev1.Capability{"ALL"}},
						},
					},
				},
				NodeSelector: nodeSelector,
				Tolerations: []corev1.Toleration{
					{
						Key:      "cattle.io/os",
						Operator: "Equal",
						Value:    "linux",
						Effect:   "NoSchedule",
					},
					{
						Key:      "node.cloudprovider.kubernetes.io/uninitialized",
						Operator: "Equal",
						Value:    "true",
						Effect:   "NoSchedule",
					},
				},
			},
		},
	}, nil
}

func (r *GitJobReconciler) newGitCloner(
	ctx context.Context,
	obj *v1alpha1.GitRepo,
	knownHosts string,
) (corev1.Container, error) {
	args := []string{"fleet", "gitcloner", obj.Spec.Repo, "/workspace"}
	volumeMounts := []corev1.VolumeMount{
		{
			Name:      gitClonerVolumeName,
			MountPath: "/workspace",
		},
		{
			Name:      emptyDirVolumeName,
			MountPath: "/tmp",
		},
	}

	branch, rev := obj.Spec.Branch, obj.Spec.Revision
	switch {
	case branch != "":
		args = append(args, "--branch", branch)
	case rev != "":
		args = append(args, "--revision", rev)
	default:
		args = append(args, "--branch", "master")
	}

	secretName := obj.Spec.ClientSecretName
	if secretName == "" {
		secretName = config.DefaultGitCredentialsSecretName
	}

	var secret corev1.Secret
	err := r.Get(ctx, types.NamespacedName{
		Namespace: obj.Namespace,
		Name:      secretName,
	}, &secret)

	if err != nil && secretName == obj.Spec.ClientSecretName {
		// Only error if an explicitly referenced secret was not found;
		// The absence of a default secret might simply mean that no credentials are needed.
		return corev1.Container{}, err
	}

	if err == nil {
		switch secret.Type {
		case corev1.SecretTypeBasicAuth:
			volumeMounts = append(volumeMounts, corev1.VolumeMount{
				Name:      gitCredentialVolumeName,
				MountPath: "/gitjob/credentials",
			})
			args = append(args, "--username", string(secret.Data[corev1.BasicAuthUsernameKey]))
			args = append(args, "--password-file", "/gitjob/credentials/"+corev1.BasicAuthPasswordKey)
		case corev1.SecretTypeSSHAuth:
			volumeMounts = append(volumeMounts, corev1.VolumeMount{
				Name:      gitCredentialVolumeName,
				MountPath: "/gitjob/ssh",
			})
			args = append(args, "--ssh-private-key-file", "/gitjob/ssh/"+corev1.SSHAuthPrivateKey)
		default:
			if fleetgithub.HasGitHubAppKeys(&secret) {
				volumeMounts = append(volumeMounts, corev1.VolumeMount{
					Name:      gitCredentialVolumeName,
					MountPath: "/gitjob/githubapp",
				})
				args = append(args,
					"--github-app-id", string(secret.Data[fleetgithub.GithubAppIDKey]),
					"--github-app-installation-id", string(secret.Data[fleetgithub.GithubAppInstallationIDKey]),
					"--github-app-key-file", "/gitjob/githubapp/"+fleetgithub.GithubAppPrivateKeyKey,
				)
			} else {
				return corev1.Container{}, fmt.Errorf("missing Github App keys in secret %s/%s", secret.Namespace, secret.Name)
			}
		}
	}

	if obj.Spec.InsecureSkipTLSverify {
		args = append(args, "--insecure-skip-tls")
	}

	var CABundleSecret corev1.Secret
	err = r.Get(ctx, types.NamespacedName{
		Namespace: obj.Namespace,
		Name:      caBundleName(obj),
	}, &CABundleSecret)
	if client.IgnoreNotFound(err) != nil {
		return corev1.Container{}, err
	}

	if !apierrors.IsNotFound(err) {
		volumeMounts = append(volumeMounts, corev1.VolumeMount{
			Name:      bundleCAVolumeName,
			MountPath: "/gitjob/cabundle",
		})
		args = append(args, "--ca-bundle-file", "/gitjob/cabundle/"+bundleCAFile)
	}

	env := []corev1.EnvVar{
		{
			Name:  fleetapply.JSONOutputEnvVar,
			Value: "true",
		},
	}
	env = append(env, proxyEnvVars()...)

	// If strict host key checks are enabled but no entries are available, another error will be shown by the known
	// hosts getter, as that means that the Fleet deployment is incomplete.
	// On the other hand, we do not want to feed entries to the cloner if strict host key checks are disabled, as that
	// would lead it to unduly reject SSH connection attempts.
	if r.KnownHosts.IsStrict() {
		env = append(env, corev1.EnvVar{Name: ssh.KnownHostsEnvVar, Value: knownHosts})
	}

	return corev1.Container{
		Command:      []string{"log.sh"},
		Args:         args,
		Image:        r.Image,
		Name:         "gitcloner-initializer",
		VolumeMounts: volumeMounts,
		Env:          env,
		SecurityContext: &corev1.SecurityContext{
			AllowPrivilegeEscalation: &[]bool{false}[0],
			ReadOnlyRootFilesystem:   &[]bool{true}[0],
			Privileged:               &[]bool{false}[0],
			Capabilities:             &corev1.Capabilities{Drop: []corev1.Capability{"ALL"}},
			RunAsNonRoot:             &[]bool{true}[0],
			SeccompProfile: &corev1.SeccompProfile{
				Type: corev1.SeccompProfileTypeRuntimeDefault,
			},
		},
	}, nil
}

// readIntEnvVar reads an integer from an environment variable using the provided getter function.
// If an error occurs, it logs the error and returns the default value.
func readIntEnvVar(logger logr.Logger, getter func() (int, error), envVarName string) int {
	val, err := getter()
	if err != nil {
		logger.Error(err, "failed parsing env variable, using defaults", "env_var_name", envVarName)
	}
	return val
}

func argsAndEnvs(
	gitrepo *v1alpha1.GitRepo,
	logger logr.Logger,
	pathOverrideCACerts string,
	knownHosts KnownHostsGetter,
	drivenScanSeparator string,
	helmInsecureSkipTLS bool,
	helmBasicHTTP bool,
) ([]string, []corev1.EnvVar) {
	args := []string{
		"fleet",
		"apply",
	}

	if logger.V(1).Enabled() {
		args = append(args, "--debug", "--debug-level", "9")
	}

	bundleLabels := labels.Merge(gitrepo.Labels, map[string]string{
		v1alpha1.RepoLabel: gitrepo.Name,
	})

	args = append(args,
		"--targets-file=/run/config/targets.yaml",
		"--label="+bundleLabels.String(),
		"--namespace", gitrepo.Namespace,
		"--service-account", gitrepo.Spec.ServiceAccount,
		fmt.Sprintf("--sync-generation=%d", gitrepo.Spec.ForceSyncGeneration),
		fmt.Sprintf("--paused=%v", gitrepo.Spec.Paused),
		"--target-namespace", gitrepo.Spec.TargetNamespace,
	)

	if gitrepo.Spec.KeepResources {
		args = append(args, "--keep-resources")
	}

	if gitrepo.Spec.DeleteNamespace {
		args = append(args, "--delete-namespace")
	}

	if gitrepo.Spec.CorrectDrift != nil && gitrepo.Spec.CorrectDrift.Enabled {
		args = append(args, "--correct-drift")
		if gitrepo.Spec.CorrectDrift.Force {
			args = append(args, "--correct-drift-force")
		}
		if gitrepo.Spec.CorrectDrift.KeepFailHistory {
			args = append(args, "--correct-drift-keep-fail-history")
		}
	}

	fleetApplyRetries := readIntEnvVar(logger, fleetapply.GetOnConflictRetries, fleetapply.FleetApplyConflictRetriesEnv)
	bundleCreationMaxConcurrency := readIntEnvVar(logger, fleetapply.GetBundleCreationMaxConcurrency, fleetapply.BundleCreationMaxConcurrencyEnv)

	env := []corev1.EnvVar{
		{
			Name:  "HOME",
			Value: fleetHomeDir,
		},
		{
			Name:  fleetapply.JSONOutputEnvVar,
			Value: "true",
		},
		{
			Name:  fleetapply.JobNameEnvVar,
			Value: jobName(gitrepo),
		},
		{
			Name:  fleetapply.FleetApplyConflictRetriesEnv,
			Value: strconv.Itoa(fleetApplyRetries),
		},
		{
			Name:  fleetapply.BundleCreationMaxConcurrencyEnv,
			Value: strconv.Itoa(bundleCreationMaxConcurrency),
		},
	}

	if gitrepo.Spec.HelmSecretNameForPaths != "" {
		helmArgs := []string{
			"--helm-credentials-by-path-file",
			"/etc/fleet/helm/secrets-path.yaml",
		}

		args = append(args, helmArgs...)
		// for ssh go-getter
		env = append(env, gitSSHCommandEnvVar(knownHosts.IsStrict()))
	} else if gitrepo.Spec.HelmSecretName != "" {
		helmArgs := []string{
			"--password-file",
			"/etc/fleet/helm/password",
			"--ssh-privatekey-file",
			"/etc/fleet/helm/ssh-privatekey",
		}

		if pathOverrideCACerts == "" {
			helmArgs = append(helmArgs,
				"--cacerts-file",
				"/etc/fleet/helm/cacerts",
			)
		}

		if gitrepo.Spec.HelmRepoURLRegex != "" {
			helmArgs = append(helmArgs, "--helm-repo-url-regex", gitrepo.Spec.HelmRepoURLRegex)
		}
		args = append(args, helmArgs...)
		// for ssh go-getter
		env = append(env, gitSSHCommandEnvVar(knownHosts.IsStrict()))
		env = append(env,
			corev1.EnvVar{
				Name: "HELM_USERNAME",
				ValueFrom: &corev1.EnvVarSource{
					SecretKeyRef: &corev1.SecretKeySelector{
						Optional: &[]bool{true}[0],
						Key:      "username",
						LocalObjectReference: corev1.LocalObjectReference{
							Name: gitrepo.Spec.HelmSecretName,
						},
					},
				},
			})
	}

	if pathOverrideCACerts != "" {
		helmArgs := []string{
			"--cacerts-file",
			pathOverrideCACerts,
		}
		if gitrepo.Spec.HelmRepoURLRegex != "" {
			helmArgs = append(helmArgs, "--helm-repo-url-regex", gitrepo.Spec.HelmRepoURLRegex)
		}
		args = append(args, helmArgs...)
		env = append(env, gitSSHCommandEnvVar(knownHosts.IsStrict()))
	}

	if !ocistorage.OCIIsEnabled() {
		env = append(env,
			corev1.EnvVar{
				Name:  ocistorage.OCIStorageFlag,
				Value: "false",
			})
	} else {
		args = append(args, "--oci-registry-secret", gitrepo.Spec.OCIRegistrySecret)
	}

	if len(gitrepo.Spec.Bundles) > 0 {
		args = append(args, "--driven-scan")
		if drivenScanSeparator != "" {
			args = append(args, "--driven-scan-sep", drivenScanSeparator)
		}
	}

	if helmInsecureSkipTLS {
		args = append(args, "--helm-insecure-skip-tls")
	}
	if helmBasicHTTP {
		args = append(args, "--helm-basic-http")
	}

	return append(args, "--", gitrepo.Name), env
}

// volumes builds sets of volumes and their volume mounts for default folders and the targets config map.
func volumes(targetsConfigName string) ([]corev1.Volume, []corev1.VolumeMount) {
	const (
		emptyDirTmpVolumeName  = "fleet-tmp-empty-dir"
		emptyDirHomeVolumeName = "fleet-home-empty-dir"
		configVolumeName       = "config"
	)

	volumes := []corev1.Volume{
		{
			Name: configVolumeName,
			VolumeSource: corev1.VolumeSource{
				ConfigMap: &corev1.ConfigMapVolumeSource{
					LocalObjectReference: corev1.LocalObjectReference{
						Name: targetsConfigName,
					},
				},
			},
		},
		{
			Name: emptyDirTmpVolumeName,
			VolumeSource: corev1.VolumeSource{
				EmptyDir: &corev1.EmptyDirVolumeSource{},
			},
		},
		{
			Name: emptyDirHomeVolumeName,
			VolumeSource: corev1.VolumeSource{
				EmptyDir: &corev1.EmptyDirVolumeSource{},
			},
		},
	}

	volumeMounts := []corev1.VolumeMount{
		{
			Name:      configVolumeName,
			MountPath: "/run/config",
		},
		{
			Name:      emptyDirTmpVolumeName,
			MountPath: "/tmp",
		},
		{
			Name:      emptyDirHomeVolumeName,
			MountPath: fleetHomeDir,
		},
	}

	return volumes, volumeMounts
}

// volumesFromSecret generates volumes and volume mounts from a Helm secret, assuming that that secret exists.
// If the secret has a cacerts key, it will be mounted into /etc/ssl/certs, too.
// It also returns a struct containing boolean values indicating if a volume has
// been created for CA bundles, along with values (defaulting to false) of the
// `insecureSkipVerify` and `basicHTTP` keys of the secret.
func volumesFromSecret(
	ctx context.Context,
	c client.Client,
	namespace string,
	secretName, volumeName, mountPath string,
) ([]corev1.Volume, []corev1.VolumeMount, helmSecretOptions) {
	if mountPath == "" {
		mountPath = "/etc/fleet/helm"
	}

	volumes := []corev1.Volume{
		{
			Name: volumeName,
			VolumeSource: corev1.VolumeSource{
				Secret: &corev1.SecretVolumeSource{
					SecretName: secretName,
				},
			},
		},
	}
	volumeMounts := []corev1.VolumeMount{
		{
			Name:      volumeName,
			MountPath: mountPath,
		},
	}

	// Mount a CA certificate, if specified in the secret. This is necessary to support Helm registries with
	// self-signed certificates.
	secret := &corev1.Secret{}
	var certVolCreated bool
	_ = c.Get(ctx, types.NamespacedName{Namespace: namespace, Name: secretName}, secret)
	if _, ok := secret.Data["cacerts"]; ok {
		volumes = append(volumes, corev1.Volume{
			Name: fmt.Sprintf("%s-cert", volumeName),
			VolumeSource: corev1.VolumeSource{
				Secret: &corev1.SecretVolumeSource{
					SecretName: secretName,
					Items: []corev1.KeyToPath{
						{
							Key:  "cacerts",
							Path: "cacert.crt",
						},
					},
				},
			},
		})
		volumeMounts = append(volumeMounts, corev1.VolumeMount{
			Name:      fmt.Sprintf("%s-cert", volumeName),
			MountPath: "/etc/ssl/certs",
		})

		certVolCreated = true
	}

	// Get the values for skipping TLS and basic HTTP connections.
	// In case of error reading the values they will be considered
	// as set to false as those values are security related.
	insecureSkipVerify := false
	if value, ok := secret.Data["insecureSkipVerify"]; ok {
		boolValue, err := strconv.ParseBool(string(value))
		if err == nil {
			insecureSkipVerify = boolValue
		}
	}

	basicHTTP := false
	if value, ok := secret.Data["basicHTTP"]; ok {
		boolValue, err := strconv.ParseBool(string(value))
		if err == nil {
			basicHTTP = boolValue
		}
	}

	secretOpts := helmSecretOptions{
		InsecureSkipTLS: insecureSkipVerify,
		BasicHTTP:       basicHTTP,
		HasCACerts:      certVolCreated,
	}

	return volumes, volumeMounts, secretOpts
}

func proxyEnvVars() []corev1.EnvVar {
	var envVars []corev1.EnvVar
	for _, envVar := range []string{"HTTP_PROXY", "HTTPS_PROXY", "NO_PROXY"} {
		if val, ok := os.LookupEnv(envVar); ok {
			envVars = append(envVars, corev1.EnvVar{Name: envVar, Value: val})
		}
	}

	return envVars
}

func gitSSHCommandEnvVar(strictChecks bool) corev1.EnvVar {
	strictVal := "no"

	if strictChecks {
		strictVal = "yes"
	}

	return corev1.EnvVar{
		Name:  "GIT_SSH_COMMAND",
		Value: fmt.Sprintf("ssh -o stricthostkeychecking=%s", strictVal),
	}
}

// getDrivenScanSeparator returns a separator that is valid for all the Bundle
// definitions in the given GitRepo.
// Since we cannot disregard the possibility that a user might have an
// unavoidable need to use the character ":" (or another character typically not
// used in directory or file paths), we need to find possible alternatives.
// The function will search for simple characters from those in
// bundleOptionsSeparatorChars, and if none of them can be used, it will return an error.
func getDrivenScanSeparator(gitrepo v1alpha1.GitRepo) (string, error) {
	for _, sep := range bundleOptionsSeparatorChars {
		if !separatorInBundleDefinitions(gitrepo, sep) {
			// We can safely use this separator
			return string(sep), nil
		}
	}

	return "", fmt.Errorf("bundle base and/or options paths contain all possible characters from %q, please update those paths to remedy this", bundleOptionsSeparatorChars)
}

func separatorInBundleDefinitions(gitrepo v1alpha1.GitRepo, sep rune) bool {
	for _, b := range gitrepo.Spec.Bundles {
		if strings.ContainsRune(b.Options, sep) {
			return true
		}

		if strings.ContainsRune(b.Base, sep) {
			return true
		}
	}

	return false
}

func jobName(obj *v1alpha1.GitRepo) string {
	return names.SafeConcatName(obj.Name, names.Hex(obj.Spec.Repo+obj.Status.Commit, 5))
}

func caBundleName(obj *v1alpha1.GitRepo) string {
	return fmt.Sprintf("%s-cabundle", obj.Name)
}

func rancherCABundleName(obj *v1alpha1.GitRepo) string {
	return fmt.Sprintf("%s-rancher-cabundle", obj.Name)
}



================================================
FILE: internal/cmd/controller/gitops/reconciler/gitjob_controller.go
================================================
package reconciler

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"sort"
	"strings"
	"time"

	"github.com/go-logr/logr"
	"github.com/reugn/go-quartz/quartz"

	"github.com/rancher/fleet/internal/cmd/controller/finalize"
	"github.com/rancher/fleet/internal/cmd/controller/imagescan"
	ctrlquartz "github.com/rancher/fleet/internal/cmd/controller/quartz"
	"github.com/rancher/fleet/internal/cmd/controller/reconciler"
	"github.com/rancher/fleet/internal/metrics"
	v1alpha1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"
	fleetevent "github.com/rancher/fleet/pkg/event"
	"github.com/rancher/fleet/pkg/sharding"

	"github.com/rancher/wrangler/v3/pkg/condition"
	"github.com/rancher/wrangler/v3/pkg/genericcondition"
	"github.com/rancher/wrangler/v3/pkg/kstatus"

	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	errutil "k8s.io/apimachinery/pkg/util/errors"
	"k8s.io/client-go/tools/record"
	"k8s.io/client-go/util/retry"
	"sigs.k8s.io/cli-utils/pkg/kstatus/status"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/builder"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
)

const (
	defaultPollingSyncInterval = 15 * time.Second
	gitPollingCondition        = "GitPolling"
	generationLabel            = "fleet.cattle.io/gitrepo-generation"
	forceSyncGenerationLabel   = "fleet.cattle.io/force-sync-generation"
	// The TTL is the grace period for short-lived metrics to be kept alive to
	// make sure Prometheus scrapes them.
	ShortLivedMetricsTTL       = 120 * time.Second
	gitJobPollingJitterPercent = 10
)

var (
	GitJobDurationBuckets = []float64{1, 2, 5, 10, 30, 60, 180, 300, 600, 1200, 1800, 3600}
	gitjobsCreatedSuccess = metrics.ObjCounter(
		"gitjobs_created_success_total",
		"Total number of failed git job creations",
	)
	gitjobsCreatedFailure = metrics.ObjCounter(
		"gitjobs_created_failure_total",
		"Total number of successfully created git jobs",
	)
	gitjobDuration = metrics.ObjHistogram(
		"gitjob_duration_seconds",
		"Duration to complete a Git job in seconds. Includes the time to fetch the git repo and to create the bundle.",
		GitJobDurationBuckets,
	)
	gitjobDurationGauge = metrics.ObjGauge(
		"gitjob_duration_seconds_gauge",
		"Duration to complete a Git job in seconds. Includes the time to fetch the git repo and to create the bundle.",
	)
	fetchLatestCommitSuccess = metrics.ObjCounter(
		"gitrepo_fetch_latest_commit_success_total",
		"Total number of successful fetches of latest commit",
	)
	fetchLatestCommitFailure = metrics.ObjCounter(
		"gitrepo_fetch_latest_commit_failure_total",
		"Total number of failed attempts to retrieve the latest commit, for any reason",
	)
	timeToFetchLatestCommit = metrics.ObjHistogram(
		"gitrepo_fetch_latest_commit_duration_seconds",
		"Duration in seconds to fetch the latest commit",
		metrics.BucketsLatency,
	)
)

type GitFetcher interface {
	LatestCommit(ctx context.Context, gitrepo *v1alpha1.GitRepo, client client.Client) (string, error)
}

// TimeGetter interface is used to mock the time.Now() call in unit tests
type TimeGetter interface {
	Now() time.Time
	Since(t time.Time) time.Duration
}

type RealClock struct{}

func (RealClock) Now() time.Time                  { return time.Now() }
func (RealClock) Since(t time.Time) time.Duration { return time.Since(t) }

type KnownHostsGetter interface {
	Get(ctx context.Context, client client.Client, namespace, secretName string) (string, error)
	IsStrict() bool
}

// GitJobReconciler reconciles a GitRepo resource to create a git cloning k8s job
type GitJobReconciler struct {
	client.Client
	Scheme          *runtime.Scheme
	Image           string
	Scheduler       quartz.Scheduler
	Workers         int
	ShardID         string
	JobNodeSelector string
	GitFetcher      GitFetcher
	Clock           TimeGetter
	Recorder        record.EventRecorder
	SystemNamespace string
	KnownHosts      KnownHostsGetter
}

func (r *GitJobReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&v1alpha1.GitRepo{},
			builder.WithPredicates(
				// do not trigger for GitRepo status changes (except for commit changes and cache sync)
				predicate.Or(
					reconciler.TypedResourceVersionUnchangedPredicate[client.Object]{},
					predicate.GenerationChangedPredicate{},
					predicate.AnnotationChangedPredicate{},
					predicate.LabelChangedPredicate{},
					commitChangedPredicate(),
				),
			),
		).
		Owns(&batchv1.Job{}, builder.WithPredicates(jobUpdatedPredicate())).
		WithEventFilter(sharding.FilterByShardID(r.ShardID)).
		WithOptions(controller.Options{MaxConcurrentReconciles: r.Workers}).
		Complete(r)
}

// Reconcile  compares the state specified by the GitRepo object against the
// actual cluster state. It checks the Git repository for new commits and
// creates a job to clone the repository if a new commit is found. In case of
// an error, the output of the job is stored in the status.
func (r *GitJobReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx).WithName("gitjob")
	gitrepo := &v1alpha1.GitRepo{}

	if err := r.Get(ctx, req.NamespacedName, gitrepo); err != nil && !apierrors.IsNotFound(err) {
		return ctrl.Result{}, err
	} else if apierrors.IsNotFound(err) {
		gitjobsCreatedSuccess.DeleteByReq(req)
		gitjobsCreatedFailure.DeleteByReq(req)
		gitjobDuration.DeleteByReq(req)
		fetchLatestCommitSuccess.DeleteByReq(req)
		fetchLatestCommitFailure.DeleteByReq(req)
		timeToFetchLatestCommit.DeleteByReq(req)

		logger.V(1).Info("Gitrepo deleted, cleaning up pull jobs")
		return ctrl.Result{}, nil
	}

	// Restrictions / Overrides, gitrepo reconciler is responsible for setting error in status
	oldStatus := gitrepo.Status.DeepCopy()
	if err := AuthorizeAndAssignDefaults(ctx, r.Client, gitrepo); err != nil {
		r.Recorder.Event(gitrepo, fleetevent.Warning, "FailedToApplyRestrictions", err.Error())
		return ctrl.Result{}, updateErrorStatus(ctx, r.Client, req.NamespacedName, *oldStatus, err)
	}

	if !gitrepo.DeletionTimestamp.IsZero() {
		if controllerutil.ContainsFinalizer(gitrepo, finalize.GitRepoFinalizer) {
			if err := r.cleanupGitRepo(ctx, logger, gitrepo); err != nil {
				return ctrl.Result{}, err
			}
		}

		return ctrl.Result{}, nil
	}

	if err := finalize.EnsureFinalizer(ctx, r.Client, gitrepo, finalize.GitRepoFinalizer); err != nil {
		return ctrl.Result{}, err
	}

	// Migration: Remove the obsolete created-by-display-name label if it exists
	if err := r.removeDisplayNameLabel(ctx, req.NamespacedName); err != nil {
		logger.V(1).Error(err, "Failed to remove display name label")
		return ctrl.Result{}, err
	}

	logger = logger.WithValues("generation", gitrepo.Generation, "commit", gitrepo.Status.Commit).WithValues("conditions", gitrepo.Status.Conditions)

	if userID := gitrepo.Labels[v1alpha1.CreatedByUserIDLabel]; userID != "" {
		logger = logger.WithValues("userID", userID)
	}

	ctx = log.IntoContext(ctx, logger)

	logger.V(1).Info("Reconciling GitRepo")

	if gitrepo.Spec.Repo == "" {
		if err := r.deletePollingJob(*gitrepo); err != nil {
			return ctrl.Result{}, updateErrorStatus(ctx, r.Client, req.NamespacedName, gitrepo.Status, err)
		}
		// TODO: return an error here, similar to what we already do for HelmOps
		return ctrl.Result{}, nil
	}

	jobUpdatedOrCreated, err := r.managePollingJob(logger, *gitrepo)
	if err != nil {
		return ctrl.Result{}, updateErrorStatus(ctx, r.Client, req.NamespacedName, gitrepo.Status, err)
	}

	if jobUpdatedOrCreated {
		// Maybe an update from the polling job will come next
		// Requeue and stop this reconcile now as moving on to gitJob creation would
		// possibly lead to conflicts.
		return ctrl.Result{RequeueAfter: durations.DefaultRequeueAfter}, nil
	}

	oldCommit := gitrepo.Status.Commit
	// maybe update the commit from webhooks or polling
	gitrepo.Status.Commit = getNextCommit(gitrepo.Status)

	res, err := r.manageGitJob(ctx, logger, gitrepo, oldCommit)
	if err != nil || res.RequeueAfter > 0 {
		return res, updateErrorStatus(ctx, r.Client, req.NamespacedName, gitrepo.Status, err)
	}

	reconciler.SetCondition(v1alpha1.GitRepoAcceptedCondition, &gitrepo.Status, nil)

	err = updateStatus(ctx, r.Client, req.NamespacedName, gitrepo.Status)
	if err != nil {
		logger.Error(err, "Reconcile failed final update to git repo status", "status", gitrepo.Status)

		return ctrl.Result{}, err
	}

	return ctrl.Result{}, nil
}

func monitorLatestCommit(obj metav1.Object, fetch func() (string, error)) (string, error) {
	start := time.Now()
	commit, err := fetch()
	if err != nil {
		fetchLatestCommitFailure.Inc(obj)
		return "", err
	}
	fetchLatestCommitSuccess.Inc(obj)
	timeToFetchLatestCommit.Observe(obj, time.Since(start).Seconds())
	return commit, nil
}

// manageGitJob is responsible for creating, updating and deleting the GitJob and setting the GitRepo's status accordingly
func (r *GitJobReconciler) manageGitJob(ctx context.Context, logger logr.Logger, gitrepo *v1alpha1.GitRepo, oldCommit string) (ctrl.Result, error) {
	if err := r.deletePreviousJob(ctx, logger, *gitrepo, oldCommit); err != nil {
		return ctrl.Result{}, err
	}

	var job batchv1.Job
	err := r.Get(ctx, types.NamespacedName{
		Namespace: gitrepo.Namespace,
		Name:      jobName(gitrepo),
	}, &job)
	if err != nil && !apierrors.IsNotFound(err) {
		err = fmt.Errorf("error retrieving git job: %w", err)
		r.Recorder.Event(gitrepo, fleetevent.Warning, "FailedToGetGitJob", err.Error())

		return ctrl.Result{}, err
	}

	if apierrors.IsNotFound(err) {
		if gitrepo.Spec.DisablePolling {
			commit, err := monitorLatestCommit(gitrepo, func() (string, error) {
				return r.GitFetcher.LatestCommit(ctx, gitrepo, r.Client)
			})
			condition.Cond(gitPollingCondition).SetError(&gitrepo.Status, "", err)
			if err == nil && commit != "" {
				gitrepo.Status.Commit = commit
			}
			if err != nil {
				r.Recorder.Event(gitrepo, fleetevent.Warning, "Failed", err.Error())
			} else if oldCommit != gitrepo.Status.Commit {
				r.Recorder.Event(gitrepo, fleetevent.Normal, "GotNewCommit", gitrepo.Status.Commit)
			}
		}

		if r.shouldCreateJob(gitrepo, oldCommit) {
			r.updateGenerationValuesIfNeeded(gitrepo)
			if err := r.validateExternalSecretExist(ctx, gitrepo); err != nil {
				r.Recorder.Event(gitrepo, fleetevent.Warning, "FailedValidatingSecret", err.Error())
				return ctrl.Result{}, fmt.Errorf("error validating external secrets: %w", err)
			}
			if err := r.createJobAndResources(ctx, gitrepo, logger); err != nil {
				gitjobsCreatedFailure.Inc(gitrepo)
				return ctrl.Result{}, err
			}
			gitjobsCreatedSuccess.Inc(gitrepo)
		}
	} else if gitrepo.Status.Commit != "" && gitrepo.Status.Commit == oldCommit {
		err, recreateGitJob := r.deleteJobIfNeeded(ctx, gitrepo, &job)
		if err != nil {
			return ctrl.Result{}, fmt.Errorf("error deleting git job: %w", err)
		}
		// job was deleted and we need to recreate it
		// Requeue so the reconciler creates the job again
		if recreateGitJob {
			return ctrl.Result{RequeueAfter: durations.DefaultRequeueAfter}, nil
		}
	}

	gitrepo.Status.ObservedGeneration = gitrepo.Generation

	if err = setStatusFromGitjob(ctx, r.Client, gitrepo, &job); err != nil {
		return ctrl.Result{}, fmt.Errorf("error setting GitRepo status from git job: %w", err)
	}

	return ctrl.Result{}, nil
}

func (r *GitJobReconciler) deletePreviousJob(ctx context.Context, logger logr.Logger, gitrepo v1alpha1.GitRepo, oldCommit string) error {
	if oldCommit == "" || oldCommit == gitrepo.Status.Commit {
		return nil
	}

	// the GitRepo is passed by value, just use the old commit
	// to calculate the job Name
	gitrepo.Status.Commit = oldCommit

	var job batchv1.Job
	err := r.Get(ctx, types.NamespacedName{
		Namespace: gitrepo.Namespace,
		Name:      jobName(&gitrepo),
	}, &job)
	if err != nil {
		if !apierrors.IsNotFound(err) {
			return err
		}

		return nil
	}

	// At this point we know the previous job still exists and the commit already changed.
	// Delete the previous one so we don't incur in conflicts
	logger.Info("Deleting previous job to avoid conflicts")
	return r.Delete(ctx, &job)
}

func (r *GitJobReconciler) cleanupGitRepo(ctx context.Context, logger logr.Logger, gitrepo *v1alpha1.GitRepo) error {
	logger.Info("Gitrepo deleted, deleting bundle, image scans")

	metrics.GitRepoCollector.Delete(gitrepo.Name, gitrepo.Namespace)
	_ = r.deletePollingJob(*gitrepo)

	nsName := types.NamespacedName{Name: gitrepo.Name, Namespace: gitrepo.Namespace}
	if err := finalize.PurgeBundles(ctx, r.Client, nsName, v1alpha1.RepoLabel); err != nil {
		return err
	}

	// remove the job scheduled by imagescan, if any
	_ = r.Scheduler.DeleteJob(imagescan.GitCommitKey(gitrepo.Namespace, gitrepo.Name))

	if err := finalize.PurgeImageScans(ctx, r.Client, nsName); err != nil {
		return err
	}

	err := retry.RetryOnConflict(retry.DefaultRetry, func() error {
		if err := r.Get(ctx, nsName, gitrepo); err != nil {
			return err
		}

		controllerutil.RemoveFinalizer(gitrepo, finalize.GitRepoFinalizer)

		return r.Update(ctx, gitrepo)
	})

	if client.IgnoreNotFound(err) != nil {
		return err
	}

	return nil
}

// shouldCreateJob checks if the conditions to create a new job are met.
// It checks for all the conditions so, in case more than one is met, it sets all the
// values related in one single reconciler loop
func (r *GitJobReconciler) shouldCreateJob(gitrepo *v1alpha1.GitRepo, oldCommit string) bool {
	if gitrepo.Status.Commit != "" && gitrepo.Status.Commit != oldCommit {
		return true
	}

	if gitrepo.Spec.ForceSyncGeneration != gitrepo.Status.UpdateGeneration {
		return true
	}

	// k8s Jobs are immutable. Recreate the job if the GitRepo Spec has changed.
	// Avoid deleting the job twice
	if generationChanged(gitrepo) {
		return true
	}

	return false
}

func (r *GitJobReconciler) updateGenerationValuesIfNeeded(gitrepo *v1alpha1.GitRepo) {
	if gitrepo.Spec.ForceSyncGeneration != gitrepo.Status.UpdateGeneration {
		gitrepo.Status.UpdateGeneration = gitrepo.Spec.ForceSyncGeneration
	}

	if generationChanged(gitrepo) {
		gitrepo.Status.ObservedGeneration = gitrepo.Generation
	}
}

// removeDisplayNameLabel removes the obsolete created-by-display-name label from the gitrepo if it exists.
func (r *GitJobReconciler) removeDisplayNameLabel(ctx context.Context, nsName types.NamespacedName) error {
	return retry.RetryOnConflict(retry.DefaultRetry, func() error {
		gitrepo := &v1alpha1.GitRepo{}
		if err := r.Get(ctx, nsName, gitrepo); err != nil {
			return err
		}

		if gitrepo.Labels == nil {
			return nil
		}

		const deprecatedLabel = "fleet.cattle.io/created-by-display-name"
		if _, exists := gitrepo.Labels[deprecatedLabel]; !exists {
			return nil
		}

		delete(gitrepo.Labels, deprecatedLabel)
		return r.Update(ctx, gitrepo)
	})
}

func (r *GitJobReconciler) validateExternalSecretExist(ctx context.Context, gitrepo *v1alpha1.GitRepo) error {
	if gitrepo.Spec.HelmSecretNameForPaths != "" {
		if err := r.Get(ctx, types.NamespacedName{Namespace: gitrepo.Namespace, Name: gitrepo.Spec.HelmSecretNameForPaths}, &corev1.Secret{}); err != nil {
			return fmt.Errorf("failed to look up HelmSecretNameForPaths, error: %w", err)
		}
	} else if gitrepo.Spec.HelmSecretName != "" {
		if err := r.Get(ctx, types.NamespacedName{Namespace: gitrepo.Namespace, Name: gitrepo.Spec.HelmSecretName}, &corev1.Secret{}); err != nil {
			return fmt.Errorf("failed to look up helmSecretName, error: %w", err)
		}
	}
	return nil
}

func (r *GitJobReconciler) deleteJobIfNeeded(ctx context.Context, gitRepo *v1alpha1.GitRepo, job *batchv1.Job) (error, bool) {
	logger := log.FromContext(ctx)

	// the following cases imply that the job is still running but we need to stop it and
	// create a new one
	if gitRepo.Spec.ForceSyncGeneration != gitRepo.Status.UpdateGeneration {
		if forceSync, ok := job.Labels[forceSyncGenerationLabel]; ok {
			t := fmt.Sprintf("%d", gitRepo.Spec.ForceSyncGeneration)
			if t != forceSync {
				jobDeletedMessage := "job deletion triggered because of ForceUpdateGeneration"
				logger.V(1).Info(jobDeletedMessage)
				if err := r.Delete(ctx, job, client.PropagationPolicy(metav1.DeletePropagationBackground)); err != nil && !apierrors.IsNotFound(err) {
					return err, true
				}
				return nil, true
			}
		}
	}

	// k8s Jobs are immutable. Recreate the job if the GitRepo Spec has changed.
	// Avoid deleting the job twice
	if generationChanged(gitRepo) {
		if gen, ok := job.Labels[generationLabel]; ok {
			t := fmt.Sprintf("%d", gitRepo.Generation)
			if t != gen {
				jobDeletedMessage := "job deletion triggered because of generation change"
				logger.V(1).Info(jobDeletedMessage)
				if err := r.Delete(ctx, job, client.PropagationPolicy(metav1.DeletePropagationBackground)); err != nil && !apierrors.IsNotFound(err) {
					return err, true
				}
				return nil, true
			}
		}
	}

	// check if the job finished and was successful
	if job.Status.Succeeded == 1 {
		if job.Status.StartTime != nil && job.Status.CompletionTime != nil {
			duration := job.Status.CompletionTime.Sub(job.Status.StartTime.Time)
			gitjobDuration.Observe(gitRepo, duration.Seconds())
			gitjobDurationGauge.Set(gitRepo, duration.Seconds())

			go func() {
				time.Sleep(ShortLivedMetricsTTL)
				gitjobDurationGauge.Delete(gitRepo)
			}()
		}
		jobDeletedMessage := "job deletion triggered because job succeeded"
		logger.Info(jobDeletedMessage)
		if err := r.Delete(ctx, job, client.PropagationPolicy(metav1.DeletePropagationBackground)); err != nil && !apierrors.IsNotFound(err) {
			return err, false
		}
		r.Recorder.Event(gitRepo, fleetevent.Normal, "JobDeleted", jobDeletedMessage)
	}

	return nil, false
}

func jobKey(g v1alpha1.GitRepo) *quartz.JobKey {
	return quartz.NewJobKey(string(g.UID))
}

// deletePollingJob deletes the polling job scheduled for the provided gitrepo, if any, and returns any error that may
// have happened in the process.
// Returns a nil error if the job could be deleted or if none existed.
func (r *GitJobReconciler) deletePollingJob(gitrepo v1alpha1.GitRepo) error {
	if r.Scheduler == nil {
		return nil
	}
	jobKey := jobKey(gitrepo)
	if _, err := r.Scheduler.GetScheduledJob(jobKey); err == nil {
		if err = r.Scheduler.DeleteJob(jobKey); err != nil {
			return fmt.Errorf("failed to delete outdated polling job: %w", err)
		}
	} else if !errors.Is(err, quartz.ErrJobNotFound) {
		return fmt.Errorf("failed to get outdated polling job for deletion: %w", err)
	}

	return nil
}

// managePollingJob creates, updates or deletes a polling job for the provided GitRepo.
func (r *GitJobReconciler) managePollingJob(logger logr.Logger, gitrepo v1alpha1.GitRepo) (bool, error) {
	jobUpdatedOrCreated := false
	if r.Scheduler == nil {
		logger.V(1).Info("Scheduler is not set; this should only happen in tests")
		return jobUpdatedOrCreated, nil
	}

	jobKey := jobKey(gitrepo)
	scheduled, err := r.Scheduler.GetScheduledJob(jobKey)

	if err != nil && !errors.Is(err, quartz.ErrJobNotFound) {
		return jobUpdatedOrCreated, fmt.Errorf("an unknown error occurred when looking for a polling job: %w", err)
	}

	if !gitrepo.Spec.DisablePolling {
		scheduledJobDescription := ""

		if err == nil {
			if detail := scheduled.JobDetail(); detail != nil {
				scheduledJobDescription = detail.Job().Description()
			}
		}

		newJob := newGitPollingJob(r.Client, r.Recorder, gitrepo, r.GitFetcher)
		currentTrigger := ctrlquartz.NewControllerTrigger(
			getPollingIntervalDuration(&gitrepo),
			gitJobPollingJitterPercent,
		)

		if errors.Is(err, quartz.ErrJobNotFound) ||
			scheduled.Trigger().Description() != currentTrigger.Description() ||
			scheduledJobDescription != newJob.Description() {
			err = r.Scheduler.ScheduleJob(
				quartz.NewJobDetailWithOptions(
					newJob,
					jobKey,
					&quartz.JobDetailOptions{
						Replace: true,
					},
				),
				currentTrigger,
			)

			if err != nil {
				return jobUpdatedOrCreated, fmt.Errorf("failed to schedule polling job: %w", err)
			}

			logger.V(1).Info("Scheduled new polling job")
			jobUpdatedOrCreated = true
		}
	} else if err == nil {
		// A job still exists, but is no longer needed; delete it.
		if err = r.Scheduler.DeleteJob(jobKey); err != nil {
			return jobUpdatedOrCreated, fmt.Errorf("failed to delete polling job: %w", err)
		}
	}

	return jobUpdatedOrCreated, nil
}

func generationChanged(r *v1alpha1.GitRepo) bool {
	// checks if generation changed.
	// it ignores the case when Status.ObservedGeneration=0 because that's
	// the initial value of a just created GitRepo and the initial value
	// for Generation in k8s is 1.
	// If we don't ignore we would be deleting the gitjob that was just created
	// until later we reconcile ObservedGeneration with Generation
	return (r.Generation != r.Status.ObservedGeneration) && r.Status.ObservedGeneration > 0
}

func getPollingIntervalDuration(gitrepo *v1alpha1.GitRepo) time.Duration {
	if gitrepo.Spec.PollingInterval == nil || gitrepo.Spec.PollingInterval.Duration == 0 {
		return defaultPollingSyncInterval
	}

	return gitrepo.Spec.PollingInterval.Duration
}

// setStatusFromGitjob sets the status fields relative to the given job in the gitRepo
func setStatusFromGitjob(ctx context.Context, c client.Client, gitRepo *v1alpha1.GitRepo, job *batchv1.Job) error {
	obj, err := runtime.DefaultUnstructuredConverter.ToUnstructured(job)
	if err != nil {
		return err
	}
	uJob := &unstructured.Unstructured{Object: obj}

	result, err := status.Compute(uJob)
	if err != nil {
		return err
	}

	terminationMessage := ""
	if result.Status == status.FailedStatus {
		selector := labels.SelectorFromSet(labels.Set{"job-name": job.Name})
		podList := &corev1.PodList{}
		err := c.List(ctx, podList, &client.ListOptions{LabelSelector: selector})
		if err != nil {
			return err
		}

		sort.Slice(podList.Items, func(i, j int) bool {
			return podList.Items[i].CreationTimestamp.Before(&podList.Items[j].CreationTimestamp)
		})

		terminationMessage = result.Message
		if len(podList.Items) > 0 {
			for _, podStatus := range podList.Items[len(podList.Items)-1].Status.ContainerStatuses {
				if podStatus.Name != "step-git-source" && podStatus.State.Terminated != nil {
					terminationMessage += podStatus.State.Terminated.Message
				}
			}

			// set also the message from init containers (if they failed)
			for _, podStatus := range podList.Items[len(podList.Items)-1].Status.InitContainerStatuses {
				if podStatus.Name != "step-git-source" &&
					podStatus.State.Terminated != nil &&
					podStatus.State.Terminated.ExitCode != 0 {
					terminationMessage += podStatus.State.Terminated.Message
				}
			}
		}
	}

	gitRepo.Status.GitJobStatus = result.Status.String()

	for _, con := range result.Conditions {
		if con.Type.String() == "Ready" {
			continue
		}
		condition.Cond(con.Type.String()).SetStatus(gitRepo, string(con.Status))
		condition.Cond(con.Type.String()).SetMessageIfBlank(gitRepo, con.Message)
		condition.Cond(con.Type.String()).Reason(gitRepo, con.Reason)
	}

	// status.Compute() possible results are
	//   - InProgress
	//   - Current
	//   - Failed
	//   - Terminating
	switch result.Status {
	case status.FailedStatus:
		kstatus.SetError(gitRepo, filterFleetCLIJobOutput(terminationMessage))
	case status.CurrentStatus:
		if strings.Contains(result.Message, "Job Completed") {
			gitRepo.Status.Commit = job.Annotations["commit"]
		}
		kstatus.SetActive(gitRepo)
	case status.InProgressStatus:
		kstatus.SetTransitioning(gitRepo, "")
	case status.TerminatingStatus:
		// set active set both conditions to False
		// the job is terminating so avoid reporting errors in
		// that case
		kstatus.SetActive(gitRepo)
	}

	return nil
}

// updateErrorStatus sets the condition in the status and tries to update the resource
func updateErrorStatus(ctx context.Context, c client.Client, req types.NamespacedName, status v1alpha1.GitRepoStatus, orgErr error) error {
	reconciler.SetCondition(v1alpha1.GitRepoAcceptedCondition, &status, orgErr)

	if statusErr := updateStatus(ctx, c, req, status); statusErr != nil {
		merr := []error{orgErr, fmt.Errorf("failed to update the status: %w", statusErr)}
		return errutil.NewAggregate(merr)
	}
	return orgErr
}

// updateStatus updates the status for the GitRepo resource. It retries on
// conflict. If the status was updated successfully, it also collects (as in
// updates) metrics for the resource GitRepo resource.
func updateStatus(ctx context.Context, c client.Client, req types.NamespacedName, status v1alpha1.GitRepoStatus) error {
	return retry.RetryOnConflict(retry.DefaultRetry, func() error {
		t := &v1alpha1.GitRepo{}
		err := c.Get(ctx, req, t)
		if err != nil {
			return err
		}

		commit := t.Status.Commit

		// selectively update the status fields this reconciler is responsible for
		t.Status.Commit = status.Commit
		t.Status.GitJobStatus = status.GitJobStatus
		t.Status.PollingCommit = status.PollingCommit
		t.Status.LastPollingTime = status.LastPollingTime
		t.Status.ObservedGeneration = status.ObservedGeneration
		t.Status.UpdateGeneration = status.UpdateGeneration

		// only keep the Ready condition from live status, it's calculated by the status reconciler
		conds := []genericcondition.GenericCondition{}
		for _, c := range t.Status.Conditions {
			if c.Type == "Ready" {
				conds = append(conds, c)
				break
			}
		}
		for _, c := range status.Conditions {
			if c.Type == "Ready" {
				continue
			}
			conds = append(conds, c)
		}
		t.Status.Conditions = conds

		if commit != "" && status.Commit == "" {
			// we could incur in a race condition between the poller job
			// setting the Commit and the first time the reconciler runs.
			// The poller could be faster than the reconciler setting the
			// Commit and we could reset back to "" in here
			t.Status.Commit = commit
		}

		err = c.Status().Update(ctx, t)
		if err != nil {
			return err
		}

		metrics.GitRepoCollector.Collect(ctx, t)

		return nil
	})
}

func filterFleetCLIJobOutput(output string) string {
	// first split the output in lines
	lines := strings.Split(output, "\n")
	s := ""
	for _, l := range lines {
		s += getFleetCLIErrorsFromLine(l)
	}

	s = strings.Trim(s, "\n")
	// in the case that all the messages from fleet apply are from libraries
	// we just report an unknown error
	if s == "" {
		s = "Unknown error"
	}

	return s
}

func getFleetCLIErrorsFromLine(l string) string {
	type LogEntry struct {
		Level         string `json:"level"`
		FleetErrorMsg string `json:"fleetErrorMessage"`
		Time          string `json:"time"`
		Msg           string `json:"msg"`
	}
	s := ""
	open := strings.IndexByte(l, '{')
	if open == -1 {
		// line does not contain a valid json string
		return ""
	}
	close := strings.IndexByte(l, '}')
	if close != -1 {
		if close < open {
			// looks like there is some garbage before a possible json string
			// ignore everything up to that closing bracked and try again
			return getFleetCLIErrorsFromLine(l[close+1:])
		}
	} else if close == -1 {
		// line does not contain a valid json string
		return ""
	}
	var entry LogEntry
	if err := json.Unmarshal([]byte(l[open:close+1]), &entry); err == nil {
		if entry.FleetErrorMsg != "" {
			s = s + entry.FleetErrorMsg + "\n"
		}
	}
	// check if there's more to parse
	if close+1 < len(l) {
		s += getFleetCLIErrorsFromLine(l[close+1:])
	}

	return s
}

// getNextCommit returns a commit SHA coming either from the status' webhook
// commit or, with lower precedence, from the polling commit.
func getNextCommit(status v1alpha1.GitRepoStatus) string {
	commit := status.Commit
	if status.PollingCommit != "" && status.PollingCommit != commit {
		commit = status.PollingCommit
	}
	// We could be using polling but webhooks react immediately to updates.
	// Give preference to the webhook commit.
	if status.WebhookCommit != "" && status.WebhookCommit != commit {
		commit = status.WebhookCommit
	}

	return commit
}



================================================
FILE: internal/cmd/controller/gitops/reconciler/gitjob_test.go
================================================
//go:generate mockgen --build_flags=--mod=mod -destination=../../../../mocks/client_mock.go -package=mocks sigs.k8s.io/controller-runtime/pkg/client Client,SubResourceWriter

package reconciler

import (
	"context"
	"errors"
	"fmt"
	"os"
	"slices"
	"strings"
	"testing"
	"time"

	"github.com/google/go-cmp/cmp"
	fleetapply "github.com/rancher/fleet/internal/cmd/cli/apply"
	"github.com/rancher/fleet/internal/cmd/controller/finalize"
	"github.com/rancher/fleet/internal/config"
	"github.com/rancher/fleet/internal/mocks"
	"github.com/rancher/fleet/internal/ssh"
	fleetv1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/wrangler/v3/pkg/genericcondition"
	"go.uber.org/mock/gomock"

	fleetevent "github.com/rancher/fleet/pkg/event"
	appsv1 "k8s.io/api/apps/v1"
	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/apimachinery/pkg/types"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/client/fake"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
)

func getCondition(gitrepo *fleetv1.GitRepo, condType string) (genericcondition.GenericCondition, bool) {
	for _, cond := range gitrepo.Status.Conditions {
		if cond.Type == condType {
			return cond, true
		}
	}
	return genericcondition.GenericCondition{}, false
}

type ClockMock struct {
	t time.Time
}

func (m ClockMock) Now() time.Time                  { return m.t }
func (m ClockMock) Since(t time.Time) time.Duration { return m.t.Sub(t) }

// gitRepoMatcher implements a gomock matcher that checks for gitrepos.
// It only checks for the expected name and namespace so far
type gitRepoMatcher struct {
	gitrepo fleetv1.GitRepo
}

func (m gitRepoMatcher) Matches(x interface{}) bool {
	gitrepo, ok := x.(*fleetv1.GitRepo)
	if !ok {
		return false
	}
	return m.gitrepo.Name == gitrepo.Name && m.gitrepo.Namespace == gitrepo.Namespace
}

func (m gitRepoMatcher) String() string {
	return fmt.Sprintf("Gitrepo %s-%s", m.gitrepo.Name, m.gitrepo.Namespace)
}

type gitRepoPointerMatcher struct {
}

func (m gitRepoPointerMatcher) Matches(x interface{}) bool {
	_, ok := x.(*fleetv1.GitRepo)
	return ok
}

func (m gitRepoPointerMatcher) String() string {
	return ""
}

func TestReconcile_Error_WhenGitrepoRestrictionsAreNotMet(t *testing.T) {
	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	scheme := runtime.NewScheme()
	utilruntime.Must(batchv1.AddToScheme(scheme))
	gitRepo := fleetv1.GitRepo{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "gitrepo",
			Namespace: "default",
		},
	}
	namespacedName := types.NamespacedName{Name: gitRepo.Name, Namespace: gitRepo.Namespace}
	mockClient := mocks.NewMockK8sClient(mockCtrl)
	mockClient.EXPECT().List(gomock.Any(), gomock.Any(), gomock.Any()).AnyTimes().DoAndReturn(
		func(ctx context.Context, restrictions *fleetv1.GitRepoRestrictionList, ns client.InNamespace) error {
			// fill the restrictions with a couple of allowed namespaces.
			// As the gitrepo has no target namespace restrictions won't be met
			restriction := fleetv1.GitRepoRestriction{AllowedTargetNamespaces: []string{"ns1", "ns2"}}
			restrictions.Items = append(restrictions.Items, restriction)
			return nil
		},
	)

	mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), &gitRepoPointerMatcher{}, gomock.Any()).Times(2).DoAndReturn(
		func(ctx context.Context, req types.NamespacedName, gitrepo *fleetv1.GitRepo, opts ...interface{}) error {
			gitrepo.Name = gitRepo.Name
			gitrepo.Namespace = gitRepo.Namespace
			return nil
		},
	)
	statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
	mockClient.EXPECT().Status().Times(1).Return(statusClient)
	statusClient.EXPECT().Update(gomock.Any(), gomock.Any(), gomock.Any()).Do(
		func(ctx context.Context, repo *fleetv1.GitRepo, opts ...interface{}) {
			if len(repo.Status.Conditions) == 0 {
				t.Errorf("expecting to have Conditions, got none")
			}
			if repo.Status.Conditions[0].Message != "empty targetNamespace denied, because allowedTargetNamespaces restriction is present" {
				t.Errorf("Expecting condition message [empty targetNamespace denied, because allowedTargetNamespaces restriction is present], got [%s]", repo.Status.Conditions[0].Message)
			}
		},
	)

	recorderMock := mocks.NewMockEventRecorder(mockCtrl)
	recorderMock.EXPECT().Event(
		&gitRepoMatcher{gitRepo},
		fleetevent.Warning,
		"FailedToApplyRestrictions",
		"empty targetNamespace denied, because allowedTargetNamespaces restriction is present",
	)

	r := GitJobReconciler{
		Client:   mockClient,
		Scheme:   scheme,
		Image:    "",
		Clock:    RealClock{},
		Recorder: recorderMock,
	}

	ctx := context.TODO()
	_, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
	if err == nil {
		t.Errorf("expecting an error, got nil")
	}
	if err.Error() != "empty targetNamespace denied, because allowedTargetNamespaces restriction is present" {
		t.Errorf("unexpected error %v", err)
	}
}

func TestReconcile_Error_WhenGetGitJobErrors(t *testing.T) {
	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	scheme := runtime.NewScheme()
	utilruntime.Must(batchv1.AddToScheme(scheme))
	gitRepo := fleetv1.GitRepo{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "gitrepo",
			Namespace: "default",
		},
	}
	namespacedName := types.NamespacedName{Name: gitRepo.Name, Namespace: gitRepo.Namespace}
	mockClient := mocks.NewMockK8sClient(mockCtrl)
	mockClient.EXPECT().List(gomock.Any(), gomock.Any(), gomock.Any()).AnyTimes().Return(nil)

	mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&fleetv1.GitRepo{}), gomock.Any()).
		Times(3).
		DoAndReturn(
			func(ctx context.Context, req types.NamespacedName, gitrepo *fleetv1.GitRepo, opts ...interface{}) error {
				gitrepo.Name = gitRepo.Name
				gitrepo.Namespace = gitRepo.Namespace
				gitrepo.Spec.Repo = "repo"
				controllerutil.AddFinalizer(gitrepo, finalize.GitRepoFinalizer)
				gitrepo.Status.Commit = "dd45c7ad68e10307765104fea4a1f5997643020f"
				return nil
			},
		)

	mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Times(1).DoAndReturn(
		func(ctx context.Context, req types.NamespacedName, job *batchv1.Job, opts ...interface{}) error {
			return fmt.Errorf("GITJOB ERROR")
		},
	)

	statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
	mockClient.EXPECT().Status().Times(1).Return(statusClient)
	statusClient.EXPECT().Update(gomock.Any(), gomock.Any(), gomock.Any()).Do(
		func(ctx context.Context, repo *fleetv1.GitRepo, opts ...interface{}) {
			c, found := getCondition(repo, fleetv1.GitRepoAcceptedCondition)
			if !found {
				t.Errorf("expecting to find the %s condition and could not find it.", fleetv1.GitRepoAcceptedCondition)
			}
			if !strings.Contains(c.Message, "GITJOB ERROR") {
				t.Errorf("expecting message containing [GITJOB ERROR] in condition, got [%s]", c.Message)
			}
		},
	)

	recorderMock := mocks.NewMockEventRecorder(mockCtrl)

	recorderMock.EXPECT().Event(
		&gitRepoMatcher{gitRepo},
		fleetevent.Warning,
		"FailedToGetGitJob",
		"error retrieving git job: GITJOB ERROR",
	)

	r := GitJobReconciler{
		Client:   mockClient,
		Scheme:   scheme,
		Image:    "",
		Clock:    RealClock{},
		Recorder: recorderMock,
	}

	ctx := context.TODO()
	_, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
	if err == nil {
		t.Errorf("expecting an error, got nil")
	}
	if err.Error() != "error retrieving git job: GITJOB ERROR" {
		t.Errorf("unexpected error %v", err)
	}
}

func TestReconcile_Error_WhenSecretDoesNotExist(t *testing.T) {
	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	scheme := runtime.NewScheme()
	utilruntime.Must(batchv1.AddToScheme(scheme))
	gitRepo := fleetv1.GitRepo{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "gitrepo",
			Namespace: "default",
		},
	}
	namespacedName := types.NamespacedName{Name: gitRepo.Name, Namespace: gitRepo.Namespace}
	mockClient := mocks.NewMockK8sClient(mockCtrl)
	mockClient.EXPECT().List(gomock.Any(), gomock.Any(), gomock.Any()).AnyTimes().Return(nil)

	mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), &gitRepoPointerMatcher{}, gomock.Any()).Times(3).DoAndReturn(
		func(ctx context.Context, req types.NamespacedName, gitrepo *fleetv1.GitRepo, opts ...interface{}) error {
			gitrepo.Name = gitRepo.Name
			gitrepo.Namespace = gitRepo.Namespace
			gitrepo.Spec.Repo = "repo"
			gitrepo.Spec.HelmSecretNameForPaths = "somevalue"
			controllerutil.AddFinalizer(gitrepo, finalize.GitRepoFinalizer)
			gitrepo.Status.Commit = "dd45c7ad68e10307765104fea4a1f5997643020f"
			// use a different polling commit to force the creation of the gitjob
			gitrepo.Status.PollingCommit = "1883fd54bc5dfd225acf02aecbb6cb8020458e33"
			return nil
		},
	)

	// we need to return a NotFound error, so the code tries to create it.
	mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&batchv1.Job{}), gomock.Any()).
		Times(1).
		DoAndReturn(
			func(ctx context.Context, req types.NamespacedName, job *batchv1.Job, opts ...interface{}) error {
				return apierrors.NewNotFound(schema.GroupResource{}, "TEST ERROR")
			},
		).Times(2)

	mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Times(1).DoAndReturn(
		func(ctx context.Context, req types.NamespacedName, job *corev1.Secret, opts ...interface{}) error {
			return fmt.Errorf("SECRET ERROR")
		},
	)

	recorderMock := mocks.NewMockEventRecorder(mockCtrl)

	recorderMock.EXPECT().Event(
		&gitRepoMatcher{gitRepo},
		fleetevent.Warning,
		"FailedValidatingSecret",
		"failed to look up HelmSecretNameForPaths, error: SECRET ERROR",
	)

	statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
	mockClient.EXPECT().Status().Times(1).Return(statusClient)
	statusClient.EXPECT().Update(gomock.Any(), gomock.Any(), gomock.Any()).Do(
		func(ctx context.Context, repo *fleetv1.GitRepo, opts ...interface{}) {
			c, found := getCondition(repo, fleetv1.GitRepoAcceptedCondition)
			if !found {
				t.Errorf("expecting to find the %s condition and could not find it.", fleetv1.GitRepoAcceptedCondition)
			}
			if c.Message != "error validating external secrets: failed to look up HelmSecretNameForPaths, error: SECRET ERROR" {
				t.Errorf("expecting message [failed to look up HelmSecretNameForPaths, error: SECRET ERROR] in condition, got [%s]", c.Message)
			}
		},
	)

	r := GitJobReconciler{
		Client:   mockClient,
		Scheme:   scheme,
		Image:    "",
		Clock:    RealClock{},
		Recorder: recorderMock,
	}

	ctx := context.TODO()
	_, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
	if err == nil {
		t.Errorf("expecting an error, got nil")
	}
	if err.Error() != "error validating external secrets: failed to look up HelmSecretNameForPaths, error: SECRET ERROR" {
		t.Errorf("unexpected error %v", err)
	}
}

func TestNewJob(t *testing.T) {
	securityContext := &corev1.SecurityContext{
		AllowPrivilegeEscalation: &[]bool{false}[0],
		ReadOnlyRootFilesystem:   &[]bool{true}[0],
		Privileged:               &[]bool{false}[0],
		Capabilities:             &corev1.Capabilities{Drop: []corev1.Capability{"ALL"}},
		RunAsNonRoot:             &[]bool{true}[0],
		SeccompProfile: &corev1.SeccompProfile{
			Type: corev1.SeccompProfileTypeRuntimeDefault,
		},
	}
	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	scheme := runtime.NewScheme()
	utilruntime.Must(batchv1.AddToScheme(scheme))
	utilruntime.Must(fleetv1.AddToScheme(scheme))
	ctx := context.TODO()

	// define the default tolerations that all jobs have
	defaultTolerations := []corev1.Toleration{
		{
			Key:      "cattle.io/os",
			Operator: "Equal",
			Value:    "linux",
			Effect:   "NoSchedule",
		},
		{
			Key:      "node.cloudprovider.kubernetes.io/uninitialized",
			Operator: "Equal",
			Value:    "true",
			Effect:   "NoSchedule",
		},
	}
	tests := map[string]struct {
		gitrepo                *fleetv1.GitRepo
		strictHostKeyChecks    bool
		clientObjects          []runtime.Object
		deploymentTolerations  []corev1.Toleration
		expectedInitContainers []corev1.Container
		expectedContainers     []corev1.Container
		expectedVolumes        []corev1.Volume
		expectedErr            error
	}{
		"simple (no credentials, no ca, no skip tls)": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{Repo: "repo"},
			},
			expectedInitContainers: []corev1.Container{
				{
					Command: []string{
						"log.sh",
					},
					Args: []string{
						"fleet",
						"gitcloner",
						"repo",
						"/workspace",
						"--branch",
						"master",
					},
					Image: "test",
					Name:  "gitcloner-initializer",
					VolumeMounts: []corev1.VolumeMount{
						{
							Name:      gitClonerVolumeName,
							MountPath: "/workspace",
						},
						{
							Name:      emptyDirVolumeName,
							MountPath: "/tmp",
						},
					},
					SecurityContext: securityContext,
					Env: []corev1.EnvVar{
						{
							Name:  fleetapply.JSONOutputEnvVar,
							Value: "true",
						},
					},
				},
			},
			expectedVolumes: []corev1.Volume{
				{
					Name: gitClonerVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: emptyDirVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
			},
			clientObjects: []runtime.Object{
				&corev1.ConfigMap{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "known-hosts",
						Namespace: "cattle-fleet-system",
					},
					Data: map[string]string{
						// Prevent deployment error about config map not existing, but the data
						// does not matter in this test case.
						"known_hosts": "",
					},
				},
			},
		},
		"simple with custom branch": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					Repo:   "repo",
					Branch: "foo",
				},
			},
			expectedInitContainers: []corev1.Container{
				{
					Command: []string{
						"log.sh",
					},
					Args: []string{
						"fleet",
						"gitcloner",
						"repo",
						"/workspace",
						"--branch",
						"foo",
					},
					Image: "test",
					Name:  "gitcloner-initializer",
					VolumeMounts: []corev1.VolumeMount{
						{
							Name:      gitClonerVolumeName,
							MountPath: "/workspace",
						},
						{
							Name:      emptyDirVolumeName,
							MountPath: "/tmp",
						},
					},
					SecurityContext: securityContext,
					Env: []corev1.EnvVar{
						{
							Name:  fleetapply.JSONOutputEnvVar,
							Value: "true",
						},
					},
				},
			},
			expectedVolumes: []corev1.Volume{
				{
					Name: gitClonerVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: emptyDirVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
			},
			clientObjects: []runtime.Object{
				&corev1.ConfigMap{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "known-hosts",
						Namespace: "cattle-fleet-system",
					},
					Data: map[string]string{
						// Prevent deployment error about config map not existing, but the data
						// does not matter in this test case.
						"known_hosts": "",
					},
				},
			},
		},
		"simple with custom revision": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					Repo:     "repo",
					Revision: "foo",
				},
			},
			expectedInitContainers: []corev1.Container{
				{
					Command: []string{
						"log.sh",
					},
					Args: []string{
						"fleet",
						"gitcloner",
						"repo",
						"/workspace",
						"--revision",
						"foo",
					},
					Image: "test",
					Name:  "gitcloner-initializer",
					VolumeMounts: []corev1.VolumeMount{
						{
							Name:      gitClonerVolumeName,
							MountPath: "/workspace",
						},
						{
							Name:      emptyDirVolumeName,
							MountPath: "/tmp",
						},
					},
					SecurityContext: securityContext,
					Env: []corev1.EnvVar{
						{
							Name:  fleetapply.JSONOutputEnvVar,
							Value: "true",
						},
					},
				},
			},
			expectedVolumes: []corev1.Volume{
				{
					Name: gitClonerVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: emptyDirVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
			},
			clientObjects: []runtime.Object{
				&corev1.ConfigMap{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "known-hosts",
						Namespace: "cattle-fleet-system",
					},
					Data: map[string]string{
						// Prevent deployment error about config map not existing, but the data
						// does not matter in this test case.
						"known_hosts": "",
					},
				},
			},
		},
		"http credentials": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					Repo:             "repo",
					ClientSecretName: "secretName",
				},
			},
			expectedInitContainers: []corev1.Container{
				{
					Command: []string{
						"log.sh",
					},
					Args: []string{
						"fleet",
						"gitcloner",
						"repo",
						"/workspace",
						"--branch",
						"master",
						"--username",
						"user",
						"--password-file",
						"/gitjob/credentials/" + corev1.BasicAuthPasswordKey,
					},
					Image: "test",
					Name:  "gitcloner-initializer",
					VolumeMounts: []corev1.VolumeMount{
						{
							Name:      gitClonerVolumeName,
							MountPath: "/workspace",
						},
						{
							Name:      emptyDirVolumeName,
							MountPath: "/tmp",
						},
						{
							Name:      gitCredentialVolumeName,
							MountPath: "/gitjob/credentials",
						},
					},
					SecurityContext: securityContext,
					Env: []corev1.EnvVar{
						{
							Name:  fleetapply.JSONOutputEnvVar,
							Value: "true",
						},
					},
				},
			},
			expectedVolumes: []corev1.Volume{
				{
					Name: gitClonerVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: emptyDirVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: gitCredentialVolumeName,
					VolumeSource: corev1.VolumeSource{
						Secret: &corev1.SecretVolumeSource{
							SecretName: "secretName",
						},
					},
				},
			},
			clientObjects: []runtime.Object{
				&corev1.Secret{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "secretName",
						Namespace: "default",
					},
					Data: map[string][]byte{
						corev1.BasicAuthUsernameKey: []byte("user"),
						corev1.BasicAuthPasswordKey: []byte("pass"),
					},
					Type: corev1.SecretTypeBasicAuth,
				},
				&corev1.ConfigMap{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "known-hosts",
						Namespace: "cattle-fleet-system",
					},
					Data: map[string]string{
						// Prevent deployment error about config map not existing, but the data
						// does not matter in this test case.
						"known_hosts": "",
					},
				},
			},
		},
		"ssh credentials": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					Repo:             "repo",
					ClientSecretName: "secretName",
				},
			},
			expectedInitContainers: []corev1.Container{
				{
					Command: []string{
						"log.sh",
					},
					Args: []string{
						"fleet",
						"gitcloner",
						"repo",
						"/workspace",
						"--branch",
						"master",
						"--ssh-private-key-file",
						"/gitjob/ssh/" + corev1.SSHAuthPrivateKey,
					},
					// FLEET_KNOWN_HOSTS not expected here as strict host key checks are disabled
					Image: "test",
					Name:  "gitcloner-initializer",
					VolumeMounts: []corev1.VolumeMount{
						{
							Name:      gitClonerVolumeName,
							MountPath: "/workspace",
						},
						{
							Name:      emptyDirVolumeName,
							MountPath: "/tmp",
						},
						{
							Name:      gitCredentialVolumeName,
							MountPath: "/gitjob/ssh",
						},
					},
					SecurityContext: securityContext,
					Env: []corev1.EnvVar{
						{
							Name:  fleetapply.JSONOutputEnvVar,
							Value: "true",
						},
					},
				},
			},
			expectedVolumes: []corev1.Volume{
				{
					Name: gitClonerVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: emptyDirVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: gitCredentialVolumeName,
					VolumeSource: corev1.VolumeSource{
						Secret: &corev1.SecretVolumeSource{
							SecretName: "secretName",
						},
					},
				},
			},
			clientObjects: []runtime.Object{
				&corev1.Secret{
					ObjectMeta: metav1.ObjectMeta{Namespace: "default", Name: "secretName"},
					Data: map[string][]byte{
						corev1.SSHAuthPrivateKey: []byte("ssh key"),
						"known_hosts":            []byte("foo"),
					},
					Type: corev1.SecretTypeSSHAuth,
				},
			},
		},
		"no ssh credentials, known_hosts info found in gitcredential secret": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					Repo: "ssh://repo",
				},
			},
			strictHostKeyChecks: true,
			expectedInitContainers: []corev1.Container{
				{
					Command: []string{
						"log.sh",
					},
					Args: []string{
						"fleet",
						"gitcloner",
						"ssh://repo",
						"/workspace",
						"--branch",
						"master",
						"--ssh-private-key-file",
						"/gitjob/ssh/" + corev1.SSHAuthPrivateKey,
					},
					Env: []corev1.EnvVar{
						{
							Name:  fleetapply.JSONOutputEnvVar,
							Value: "true",
						},
						{
							Name:  "FLEET_KNOWN_HOSTS",
							Value: "some known hosts",
						},
					},
					Image: "test",
					Name:  "gitcloner-initializer",
					VolumeMounts: []corev1.VolumeMount{
						{
							Name:      gitClonerVolumeName,
							MountPath: "/workspace",
						},
						{
							Name:      emptyDirVolumeName,
							MountPath: "/tmp",
						},
						{
							Name:      gitCredentialVolumeName,
							MountPath: "/gitjob/ssh",
						},
					},
					SecurityContext: securityContext,
				},
			},
			expectedVolumes: []corev1.Volume{
				{
					Name: gitClonerVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: emptyDirVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: gitCredentialVolumeName,
					VolumeSource: corev1.VolumeSource{
						Secret: &corev1.SecretVolumeSource{
							SecretName: "gitcredential",
						},
					},
				},
			},
			clientObjects: []runtime.Object{
				&corev1.Secret{
					ObjectMeta: metav1.ObjectMeta{
						Namespace: "default",
						Name:      "gitcredential",
					},
					Data: map[string][]byte{
						corev1.SSHAuthPrivateKey: []byte("ssh key"),
						"known_hosts":            []byte("some known hosts"),
					},
					Type: corev1.SecretTypeSSHAuth,
				},
			},
		},
		"ssh credentials, incomplete secret, known_hosts found in config map": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					Repo:             "ssh://repo",
					ClientSecretName: "secretName",
				},
			},
			strictHostKeyChecks: true,
			expectedInitContainers: []corev1.Container{
				{
					Command: []string{
						"log.sh",
					},
					Args: []string{
						"fleet",
						"gitcloner",
						"ssh://repo",
						"/workspace",
						"--branch",
						"master",
						"--ssh-private-key-file",
						"/gitjob/ssh/" + corev1.SSHAuthPrivateKey,
					},
					Env: []corev1.EnvVar{
						{
							Name:  fleetapply.JSONOutputEnvVar,
							Value: "true",
						},
						{
							Name:  "FLEET_KNOWN_HOSTS",
							Value: "foo",
						},
					},
					Image: "test",
					Name:  "gitcloner-initializer",
					VolumeMounts: []corev1.VolumeMount{
						{
							Name:      gitClonerVolumeName,
							MountPath: "/workspace",
						},
						{
							Name:      emptyDirVolumeName,
							MountPath: "/tmp",
						},
						{
							Name:      gitCredentialVolumeName,
							MountPath: "/gitjob/ssh",
						},
					},
					SecurityContext: securityContext,
				},
			},
			expectedVolumes: []corev1.Volume{
				{
					Name: gitClonerVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: emptyDirVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: gitCredentialVolumeName,
					VolumeSource: corev1.VolumeSource{
						Secret: &corev1.SecretVolumeSource{
							SecretName: "secretName",
						},
					},
				},
			},
			clientObjects: []runtime.Object{
				&corev1.ConfigMap{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "known-hosts",
						Namespace: "cattle-fleet-system",
					},
					Data: map[string]string{
						"known_hosts": "foo",
					},
				},
				&corev1.Secret{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "secretName",
						Namespace: "default",
					},
					Data: map[string][]byte{
						corev1.SSHAuthPrivateKey: []byte("ssh key"),
					},
					Type: corev1.SecretTypeSSHAuth,
				},
			},
		},
		"ssh credentials, no secret, known_hosts found in config map": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					Repo: "ssh://repo",
				},
			},
			strictHostKeyChecks: true,
			expectedInitContainers: []corev1.Container{
				{
					Command: []string{
						"log.sh",
					},
					Args: []string{
						"fleet",
						"gitcloner",
						"ssh://repo",
						"/workspace",
						"--branch",
						"master",
					},
					Env: []corev1.EnvVar{
						{
							Name:  fleetapply.JSONOutputEnvVar,
							Value: "true",
						},
						{
							Name:  "FLEET_KNOWN_HOSTS",
							Value: "foo",
						},
					},
					Image: "test",
					Name:  "gitcloner-initializer",
					VolumeMounts: []corev1.VolumeMount{
						{
							Name:      gitClonerVolumeName,
							MountPath: "/workspace",
						},
						{
							Name:      emptyDirVolumeName,
							MountPath: "/tmp",
						},
					},
					SecurityContext: securityContext,
				},
			},
			expectedVolumes: []corev1.Volume{
				{
					Name: gitClonerVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: emptyDirVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
			},
			clientObjects: []runtime.Object{
				&corev1.ConfigMap{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "known-hosts",
						Namespace: "cattle-fleet-system",
					},
					Data: map[string]string{
						"known_hosts": "foo",
					},
				},
			},
		},
		"github app credentials": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					Repo:             "repo",
					ClientSecretName: "secretName",
				},
			},
			expectedInitContainers: []corev1.Container{
				{
					Command: []string{
						"log.sh",
					},
					Args: []string{
						"fleet",
						"gitcloner",
						"repo",
						"/workspace",
						"--branch",
						"master",
						"--github-app-id",
						"123",
						"--github-app-installation-id",
						"456",
						"--github-app-key-file",
						"/gitjob/githubapp/github_app_private_key",
					},
					Image: "test",
					Name:  "gitcloner-initializer",
					VolumeMounts: []corev1.VolumeMount{
						{
							Name:      gitClonerVolumeName,
							MountPath: "/workspace",
						},
						{
							Name:      emptyDirVolumeName,
							MountPath: "/tmp",
						},
						{
							Name:      gitCredentialVolumeName,
							MountPath: "/gitjob/githubapp",
						},
					},
					SecurityContext: securityContext,
					Env: []corev1.EnvVar{
						{
							Name:  fleetapply.JSONOutputEnvVar,
							Value: "true",
						},
					},
				},
			},
			expectedVolumes: []corev1.Volume{
				{
					Name: gitClonerVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: emptyDirVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: gitCredentialVolumeName,
					VolumeSource: corev1.VolumeSource{
						Secret: &corev1.SecretVolumeSource{
							SecretName: "secretName",
						},
					},
				},
			},
			clientObjects: []runtime.Object{
				&corev1.Secret{
					ObjectMeta: metav1.ObjectMeta{Namespace: "default", Name: "secretName"},
					Data: map[string][]byte{
						"github_app_id":              []byte("123"),
						"github_app_installation_id": []byte("456"),
						"github_app_private_key":     []byte("private key"),
					},
					Type: corev1.SecretTypeOpaque,
				},
				&corev1.ConfigMap{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "known-hosts",
						Namespace: "cattle-fleet-system",
					},
					Data: map[string]string{
						// Prevent deployment error about config map not existing, but the data
						// does not matter in this test case.
						"known_hosts": "",
					},
				},
			},
		},
		"custom CA": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					CABundle: []byte("ca"),
					Repo:     "repo",
				},
			},
			expectedInitContainers: []corev1.Container{
				{
					Command: []string{
						"log.sh",
					},
					Args: []string{
						"fleet",
						"gitcloner",
						"repo",
						"/workspace",
						"--branch",
						"master",
						"--ca-bundle-file",
						"/gitjob/cabundle/" + bundleCAFile,
					},
					Image: "test",
					Name:  "gitcloner-initializer",
					VolumeMounts: []corev1.VolumeMount{
						{
							Name:      gitClonerVolumeName,
							MountPath: "/workspace",
						},
						{
							Name:      emptyDirVolumeName,
							MountPath: "/tmp",
						},
						{
							Name:      bundleCAVolumeName,
							MountPath: "/gitjob/cabundle",
						},
					},
					SecurityContext: securityContext,
					Env: []corev1.EnvVar{
						{
							Name:  fleetapply.JSONOutputEnvVar,
							Value: "true",
						},
					},
				},
			},
			expectedVolumes: []corev1.Volume{
				{
					Name: gitClonerVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: emptyDirVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: bundleCAVolumeName,
					VolumeSource: corev1.VolumeSource{
						Secret: &corev1.SecretVolumeSource{
							SecretName: "gitrepo-cabundle",
						},
					},
				},
			},
			clientObjects: []runtime.Object{
				&corev1.Secret{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "gitrepo-cabundle",
						Namespace: "default",
					},
					Data: map[string][]byte{
						"cacerts": []byte("foo"),
					},
					Type: corev1.SecretTypeSSHAuth,
				},
				&corev1.ConfigMap{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "known-hosts",
						Namespace: "cattle-fleet-system",
					},
					Data: map[string]string{
						// Prevent deployment error about config map not existing, but the data
						// does not matter in this test case.
						"known_hosts": "",
					},
				},
			},
		},
		"no custom CA but Rancher CA secret exists": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					Repo: "repo",
				},
			},
			expectedInitContainers: []corev1.Container{
				{
					Command: []string{
						"log.sh",
					},
					Args: []string{
						"fleet",
						"gitcloner",
						"repo",
						"/workspace",
						"--branch",
						"master",
						"--ca-bundle-file",
						"/gitjob/cabundle/" + bundleCAFile,
					},
					Image: "test",
					Name:  "gitcloner-initializer",
					VolumeMounts: []corev1.VolumeMount{
						{
							Name:      gitClonerVolumeName,
							MountPath: "/workspace",
						},
						{
							Name:      emptyDirVolumeName,
							MountPath: "/tmp",
						},
						{
							Name:      bundleCAVolumeName,
							MountPath: "/gitjob/cabundle",
						},
					},
					SecurityContext: securityContext,
					Env: []corev1.EnvVar{
						{
							Name:  fleetapply.JSONOutputEnvVar,
							Value: "true",
						},
					},
				},
			},
			expectedVolumes: []corev1.Volume{
				{
					Name: gitClonerVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: emptyDirVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: bundleCAVolumeName,
					VolumeSource: corev1.VolumeSource{
						Secret: &corev1.SecretVolumeSource{
							SecretName: "gitrepo-cabundle",
						},
					},
				},
				{
					Name: "rancher-helm-secret-cert",
					VolumeSource: corev1.VolumeSource{
						Secret: &corev1.SecretVolumeSource{
							SecretName: "gitrepo-rancher-cabundle",
							Items: []corev1.KeyToPath{
								{
									Key:  "cacerts",
									Path: "cacert.crt",
								},
							},
						},
					},
				},
			},
			expectedContainers: []corev1.Container{
				{
					Args: []string{
						"--cacerts-file",
						"/etc/rancher/certs/cacerts",
					},
					Name: "fleet",
					VolumeMounts: []corev1.VolumeMount{
						{
							Name:      "rancher-helm-secret-cert",
							MountPath: "/etc/ssl/certs",
						},
					},
				},
			},
			clientObjects: []runtime.Object{
				&corev1.Secret{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "tls-ca-additional",
						Namespace: "cattle-system",
					},
					Data: map[string][]byte{
						"ca-additional.pem": []byte("foo"),
					},
				},
				&corev1.Secret{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "gitrepo-cabundle",
						Namespace: "default",
					},
					Data: map[string][]byte{
						"additional-ca.crt": []byte("foo"),
					},
				},
				&corev1.ConfigMap{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "known-hosts",
						Namespace: "cattle-fleet-system",
					},
					Data: map[string]string{
						// Prevent deployment error about config map not existing, but the data
						// does not matter in this test case.
						"known_hosts": "",
					},
				},
			},
		},
		"skip tls": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					InsecureSkipTLSverify: true,
					Repo:                  "repo",
				},
			},
			expectedInitContainers: []corev1.Container{
				{
					Command: []string{
						"log.sh",
					},
					Args: []string{
						"fleet",
						"gitcloner",
						"repo",
						"/workspace",
						"--branch",
						"master",
						"--insecure-skip-tls",
					},
					Image: "test",
					Name:  "gitcloner-initializer",
					VolumeMounts: []corev1.VolumeMount{
						{
							Name:      gitClonerVolumeName,
							MountPath: "/workspace",
						},
						{
							Name:      emptyDirVolumeName,
							MountPath: "/tmp",
						},
					},
					SecurityContext: securityContext,
					Env: []corev1.EnvVar{
						{
							Name:  fleetapply.JSONOutputEnvVar,
							Value: "true",
						},
					},
				},
			},
			expectedVolumes: []corev1.Volume{
				{
					Name: gitClonerVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: emptyDirVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
			},
			clientObjects: []runtime.Object{
				&corev1.ConfigMap{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "known-hosts",
						Namespace: "cattle-fleet-system",
					},
					Data: map[string]string{
						// Prevent deployment error about config map not existing, but the data
						// does not matter in this test case.
						"known_hosts": "",
					},
				},
			},
		},
		"simple with tolerations": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{Repo: "repo"},
			},
			expectedInitContainers: []corev1.Container{
				{
					Command: []string{
						"log.sh",
					},
					Args: []string{
						"fleet",
						"gitcloner",
						"repo",
						"/workspace",
						"--branch",
						"master",
					},
					Image: "test",
					Name:  "gitcloner-initializer",
					VolumeMounts: []corev1.VolumeMount{
						{
							Name:      gitClonerVolumeName,
							MountPath: "/workspace",
						},
						{
							Name:      emptyDirVolumeName,
							MountPath: "/tmp",
						},
					},
					SecurityContext: securityContext,
					Env: []corev1.EnvVar{
						{
							Name:  fleetapply.JSONOutputEnvVar,
							Value: "true",
						},
					},
				},
			},
			expectedVolumes: []corev1.Volume{
				{
					Name: gitClonerVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
				{
					Name: emptyDirVolumeName,
					VolumeSource: corev1.VolumeSource{
						EmptyDir: &corev1.EmptyDirVolumeSource{},
					},
				},
			},
			deploymentTolerations: []corev1.Toleration{
				{
					Effect:   "NoSchedule",
					Key:      "key1",
					Value:    "value1",
					Operator: "Equals",
				},
				{
					Effect:   "NoExecute",
					Key:      "key2",
					Value:    "value2",
					Operator: "Exists",
				},
			},
			clientObjects: []runtime.Object{
				&corev1.ConfigMap{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "known-hosts",
						Namespace: "cattle-fleet-system",
					},
					Data: map[string]string{
						// Prevent deployment error about config map not existing, but the data
						// does not matter in this test case.
						"known_hosts": "",
					},
				},
			},
		},
	}

	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			r := GitJobReconciler{
				Client:          getFakeClient(test.deploymentTolerations, test.clientObjects...),
				Scheme:          scheme,
				Image:           "test",
				Clock:           RealClock{},
				SystemNamespace: config.DefaultNamespace,
				KnownHosts:      ssh.KnownHosts{EnforceHostKeyChecks: test.strictHostKeyChecks},
			}

			job, err := r.newGitJob(ctx, test.gitrepo)
			if err != nil {
				t.Fatalf("unexpected error: %v", err)
			}

			if !cmp.Equal(job.Spec.Template.Spec.InitContainers, test.expectedInitContainers) {
				t.Fatalf("expected initContainers:\n\t%v,\n got:\n\t%v", test.expectedInitContainers, job.Spec.Template.Spec.InitContainers)
			}

			for _, eCont := range test.expectedContainers {
				found := false
				for _, cont := range job.Spec.Template.Spec.Containers {
					if cont.Name == eCont.Name {
						found = true

						for _, eArg := range eCont.Args {
							if argFound := slices.Contains(cont.Args, eArg); !argFound {
								t.Fatalf("expected arg %q not found in container %s with args %#v", eArg, eCont.Name, cont.Args)
							}
						}

						for _, eVM := range eCont.VolumeMounts {
							vmFound := false
							for _, vm := range cont.VolumeMounts {
								if vm.Name != eVM.Name {
									continue
								}
								vmFound = true
								if vm != eVM {
									t.Fatalf("expected volume mount %v in container %s, got %v", eVM, eCont.Name, vm)
								}
							}
							if !vmFound {
								t.Fatalf("expected volume mount %v not found in container %s", eVM, eCont.Name)
							}
						}
					}
				}
				if !found {
					t.Fatalf("expected container %s not found", eCont.Name)
				}
			}

			if len(test.expectedContainers) > 0 && len(test.expectedContainers) != len(job.Spec.Template.Spec.Containers) {
				t.Fatalf(
					"expected %d Containers:\n\t%v\ngot %d:\n\t%v",
					len(test.expectedContainers),
					test.expectedContainers,
					len(job.Spec.Template.Spec.Containers),
					job.Spec.Template.Spec.Containers,
				)
			}

			for _, expCont := range test.expectedContainers {
				found := false
				for _, cont := range job.Spec.Template.Spec.Containers {
					if cont.Name == expCont.Name {
						found = true

						for _, expMount := range expCont.VolumeMounts {
							foundMount := false
							for _, mount := range cont.VolumeMounts {
								if mount == expMount {
									foundMount = true
								}
							}

							if !foundMount {
								t.Fatalf("expected volume mount %v for container %v not found in\n\t%v", expMount, expCont, cont.VolumeMounts)
							}
						}
					}
				}

				if !found {
					t.Fatalf("expected container %v not found in\n\t%v", expCont, job.Spec.Template.Spec.Containers)
				}
			}

			for _, evol := range test.expectedVolumes {
				found := false
				for _, tvol := range job.Spec.Template.Spec.Volumes {
					if cmp.Equal(evol, tvol) {
						found = true
						break
					}
				}
				if !found {
					t.Fatalf("volume %v not found in \n\t%v", evol, job.Spec.Template.Spec.Volumes)
				}
			}

			// tolerations check
			// tolerations will be the default ones plus the deployment ones
			expectedTolerations := append([]corev1.Toleration{}, defaultTolerations...)
			expectedTolerations = append(expectedTolerations, test.deploymentTolerations...)
			if !cmp.Equal(expectedTolerations, job.Spec.Template.Spec.Tolerations) {
				t.Fatalf("job tolerations differ. Expecting: %v and found: %v", test.deploymentTolerations, job.Spec.Template.Spec.Tolerations)
			}
		})
	}
}

func TestGenerateJob_EnvVars(t *testing.T) {
	ctx := context.TODO()

	tests := map[string]struct {
		gitrepo                      *fleetv1.GitRepo
		strictSSHHostKeyChecks       bool
		osEnv                        map[string]string
		expectedContainerEnvVars     []corev1.EnvVar
		expectedInitContainerEnvVars []corev1.EnvVar
	}{
		"Helm secret name": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					HelmSecretName: "foo",
					Repo:           "https://github.com/rancher/fleet-examples",
				},
				Status: fleetv1.GitRepoStatus{
					Commit: "commit",
				},
			},
			expectedContainerEnvVars: []corev1.EnvVar{
				{
					Name:  "HOME",
					Value: "/fleet-home",
				},
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
				{
					Name:  fleetapply.JobNameEnvVar,
					Value: "gitrepo-b7eaf",
				},
				{
					Name:  "FLEET_APPLY_CONFLICT_RETRIES",
					Value: "1",
				},
				{
					Name:  "FLEET_BUNDLE_CREATION_MAX_CONCURRENCY",
					Value: "4",
				},
				{
					Name:  "GIT_SSH_COMMAND",
					Value: "ssh -o stricthostkeychecking=no",
				},
				{
					Name: "HELM_USERNAME",
					ValueFrom: &corev1.EnvVarSource{
						SecretKeyRef: &corev1.SecretKeySelector{
							Optional: &[]bool{true}[0],
							Key:      "username",
							LocalObjectReference: corev1.LocalObjectReference{
								Name: "foo",
							},
						},
					},
				},
				{
					Name:  "COMMIT",
					Value: "commit",
				},
			},
			expectedInitContainerEnvVars: []corev1.EnvVar{
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
			},
		},
		"Helm secret name with strict host key checks": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					HelmSecretName: "foo",
					Repo:           "https://github.com/rancher/fleet-examples",
				},
				Status: fleetv1.GitRepoStatus{
					Commit: "commit",
				},
			},
			strictSSHHostKeyChecks: true,
			expectedInitContainerEnvVars: []corev1.EnvVar{
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
				{
					Name: "FLEET_KNOWN_HOSTS",
				},
			},
			expectedContainerEnvVars: []corev1.EnvVar{
				{
					Name:  "HOME",
					Value: "/fleet-home",
				},
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
				{
					Name:  fleetapply.JobNameEnvVar,
					Value: "gitrepo-b7eaf",
				},
				{
					Name:  "FLEET_APPLY_CONFLICT_RETRIES",
					Value: "1",
				},
				{
					Name:  "FLEET_BUNDLE_CREATION_MAX_CONCURRENCY",
					Value: "4",
				},
				{
					Name:  "GIT_SSH_COMMAND",
					Value: "ssh -o stricthostkeychecking=yes",
				},
				{
					Name: "HELM_USERNAME",
					ValueFrom: &corev1.EnvVarSource{
						SecretKeyRef: &corev1.SecretKeySelector{
							Optional: &[]bool{true}[0],
							Key:      "username",
							LocalObjectReference: corev1.LocalObjectReference{
								Name: "foo",
							},
						},
					},
				},
				{
					Name:  "COMMIT",
					Value: "commit",
				},
			},
		},
		"Helm secret name for paths": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					HelmSecretNameForPaths: "foo",
					Repo:                   "https://github.com/rancher/fleet-examples",
				},
				Status: fleetv1.GitRepoStatus{
					Commit: "commit",
				},
			},
			expectedContainerEnvVars: []corev1.EnvVar{
				{
					Name:  "HOME",
					Value: "/fleet-home",
				},
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
				{
					Name:  fleetapply.JobNameEnvVar,
					Value: "gitrepo-b7eaf",
				},
				{
					Name:  "FLEET_APPLY_CONFLICT_RETRIES",
					Value: "1",
				},
				{
					Name:  "FLEET_BUNDLE_CREATION_MAX_CONCURRENCY",
					Value: "4",
				},
				{
					Name:  "GIT_SSH_COMMAND",
					Value: "ssh -o stricthostkeychecking=no",
				},
				{
					Name:  "COMMIT",
					Value: "commit",
				},
			},
			expectedInitContainerEnvVars: []corev1.EnvVar{
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
			},
		},
		"Helm secret name for paths with strict host key checks": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					HelmSecretNameForPaths: "foo",
					Repo:                   "https://github.com/rancher/fleet-examples",
				},
				Status: fleetv1.GitRepoStatus{
					Commit: "commit",
				},
			},
			strictSSHHostKeyChecks: true,
			expectedInitContainerEnvVars: []corev1.EnvVar{
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
				{
					Name: "FLEET_KNOWN_HOSTS",
				},
			},
			expectedContainerEnvVars: []corev1.EnvVar{
				{
					Name:  "HOME",
					Value: "/fleet-home",
				},
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
				{
					Name:  fleetapply.JobNameEnvVar,
					Value: "gitrepo-b7eaf",
				},
				{
					Name:  "FLEET_APPLY_CONFLICT_RETRIES",
					Value: "1",
				},
				{
					Name:  "FLEET_BUNDLE_CREATION_MAX_CONCURRENCY",
					Value: "4",
				},
				{
					Name:  "GIT_SSH_COMMAND",
					Value: "ssh -o stricthostkeychecking=yes",
				},
				{
					Name:  "COMMIT",
					Value: "commit",
				},
			},
		},
		"proxy": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					Repo: "https://github.com/rancher/fleet-examples",
				},
				Status: fleetv1.GitRepoStatus{
					Commit: "commit",
				},
			},
			expectedContainerEnvVars: []corev1.EnvVar{
				{
					Name:  "HOME",
					Value: "/fleet-home",
				},
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
				{
					Name:  fleetapply.JobNameEnvVar,
					Value: "gitrepo-b7eaf",
				},
				{
					Name:  "FLEET_APPLY_CONFLICT_RETRIES",
					Value: "1",
				},
				{
					Name:  "FLEET_BUNDLE_CREATION_MAX_CONCURRENCY",
					Value: "4",
				},
				{
					Name:  "COMMIT",
					Value: "commit",
				},
				{
					Name:  "HTTP_PROXY",
					Value: "httpProxy",
				},
				{
					Name:  "HTTPS_PROXY",
					Value: "httpsProxy",
				},
			},
			expectedInitContainerEnvVars: []corev1.EnvVar{
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
				{
					Name:  "HTTP_PROXY",
					Value: "httpProxy",
				},
				{
					Name:  "HTTPS_PROXY",
					Value: "httpsProxy",
				},
			},
			osEnv: map[string]string{"HTTP_PROXY": "httpProxy", "HTTPS_PROXY": "httpsProxy"},
		},
		"retries_valid": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					Repo: "https://github.com/rancher/fleet-examples",
				},
				Status: fleetv1.GitRepoStatus{
					Commit: "commit",
				},
			},
			expectedContainerEnvVars: []corev1.EnvVar{
				{
					Name:  "HOME",
					Value: "/fleet-home",
				},
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
				{
					Name:  fleetapply.JobNameEnvVar,
					Value: "gitrepo-b7eaf",
				},
				{
					Name:  "FLEET_APPLY_CONFLICT_RETRIES",
					Value: "3",
				},
				{
					Name:  "FLEET_BUNDLE_CREATION_MAX_CONCURRENCY",
					Value: "4",
				},
				{
					Name:  "COMMIT",
					Value: "commit",
				},
			},
			osEnv: map[string]string{"FLEET_APPLY_CONFLICT_RETRIES": "3"},
			expectedInitContainerEnvVars: []corev1.EnvVar{
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
			},
		},
		"retries_not_valid": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					Repo: "https://github.com/rancher/fleet-examples",
				},
				Status: fleetv1.GitRepoStatus{
					Commit: "commit",
				},
			},
			expectedContainerEnvVars: []corev1.EnvVar{
				{
					Name:  "HOME",
					Value: "/fleet-home",
				},
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
				{
					Name:  fleetapply.JobNameEnvVar,
					Value: "gitrepo-b7eaf",
				},
				{
					Name:  "FLEET_APPLY_CONFLICT_RETRIES",
					Value: "1",
				},
				{
					Name:  "FLEET_BUNDLE_CREATION_MAX_CONCURRENCY",
					Value: "4",
				},
				{
					Name:  "COMMIT",
					Value: "commit",
				},
			},
			osEnv: map[string]string{"FLEET_APPLY_CONFLICT_RETRIES": "this_is_not_an_int"},
			expectedInitContainerEnvVars: []corev1.EnvVar{
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
			},
		},
		"bundle_creation_max_concurrency_valid": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					Repo: "https://github.com/rancher/fleet-examples",
				},
				Status: fleetv1.GitRepoStatus{
					Commit: "commit",
				},
			},
			expectedContainerEnvVars: []corev1.EnvVar{
				{
					Name:  "HOME",
					Value: "/fleet-home",
				},
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
				{
					Name:  fleetapply.JobNameEnvVar,
					Value: "gitrepo-b7eaf",
				},
				{
					Name:  "FLEET_APPLY_CONFLICT_RETRIES",
					Value: "1",
				},
				{
					Name:  "FLEET_BUNDLE_CREATION_MAX_CONCURRENCY",
					Value: "8",
				},
				{
					Name:  "COMMIT",
					Value: "commit",
				},
			},
			osEnv: map[string]string{"FLEET_BUNDLE_CREATION_MAX_CONCURRENCY": "8"},
			expectedInitContainerEnvVars: []corev1.EnvVar{
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
			},
		},
		"bundle_creation_max_concurrency_invalid": {
			gitrepo: &fleetv1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "gitrepo",
					Namespace: "default",
				},
				Spec: fleetv1.GitRepoSpec{
					Repo: "https://github.com/rancher/fleet-examples",
				},
				Status: fleetv1.GitRepoStatus{
					Commit: "commit",
				},
			},
			expectedContainerEnvVars: []corev1.EnvVar{
				{
					Name:  "HOME",
					Value: "/fleet-home",
				},
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
				{
					Name:  fleetapply.JobNameEnvVar,
					Value: "gitrepo-b7eaf",
				},
				{
					Name:  "FLEET_APPLY_CONFLICT_RETRIES",
					Value: "1",
				},
				{
					Name:  "FLEET_BUNDLE_CREATION_MAX_CONCURRENCY",
					Value: "4",
				},
				{
					Name:  "COMMIT",
					Value: "commit",
				},
			},
			osEnv: map[string]string{"FLEET_BUNDLE_CREATION_MAX_CONCURRENCY": "this_is_not_an_int"},
			expectedInitContainerEnvVars: []corev1.EnvVar{
				{
					Name:  fleetapply.JSONOutputEnvVar,
					Value: "true",
				},
			},
		},
	}

	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			for k, v := range test.osEnv {
				err := os.Setenv(k, v)
				if err != nil {
					t.Errorf("unexpected error: %v", err)
				}
			}

			r := GitJobReconciler{
				Client:          getFakeClient([]corev1.Toleration{}),
				Image:           "test",
				Clock:           RealClock{},
				SystemNamespace: config.DefaultNamespace,
				KnownHosts: mockKnownHostsGetter{
					strict: test.strictSSHHostKeyChecks,
				},
			}
			job, err := r.newGitJob(ctx, test.gitrepo)
			if err != nil {
				t.Errorf("unexpected error: %v", err)
			}
			if !cmp.Equal(job.Spec.Template.Spec.Containers[0].Env, test.expectedContainerEnvVars) {
				t.Errorf("unexpected envVars. expected %v, but got %v", test.expectedContainerEnvVars, job.Spec.Template.Spec.Containers[0].Env)
			}
			if !cmp.Equal(job.Spec.Template.Spec.InitContainers[0].Env, test.expectedInitContainerEnvVars) {
				t.Errorf("unexpected envVars. expected %v, but got %v", test.expectedInitContainerEnvVars, job.Spec.Template.Spec.InitContainers[0].Env)
			}

			for k := range test.osEnv {
				err := os.Unsetenv(k)
				if err != nil {
					t.Errorf("unexpected error: %v", err)
				}
			}
		})
	}
}

type mockKnownHostsGetter struct {
	data   string
	strict bool
	err    error
}

func (m mockKnownHostsGetter) Get(ctx context.Context, c client.Client, ns string, secretName string) (string, error) {
	return m.data, m.err
}

func (m mockKnownHostsGetter) IsStrict() bool {
	return m.strict
}

func TestGitClonerSSH(t *testing.T) {
	tests := map[string]struct {
		gitrepo               *fleetv1.GitRepo
		knownHostsData        string
		knownHostsErr         error
		expectedContainerArgs []string
		expectedErr           error
	}{
		"known hosts check would fail, non-SSH repo, no found known_hosts data": {
			gitrepo: &fleetv1.GitRepo{
				Spec: fleetv1.GitRepoSpec{
					Repo: "foo",
				},
			},
			// The error does not matter, as known hosts checks should not be called for non-SSH repos
			knownHostsErr: errors.New("something happened"),
			expectedContainerArgs: []string{
				"fleet",
				"gitcloner",
				"foo",
				"/workspace",
				"--branch",
				"master",
			},
			expectedErr: nil,
		},
		"known hosts check passes, SSH repo, no found known_hosts data": {
			gitrepo: &fleetv1.GitRepo{
				Spec: fleetv1.GitRepoSpec{
					Repo: "ssh://foo",
				},
			},
			expectedContainerArgs: []string{
				"fleet",
				"gitcloner",
				"ssh://foo",
				"/workspace",
				"--branch",
				"master",
			},
			expectedErr: nil,
		},
		"known hosts check would pass, non-SSH repo, no found known_hosts data": {
			gitrepo: &fleetv1.GitRepo{
				Spec: fleetv1.GitRepoSpec{
					Repo: "foo",
				},
			},
			expectedContainerArgs: []string{
				"fleet",
				"gitcloner",
				"foo",
				"/workspace",
				"--branch",
				"master",
			},
			expectedErr: nil,
		},
		"SSH repo, found known_hosts data": {
			gitrepo: &fleetv1.GitRepo{
				Spec: fleetv1.GitRepoSpec{
					Repo: "ssh://foo",
				},
			},
			knownHostsData: "foo",
			expectedContainerArgs: []string{
				"fleet",
				"gitcloner",
				"ssh://foo",
				"/workspace",
				"--branch",
				"master",
			},
		},
	}

	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			mockCtrl := gomock.NewController(t)
			defer mockCtrl.Finish()

			r := GitJobReconciler{
				Client: fake.NewFakeClient(),
				Image:  "test",
				KnownHosts: mockKnownHostsGetter{
					data: test.knownHostsData,
					err:  test.knownHostsErr,
				},
			}

			cont, err := r.newGitCloner(context.TODO(), test.gitrepo, test.knownHostsData)
			if (err != nil && test.expectedErr == nil) || (err == nil && test.expectedErr != nil) {
				t.Errorf("expecting error %v, got %v", test.expectedErr, err)
			}
			if err != nil && test.expectedErr != nil && err.Error() != test.expectedErr.Error() {
				t.Errorf("expecting error %v, got %v", test.expectedErr, err)
			}
			if len(cont.Args) != len(test.expectedContainerArgs) {
				t.Fatalf("expecting args %v, got %v", test.expectedContainerArgs, cont.Args)
			}

			for idx := range test.expectedContainerArgs {
				if cont.Args[idx] != test.expectedContainerArgs[idx] {
					t.Errorf("expecting arg %q at index %d, got %q", test.expectedContainerArgs[idx], idx, cont.Args[idx])

				}
			}
		})
	}
}

func TestDrivenScanSeparator(t *testing.T) {
	tests := map[string]struct {
		bundles        []fleetv1.BundlePath
		expectError    bool
		expectedResult string
	}{
		"Bundle definitions have no separator character": {
			bundles: []fleetv1.BundlePath{
				{
					Base:    "test/one/two",
					Options: "options.yaml",
				},
				{
					Base:    "test/",
					Options: "options2.yaml",
				},
			},
			expectError:    false,
			expectedResult: ":",
		},
		"Bundle definitions have : separator character": {
			bundles: []fleetv1.BundlePath{
				{
					Base:    "test/one:two",
					Options: "options.yaml",
				},
				{
					Base:    "test/",
					Options: "options2.yaml",
				},
			},
			expectError:    false,
			expectedResult: ",",
		},
		"Bundle definitions have : and , separator characters": {
			bundles: []fleetv1.BundlePath{
				{
					Base:    "test/one:two",
					Options: "options.yaml",
				},
				{
					Base:    "test,one",
					Options: "options2.yaml",
				},
			},
			expectError:    false,
			expectedResult: "|",
		},
		"Bundle definitions have : , and | separator characters": {
			bundles: []fleetv1.BundlePath{
				{
					Base:    "test/one:two",
					Options: "options.yaml",
				},
				{
					Base:    "test,one",
					Options: "options2|.yaml",
				},
			},
			expectError:    false,
			expectedResult: "?",
		},
		"Bundle definitions have : ,  | and ? separator characters": {
			bundles: []fleetv1.BundlePath{
				{
					Base:    "test?one:two",
					Options: "options.yaml",
				},
				{
					Base:    "test,one",
					Options: "options2|.yaml",
				},
			},
			expectError:    false,
			expectedResult: "<",
		},
		"Bundle definitions have : ,  |  ? and < separator characters": {
			bundles: []fleetv1.BundlePath{
				{
					Base:    "test?one:two",
					Options: "options.yaml",
				},
				{
					Base:    "test,one<",
					Options: "options2|.yaml",
				},
			},
			expectError:    false,
			expectedResult: ">",
		},
		"Bundle definitions have all separator characters": {
			bundles: []fleetv1.BundlePath{
				{
					Base:    "test?one:two",
					Options: "options.yaml",
				},
				{
					Base:    "test,one<>",
					Options: "options2|.yaml",
				},
			},
			expectError:    true,
			expectedResult: "",
		},
	}
	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			gitrepo := &fleetv1.GitRepo{
				Spec: fleetv1.GitRepoSpec{
					Bundles: test.bundles,
				},
			}
			separator, err := getDrivenScanSeparator(*gitrepo)
			if !test.expectError {
				if err != nil {
					t.Errorf("unexpected error: %v", err)
				}
				if test.expectedResult != separator {
					t.Errorf("expecting separator to be: %q, got: %q", test.expectedResult, separator)
				}
			} else {
				expectedErrorMessage := fmt.Sprintf("bundle base and/or options paths contain all possible characters from %q, please update those paths to remedy this", bundleOptionsSeparatorChars)
				if err == nil {
					t.Errorf("expecting error, got none")
				}
				if err.Error() != expectedErrorMessage {
					t.Errorf("expecting error %q, got %q", expectedErrorMessage, err.Error())
				}
			}
		})
	}
}

func TestFilterFleetApplyJobOutput(t *testing.T) {
	tests := map[string]struct {
		input          string
		expectedOutput string
	}{
		"Filter a few lines and return a couple from Fleet": {
			input: `this line should be ignored
		this line should be ignored, too
		{"level":"fatal","msg":"This line is not from fleet cli","time":"2025-04-15T14:53:15+02:00"}
		{"fleetErrorMessage":"fleet line 1","level":"fatal","msg":"Fleet cli failed","time":"2025-04-15T14:53:15+02:00"}
		ignore this line as well
		{"fleetErrorMessage":"fleet line 2","level":"fatal","msg":"Fleet cli failed","time":"2025-04-15T14:53:15+02:00"}`,
			expectedOutput: "fleet line 1\nfleet line 2",
		},
		"There are no lines from fleet apply": {
			input: `this line should be ignored
		this line should be ignored, too
		{"level":"fatal","msg":"This line is not from fleet cli","time":"2025-04-15T14:53:15+02:00"}
		ignore this line as well`,
			expectedOutput: "Unknown error",
		},
		"The output is from fleet apply, but it's not in json format": {
			input:          "FATA[0000] no resource found at the following paths to deploy: [tt]",
			expectedOutput: "Unknown error",
		},
		"Valid message with some extra text before and after": {
			input: `this line should be ignored
		this line should be ignored, too
		This line is OK{"fleetErrorMessage":"fleet line error 1","level":"fatal","msg":"Fleet cli failed","time":"2025-04-15T14:53:15+02:00"}This part should be ignored
		ignore this line as well`,
			expectedOutput: "fleet line error 1",
		},
		"Valid message with some extra text before": {
			input: `this line should be ignored
		this line should be ignored, too
		This line is OK{"fleetErrorMessage":"fleet line error 1","level":"fatal","msg":"Fleet cli failed","time":"2025-04-15T14:53:15+02:00"}
		ignore this line as well`,
			expectedOutput: "fleet line error 1",
		},
		"Valid message with some extra text after": {
			input: `this line should be ignored
		this line should be ignored, too
		This line is OK{"fleetErrorMessage":"fleet line error 1","level":"fatal","msg":"Fleet cli failed","time":"2025-04-15T14:53:15+02:00"}
		ignore this line as well`,
			expectedOutput: "fleet line error 1",
		},
		"Not valid json message": {
			input: `this line should be ignored
		this line should be ignored, too
		This lin}e is OK "{fleetErrorMessage":"fleet line error 1","level":"fatal","msg":"Fleet cli failed","time":"2025-04-15T14:53:15+02:00"}
		ignore this line as well`,
			expectedOutput: "Unknown error",
		},
		"More than one error in the same line": {
			input: `this line should be ignored
this line should be ignored, too
T}his{ lin}e is OK "{"fleetErrorMessage":"fleet line error 1","level":"fatal","msg":"Fleet cli failed","time":"2025-04-15T14:53:15+02:00"}more garbage{"fleetErrorMessage":"fleet line error 2","level":"fatal","msg":"Fleet cli failed","time":"2025-04-15T14:53:15+02:00"}some more noise
ignore this line as well`,
			expectedOutput: "fleet line error 1\nfleet line error 2",
		},
		"Empty input": {
			input:          "",
			expectedOutput: "Unknown error",
		},

		"The fleetErrorMessage field is empty": {
			input:          `{"fleetErrorMessage":"","level":"fatal","msg":"Fleet cli failed","time":"2025-04-15T14:53:15+02:00"}`,
			expectedOutput: "Unknown error",
		},
	}
	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			filteredMsg := filterFleetCLIJobOutput(test.input)
			if filteredMsg != test.expectedOutput {
				t.Errorf("expecting output %q, got %q", test.expectedOutput, filteredMsg)
			}
		})
	}
}

func getFakeClient(tolerations []corev1.Toleration, objs ...runtime.Object) client.Client {
	scheme := runtime.NewScheme()
	utilruntime.Must(corev1.AddToScheme(scheme))
	utilruntime.Must(appsv1.AddToScheme(scheme))

	return fake.NewClientBuilder().
		WithScheme(scheme).
		WithRuntimeObjects(objs...).
		WithRuntimeObjects(getFleetControllerDeployment(tolerations)).Build()
}

func getFleetControllerDeployment(tolerations []corev1.Toleration) *appsv1.Deployment {
	return &appsv1.Deployment{
		ObjectMeta: metav1.ObjectMeta{
			Name:      config.ManagerConfigName,
			Namespace: config.DefaultNamespace,
		},
		Spec: appsv1.DeploymentSpec{
			Template: corev1.PodTemplateSpec{
				Spec: corev1.PodSpec{
					Tolerations: tolerations,
				},
			},
		},
	}
}



================================================
FILE: internal/cmd/controller/gitops/reconciler/polling_job.go
================================================
// Copyright (c) 2025 SUSE LLC

package reconciler

import (
	"context"
	"fmt"
	"time"

	"github.com/reugn/go-quartz/quartz"
	"golang.org/x/sync/semaphore"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	fleetevent "github.com/rancher/fleet/pkg/event"

	"github.com/rancher/wrangler/v3/pkg/condition"
	"github.com/rancher/wrangler/v3/pkg/kstatus"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	errutil "k8s.io/apimachinery/pkg/util/errors"
	"k8s.io/client-go/tools/record"
	"k8s.io/client-go/util/retry"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

var _ quartz.Job = &gitPollingJob{}

type gitPollingJob struct {
	sem    *semaphore.Weighted
	client client.Client

	namespace string
	name      string

	repo   string
	branch string

	recorder   record.EventRecorder
	gitFetcher GitFetcher
}

func newGitPollingJob(c client.Client, r record.EventRecorder, repo fleet.GitRepo, fetcher GitFetcher) *gitPollingJob {
	return &gitPollingJob{
		sem:        semaphore.NewWeighted(1),
		client:     c,
		recorder:   r,
		gitFetcher: fetcher,

		namespace: repo.Namespace,
		name:      repo.Name,

		repo:   repo.Spec.Repo,
		branch: repo.Spec.Branch,
	}
}

func (j *gitPollingJob) Execute(ctx context.Context) error {
	logger := log.FromContext(ctx)

	if !j.sem.TryAcquire(1) {
		// already running
		logger.V(1).Info("skipping polling job execution: already running")

		return nil
	}
	defer j.sem.Release(1)

	return j.pollGitRepo(ctx)
}

// Description returns a description for the job.
// This is needed to implement the Quartz Job interface.
func (j *gitPollingJob) Description() string {
	return fmt.Sprintf("gitops-polling-%s-%s-%s-%s", j.namespace, j.name, j.repo, j.branch)
}

func (j *gitPollingJob) pollGitRepo(ctx context.Context) error {
	gitrepo := &fleet.GitRepo{}
	nsName := types.NamespacedName{
		Name:      j.name,
		Namespace: j.namespace,
	}
	if err := j.client.Get(ctx, nsName, gitrepo); err != nil {
		return fmt.Errorf("could not get GitRepo resource from polling job: %w", err)
	}

	pollingTimestamp := time.Now().UTC()

	fail := func(origErr error) error {
		j.recorder.Event(gitrepo, fleetevent.Warning, "FailedToCheckCommit", origErr.Error())
		return j.updateErrorStatus(ctx, gitrepo, pollingTimestamp, origErr)
	}

	commit, err := monitorLatestCommit(gitrepo, func() (string, error) {
		return j.gitFetcher.LatestCommit(ctx, gitrepo, j.client)
	})
	if err != nil {
		return fail(err)
	}

	if commit != gitrepo.Status.Commit {
		j.recorder.Event(gitrepo, fleetevent.Normal, "GotNewCommit", commit)
	}

	err = retry.RetryOnConflict(retry.DefaultRetry, func() error {
		t := &fleet.GitRepo{}
		if err := j.client.Get(ctx, nsName, t); err != nil {
			return fmt.Errorf("could not get GitRepo to update its status: %w", err)
		}

		t.Status.LastPollingTime = metav1.Time{Time: pollingTimestamp}
		t.Status.PollingCommit = commit

		condition.Cond(gitPollingCondition).SetError(&t.Status, "", nil)

		statusPatch := client.MergeFrom(gitrepo)
		if patchData, err := statusPatch.Data(t); err == nil && string(patchData) == "{}" {
			// skip update if patch is empty
			return nil
		}
		return j.client.Status().Patch(ctx, t, statusPatch)
	})
	if err != nil {
		return fail(fmt.Errorf("could not update GitRepo status with polling timestamp: %w", err))
	}

	return nil
}

// updateErrorStatus updates the provided gitrepo's status to reflect the provided orgErr.
// This includes updating the gitrepo's polling timestamp, if provided.
func (j *gitPollingJob) updateErrorStatus(
	ctx context.Context,
	gitrepo *fleet.GitRepo,
	pollingTimestamp time.Time,
	orgErr error,
) error {
	nsn := types.NamespacedName{Name: gitrepo.Name, Namespace: gitrepo.Namespace}

	merr := []error{orgErr}
	err := retry.RetryOnConflict(retry.DefaultRetry, func() error {
		t := &fleet.GitRepo{}
		if err := j.client.Get(ctx, nsn, t); err != nil {
			return fmt.Errorf("could not get GitRepo to update its status: %w", err)
		}

		condition.Cond(gitPollingCondition).SetError(&t.Status, "", orgErr)
		kstatus.SetError(t, orgErr.Error())

		if !pollingTimestamp.IsZero() {
			t.Status.LastPollingTime = metav1.Time{Time: pollingTimestamp}
		}

		statusPatch := client.MergeFrom(gitrepo)
		return j.client.Status().Patch(ctx, t, statusPatch)
	})
	if err != nil {
		merr = append(merr, err)
	}
	return errutil.NewAggregate(merr)
}



================================================
FILE: internal/cmd/controller/gitops/reconciler/polling_job_test.go
================================================
package reconciler

import (
	"context"
	"errors"
	"testing"

	"github.com/rancher/fleet/internal/mocks"
	"github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	gitmocks "github.com/rancher/fleet/pkg/git/mocks"
	"github.com/rancher/wrangler/v3/pkg/genericcondition"
	"go.uber.org/mock/gomock"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/tools/record"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func TestPollGitRepo(t *testing.T) {
	const (
		namespace = "test-ns"
		name      = "test-repo"
		repoURL   = "https://github.com/rancher/fleet-examples"
		branch    = "main"
	)

	type testcase struct {
		name            string
		gitrepo         *v1alpha1.GitRepo
		setupMocks      func(*mocks.MockK8sClient, *mocks.MockStatusWriter, *gitmocks.MockGitFetcher, *record.FakeRecorder)
		patchErr        string
		expectedErr     string
		expectedEvents  []string
		validateGitRepo func(*testing.T, *v1alpha1.GitRepo)
	}

	testCases := []testcase{
		{
			name: "New commit found",
			gitrepo: &v1alpha1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{Name: name, Namespace: namespace},
				Spec:       v1alpha1.GitRepoSpec{Repo: repoURL, Branch: branch},
				Status:     v1alpha1.GitRepoStatus{Commit: "old-commit"},
			},
			setupMocks: func(c *mocks.MockK8sClient, sw *mocks.MockStatusWriter, gf *gitmocks.MockGitFetcher, r *record.FakeRecorder) {
				nsName := types.NamespacedName{Name: name, Namespace: namespace}
				c.EXPECT().Get(gomock.Any(), nsName, gomock.Any()).Times(2).DoAndReturn(func(_ context.Context, _ types.NamespacedName, obj *v1alpha1.GitRepo, _ ...client.GetOption) error {
					obj.Name = name
					obj.Namespace = namespace
					obj.Spec.Repo = repoURL
					obj.Spec.Branch = branch
					obj.Status.Commit = "old-commit"
					return nil
				})
				gf.EXPECT().LatestCommit(gomock.Any(), gomock.Any(), gomock.Any()).Return("new-commit", nil)
				c.EXPECT().Status().Return(sw)
			},
			expectedEvents: []string{"Normal GotNewCommit new-commit"},
			validateGitRepo: func(t *testing.T, gr *v1alpha1.GitRepo) {
				t.Helper()
				if gr.Status.PollingCommit != "new-commit" {
					t.Errorf("expected PollingCommit to be 'new-commit', got %s", gr.Status.PollingCommit)
				}
				cond := findStatusCondition(gr.Status.Conditions, gitPollingCondition)
				if cond == nil || cond.Status != "True" {
					t.Errorf("expected GitPolling condition to be True")
				}
			},
		},
		{
			name: "No new commit",
			gitrepo: &v1alpha1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{Name: name, Namespace: namespace},
				Spec:       v1alpha1.GitRepoSpec{Repo: repoURL, Branch: branch},
				Status:     v1alpha1.GitRepoStatus{Commit: "same-commit"},
			},
			setupMocks: func(c *mocks.MockK8sClient, sw *mocks.MockStatusWriter, gf *gitmocks.MockGitFetcher, r *record.FakeRecorder) {
				c.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any()).Times(2).DoAndReturn(func(_ context.Context, _ client.ObjectKey, obj *v1alpha1.GitRepo, _ ...client.GetOption) error {
					obj.Name = name
					obj.Namespace = namespace
					obj.Spec.Repo = repoURL
					obj.Spec.Branch = branch
					obj.Status.Commit = "same-commit"
					return nil
				})
				gf.EXPECT().LatestCommit(gomock.Any(), gomock.Any(), gomock.Any()).Return("same-commit", nil)
				c.EXPECT().Status().Return(sw)
			},
			validateGitRepo: func(t *testing.T, gr *v1alpha1.GitRepo) {
				t.Helper()
				if gr.Status.PollingCommit != "same-commit" {
					t.Errorf("expected PollingCommit to be 'same-commit', got %s", gr.Status.PollingCommit)
				}
			},
		},
		{
			name: "Git fetch error",
			gitrepo: &v1alpha1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{Name: name, Namespace: namespace},
				Spec:       v1alpha1.GitRepoSpec{Repo: repoURL, Branch: branch},
			},
			setupMocks: func(c *mocks.MockK8sClient, sw *mocks.MockStatusWriter, gf *gitmocks.MockGitFetcher, r *record.FakeRecorder) {
				nsName := types.NamespacedName{Name: name, Namespace: namespace}
				c.EXPECT().Get(gomock.Any(), nsName, gomock.Any()).Times(2).DoAndReturn(func(_ context.Context, _ client.ObjectKey, obj *v1alpha1.GitRepo, _ ...client.GetOption) error {
					obj.Name = name
					obj.Namespace = namespace
					obj.Spec.Repo = repoURL
					obj.Spec.Branch = branch
					obj.Status.Commit = "commit"
					return nil
				})
				gf.EXPECT().LatestCommit(gomock.Any(), gomock.Any(), gomock.Any()).Return("", errors.New("git error"))
				c.EXPECT().Status().Return(sw)
			},
			expectedErr:    "git error",
			expectedEvents: []string{"Warning FailedToCheckCommit git error"},
			validateGitRepo: func(t *testing.T, gr *v1alpha1.GitRepo) {
				t.Helper()
				cond := findStatusCondition(gr.Status.Conditions, gitPollingCondition)
				if cond == nil || cond.Status != "False" || cond.Message != "git error" {
					t.Errorf("expected GitPolling condition to be False with message 'git error', got %+v", cond)
				}
			},
		},
		{
			name: "Update status error",
			gitrepo: &v1alpha1.GitRepo{
				ObjectMeta: metav1.ObjectMeta{Name: name, Namespace: namespace},
				Spec:       v1alpha1.GitRepoSpec{Repo: repoURL, Branch: branch},
			},
			setupMocks: func(c *mocks.MockK8sClient, sw *mocks.MockStatusWriter, gf *gitmocks.MockGitFetcher, r *record.FakeRecorder) {
				nsName := types.NamespacedName{Name: name, Namespace: namespace}
				c.EXPECT().Get(gomock.Any(), nsName, gomock.Any()).Times(3).DoAndReturn(func(_ context.Context, _ client.ObjectKey, obj *v1alpha1.GitRepo, _ ...client.GetOption) error {
					obj.Name = name
					obj.Namespace = namespace
					obj.Spec.Repo = repoURL
					obj.Spec.Branch = branch
					obj.Status.Commit = "commit"
					return nil
				})
				gf.EXPECT().LatestCommit(gomock.Any(), gomock.Any(), gomock.Any()).Return("new-commit", nil)
				c.EXPECT().Status().Return(sw).Times(2)
			},
			patchErr:    "update error",
			expectedErr: "could not update GitRepo status with polling timestamp: update error",
		},
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			ctrl := gomock.NewController(t)
			defer ctrl.Finish()

			mockClient := mocks.NewMockK8sClient(ctrl)
			mockStatusWriter := mocks.NewMockStatusWriter(ctrl)
			mockGitFetcher := gitmocks.NewMockGitFetcher(ctrl)
			recorder := record.NewFakeRecorder(10)

			if tc.setupMocks != nil {
				tc.setupMocks(mockClient, mockStatusWriter, mockGitFetcher, recorder)
			}

			job := newGitPollingJob(mockClient, recorder, *tc.gitrepo, mockGitFetcher)

			// We are testing pollGitRepo directly, which modifies the gitrepo object in place for status updates.
			// So we need a mechanism to capture the final state of the gitrepo.
			// The patch function in the mock will be our capture point.
			var finalGitRepo *v1alpha1.GitRepo

			mockStatusWriter.EXPECT().Patch(gomock.Any(), gomock.Any(), gomock.Any()).DoAndReturn(
				func(_ context.Context, obj *v1alpha1.GitRepo, _ client.Patch, _ ...client.PatchOption) error {
					finalGitRepo = obj.DeepCopy()
					if tc.patchErr != "" {
						return errors.New(tc.patchErr)
					}
					return nil
				}).Times(1)
			if tc.patchErr != "" {
				// this second call is to set the error in the condition
				mockStatusWriter.EXPECT().Patch(gomock.Any(), gomock.Any(), gomock.Any()).DoAndReturn(
					func(_ context.Context, obj *v1alpha1.GitRepo, _ client.Patch, _ ...client.PatchOption) error {
						finalGitRepo = obj.DeepCopy()
						return nil
					}).Times(1)
			}

			err := job.pollGitRepo(context.Background())

			if tc.expectedErr != "" {
				if err == nil || err.Error() != tc.expectedErr {
					t.Errorf("expected error '%q', got '%v'", tc.expectedErr, err)
				}
			} else if err != nil {
				t.Errorf("unexpected error: %v", err)
			}

			close(recorder.Events)
			if len(tc.expectedEvents) > 0 {
				if len(recorder.Events) != len(tc.expectedEvents) {
					t.Errorf("expected %d events, got %d", len(tc.expectedEvents), len(recorder.Events))
				}
				for i, expectedEvent := range tc.expectedEvents {
					if event, ok := <-recorder.Events; ok && event != expectedEvent {
						t.Errorf("expected event %d to be '%q', got '%q'", i, expectedEvent, event)
					}
				}
			}

			if tc.validateGitRepo != nil {
				// The gitrepo passed to pollGitRepo is modified in-place for status updates.
				// We use the captured object from the Patch mock.
				if finalGitRepo == nil {
					t.Fatal("Status().Patch() was not called, cannot validate final gitrepo state")
				}
				tc.validateGitRepo(t, finalGitRepo)
			}
		})
	}
}

func TestGitPollingJob_Description(t *testing.T) {
	job := newGitPollingJob(nil, nil, v1alpha1.GitRepo{
		ObjectMeta: metav1.ObjectMeta{Name: "test-repo", Namespace: "test-ns"},
		Spec:       v1alpha1.GitRepoSpec{Repo: "http://a.b/c.git", Branch: "develop"},
	}, nil)

	expected := "gitops-polling-test-ns-test-repo-http://a.b/c.git-develop"
	if job.Description() != expected {
		t.Errorf("expected description '%q', got '%q'", expected, job.Description())
	}
}

// findStatusCondition finds the conditionType in conditions.
func findStatusCondition(conditions []genericcondition.GenericCondition, conditionType string) *genericcondition.GenericCondition {
	for i := range conditions {
		if conditions[i].Type == conditionType {
			return &conditions[i]
		}
	}

	return nil
}



================================================
FILE: internal/cmd/controller/gitops/reconciler/predicate.go
================================================
package reconciler

import (
	"reflect"

	v1alpha1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	batchv1 "k8s.io/api/batch/v1"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
)

func jobUpdatedPredicate() predicate.Funcs {
	return predicate.Funcs{
		CreateFunc: func(e event.CreateEvent) bool {
			return false
		},
		UpdateFunc: func(e event.UpdateEvent) bool {
			n, isJob := e.ObjectNew.(*batchv1.Job)
			if !isJob {
				return false
			}
			o := e.ObjectOld.(*batchv1.Job)
			if n == nil || o == nil {
				return false
			}
			return !reflect.DeepEqual(n.Status, o.Status) ||
				(n.DeletionTimestamp != nil && o.DeletionTimestamp == nil)
		},
		DeleteFunc: func(e event.DeleteEvent) bool {
			return false
		},
	}
}

func commitChangedPredicate() predicate.Predicate {
	return predicate.Funcs{
		UpdateFunc: func(e event.UpdateEvent) bool {
			oldGitRepo, ok := e.ObjectOld.(*v1alpha1.GitRepo)
			if !ok {
				return true
			}
			newGitRepo, ok := e.ObjectNew.(*v1alpha1.GitRepo)
			if !ok {
				return true
			}
			return (oldGitRepo.Status.WebhookCommit != newGitRepo.Status.WebhookCommit) ||
				(oldGitRepo.Status.PollingCommit != newGitRepo.Status.PollingCommit)
		},
	}
}



================================================
FILE: internal/cmd/controller/gitops/reconciler/rbac.go
================================================
// Copyright (c) 2021-2025 SUSE LLC

package reconciler

import (
	"context"

	"github.com/rancher/fleet/internal/names"

	v1alpha1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	corev1 "k8s.io/api/core/v1"
	rbacv1 "k8s.io/api/rbac/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
)

func (r *GitJobReconciler) createJobRBAC(ctx context.Context, gitRepo *v1alpha1.GitRepo) error {
	saName := names.SafeConcatName("git", gitRepo.Name)

	if err := r.createServiceAccount(ctx, gitRepo, saName); err != nil {
		return err
	}

	if err := r.createOrUpdateRole(ctx, gitRepo, saName); err != nil {
		return err
	}

	if err := r.createOrUpdateRoleBinding(ctx, gitRepo, saName); err != nil {
		return err
	}

	return nil
}

func (r *GitJobReconciler) createServiceAccount(ctx context.Context, gitRepo *v1alpha1.GitRepo, saName string) error {
	sa := &corev1.ServiceAccount{
		ObjectMeta: metav1.ObjectMeta{
			Name:      saName,
			Namespace: gitRepo.Namespace,
		},
	}
	if err := controllerutil.SetControllerReference(gitRepo, sa, r.Scheme); err != nil {
		return err
	}
	// No update needed, values are the same. So we ignore AlreadyExists.
	if err := r.Create(ctx, sa); err != nil && !errors.IsAlreadyExists(err) {
		return err
	}
	return nil
}

func (r *GitJobReconciler) createOrUpdateRole(ctx context.Context, gitRepo *v1alpha1.GitRepo, saName string) error {
	role := &rbacv1.Role{ObjectMeta: metav1.ObjectMeta{Namespace: gitRepo.Namespace, Name: saName}}
	if err := controllerutil.SetControllerReference(gitRepo, role, r.Scheme); err != nil {
		return err
	}
	if _, err := controllerutil.CreateOrUpdate(ctx, r.Client, role, func() error {
		role.Rules = []rbacv1.PolicyRule{
			{
				Verbs:     []string{"get", "create", "update", "list", "delete"},
				APIGroups: []string{"fleet.cattle.io"},
				Resources: []string{"bundles", "imagescans"},
			},
			{
				Verbs:     []string{"get"},
				APIGroups: []string{"fleet.cattle.io"},
				Resources: []string{"gitrepos"},
			},
			{
				Verbs:     []string{"get", "create", "update", "delete"},
				APIGroups: []string{""},
				Resources: []string{"secrets"},
			},
			{
				Verbs:     []string{"create"},
				APIGroups: []string{""},
				Resources: []string{"events"},
			},
		}
		return nil
	}); err != nil {
		return err
	}
	return nil
}

func (r *GitJobReconciler) createOrUpdateRoleBinding(ctx context.Context, gitRepo *v1alpha1.GitRepo, saName string) error {
	rb := &rbacv1.RoleBinding{
		ObjectMeta: metav1.ObjectMeta{
			Name:      saName,
			Namespace: gitRepo.Namespace,
		},
	}
	if err := controllerutil.SetControllerReference(gitRepo, rb, r.Scheme); err != nil {
		return err
	}
	if _, err := controllerutil.CreateOrUpdate(ctx, r.Client, rb, func() error {
		rb.Subjects = []rbacv1.Subject{{
			Kind:      "ServiceAccount",
			Name:      saName,
			Namespace: gitRepo.Namespace,
		}}
		rb.RoleRef = rbacv1.RoleRef{
			APIGroup: "rbac.authorization.k8s.io",
			Kind:     "Role",
			Name:     saName,
		}
		return nil
	}); err != nil {
		return err
	}
	return nil
}



================================================
FILE: internal/cmd/controller/gitops/reconciler/restrictions.go
================================================
package reconciler

import (
	"context"
	"fmt"
	"regexp"
	"sort"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"sigs.k8s.io/controller-runtime/pkg/client"
)

// AuthorizeAndAssignDefaults applies restrictions and mutates the passed in
// GitRepo if it passes the restrictions
func AuthorizeAndAssignDefaults(ctx context.Context, c client.Client, gitrepo *fleet.GitRepo) error {
	restrictions := &fleet.GitRepoRestrictionList{}
	err := c.List(ctx, restrictions, client.InNamespace(gitrepo.Namespace))
	if err != nil {
		return err
	}

	if len(restrictions.Items) == 0 {
		return nil
	}

	restriction := aggregate(restrictions.Items)

	if len(restriction.AllowedTargetNamespaces) > 0 && gitrepo.Spec.TargetNamespace == "" {
		return fmt.Errorf("empty targetNamespace denied, because allowedTargetNamespaces restriction is present")
	}

	targetNamespace, err := isAllowed(gitrepo.Spec.TargetNamespace, "", restriction.AllowedTargetNamespaces)
	if err != nil {
		return fmt.Errorf("disallowed targetNamespace %s: %w", gitrepo.Spec.TargetNamespace, err)
	}

	serviceAccount, err := isAllowed(gitrepo.Spec.ServiceAccount,
		restriction.DefaultServiceAccount,
		restriction.AllowedServiceAccounts)
	if err != nil {
		return fmt.Errorf("disallowed serviceAccount %s: %w", gitrepo.Spec.ServiceAccount, err)
	}

	repo, err := isAllowedByRegex(gitrepo.Spec.Repo, "", restriction.AllowedRepoPatterns)
	if err != nil {
		return fmt.Errorf("disallowed repo %s: %w", gitrepo.Spec.Repo, err)
	}

	clientSecretName, err := isAllowed(gitrepo.Spec.ClientSecretName,
		restriction.DefaultClientSecretName,
		restriction.AllowedClientSecretNames)
	if err != nil {
		return fmt.Errorf("disallowed clientSecretName %s: %w", gitrepo.Spec.ClientSecretName, err)
	}

	// set the defaults back to the GitRepo
	gitrepo.Spec.TargetNamespace = targetNamespace
	gitrepo.Spec.ServiceAccount = serviceAccount
	gitrepo.Spec.Repo = repo
	gitrepo.Spec.ClientSecretName = clientSecretName

	return nil
}

func aggregate(restrictions []fleet.GitRepoRestriction) (result fleet.GitRepoRestriction) {
	sort.Slice(restrictions, func(i, j int) bool {
		return restrictions[i].Name < restrictions[j].Name
	})
	for _, restriction := range restrictions {
		if result.DefaultServiceAccount == "" {
			result.DefaultServiceAccount = restriction.DefaultServiceAccount
		}
		if result.DefaultClientSecretName == "" {
			result.DefaultClientSecretName = restriction.DefaultClientSecretName
		}
		result.AllowedServiceAccounts = append(result.AllowedServiceAccounts, restriction.AllowedServiceAccounts...)
		result.AllowedClientSecretNames = append(result.AllowedClientSecretNames, restriction.AllowedClientSecretNames...)
		result.AllowedRepoPatterns = append(result.AllowedRepoPatterns, restriction.AllowedRepoPatterns...)
		result.AllowedTargetNamespaces = append(result.AllowedTargetNamespaces, restriction.AllowedTargetNamespaces...)
	}
	return
}

func isAllowed(currentValue, defaultValue string, allowedValues []string) (string, error) {
	if currentValue == "" {
		return defaultValue, nil
	}
	if len(allowedValues) == 0 {
		return currentValue, nil
	}
	for _, allowedValue := range allowedValues {
		if allowedValue == currentValue {
			return currentValue, nil
		}
	}

	return currentValue, fmt.Errorf("%s not in allowed set %v", currentValue, allowedValues)
}

func isAllowedByRegex(currentValue, defaultValue string, patterns []string) (string, error) {
	if currentValue == "" {
		return defaultValue, nil
	}
	if len(patterns) == 0 {
		return currentValue, nil
	}
	for _, pattern := range patterns {
		// for compatibility with previous versions, the patterns can match verbatim
		if pattern == currentValue {
			return currentValue, nil
		}

		p, err := regexp.Compile(pattern)
		if err != nil {
			return currentValue, fmt.Errorf("GitRepoRestriction failed to compile regex '%s': %w", pattern, err)
		}
		if p.MatchString(currentValue) {
			return currentValue, nil
		}
	}

	return currentValue, fmt.Errorf("%s not in allowed set %v", currentValue, patterns)
}



================================================
FILE: internal/cmd/controller/gitops/reconciler/restrictions_test.go
================================================
package reconciler_test

import (
	"context"
	"errors"
	"reflect"
	"testing"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"go.uber.org/mock/gomock"
	crclient "sigs.k8s.io/controller-runtime/pkg/client"

	"github.com/rancher/fleet/internal/cmd/controller/gitops/reconciler"
	"github.com/rancher/fleet/internal/mocks"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

func TestAuthorizeAndAssignDefaults(t *testing.T) {
	dummyErrMsg := "something happened"
	dummyErr := errors.New(dummyErrMsg)
	cases := []struct {
		name                string
		inputGr             fleet.GitRepo
		restrictions        *fleet.GitRepoRestrictionList
		restrictionsListErr error
		expectedGr          fleet.GitRepo
		expectedErr         string
	}{
		{
			name:                "fail when listing GitRepo restrictions errors",
			inputGr:             fleet.GitRepo{},
			restrictionsListErr: dummyErr,
			expectedGr:          fleet.GitRepo{},
			expectedErr:         dummyErrMsg,
		},
		{
			name:         "pass when list of GitRepo restrictions is empty",
			inputGr:      fleet.GitRepo{},
			restrictions: &fleet.GitRepoRestrictionList{},
			expectedGr:   fleet.GitRepo{},
		},
		{
			name:    "deny empty targetNamespace when allowedTargetNamespaces restriction present",
			inputGr: fleet.GitRepo{}, // no target ns provided
			restrictions: &fleet.GitRepoRestrictionList{
				Items: []fleet.GitRepoRestriction{
					{
						AllowedTargetNamespaces: []string{"foo"},
					},
				},
			},
			expectedGr:  fleet.GitRepo{},
			expectedErr: "empty targetNamespace denied.*allowedTargetNamespaces restriction is present",
		},
		{
			name: "deny disallowed targetNamespace",
			inputGr: fleet.GitRepo{
				Spec: fleet.GitRepoSpec{
					TargetNamespace: "not-foo",
				},
			},
			restrictions: &fleet.GitRepoRestrictionList{
				Items: []fleet.GitRepoRestriction{
					{
						AllowedTargetNamespaces: []string{"foo"},
					},
				},
			},
			expectedGr: fleet.GitRepo{
				Spec: fleet.GitRepoSpec{
					TargetNamespace: "not-foo",
				},
			},
			expectedErr: "disallowed targetNamespace.*",
		},
		{
			name: "deny disallowed service account",
			inputGr: fleet.GitRepo{
				Spec: fleet.GitRepoSpec{
					ServiceAccount: "not-foo",
				},
			},
			restrictions: &fleet.GitRepoRestrictionList{
				Items: []fleet.GitRepoRestriction{
					{
						AllowedServiceAccounts: []string{"foo"},
					},
				},
			},
			expectedGr: fleet.GitRepo{
				Spec: fleet.GitRepoSpec{
					ServiceAccount: "not-foo",
				},
			},
			expectedErr: "disallowed serviceAccount.*",
		},
		{
			name: "deny disallowed repo pattern",
			inputGr: fleet.GitRepo{
				Spec: fleet.GitRepoSpec{
					Repo: "bar",
				},
			},
			restrictions: &fleet.GitRepoRestrictionList{
				Items: []fleet.GitRepoRestriction{
					{
						AllowedRepoPatterns: []string{"baz"},
					},
				},
			},
			expectedGr: fleet.GitRepo{
				Spec: fleet.GitRepoSpec{
					Repo: "bar",
				},
			},
			expectedErr: "disallowed repo.*",
		},
		{
			name: "deny disallowed client secret name",
			inputGr: fleet.GitRepo{
				Spec: fleet.GitRepoSpec{
					ClientSecretName: "not-foo",
				},
			},
			restrictions: &fleet.GitRepoRestrictionList{
				Items: []fleet.GitRepoRestriction{
					{
						AllowedClientSecretNames: []string{"foo"},
					},
				},
			},
			expectedGr: fleet.GitRepo{
				Spec: fleet.GitRepoSpec{
					ClientSecretName: "not-foo",
				},
			},
			expectedErr: "disallowed clientSecretName.*",
		},
		{
			name: "pass when no restrictions nor disallowed values exist",
			inputGr: fleet.GitRepo{
				Spec: fleet.GitRepoSpec{
					Repo: "http://foo.bar/baz",
				},
			},
			restrictions: &fleet.GitRepoRestrictionList{},
			expectedGr: fleet.GitRepo{
				Spec: fleet.GitRepoSpec{
					Repo: "http://foo.bar/baz",
				},
			},
		},
		{
			name: "pass when restrictions exist and the GitRepo matches allowed values",
			inputGr: fleet.GitRepo{
				Spec: fleet.GitRepoSpec{
					ClientSecretName: "csn",
					Repo:             "http://foo.bar/baz",
					ServiceAccount:   "sacc",
					TargetNamespace:  "tns",
				},
			},
			restrictions: &fleet.GitRepoRestrictionList{
				Items: []fleet.GitRepoRestriction{
					{
						AllowedClientSecretNames: []string{"csn"},
						AllowedRepoPatterns:      []string{".*foo.bar.*"},
						AllowedServiceAccounts:   []string{"sacc"},
						AllowedTargetNamespaces:  []string{"tns"},
					},
				},
			},
			expectedGr: fleet.GitRepo{
				Spec: fleet.GitRepoSpec{
					ClientSecretName: "csn",
					Repo:             "http://foo.bar/baz",
					ServiceAccount:   "sacc",
					TargetNamespace:  "tns",
				},
			},
		},
		{
			name: "pass and mutate repo with defaults when restrictions exist and the GitRepo matches allowed values",
			inputGr: fleet.GitRepo{
				Spec: fleet.GitRepoSpec{
					Repo:            "http://foo.bar/baz",
					TargetNamespace: "tns",
				},
			},
			restrictions: &fleet.GitRepoRestrictionList{
				Items: []fleet.GitRepoRestriction{
					{
						AllowedClientSecretNames: []string{"csn"},
						AllowedRepoPatterns:      []string{".*foo.bar.*"},
						AllowedServiceAccounts:   []string{"sacc"},
						AllowedTargetNamespaces:  []string{"tns"},
						DefaultClientSecretName:  "dcsn",
						DefaultServiceAccount:    "dsacc",
					},
				},
			},
			expectedGr: fleet.GitRepo{
				Spec: fleet.GitRepoSpec{
					ClientSecretName: "dcsn",
					Repo:             "http://foo.bar/baz",
					ServiceAccount:   "dsacc",
					TargetNamespace:  "tns",
				},
			},
		},
	}

	for _, c := range cases {
		t.Run(c.name, func(t *testing.T) {
			mockCtrl := gomock.NewController(t)
			defer mockCtrl.Finish()

			client := mocks.NewMockK8sClient(mockCtrl)

			client.EXPECT().List(gomock.Any(), gomock.Any(), gomock.Any()).AnyTimes().DoAndReturn(
				func(_ context.Context, rl *fleet.GitRepoRestrictionList, ns crclient.InNamespace) error {
					if c.restrictions != nil && len(c.restrictions.Items) > 0 {
						rl.Items = c.restrictions.Items
					}

					return c.restrictionsListErr
				},
			)

			err := reconciler.AuthorizeAndAssignDefaults(context.TODO(), client, &c.inputGr)

			if len(c.expectedErr) > 0 {
				require.Error(t, err)
				assert.Regexp(t, c.expectedErr, err.Error())
			} else {
				require.NoError(t, err)
			}

			if !reflect.DeepEqual(c.inputGr, c.expectedGr) {
				t.Errorf("Expected res %v, got %v", c.expectedGr, c.inputGr)
			}
		})
	}
}



================================================
FILE: internal/cmd/controller/gitops/reconciler/status_controller.go
================================================
package reconciler

import (
	"context"
	"fmt"
	"sort"
	"time"

	"github.com/rancher/fleet/internal/cmd/controller/status"
	"github.com/rancher/fleet/internal/cmd/controller/summary"
	"github.com/rancher/fleet/internal/resourcestatus"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"
	"github.com/rancher/fleet/pkg/sharding"
	"github.com/rancher/wrangler/v3/pkg/genericcondition"
	v1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/util/workqueue"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/source"
)

// enqueueDelay is used as an artificial delay for enqueuing GitRepo status reconciliation requests
// This allows aggregating multiple consecutive Bundle update events, reducing the number of GitRepo status changes at the cost of introducing a delay in the notification
const enqueueDelay = 3 * time.Second

type StatusReconciler struct {
	client.Client
	Scheme  *runtime.Scheme
	Workers int
	ShardID string
}

func (r *StatusReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&fleet.GitRepo{}).
		WatchesRawSource(wrapSourceWithForcedDelay(enqueueDelay, source.TypedKind(
			// Fan out from bundle to gitrepo
			mgr.GetCache(),
			&fleet.Bundle{},
			handler.TypedEnqueueRequestsFromMapFunc(func(ctx context.Context, a *fleet.Bundle) []ctrl.Request {
				repo := a.GetLabels()[fleet.RepoLabel]
				if repo != "" {
					return []ctrl.Request{{
						NamespacedName: types.NamespacedName{
							Namespace: a.GetNamespace(),
							Name:      repo,
						},
					}}
				}

				return []ctrl.Request{}
			}),
			sharding.TypedFilterByShardID[*fleet.Bundle](r.ShardID), // WatchesRawSources ignores event filters, we need to use a predicate
			status.BundleStatusChangedPredicate(),
		))).
		WithEventFilter(sharding.FilterByShardID(r.ShardID)).
		WithOptions(controller.Options{MaxConcurrentReconciles: r.Workers}).
		Named("GitRepoStatus").
		Complete(r)
}

// Reconcile reads the stat of the GitRepo and BundleDeployments and
// computes status fields for the GitRepo. This status is used to
// display information to the user.
func (r *StatusReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx).WithName("gitops-status")

	gitrepo := &fleet.GitRepo{}
	if err := r.Get(ctx, req.NamespacedName, gitrepo); err != nil && !errors.IsNotFound(err) {
		return ctrl.Result{}, err
	} else if errors.IsNotFound(err) {
		logger.V(1).Info("Gitrepo deleted, cleaning up poll jobs")
		return ctrl.Result{}, nil
	}

	orig := gitrepo.DeepCopy()

	// Restrictions / Overrides, gitrepo reconciler is responsible for setting error in status
	if err := AuthorizeAndAssignDefaults(ctx, r.Client, gitrepo); err != nil {
		// the gitjob_controller will handle the error (records event, updates status)
		// status_controller just computes display status, no action needed on restriction errors
		//nolint:nilerr // Intentionally delegating error handling to gitjob_controller
		return ctrl.Result{}, nil
	}

	if !gitrepo.DeletionTimestamp.IsZero() {
		// the gitjob_controller will handle deletion
		return ctrl.Result{}, nil
	}

	if gitrepo.Spec.Repo == "" {
		return ctrl.Result{}, nil
	}

	logger = logger.WithValues("generation", gitrepo.Generation, "commit", gitrepo.Status.Commit).WithValues("conditions", gitrepo.Status.Conditions)
	ctx = log.IntoContext(ctx, logger)

	logger.V(1).Info("Reconciling GitRepo status")

	bdList := &fleet.BundleDeploymentList{}
	err := r.List(ctx, bdList, client.MatchingLabels{
		fleet.RepoLabel:            gitrepo.Name,
		fleet.BundleNamespaceLabel: gitrepo.Namespace,
	})
	if err != nil {
		return ctrl.Result{}, err
	}

	err = setStatus(bdList, gitrepo)
	if err != nil {
		return ctrl.Result{}, err
	}

	if gitrepo.Status.GitJobStatus != "Current" {
		gitrepo.Status.Display.State = "GitUpdating"
	}

	// We're explicitly setting the ready status from a bundle here, but only if it isn't ready.
	//
	// - If the bundle has no deployments, there is no status to be copied from the setStatus
	// function, so that we won't overwrite anything.
	//
	// - If the bundle has rendering issues and there are deployments of which there is at least one
	// in a failed state, the status of the bundle deployments would be overwritten by the bundle
	// status.
	//
	// - If the bundle has no rendering issues but there are deployments in a failed state, the code
	// will overwrite the gitrepo's ready status condition with the ready status condition coming
	// from the bundle. Because both have the same content, we can unconditionally set the status
	// from the bundle.
	//
	// So we're basically just making sure the status from the bundle is being set on the gitrepo,
	// even if there are no bundle deployments, which is the case for issues with rendering the
	// manifests, for instance. In that case no bundle deployments are created, but an error is set
	// in a ready status condition on the bundle.
	err = r.setReadyStatusFromBundle(ctx, gitrepo)
	if err != nil {
		return ctrl.Result{}, err
	}

	if err := r.updateStatus(ctx, orig, gitrepo); err != nil {
		logger.Error(err, "Reconcile failed update to git repo status", "status", gitrepo.Status)
		return ctrl.Result{RequeueAfter: durations.GitRepoStatusDelay}, nil
	}

	return ctrl.Result{}, nil
}

func (r *StatusReconciler) updateStatus(ctx context.Context, orig *fleet.GitRepo, obj *fleet.GitRepo) error {
	statusPatch := client.MergeFrom(orig)
	if patchData, err := statusPatch.Data(obj); err == nil && string(patchData) == "{}" {
		// skip update if patch is empty
		return nil
	}
	return r.Client.Status().Patch(ctx, obj, statusPatch)
}

func setStatus(list *fleet.BundleDeploymentList, gitrepo *fleet.GitRepo) error {
	// sort bundledeployments so lists in status are always in the same order
	sort.Slice(list.Items, func(i, j int) bool {
		return list.Items[i].UID < list.Items[j].UID
	})

	err := status.SetFields(list, &gitrepo.Status.StatusBase)
	if err != nil {
		return err
	}

	resourcestatus.SetResources(list.Items, &gitrepo.Status.StatusBase)

	summary.SetReadyConditions(&gitrepo.Status, "Bundle", gitrepo.Status.Summary)

	gitrepo.Status.Display.ReadyBundleDeployments = fmt.Sprintf("%d/%d",
		gitrepo.Status.Summary.Ready,
		gitrepo.Status.Summary.DesiredReady)

	return nil
}

// setReadyStatusFromBundle fetches all bundles from a given gitrepo, checks the ready status conditions
// from the bundles and applies one on the gitrepo if it isn't ready. The purpose is to make
// rendering issues visible in the gitrepo status. Those issues need to be made explicitly visible
// since the other statuses are calculated from bundle deployments, which do not exist when
// rendering manifests fail. Should an issue be on the bundle, it will be copied to the gitrepo.
func (r StatusReconciler) setReadyStatusFromBundle(ctx context.Context, gitrepo *fleet.GitRepo) error {
	bList := &fleet.BundleList{}
	err := r.List(ctx, bList, client.MatchingLabels{
		fleet.RepoLabel: gitrepo.Name,
	}, client.InNamespace(gitrepo.Namespace))
	if err != nil {
		return err
	}

	// Make sure the bundles are always iterated in the same order
	// The code below will pick the first element matching the condition, so successive executions should produce the same result.
	sort.Slice(bList.Items, func(i, j int) bool {
		return bList.Items[i].UID < bList.Items[j].UID
	})

	found := false
	// Find a ready status condition in a bundle which is not ready.
	var condition genericcondition.GenericCondition
bundles:
	for _, bundle := range bList.Items {
		if bundle.Status.Conditions == nil {
			continue
		}

		for _, c := range bundle.Status.Conditions {
			if c.Type == string(fleet.Ready) && c.Status == v1.ConditionFalse {
				condition = c
				found = true
				break bundles
			}
		}
	}

	// No ready condition found in any bundle, nothing to do here.
	if !found {
		return nil
	}

	found = false
	newConditions := make([]genericcondition.GenericCondition, 0, len(gitrepo.Status.Conditions))
	for _, c := range gitrepo.Status.Conditions {
		if c.Type == string(fleet.Ready) {
			// Replace the ready condition with the one from the bundle
			newConditions = append(newConditions, condition)
			found = true
			continue
		}
		newConditions = append(newConditions, c)
	}
	if !found {
		// Add the ready condition from the bundle to the gitrepo.
		newConditions = append(newConditions, condition)
	}
	gitrepo.Status.Conditions = newConditions

	return nil
}

type forcedDelayingSource[R comparable] struct {
	source.TypedSource[R]
	delay time.Duration
}

func wrapSourceWithForcedDelay[R comparable](delay time.Duration, delegate source.TypedSource[R]) source.TypedSource[R] {
	return &forcedDelayingSource[R]{TypedSource: delegate, delay: delay}
}

func (s *forcedDelayingSource[R]) Start(ctx context.Context, delegate workqueue.TypedRateLimitingInterface[R]) error {
	return s.TypedSource.Start(ctx, &forcedDelayingQueue[R]{delegate, s.delay})
}

type forcedDelayingQueue[R comparable] struct {
	workqueue.TypedRateLimitingInterface[R]
	delay time.Duration
}

func (f *forcedDelayingQueue[R]) Add(obj R) {
	f.AddAfter(obj, f.delay)
}



================================================
FILE: internal/cmd/controller/gitops/reconciler/suite_test.go
================================================
package reconciler

import (
	"testing"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

func TestGitOpsReconciler(t *testing.T) {
	RegisterFailHandler(Fail)
	RunSpecs(t, "GitOps Reconciler Suite")
}

var _ = BeforeSuite(func() {
})



================================================
FILE: internal/cmd/controller/gitops/reconciler/targetsyaml.go
================================================
package reconciler

import (
	"encoding/json"

	"github.com/rancher/fleet/internal/names"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// newTargetsConfigMap builds a config map, containing the GitTarget cluster matchers, converted to BundleTargets.
// The BundleTargets are duplicated into TargetRestrictions. TargetRestrictions is a whitelist. A BundleDeployment
// will be created for a Target just if it is inside a TargetRestrictions. If it is not inside TargetRestrictions a Target
// is a TargetCustomization.
func newTargetsConfigMap(repo *fleet.GitRepo) (*corev1.ConfigMap, error) {
	spec := &fleet.BundleSpec{}
	for _, target := range targetsOrDefault(repo.Spec.Targets) {
		spec.Targets = append(spec.Targets, fleet.BundleTarget{
			Name:                 target.Name,
			ClusterName:          target.ClusterName,
			ClusterSelector:      target.ClusterSelector,
			ClusterGroup:         target.ClusterGroup,
			ClusterGroupSelector: target.ClusterGroupSelector,
		})
		spec.TargetRestrictions = append(spec.TargetRestrictions, fleet.BundleTargetRestriction(target))
	}
	data, err := json.Marshal(spec)
	if err != nil {
		return nil, err
	}

	hash := names.KeyHash(string(data))
	return &corev1.ConfigMap{
		ObjectMeta: metav1.ObjectMeta{
			Name:      names.SafeConcatName(repo.Name, "config", hash),
			Namespace: repo.Namespace,
		},
		BinaryData: map[string][]byte{
			"targets.yaml": data,
		},
	}, nil
}

func targetsOrDefault(targets []fleet.GitTarget) []fleet.GitTarget {
	if len(targets) == 0 {
		return []fleet.GitTarget{
			{
				Name:         "default",
				ClusterGroup: "default",
			},
		}
	}
	return targets
}



================================================
FILE: internal/cmd/controller/helmops/operator.go
================================================
package helmops

import (
	"fmt"
	"os"
	"strconv"

	"github.com/reugn/go-quartz/quartz"
	"github.com/spf13/cobra"
	"golang.org/x/sync/errgroup"

	"k8s.io/apimachinery/pkg/runtime"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	clientgoscheme "k8s.io/client-go/kubernetes/scheme"
	ctrl "sigs.k8s.io/controller-runtime"
	clog "sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"
	metricsserver "sigs.k8s.io/controller-runtime/pkg/metrics/server"

	command "github.com/rancher/fleet/internal/cmd"
	"github.com/rancher/fleet/internal/cmd/controller/helmops/reconciler"
	fcreconciler "github.com/rancher/fleet/internal/cmd/controller/reconciler"
	"github.com/rancher/fleet/internal/metrics"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/version"
)

var (
	scheme   = runtime.NewScheme()
	setupLog = ctrl.Log.WithName("setup")
	zopts    *zap.Options
)

func init() {
	utilruntime.Must(clientgoscheme.AddToScheme(scheme))
	utilruntime.Must(fleet.AddToScheme(scheme))
}

type HelmOperator struct {
	command.DebugConfig
	Kubeconfig           string `usage:"Kubeconfig file"`
	Namespace            string `usage:"namespace to watch" default:"cattle-fleet-system" env:"NAMESPACE"`
	MetricsAddr          string `name:"metrics-bind-address" default:":8081" usage:"The address the metric endpoint binds to."`
	DisableMetrics       bool   `name:"disable-metrics" usage:"Disable the metrics server."`
	EnableLeaderElection bool   `name:"leader-elect" default:"true" usage:"Enable leader election for controller manager. Enabling this will ensure there is only one active controller manager."`
	ShardID              string `usage:"only manage resources labeled with a specific shard ID" name:"shard-id"`
}

func App(zo *zap.Options) *cobra.Command {
	zopts = zo
	return command.Command(&HelmOperator{}, cobra.Command{
		Version: version.FriendlyVersion(),
		Use:     "helmops",
	})
}

// HelpFunc hides the global flag from the help output
func (c *HelmOperator) HelpFunc(cmd *cobra.Command, strings []string) {
	cmd.Parent().HelpFunc()(cmd, strings)
}

func (g *HelmOperator) PersistentPre(_ *cobra.Command, _ []string) error {
	if err := g.SetupDebug(); err != nil {
		return fmt.Errorf("failed to setup debug logging: %w", err)
	}
	zopts = g.OverrideZapOpts(zopts)

	return nil
}

func (g *HelmOperator) Run(cmd *cobra.Command, args []string) error {
	ctrl.SetLogger(zap.New(zap.UseFlagOptions(zopts)))
	ctx := clog.IntoContext(cmd.Context(), ctrl.Log.WithName("helmop-reconciler"))

	namespace := g.Namespace

	leaderOpts, err := command.NewLeaderElectionOptions()
	if err != nil {
		return err
	}

	var shardIDSuffix string
	if g.ShardID != "" {
		shardIDSuffix = fmt.Sprintf("-%s", g.ShardID)
	}

	mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{
		Scheme:                  scheme,
		Metrics:                 g.setupMetrics(),
		LeaderElection:          g.EnableLeaderElection,
		LeaderElectionID:        fmt.Sprintf("fleet-helmops-leader-election-shard%s", shardIDSuffix),
		LeaderElectionNamespace: namespace,
		LeaseDuration:           &leaderOpts.LeaseDuration,
		RenewDeadline:           &leaderOpts.RenewDeadline,
		RetryPeriod:             &leaderOpts.RetryPeriod,
	})

	if err != nil {
		return err
	}

	sched, err := quartz.NewStdScheduler()
	if err != nil {
		return fmt.Errorf("failed to create scheduler: %w", err)
	}

	var workers int
	if d := os.Getenv("HELMOPS_RECONCILER_WORKERS"); d != "" {
		w, err := strconv.Atoi(d)
		if err != nil {
			setupLog.Error(err, "failed to parse HELMOPS_RECONCILER_WORKERS", "value", d)
		}
		workers = w
	}

	helmOpReconciler := &reconciler.HelmOpReconciler{
		Client:    mgr.GetClient(),
		Scheme:    mgr.GetScheme(),
		Scheduler: sched,
		Workers:   workers,
		ShardID:   g.ShardID,
		Recorder:  mgr.GetEventRecorderFor(fmt.Sprintf("fleet-helmops%s", shardIDSuffix)),
	}

	helmOpStatusReconciler := &reconciler.HelmOpStatusReconciler{
		Client:  mgr.GetClient(),
		Scheme:  mgr.GetScheme(),
		ShardID: g.ShardID,
		Workers: workers,
	}

	configReconciler := &fcreconciler.ConfigReconciler{
		Client:          mgr.GetClient(),
		Scheme:          mgr.GetScheme(),
		SystemNamespace: namespace,
		ShardID:         g.ShardID,
	}

	if err := fcreconciler.Load(ctx, mgr.GetAPIReader(), namespace); err != nil {
		setupLog.Error(err, "failed to load config")
		return err
	}

	group, ctx := errgroup.WithContext(ctx)
	group.Go(func() error {
		setupLog.Info("starting config controller")
		if err = configReconciler.SetupWithManager(mgr); err != nil {
			return err
		}

		setupLog.Info("starting helmops controller")
		if err = helmOpReconciler.SetupWithManager(mgr); err != nil {
			return err
		}

		setupLog.Info("starting helmops status controller")
		if err = helmOpStatusReconciler.SetupWithManager(mgr); err != nil {
			return err
		}

		return mgr.Start(ctx)
	})

	helmOpReconciler.Scheduler.Start(ctx)

	return group.Wait()
}

func (g *HelmOperator) setupMetrics() metricsserver.Options {
	if g.DisableMetrics {
		return metricsserver.Options{BindAddress: "0"}
	}

	metricsAddr := g.MetricsAddr
	if d := os.Getenv("HELMOPS_METRICS_BIND_ADDRESS"); d != "" {
		metricsAddr = d
	}

	metricServerOpts := metricsserver.Options{BindAddress: metricsAddr}
	metrics.RegisterHelmOpsMetrics() // enable helmops related metrics

	return metricServerOpts
}



================================================
FILE: internal/cmd/controller/helmops/reconciler/helmop_controller.go
================================================
package reconciler

import (
	"context"
	"errors"
	"fmt"
	"strings"
	"time"

	"k8s.io/apimachinery/pkg/api/equality"
	k8serrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	errutil "k8s.io/apimachinery/pkg/util/errors"
	"k8s.io/client-go/tools/record"
	"k8s.io/client-go/util/retry"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/builder"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/predicate"

	"github.com/Masterminds/semver/v3"
	"github.com/go-logr/logr"
	"github.com/rancher/wrangler/v3/pkg/condition"
	"github.com/rancher/wrangler/v3/pkg/genericcondition"
	"github.com/reugn/go-quartz/quartz"

	"github.com/rancher/fleet/internal/bundlereader"
	fleetutil "github.com/rancher/fleet/internal/cmd/controller/errorutil"
	"github.com/rancher/fleet/internal/cmd/controller/finalize"
	ctrlquartz "github.com/rancher/fleet/internal/cmd/controller/quartz"
	"github.com/rancher/fleet/internal/metrics"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"
	"github.com/rancher/fleet/pkg/sharding"
)

// HelmOpReconciler reconciles a HelmOp resource to create and apply bundles for helm charts
type HelmOpReconciler struct {
	client.Client
	Scheme    *runtime.Scheme
	Scheduler quartz.Scheduler
	Workers   int
	ShardID   string
	Recorder  record.EventRecorder
}

func (r *HelmOpReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&fleet.HelmOp{},
			builder.WithPredicates(
				predicate.Or(
					// Note: These predicates prevent cache
					// syncPeriod from triggering reconcile, since
					// cache sync is an Update event.
					predicate.GenerationChangedPredicate{},
					predicate.AnnotationChangedPredicate{},
					predicate.LabelChangedPredicate{},
				),
			),
		).
		WithEventFilter(sharding.FilterByShardID(r.ShardID)).
		WithOptions(controller.Options{MaxConcurrentReconciles: r.Workers}).
		Complete(r)
}

// Reconcile is part of the main kubernetes reconciliation loop which aims to
// move the current state of the cluster closer to the desired state.
// The Reconcile function compares the state specified by
// the HelmOp object against the actual cluster state, and then
// performs operations to make the cluster state reflect the state specified by
// the user.
//
// For more details, check Reconcile and its Result here:
// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.15.0/pkg/reconcile
func (r *HelmOpReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx).WithName("HelmOp")
	helmop := &fleet.HelmOp{}

	if err := r.Get(ctx, req.NamespacedName, helmop); err != nil && !k8serrors.IsNotFound(err) {
		return ctrl.Result{}, err
	} else if k8serrors.IsNotFound(err) {
		return ctrl.Result{}, nil
	}

	if userID := helmop.Labels[fleet.CreatedByUserIDLabel]; userID != "" {
		logger = logger.WithValues("userID", userID)
	}

	ctx = log.IntoContext(ctx, logger)

	// Finalizer handling
	purgeBundlesFn := func() error {
		nsName := types.NamespacedName{Name: helmop.Name, Namespace: helmop.Namespace}
		if err := finalize.PurgeBundles(ctx, r.Client, nsName, fleet.HelmOpLabel); err != nil {
			return err
		}
		return nil
	}

	if !helmop.GetDeletionTimestamp().IsZero() {
		metrics.HelmCollector.Delete(helmop.Name, helmop.Namespace)

		if err := purgeBundlesFn(); err != nil {
			return ctrl.Result{}, err
		}
		if controllerutil.ContainsFinalizer(helmop, finalize.HelmOpFinalizer) {
			if err := deleteFinalizer(ctx, r.Client, helmop, finalize.HelmOpFinalizer); err != nil {
				return ctrl.Result{}, err
			}
		}

		err := r.deletePollingJob(*helmop)

		return ctrl.Result{}, err
	}

	if err := finalize.EnsureFinalizer(ctx, r.Client, helmop, finalize.HelmOpFinalizer); err != nil {
		return ctrl.Result{}, err
	}

	if err := validate(*helmop); err != nil {
		if delErr := r.deletePollingJob(*helmop); delErr != nil {
			err = errutil.NewAggregate([]error{err, delErr})
		}
		return ctrl.Result{}, updateErrorStatusHelm(ctx, r.Client, req.NamespacedName, helmop, err)
	}

	// Reconciling
	logger = logger.WithValues("generation", helmop.Generation, "chart", helmop.Spec.Helm.Chart)
	ctx = log.IntoContext(ctx, logger)

	logger.V(1).Info("Reconciling HelmOp")

	if _, err := r.createUpdateBundle(ctx, helmop); err != nil {
		return ctrl.Result{}, updateErrorStatusHelm(ctx, r.Client, req.NamespacedName, helmop, err)
	}

	// Running this logic after creating/updating the bundle to avoid scheduling a job if the bundle has not been created.
	if err := r.managePollingJob(logger, *helmop); err != nil {
		return ctrl.Result{}, updateErrorStatusHelm(ctx, r.Client, req.NamespacedName, helmop, err)
	}

	err := updateStatus(ctx, r.Client, req.NamespacedName, helmop, nil)
	if err != nil {
		logger.Error(err, "Reconcile failed final update to HelmOp status", "status", helmop.Status)

		return ctrl.Result{RequeueAfter: durations.DefaultRequeueAfter}, err
	}

	return ctrl.Result{}, err
}

func (r *HelmOpReconciler) createUpdateBundle(ctx context.Context, helmop *fleet.HelmOp) (*fleet.Bundle, error) {
	b := &fleet.Bundle{}
	nsName := types.NamespacedName{
		Name:      helmop.Name,
		Namespace: helmop.Namespace,
	}

	err := r.Get(ctx, nsName, b)
	if err != nil && !k8serrors.IsNotFound(err) {
		return nil, err
	}

	if err == nil && b.Spec.HelmOpOptions == nil {
		// A gitOps bundle with the same name exists; abort.
		return nil, fmt.Errorf("a non-helmops bundle already exists with name %s; aborting", helmop.Name)
	}

	// calculate the new representation of the helmop resource
	bundle := r.calculateBundle(helmop)

	if err := r.handleVersion(ctx, b, bundle, helmop); err != nil {
		return nil, err
	}

	updated := bundle.DeepCopy()
	_, err = controllerutil.CreateOrUpdate(ctx, r.Client, bundle, func() error {
		bundle.Spec = updated.Spec
		bundle.Annotations = updated.Annotations
		bundle.Labels = updated.Labels
		return nil
	})

	return bundle, err
}

// Calculates the bundle representation of the given HelmOp resource
func (r *HelmOpReconciler) calculateBundle(helmop *fleet.HelmOp) *fleet.Bundle {
	spec := helmop.Spec.BundleSpec

	// set target names
	for i, target := range spec.Targets {
		if target.Name == "" {
			spec.Targets[i].Name = fmt.Sprintf("target%03d", i)
		}
	}

	propagateHelmOpProperties(&spec)

	bundle := &fleet.Bundle{
		ObjectMeta: metav1.ObjectMeta{
			Namespace: helmop.Namespace,
			Name:      helmop.Name,
		},
		// We ensure the bundle and HelmOp spec are independent. This prevents versions constraints from being overwritten
		// in the HelmOp spec with actual versions resolved from Helm when the bundle version is updated.
		Spec: *spec.DeepCopy(),
	}
	if len(bundle.Spec.Targets) == 0 {
		bundle.Spec.Targets = []fleet.BundleTarget{
			{
				Name:         "default",
				ClusterGroup: "default",
			},
		}
	}

	// apply additional labels from spec
	for k, v := range helmop.Spec.Labels {
		if bundle.Labels == nil {
			bundle.Labels = make(map[string]string)
		}
		bundle.Labels[k] = v
	}
	bundle.Labels = labels.Merge(bundle.Labels, map[string]string{
		fleet.HelmOpLabel: helmop.Name,
	})

	// Setting the Resources to nil, the agent will download the helm chart
	bundle.Spec.Resources = nil
	// store the helm options (this will also enable the helm chart deployment in the bundle)
	bundle.Spec.HelmOpOptions = &fleet.BundleHelmOptions{
		SecretName:            helmop.Spec.HelmSecretName,
		InsecureSkipTLSverify: helmop.Spec.InsecureSkipTLSverify,
	}

	return bundle
}

// handleVersion validates the version configured on the provided HelmOp.
// In particular:
//   - it returns an error in case that version represents an invalid semver constraint.
//   - it handles empty or * versions, downloading the current version from the registry
//
// This is calculated in the upstream cluster so all downstream bundle deployments have the same
// version. (Potentially we could be gathering the version at the very moment it is being updated, for example)
func (r *HelmOpReconciler) handleVersion(ctx context.Context, oldBundle *fleet.Bundle, bundle *fleet.Bundle, helmop *fleet.HelmOp) error {
	if helmop == nil {
		return fmt.Errorf("the provided HelmOp is nil; this should not happen")
	}

	if !helmChartSpecChanged(oldBundle.Spec.Helm, bundle.Spec.Helm, helmop.Status.Version) {
		bundle.Spec.Helm.Version = helmop.Status.Version

		return nil
	}

	version, err := getChartVersion(ctx, r.Client, *helmop)
	if err != nil {
		return err
	}

	if usesPolling(*helmop) {
		return nil // Field updates will be run from the polling job, to prevent race conditions.
	}

	bundle.Spec.Helm.Version = version
	helmop.Status.Version = bundle.Spec.Helm.Version

	return nil
}

// deletePollingJob deletes the polling job scheduled for the provided helmop, if any, and returns any error that may
// have happened in the process.
// Returns a nil error if the job could be deleted or if none existed.
func (r *HelmOpReconciler) deletePollingJob(helmop fleet.HelmOp) error {
	jobKey := jobKey(helmop)
	if _, err := r.Scheduler.GetScheduledJob(jobKey); err == nil {
		if err = r.Scheduler.DeleteJob(jobKey); err != nil {
			return fmt.Errorf("failed to delete outdated polling job: %w", err)
		}
	} else if !errors.Is(err, quartz.ErrJobNotFound) {
		return fmt.Errorf("failed to get outdated polling job for deletion: %w", err)
	}

	return nil
}

// managePollingJob creates, updates or deletes a polling job for the provided HelmOp.
func (r *HelmOpReconciler) managePollingJob(logger logr.Logger, helmop fleet.HelmOp) error {
	if r.Scheduler == nil {
		logger.V(1).Info("Scheduler is not set; this should only happen in tests")
		return nil
	}

	jobKey := jobKey(helmop)
	scheduled, err := r.Scheduler.GetScheduledJob(jobKey)

	if err != nil && !errors.Is(err, quartz.ErrJobNotFound) {
		return fmt.Errorf("an unknown error occurred when looking for a polling job: %w", err)
	}

	if usesPolling(helmop) {
		scheduledJobDescription := ""

		if err == nil {
			if detail := scheduled.JobDetail(); detail != nil {
				scheduledJobDescription = detail.Job().Description()
			}
		}

		newJob := newHelmPollingJob(r.Client, r.Recorder, helmop.Namespace, helmop.Name, *helmop.Spec.Helm)
		currentTrigger := ctrlquartz.NewControllerTrigger(helmop.Spec.PollingInterval.Duration, 0)
		// A changing trigger description would indicate the polling interval has changed.
		// On the other hand, if the job description changes, this implies that one of the following fields has
		// been updated:
		// * Helm repo
		// * Helm chart
		// * Helm version constraint
		if errors.Is(err, quartz.ErrJobNotFound) ||
			scheduled.Trigger().Description() != currentTrigger.Description() ||
			scheduledJobDescription != newJob.Description() {
			err = r.Scheduler.ScheduleJob(
				quartz.NewJobDetailWithOptions(
					newJob,
					jobKey,
					&quartz.JobDetailOptions{
						Replace: true,
					},
				),
				currentTrigger,
			)

			if err != nil {
				return fmt.Errorf("failed to schedule polling job: %w", err)
			}

			logger.V(1).Info("Scheduled new polling job")
		}
	} else if err == nil {
		// A job still exists, but is no longer needed; delete it.
		if err = r.Scheduler.DeleteJob(jobKey); err != nil {
			return fmt.Errorf("failed to delete polling job: %w", err)
		}
	}

	return nil
}

// propagateHelmOpProperties propagates root Helm chart properties to the child targets.
// This is necessary, so we can download the correct chart version for each target.
func propagateHelmOpProperties(spec *fleet.BundleSpec) {
	// Check if there is anything to propagate
	if spec.Helm == nil {
		return
	}
	for _, target := range spec.Targets {
		if target.Helm == nil {
			// This target has nothing to propagate to
			continue
		}
		if target.Helm.Repo == "" {
			target.Helm.Repo = spec.Helm.Repo
		}
		if target.Helm.Chart == "" {
			target.Helm.Chart = spec.Helm.Chart
		}
		if target.Helm.Version == "" {
			target.Helm.Version = spec.Helm.Version
		}
	}
}

func deleteFinalizer[T client.Object](ctx context.Context, c client.Client, obj T, finalizer string) error {
	err := retry.RetryOnConflict(retry.DefaultRetry, func() error {
		nsName := types.NamespacedName{Name: obj.GetName(), Namespace: obj.GetNamespace()}
		if err := c.Get(ctx, nsName, obj); err != nil {
			return err
		}

		controllerutil.RemoveFinalizer(obj, finalizer)

		return c.Update(ctx, obj)
	})
	if client.IgnoreNotFound(err) != nil {
		return err
	}
	return nil
}

// usesPolling returns a boolean indicating whether polling makes sense for the provided helmop.
func usesPolling(helmop fleet.HelmOp) bool {
	if helmop.Spec.PollingInterval == nil || helmop.Spec.PollingInterval.Duration == 0 {
		return false
	}

	// Polling does not apply to tarball charts, where no index.yaml file nor set of tags is available to check for new
	// chart versions.
	if strings.HasSuffix(strings.ToLower(helmop.Spec.Helm.Chart), ".tgz") {
		return false
	}

	// we only need to poll if the version is set to a constraint on versions, which may resolve to
	// different available versions as the contents of the Helm repository evolves over time.
	_, err := semver.StrictNewVersion(helmop.Spec.Helm.Version)

	return err != nil
}

// updateStatus updates the status for the HelmOp resource. It retries on
// conflict. If the status was updated successfully, it also collects (as in
// updates) metrics for the HelmOp resource.
func updateStatus(ctx context.Context, c client.Client, req types.NamespacedName, helmop *fleet.HelmOp, orgErr error) error {
	if helmop == nil {
		return fmt.Errorf("the HelmOp provided for a status update is nil; this should not happen")
	}

	objToPatchFrom := helmop.DeepCopy()

	return retry.RetryOnConflict(retry.DefaultRetry, func() error {
		t := &fleet.HelmOp{}
		if err := c.Get(ctx, req, t); err != nil {
			return err
		}

		// selectively update the status fields this reconciler is responsible for
		if t.Status.Version != objToPatchFrom.Status.Version && objToPatchFrom.Status.Version != "" {
			t.Status.Version = objToPatchFrom.Status.Version
			// (#3883)
			// If orig.Status.Version is, for example, equal to 1.0.0, when
			// assigning the Version to t.Status.Version both will be 1.0.0.
			// When calculating the Patch data, Status.Version will be ignored because
			// both objects have the same value.
			// The following cleanup prevents that so Status.Version is taken into
			// account when calculating the patch data.
			objToPatchFrom.Status.Version = ""
		}

		// only keep the Ready condition from live status, it's calculated by the status reconciler
		conds := []genericcondition.GenericCondition{}
		for _, c := range t.Status.Conditions {
			if c.Type == "Ready" {
				conds = append(conds, c)
				break
			}
		}
		for _, c := range objToPatchFrom.Status.Conditions {
			if c.Type == "Ready" {
				continue
			}
			conds = append(conds, c)
		}
		t.Status.Conditions = conds

		setAcceptedConditionHelm(&t.Status, orgErr)

		statusPatch := client.MergeFrom(objToPatchFrom)
		if patchData, err := statusPatch.Data(t); err == nil && string(patchData) == "{}" {
			metrics.HelmCollector.Collect(ctx, t)
			// skip update if patch is empty
			return nil
		}

		if err := c.Status().Patch(ctx, t, statusPatch); err != nil {
			return err
		}

		metrics.HelmCollector.Collect(ctx, t)

		return nil
	})
}

// updateErrorStatusHelm sets the condition in the status and tries to update the resource
func updateErrorStatusHelm(ctx context.Context, c client.Client, req types.NamespacedName, helmOp *fleet.HelmOp, orgErr error) error {
	if statusErr := updateStatus(ctx, c, req, helmOp, orgErr); statusErr != nil {
		merr := []error{orgErr, fmt.Errorf("failed to update the status: %w", statusErr)}
		return errutil.NewAggregate(merr)
	}
	return orgErr
}

// setAcceptedCondition sets the condition and updates the timestamp, if the condition changed
func setAcceptedConditionHelm(status *fleet.HelmOpStatus, err error) {
	cond := condition.Cond(fleet.HelmOpAcceptedCondition)
	origStatus := status.DeepCopy()
	cond.SetError(status, "", fleetutil.IgnoreConflict(err))
	if !equality.Semantic.DeepEqual(origStatus, status) {
		cond.LastUpdated(status, time.Now().UTC().Format(time.RFC3339))
	}
}

func helmChartSpecChanged(o *fleet.HelmOptions, n *fleet.HelmOptions, statusVersion string) bool {
	if o == nil {
		// still not set
		return true
	}
	if o.Repo != n.Repo {
		return true
	}
	if o.Chart != n.Chart {
		return true
	}
	// check also against statusVersion in case that Reconcile is called
	// before the status subresource has been fully updated in the cluster (and the cache)
	if o.Version != n.Version && statusVersion == o.Version {
		return true
	}
	return false
}

// getChartVersion fetches the latest chart version from the Helm registry referenced by helmop, and returns it.
// If this fails, it returns an empty version along with an error.
func getChartVersion(ctx context.Context, c client.Client, helmop fleet.HelmOp) (string, error) {
	auth := bundlereader.Auth{}
	if helmop.Spec.HelmSecretName != "" {
		req := types.NamespacedName{Namespace: helmop.Namespace, Name: helmop.Spec.HelmSecretName}
		var err error
		auth, err = bundlereader.ReadHelmAuthFromSecret(ctx, c, req)
		if err != nil {
			return "", fmt.Errorf("could not read Helm auth from secret: %w", err)
		}
	}
	auth.InsecureSkipVerify = helmop.Spec.InsecureSkipTLSverify

	version, err := bundlereader.ChartVersion(ctx, *helmop.Spec.Helm, auth)
	if err != nil {
		return "", fmt.Errorf("could not get a chart version: %w", err)
	}

	return version, nil
}

func jobKey(h fleet.HelmOp) *quartz.JobKey {
	return quartz.NewJobKey(string(h.UID))
}

// validate checks combinations of Chart, Repo and Version fields in h's Helm options.
// It returns an error if those options are nil, or if they don't fall under any of these categories,
// as per https://helm.sh/docs/helm/helm_install/ :
// * tarball URL in Chart, empty Repo, empty Version
// * OCI reference in the Repo field, empty Chart, optional Version
// * non-empty Repo URL, non-empty Chart name, optional Version
func validate(h fleet.HelmOp) error {
	if h.Spec.Helm == nil {
		return fmt.Errorf("helm options are empty in the HelmOp's spec")
	}

	fail := func(msg string) error {
		return fmt.Errorf("helm options invalid: %s", msg)
	}

	switch {
	case strings.HasSuffix(strings.ToLower(h.Spec.Helm.Chart), ".tgz"):
		if len(h.Spec.Helm.Repo) > 0 {
			return fail("tarball chart with a non-empty repo field")
		}

		if len(h.Spec.Helm.Version) > 0 {
			return fail("tarball chart with a non-empty version field")
		}
	case strings.HasPrefix(strings.ToLower(h.Spec.Helm.Repo), "oci://"):
		if len(h.Spec.Helm.Chart) > 0 {
			return fail("OCI repository with a non-empty chart field")
		}
	default: // Expecting full reference: chart + repo + optional version
		if len(h.Spec.Helm.Chart) == 0 {
			return fail("non-OCI repository with an empty chart field")
		}

		if len(h.Spec.Helm.Repo) == 0 {
			return fail("non-tarball chart with an empty repo field")
		}
	}

	return nil
}



================================================
FILE: internal/cmd/controller/helmops/reconciler/helmop_controller_test.go
================================================
//go:generate mockgen --build_flags=--mod=mod -destination=../../../../mocks/client_mock.go -package=mocks sigs.k8s.io/controller-runtime/pkg/client Client,SubResourceWriter
//go:generate mockgen --build_flags=--mod=mod -destination=../../../../mocks/scheduler_mock.go -package=mocks github.com/reugn/go-quartz/quartz Scheduler,ScheduledJob

package reconciler

import (
	"context"
	"errors"
	"fmt"
	"net/http"
	"net/http/httptest"
	"reflect"
	"regexp"
	"strings"
	"testing"
	"time"

	"go.uber.org/mock/gomock"

	"github.com/rancher/fleet/internal/cmd/controller/finalize"
	ctrlquartz "github.com/rancher/fleet/internal/cmd/controller/quartz"
	"github.com/rancher/fleet/internal/mocks"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/wrangler/v3/pkg/genericcondition"
	"github.com/reugn/go-quartz/quartz"

	batchv1 "k8s.io/api/batch/v1"
	k8serrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/apimachinery/pkg/types"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	ctrl "sigs.k8s.io/controller-runtime"
	crclient "sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
)

const (
	helmRepoIndex = `apiVersion: v1
entries:
  alpine:
    - created: 2016-10-06T16:23:20.499814565-06:00
      description: Deploy a basic Alpine Linux pod
      digest: 99c76e403d752c84ead610644d4b1c2f2b453a74b921f422b9dcb8a7c8b559cd
      home: https://helm.sh/helm
      name: alpine
      sources:
      - https://github.com/helm/helm
      urls:
      - https://technosophos.github.io/tscharts/alpine-0.2.0.tgz
      version: 0.2.0
    - created: 2016-10-06T16:23:20.499543808-06:00
      description: Deploy a basic Alpine Linux pod
      digest: 515c58e5f79d8b2913a10cb400ebb6fa9c77fe813287afbacf1a0b897cd78727
      home: https://helm.sh/helm
      name: alpine
      sources:
      - https://github.com/helm/helm
      urls:
      - https://technosophos.github.io/tscharts/alpine-0.1.0.tgz
      version: 0.1.0
  nginx:
    - created: 2016-10-06T16:23:20.499543808-06:00
      description: Create a basic nginx HTTP server
      digest: aaff4545f79d8b2913a10cb400ebb6fa9c77fe813287afbacf1a0b897cdffffff
      home: https://helm.sh/helm
      name: nginx
      sources:
      - https://github.com/helm/charts
      urls:
      - https://technosophos.github.io/tscharts/nginx-0.1.0.tgz
      version: 0.1.0
generated: 2016-10-06T16:23:20.499029981-06:00`
)

func createHelmServer() *httptest.Server {
	return httptest.NewTLSServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusOK)
		fmt.Fprint(w, helmRepoIndex)
	}))
}

func createHelmServerWithErrorHTML() *httptest.Server {
	return httptest.NewTLSServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusNotFound)
		fmt.Fprint(w, `<!DOCTYPE html><html><head><meta charset=\"utf-8\"><body><p>Some irrelevant content here</p></body></html>`)
	}))
}

func getCondition(helmop *fleet.HelmOp, condType string) (genericcondition.GenericCondition, bool) {
	for _, cond := range helmop.Status.Conditions {
		if cond.Type == condType {
			return cond, true
		}
	}
	return genericcondition.GenericCondition{}, false
}

func TestReconcile_Validate(t *testing.T) {
	cases := []struct {
		name   string
		helmop fleet.HelmOp
		err    string
	}{
		{
			name: "error if Helm spec is empty",
			helmop: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{},
			},
			err: "helm options are empty",
		},
		{
			name: "error if tarball chart with non-empty repo",
			helmop: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Chart: "https://foo.bar/baz.tgz",
								Repo:  "non-empty",
							},
						},
					},
				},
			},
			err: "tarball chart with a non-empty repo field",
		},
		{
			name: "error if tarball chart with non-empty nor wildcard version",
			helmop: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Chart:   "https://foo.bar/baz.tgz",
								Version: "1.2.3",
							},
						},
					},
				},
			},
			err: "tarball chart with a non-empty version field",
		},
		{
			name: "error if tarball chart with 'latest' version",
			helmop: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Chart:   "https://foo.bar/baz.tgz",
								Version: "latest",
							},
						},
					},
				},
			},
			err: "tarball chart with a non-empty version field",
		},
		{
			name: "error if tarball chart with wildcard version",
			helmop: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Chart:   "https://foo.bar/baz.tgz",
								Version: "*",
							},
						},
					},
				},
			},
			err: "tarball chart with a non-empty version field",
		},
		{
			name: "error if OCI repo with non-empty chart",
			helmop: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Chart:   "hello",
								Repo:    "oci://foo/bar/baz",
								Version: "*",
							},
						},
					},
				},
			},
			err: "OCI repository with a non-empty chart field",
		},
		{
			name: "error if non-OCI repo with empty chart",
			helmop: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo: "https://foo/bar/baz",
							},
						},
					},
				},
			},
			err: "non-OCI repository with an empty chart field",
		},
		{
			name: "error if non-tarball chart with empty repo",
			helmop: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Chart: "foo",
							},
						},
					},
				},
			},
			err: "non-tarball chart with an empty repo field",
		},
	}

	for _, c := range cases {
		t.Run(c.name, func(t *testing.T) {
			mockCtrl := gomock.NewController(t)
			defer mockCtrl.Finish()
			scheme := runtime.NewScheme()
			utilruntime.Must(batchv1.AddToScheme(scheme))

			namespacedName := types.NamespacedName{Name: c.helmop.Name, Namespace: c.helmop.Namespace}
			client := mocks.NewMockK8sClient(mockCtrl)
			client.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Times(1).DoAndReturn(
				func(ctx context.Context, req types.NamespacedName, fh *fleet.HelmOp, opts ...interface{}) error {
					fh.Name = c.helmop.Name
					fh.Namespace = c.helmop.Namespace
					fh.Spec = c.helmop.Spec

					controllerutil.AddFinalizer(fh, finalize.HelmOpFinalizer)
					return nil
				},
			)

			client.EXPECT().Get(gomock.Any(), namespacedName, matchesBundle(c.helmop.Name, c.helmop.Namespace), gomock.Any()).
				DoAndReturn(
					func(ctx context.Context, req types.NamespacedName, bundle *fleet.Bundle, opts ...interface{}) error {
						bundle.ObjectMeta = metav1.ObjectMeta{
							Name:      c.helmop.Name,
							Namespace: c.helmop.Namespace,
						}
						bundle.Spec.HelmOpOptions = &fleet.BundleHelmOptions{
							SecretName: "foo", // prevent collision errors; the value does not matter.
						}

						return nil
					},
				).AnyTimes()

			// expected from addFinalizer
			client.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any()).AnyTimes().Return(nil)
			client.EXPECT().Update(gomock.Any(), gomock.Any()).AnyTimes().Return(nil)

			statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
			client.EXPECT().Status().Return(statusClient)
			statusClient.EXPECT().Patch(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any())

			scheduler := mocks.NewMockScheduler(mockCtrl)

			if len(c.err) > 0 {
				job := mocks.NewMockScheduledJob(mockCtrl)
				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(job, nil)
				scheduler.EXPECT().DeleteJob(gomock.Any()).Return(nil)
			}

			r := HelmOpReconciler{
				Client:    client,
				Scheme:    scheme,
				Scheduler: scheduler,
			}

			ctx := context.TODO()

			_, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
			if err == nil || !strings.Contains(err.Error(), c.err) {
				t.Errorf("unexpected error:\r\n\twanted\r\n\t\t%q in error message,\r\n\tgot \r\n\t\t%v", c.err, err)
			}

		})
	}
}

func TestReconcile_ErrorCreatingBundleIsShownInStatus(t *testing.T) {
	t.Run("propagating error seen when getting the bundle", func(t *testing.T) {
		mockCtrl := gomock.NewController(t)
		defer mockCtrl.Finish()
		scheme := runtime.NewScheme()
		utilruntime.Must(batchv1.AddToScheme(scheme))
		helmop := fleet.HelmOp{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "helmop",
				Namespace: "default",
			},
		}
		namespacedName := types.NamespacedName{Name: helmop.Name, Namespace: helmop.Namespace}
		client := mocks.NewMockK8sClient(mockCtrl)
		client.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Times(1).DoAndReturn(
			func(ctx context.Context, req types.NamespacedName, fh *fleet.HelmOp, opts ...interface{}) error {
				fh.Name = helmop.Name
				fh.Namespace = helmop.Namespace
				fh.Spec.Helm = &fleet.HelmOptions{
					Chart: "chart.tgz",
				}
				fh.Status = fleet.HelmOpStatus{}

				controllerutil.AddFinalizer(fh, finalize.HelmOpFinalizer)
				return nil
			},
		)

		client.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Times(1).DoAndReturn(
			func(ctx context.Context, req types.NamespacedName, bundle *fleet.Bundle, opts ...interface{}) error {
				return fmt.Errorf("this is a test error")
			},
		)

		client.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Times(1).DoAndReturn(
			func(ctx context.Context, req types.NamespacedName, bundle *fleet.HelmOp, opts ...interface{}) error {
				return nil
			},
		)

		statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
		client.EXPECT().Status().Return(statusClient).Times(1)
		statusClient.EXPECT().Patch(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(
			func(ctx context.Context, helmop *fleet.HelmOp, patch crclient.Patch, opts ...interface{}) {
				c, found := getCondition(helmop, fleet.HelmOpAcceptedCondition)
				if !found {
					t.Errorf("expecting to find the %s condition and could not find it.", fleet.HelmOpAcceptedCondition)
				}
				if c.Message != "this is a test error" {
					t.Errorf("expecting message [this is a test error] in condition, got [%s]", c.Message)
				}
			},
		).Times(1)

		r := HelmOpReconciler{
			Client: client,
			Scheme: scheme,
		}

		ctx := context.TODO()

		res, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
		if err == nil {
			t.Errorf("expecting error, got nil")
		}
		if err.Error() != "this is a test error" {
			t.Errorf("expecting error: [this is a test error], got %v", err.Error())
		}
		if res.RequeueAfter != 0 {
			t.Errorf("expecting no requeue when there's an error, but got RequeueAfter: %v", res.RequeueAfter)
		}
	})

	t.Run("propagating error from Helm registry without HTML response body", func(t *testing.T) {
		svr := createHelmServerWithErrorHTML()
		defer svr.Close()

		mockCtrl := gomock.NewController(t)
		defer mockCtrl.Finish()
		scheme := runtime.NewScheme()
		utilruntime.Must(batchv1.AddToScheme(scheme))
		helmop := fleet.HelmOp{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "helmop",
				Namespace: "default",
			},
			Spec: fleet.HelmOpSpec{
				BundleSpec: fleet.BundleSpec{
					BundleDeploymentOptions: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							Repo:    svr.URL,
							Chart:   "alpine",
							Version: "0.1.0", // static version
						},
					},
				},
				InsecureSkipTLSverify: true,
			},
		}

		namespacedName := types.NamespacedName{Name: helmop.Name, Namespace: helmop.Namespace}
		client := mocks.NewMockK8sClient(mockCtrl)
		client.EXPECT().Get(gomock.Any(), gomock.Any(), OfType(&fleet.HelmOp{}), gomock.Any()).Times(1).DoAndReturn(
			func(ctx context.Context, req types.NamespacedName, fh *fleet.HelmOp, opts ...interface{}) error {
				fh.Name = helmop.Name
				fh.Namespace = helmop.Namespace
				fh.Spec = helmop.Spec
				fh.Status = fleet.HelmOpStatus{}

				controllerutil.AddFinalizer(fh, finalize.HelmOpFinalizer)
				return nil
			},
		)

		client.EXPECT().Get(gomock.Any(), gomock.Any(), OfType(&fleet.Bundle{}), gomock.Any()).AnyTimes().DoAndReturn(
			func(ctx context.Context, req types.NamespacedName, bundle *fleet.Bundle, opts ...interface{}) error {
				bundle.Spec.HelmOpOptions = &fleet.BundleHelmOptions{}
				return nil
			},
		)

		client.EXPECT().Get(gomock.Any(), gomock.Any(), OfType(&fleet.HelmOp{}), gomock.Any()).AnyTimes().DoAndReturn(
			func(ctx context.Context, req types.NamespacedName, helmOp *fleet.HelmOp, opts ...interface{}) error {
				return nil
			},
		)

		statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
		client.EXPECT().Status().Return(statusClient).Times(1)
		statusClient.EXPECT().Patch(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(
			func(ctx context.Context, helmop *fleet.HelmOp, patch crclient.Patch, opts ...interface{}) {
				c, found := getCondition(helmop, fleet.HelmOpAcceptedCondition)
				if !found {
					t.Errorf("expecting to find the %s condition and could not find it.", fleet.HelmOpAcceptedCondition)
				}
				if !strings.Contains(c.Message, "404") {
					t.Errorf("expecting message to contain [404] in condition, got [%s]", c.Message)
				}
			},
		).Times(1)

		r := HelmOpReconciler{
			Client: client,
			Scheme: scheme,
		}

		ctx := context.TODO()

		res, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
		if err == nil {
			t.Errorf("expecting error, got nil")
		}

		errRegex := "could not get a chart version.*error code: 404"
		match, rErr := regexp.Match(errRegex, []byte(err.Error()))
		if rErr != nil {
			t.Errorf("something went wrong when compiling the regex: %v", rErr)
		}
		if !match {
			t.Errorf("expecting error matching %q, got %v", errRegex, err)
		}
		if res.RequeueAfter != 0 {
			t.Errorf("expecting no requeue when there's an error, but got RequeueAfter: %v", res.RequeueAfter)
		}
	})
}

// Validates that the HelmOps reconciler will not create a bundle if another bundle exists with the same name, for
// instance a gitOps bundle.
func TestReconcile_ErrorCreatingBundleIfBundleWithSameNameExists(t *testing.T) {
	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	scheme := runtime.NewScheme()
	utilruntime.Must(batchv1.AddToScheme(scheme))
	helmop := fleet.HelmOp{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "my-workload",
			Namespace: "default",
		},
	}
	namespacedName := types.NamespacedName{Name: helmop.Name, Namespace: helmop.Namespace}
	client := mocks.NewMockK8sClient(mockCtrl)
	client.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Times(1).DoAndReturn(
		func(ctx context.Context, req types.NamespacedName, fh *fleet.HelmOp, opts ...interface{}) error {
			fh.Name = helmop.Name
			fh.Namespace = helmop.Namespace
			fh.Spec.Helm = &fleet.HelmOptions{
				Chart: "chart.tgz",
			}
			fh.Status = fleet.HelmOpStatus{}

			controllerutil.AddFinalizer(fh, finalize.HelmOpFinalizer)
			return nil
		},
	)

	client.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any(), fleet.Bundle{}).AnyTimes().DoAndReturn(
		func(ctx context.Context, req types.NamespacedName, bundle *fleet.Bundle, opts ...interface{}) error {
			bundle.ObjectMeta = metav1.ObjectMeta{
				Name:      "my-workload",
				Namespace: "default",
			}

			return nil
		},
	)

	client.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any()).AnyTimes().Return(nil)

	client.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any(), fleet.HelmOp{}).AnyTimes().DoAndReturn(
		func(ctx context.Context, req types.NamespacedName, bundle *fleet.HelmOp, opts ...interface{}) error {
			return nil
		},
	)

	expectedErrorMsg := "non-helmops bundle already exists"
	statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
	client.EXPECT().Status().Return(statusClient).Times(1)
	statusClient.EXPECT().Patch(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(
		func(ctx context.Context, helmop *fleet.HelmOp, patch crclient.Patch, opts ...interface{}) {
			c, found := getCondition(helmop, fleet.HelmOpAcceptedCondition)
			if !found {
				t.Errorf("expecting to find the %s condition and could not find it.", fleet.HelmOpAcceptedCondition)
			}
			if !strings.Contains(c.Message, expectedErrorMsg) {
				t.Errorf("expecting message [%s] in condition, got [%s]", expectedErrorMsg, c.Message)
			}
		},
	).Times(1)

	r := HelmOpReconciler{
		Client: client,
		Scheme: scheme,
	}

	ctx := context.TODO()

	res, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
	if err == nil {
		t.Errorf("expecting error, got nil")
	}

	if err != nil && !strings.Contains(err.Error(), expectedErrorMsg) {
		t.Errorf("expecting error: [%s], got %v", expectedErrorMsg, err.Error())
	}

	if res.RequeueAfter != 0 {
		t.Errorf("expecting no requeue when there's an error, but got RequeueAfter: %v", res.RequeueAfter)
	}
}

func TestReconcile_CreatesBundleAndUpdatesStatus(t *testing.T) {
	svr1 := createHelmServer()
	defer svr1.Close()

	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	scheme := runtime.NewScheme()
	utilruntime.Must(batchv1.AddToScheme(scheme))
	helmop := fleet.HelmOp{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "helmop",
			Namespace: "default",
		},
	}
	namespacedName := types.NamespacedName{Name: helmop.Name, Namespace: helmop.Namespace}
	client := mocks.NewMockK8sClient(mockCtrl)
	client.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Times(1).DoAndReturn(
		func(ctx context.Context, req types.NamespacedName, fh *fleet.HelmOp, opts ...interface{}) error {
			fh.Name = helmop.Name
			fh.Namespace = helmop.Namespace
			fh.Spec.InsecureSkipTLSverify = true
			fh.Spec.Helm = &fleet.HelmOptions{
				Repo:    svr1.URL,
				Chart:   "alpine",
				Version: "0.x.x",
			}
			fh.Status = fleet.HelmOpStatus{}

			controllerutil.AddFinalizer(fh, finalize.HelmOpFinalizer)
			return nil
		},
	)

	client.EXPECT().Get(gomock.Any(), namespacedName, gomock.Any(), gomock.Any()).Times(2).
		DoAndReturn(
			func(ctx context.Context, req types.NamespacedName, bundle *fleet.Bundle, opts ...interface{}) error {
				return k8serrors.NewNotFound(schema.GroupResource{}, "Not found")
			},
		)

	client.EXPECT().Create(gomock.Any(), matchesBundle(helmop.Name, helmop.Namespace), gomock.Any()).
		DoAndReturn(
			func(ctx context.Context, bundle *fleet.Bundle, opts ...interface{}) error {
				return nil
			},
		)

	client.EXPECT().Get(gomock.Any(), gomock.Any(), &fleet.HelmOp{}, gomock.Any()).Times(1).DoAndReturn(
		func(ctx context.Context, req types.NamespacedName, _ *fleet.HelmOp, opts ...interface{}) error {
			return nil
		},
	)

	statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
	client.EXPECT().Status().Return(statusClient).Times(1)
	statusClient.EXPECT().Patch(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(
		func(ctx context.Context, helmop *fleet.HelmOp, patch crclient.Patch, opts ...interface{}) {
			// version in status should be the one in the spec
			if helmop.Status.Version != "0.2.0" {
				t.Errorf("expecting Status.Version == 0.2.0, got %q", helmop.Status.Version)
			}
		},
	).Times(1)

	// Note: this test case does not try to fetch the actual version from the Helm registry, because the repo field
	// is empty.

	r := HelmOpReconciler{
		Client: client,
		Scheme: scheme,
	}

	ctx := context.TODO()

	res, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
	if err != nil {
		t.Errorf("found unexpected error %v", err)
	}
	if res.RequeueAfter != 0 {
		t.Errorf("expecting no requeue on successful reconciliation, but got RequeueAfter: %v", res.RequeueAfter)
	}
}

func TestReconcile_ManagePollingJobs(t *testing.T) {
	svr1 := createHelmServer()
	defer svr1.Close()

	svr2 := createHelmServer()
	defer svr2.Close()

	cases := []struct {
		name                   string
		helmOp                 fleet.HelmOp
		expectedSchedulerCalls func(*gomock.Controller, *mocks.MockScheduler, fleet.HelmOp)
		expectedError          string
	}{
		{
			name: "does not poll if the version is static",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					InsecureSkipTLSverify: true,
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "alpine",
								Version: "0.1.0", // static version
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(_ *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(nil, quartz.ErrJobNotFound)

				// No job expected to be created nor deleted
			},
		},
		{
			name: "deletes existing polling job when the version is static",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					InsecureSkipTLSverify: true,
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "alpine",
								Version: "0.1.0", // static version
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(ctrl *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				job := mocks.NewMockScheduledJob(ctrl)
				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(job, nil)
				scheduler.EXPECT().DeleteJob(gomock.Any()).Return(nil)
			},
		},
		{
			name: "does not poll if the polling interval is not set",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					InsecureSkipTLSverify: true,
					// polling interval not set
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "alpine",
								Version: "0.x.x",
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(_ *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(nil, quartz.ErrJobNotFound)

				// No job expected to be created nor deleted
			},
		},
		{
			name: "deletes existing polling job when the polling interval is not set",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					InsecureSkipTLSverify: true,
					// polling interval not set
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "alpine",
								Version: "0.x.x",
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(ctrl *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				job := mocks.NewMockScheduledJob(ctrl)
				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(job, nil)
				scheduler.EXPECT().DeleteJob(gomock.Any()).Return(nil)
			},
		},
		{
			name: "does not poll if the polling interval is set to 0",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					InsecureSkipTLSverify: true,
					PollingInterval:       &metav1.Duration{Duration: 0 * time.Minute},
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "alpine",
								Version: "0.x.x",
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(_ *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(nil, quartz.ErrJobNotFound)

				// No job expected to be created nor deleted
			},
		},
		{
			name: "deletes existing polling job when the polling interval is set to 0",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					InsecureSkipTLSverify: true,
					PollingInterval:       &metav1.Duration{Duration: 0 * time.Minute},
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "alpine",
								Version: "0.x.x",
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(ctrl *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				job := mocks.NewMockScheduledJob(ctrl)
				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(job, nil)
				scheduler.EXPECT().DeleteJob(gomock.Any()).Return(nil)
			},
		},
		{
			name: "does not poll when referencing a tarball chart",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					PollingInterval:       &metav1.Duration{Duration: 1 * time.Minute},
					InsecureSkipTLSverify: true,
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Chart: fmt.Sprintf("%s/alpine/alpine-0.1.0.tgz", svr1.URL),
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(_ *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(nil, quartz.ErrJobNotFound)

				// No job expected to be created nor deleted
			},
		},
		{
			name: "deletes existing polling job when referencing a tarball chart",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					PollingInterval:       &metav1.Duration{Duration: 1 * time.Minute},
					InsecureSkipTLSverify: true,
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Chart: fmt.Sprintf("%s/alpine/alpine-0.1.0.tgz", svr1.URL),
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(ctrl *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				job := mocks.NewMockScheduledJob(ctrl)
				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(job, nil)
				scheduler.EXPECT().DeleteJob(gomock.Any()).Return(nil)
			},
		},
		{
			name: "returns an error when failing to delete a polling job",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					InsecureSkipTLSverify: true,
					PollingInterval:       &metav1.Duration{Duration: 0 * time.Minute},
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "alpine",
								Version: "0.x.x",
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(ctrl *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				job := mocks.NewMockScheduledJob(ctrl)
				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(job, nil)
				scheduler.EXPECT().DeleteJob(gomock.Any()).Return(errors.New("something happened!"))
			},
			expectedError: "something happened!",
		},
		{
			name: "returns an error when failing to schedule a new job replacing an existing one",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
					UID:       "foo",
				},
				Spec: fleet.HelmOpSpec{
					InsecureSkipTLSverify: true,
					PollingInterval:       &metav1.Duration{Duration: 1 * time.Minute},
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "alpine",
								Version: "0.x.x",
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(ctrl *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				trigger := quartz.NewSimpleTrigger(2 * helmop.Spec.PollingInterval.Duration)

				job := mocks.NewMockScheduledJob(ctrl)
				job.EXPECT().Trigger().Return(trigger)
				job.EXPECT().JobDetail().Return(nil)

				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(job, nil)
				scheduler.EXPECT().ScheduleJob(matchesJobDetailReplace(helmop), gomock.Any()).Return(errors.New("something happened!"))
			},
			expectedError: "something happened!",
		},
		{
			name: "returns an error when failing to schedule a new job with no existing one",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
					UID:       "foo",
				},
				Spec: fleet.HelmOpSpec{
					InsecureSkipTLSverify: true,
					PollingInterval:       &metav1.Duration{Duration: 1 * time.Minute},
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "alpine",
								Version: "0.x.x",
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(ctrl *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(nil, quartz.ErrJobNotFound)
				scheduler.EXPECT().ScheduleJob(matchesJobDetailReplace(helmop), gomock.Any()).Return(errors.New("something happened!"))
			},
			expectedError: "something happened!",
		},
		{
			name: "creates a polling job if all conditions are met",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
					UID:       "foo",
				},
				Spec: fleet.HelmOpSpec{
					InsecureSkipTLSverify: true,
					PollingInterval:       &metav1.Duration{Duration: 1 * time.Minute},
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "alpine",
								Version: "0.x.x",
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(_ *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(nil, quartz.ErrJobNotFound)
				scheduler.EXPECT().ScheduleJob(matchesJobDetailReplace(helmop), gomock.Any()).Return(nil)
			},
		},
		{
			name: "does not create a polling job if the same one already exists",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
				},
				Spec: fleet.HelmOpSpec{
					PollingInterval:       &metav1.Duration{Duration: 1 * time.Minute},
					InsecureSkipTLSverify: true,
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "alpine",
								Version: "0.x.x",
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(ctrl *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				trigger := ctrlquartz.NewControllerTrigger(helmop.Spec.PollingInterval.Duration, 0)
				job := newHelmPollingJob(nil, nil, helmop.Namespace, helmop.Name, *helmop.Spec.Helm)

				detail := quartz.NewJobDetail(job, nil)

				scheduled := mocks.NewMockScheduledJob(ctrl)
				scheduled.EXPECT().Trigger().Return(trigger)
				scheduled.EXPECT().JobDetail().Return(detail)

				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(scheduled, nil)
			},
		},
		{
			name: "creates a polling job if the version constraint has changed",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
					UID:       "foo",
				},
				Spec: fleet.HelmOpSpec{
					PollingInterval:       &metav1.Duration{Duration: 1 * time.Minute},
					InsecureSkipTLSverify: true,
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "alpine",
								Version: "0.2.x",
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(ctrl *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				oldHelmSpec := helmop.Spec.Helm.DeepCopy()
				oldHelmSpec.Version = "0.1.x"

				trigger := ctrlquartz.NewControllerTrigger(helmop.Spec.PollingInterval.Duration, 0)
				job := newHelmPollingJob(nil, nil, helmop.Namespace, helmop.Name, *oldHelmSpec)

				detail := quartz.NewJobDetail(job, nil)

				scheduled := mocks.NewMockScheduledJob(ctrl)
				scheduled.EXPECT().Trigger().Return(trigger)
				scheduled.EXPECT().JobDetail().Return(detail)

				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(scheduled, nil)
				scheduler.EXPECT().ScheduleJob(matchesJobDetailReplace(helmop), gomock.Any()).Return(nil)
			},
		},
		{
			name: "creates a polling job if the Helm repo has changed",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
					UID:       "foo",
				},
				Spec: fleet.HelmOpSpec{
					PollingInterval:       &metav1.Duration{Duration: 1 * time.Minute},
					InsecureSkipTLSverify: true,
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "alpine",
								Version: "0.2.x",
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(ctrl *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				oldHelmSpec := helmop.Spec.Helm.DeepCopy()
				oldHelmSpec.Repo = svr2.URL

				trigger := ctrlquartz.NewControllerTrigger(helmop.Spec.PollingInterval.Duration, 0)
				job := newHelmPollingJob(nil, nil, helmop.Namespace, helmop.Name, *oldHelmSpec)

				detail := quartz.NewJobDetail(job, nil)

				scheduled := mocks.NewMockScheduledJob(ctrl)
				scheduled.EXPECT().Trigger().Return(trigger)
				scheduled.EXPECT().JobDetail().Return(detail)

				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(scheduled, nil)
				scheduler.EXPECT().ScheduleJob(matchesJobDetailReplace(helmop), gomock.Any()).Return(nil)
			},
		},
		{
			name: "creates a polling job if the Helm chart has changed",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
					UID:       "foo",
				},
				Spec: fleet.HelmOpSpec{
					PollingInterval:       &metav1.Duration{Duration: 1 * time.Minute},
					InsecureSkipTLSverify: true,
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "nginx",
								Version: "0.1.x",
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(ctrl *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				oldHelmSpec := helmop.Spec.Helm.DeepCopy()
				oldHelmSpec.Chart = "alpine"

				trigger := ctrlquartz.NewControllerTrigger(helmop.Spec.PollingInterval.Duration, 0)
				job := newHelmPollingJob(nil, nil, helmop.Namespace, helmop.Name, *oldHelmSpec)

				detail := quartz.NewJobDetail(job, nil)

				scheduled := mocks.NewMockScheduledJob(ctrl)
				scheduled.EXPECT().Trigger().Return(trigger)
				scheduled.EXPECT().JobDetail().Return(detail)

				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(scheduled, nil)
				scheduler.EXPECT().ScheduleJob(matchesJobDetailReplace(helmop), gomock.Any()).Return(nil)
			},
		},
		{
			name: "creates a polling job if the polling interval has changed",
			helmOp: fleet.HelmOp{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "helmop",
					Namespace: "default",
					UID:       "foo",
				},
				Spec: fleet.HelmOpSpec{
					PollingInterval:       &metav1.Duration{Duration: 1 * time.Minute},
					InsecureSkipTLSverify: true,
					BundleSpec: fleet.BundleSpec{
						BundleDeploymentOptions: fleet.BundleDeploymentOptions{
							Helm: &fleet.HelmOptions{
								Repo:    svr1.URL,
								Chart:   "alpine",
								Version: "0.1.x",
							},
						},
					},
				},
			},
			expectedSchedulerCalls: func(ctrl *gomock.Controller, scheduler *mocks.MockScheduler, helmop fleet.HelmOp) {
				trigger := ctrlquartz.NewControllerTrigger(2*helmop.Spec.PollingInterval.Duration, 0)
				job := newHelmPollingJob(nil, nil, helmop.Namespace, helmop.Name, *helmop.Spec.Helm)

				detail := quartz.NewJobDetail(job, nil)

				scheduled := mocks.NewMockScheduledJob(ctrl)
				scheduled.EXPECT().Trigger().Return(trigger)
				scheduled.EXPECT().JobDetail().Return(detail)

				scheduler.EXPECT().GetScheduledJob(gomock.Any()).Return(scheduled, nil)
				scheduler.EXPECT().ScheduleJob(matchesJobDetailReplace(helmop), gomock.Any()).Return(nil)
			},
		},
	}

	for _, c := range cases {
		t.Run(c.name, func(t *testing.T) {

			mockCtrl := gomock.NewController(t)
			defer mockCtrl.Finish()
			namespacedName := types.NamespacedName{Name: c.helmOp.Name, Namespace: c.helmOp.Namespace}
			client := mocks.NewMockK8sClient(mockCtrl)
			scheme := runtime.NewScheme()
			scheduler := mocks.NewMockScheduler(mockCtrl)

			r := HelmOpReconciler{
				Client:    client,
				Scheme:    scheme,
				Scheduler: scheduler,
			}

			ctx := context.TODO()

			// Initial reconcile get
			client.EXPECT().Get(gomock.Any(), gomock.Any(), &fleet.HelmOp{}, gomock.Any()).DoAndReturn(
				func(ctx context.Context, req types.NamespacedName, fh *fleet.HelmOp, opts ...interface{}) error {
					fh.Name = c.helmOp.Name
					fh.Namespace = c.helmOp.Namespace
					fh.UID = c.helmOp.UID
					fh.Spec = c.helmOp.Spec
					controllerutil.AddFinalizer(fh, finalize.HelmOpFinalizer)
					return nil
				}).AnyTimes()

			// Check to create or update the bundle
			client.EXPECT().Get(gomock.Any(), namespacedName, matchesBundle(c.helmOp.Name, c.helmOp.Namespace), gomock.Any()).DoAndReturn(
				func(ctx context.Context, req types.NamespacedName, b *fleet.Bundle, opts ...interface{}) error {
					b.Spec.HelmOpOptions = &fleet.BundleHelmOptions{
						SecretName: "foo", // prevent collision errors; the value does not matter.
					}
					return nil
				}).AnyTimes()

			client.EXPECT().Get(gomock.Any(), namespacedName, &fleet.Bundle{}, gomock.Any()).DoAndReturn(
				func(ctx context.Context, req types.NamespacedName, b *fleet.Bundle, opts ...interface{}) error {
					b.Spec.HelmOpOptions = &fleet.BundleHelmOptions{
						SecretName: "foo", // prevent collision errors; the value does not matter.
					}
					return nil
				}).AnyTimes()

			// Only expected in happy cases. If errors happen, only status updates are expected.
			client.EXPECT().Update(gomock.Any(), matchesBundle(c.helmOp.Name, c.helmOp.Namespace), gomock.Any()).Return(nil).AnyTimes()

			statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
			statusClient.EXPECT().Patch(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Return(nil)

			client.EXPECT().Status().Return(statusClient).Times(1)

			c.expectedSchedulerCalls(mockCtrl, scheduler, c.helmOp)

			_, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
			if (c.expectedError == "" && err != nil) ||
				(c.expectedError != "" && (err == nil || !strings.Contains(err.Error(), c.expectedError))) {
				t.Errorf("error mismatch: want %v, got %v", c.expectedError, err)
			}

		})
	}
}

type bundleMatcher struct {
	name      string
	namespace string
}

func matchesBundle(name, namespace string) gomock.Matcher {
	return &bundleMatcher{name: name, namespace: namespace}
}

func (b *bundleMatcher) Matches(x interface{}) bool {
	ho, ok := x.(*fleet.Bundle)
	if !ok {
		return false
	}

	return ho.Name == b.name && ho.Namespace == b.namespace
}

func (b *bundleMatcher) String() string {
	return fmt.Sprintf("matches namespace %q and name %q", b.namespace, b.name)
}

type scheduledJobMatcher struct {
	replaceExisting bool
	key             *quartz.JobKey
}

func matchesJobDetailReplace(helmop fleet.HelmOp) gomock.Matcher {
	return &scheduledJobMatcher{replaceExisting: true, key: quartz.NewJobKey(string(helmop.UID))}
}

func (s *scheduledJobMatcher) Matches(x interface{}) bool {
	jd, ok := x.(*quartz.JobDetail)
	if !ok {
		return false
	}

	if jd.JobKey() == nil {
		return false
	}

	if jd.Options() == nil {
		return false
	}

	return jd.JobKey().Equals(s.key) && jd.Options().Replace == s.replaceExisting
}

func (s *scheduledJobMatcher) String() string {
	return fmt.Sprintf("matches replace %t and job key %s", s.replaceExisting, s.key)
}

type typeMatcher struct{ t interface{} }

func OfType(t interface{}) gomock.Matcher {
	return &typeMatcher{t}
}

func (tm *typeMatcher) Matches(x interface{}) bool {
	return reflect.TypeOf(x) == reflect.TypeOf(tm.t)
}

func (tm *typeMatcher) String() string {
	return "is of type " + reflect.TypeOf(tm.t).String()
}



================================================
FILE: internal/cmd/controller/helmops/reconciler/helmop_status.go
================================================
package reconciler

import (
	"context"
	"fmt"
	"sort"

	"github.com/rancher/fleet/internal/cmd/controller/status"
	"github.com/rancher/fleet/internal/cmd/controller/summary"
	"github.com/rancher/fleet/internal/resourcestatus"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"
	"github.com/rancher/fleet/pkg/sharding"
	"sigs.k8s.io/controller-runtime/pkg/source"

	"k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

type HelmOpStatusReconciler struct {
	client.Client
	Scheme  *runtime.Scheme
	Workers int
	ShardID string
}

func (r *HelmOpStatusReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&fleet.HelmOp{}).
		// Fan out from bundle to HelmOp
		WatchesRawSource(source.TypedKind(
			mgr.GetCache(),
			&fleet.Bundle{},
			handler.TypedEnqueueRequestsFromMapFunc(func(ctx context.Context, a *fleet.Bundle) []ctrl.Request {
				app := a.GetLabels()[fleet.HelmOpLabel]
				if app != "" {
					return []ctrl.Request{{
						NamespacedName: types.NamespacedName{
							Namespace: a.GetNamespace(),
							Name:      app,
						},
					}}
				}

				return []ctrl.Request{}
			}),
			sharding.TypedFilterByShardID[*fleet.Bundle](r.ShardID), // WatchesRawSources ignores event filters, we need to use a predicate
			status.BundleStatusChangedPredicate(),
		)).
		WithEventFilter(sharding.FilterByShardID(r.ShardID)).
		WithOptions(controller.Options{MaxConcurrentReconciles: r.Workers}).
		Named("HelmOpStatus").
		Complete(r)
}

// Reconcile reads the stat of the HelmOp and BundleDeployments and
// computes status fields for the HelmOp. This status is used to
// display information to the user.
func (r *HelmOpStatusReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx).WithName("helmop-status")
	helmop := &fleet.HelmOp{}

	if err := r.Get(ctx, req.NamespacedName, helmop); err != nil && !errors.IsNotFound(err) {
		return ctrl.Result{}, err
	} else if errors.IsNotFound(err) {
		return ctrl.Result{}, nil
	}

	if !helmop.DeletionTimestamp.IsZero() {
		// the HelmOp controller will handle deletion
		return ctrl.Result{}, nil
	}

	logger = logger.WithValues("generation", helmop.Generation, "chart", helmop.Spec.Helm.Chart).WithValues("conditions", helmop.Status.Conditions)
	ctx = log.IntoContext(ctx, logger)

	logger.V(1).Info("Reconciling HelmOp status")

	bdList := &fleet.BundleDeploymentList{}
	err := r.List(ctx, bdList, client.MatchingLabels{
		fleet.HelmOpLabel:          helmop.Name,
		fleet.BundleNamespaceLabel: helmop.Namespace,
	})
	if err != nil {
		return ctrl.Result{}, err
	}

	orig := helmop.DeepCopy()

	err = setStatusHelm(bdList, helmop)
	if err != nil {
		return ctrl.Result{}, err
	}

	statusPatch := client.MergeFrom(orig)
	if patchData, err := statusPatch.Data(helmop); err == nil && string(patchData) != "{}" {
		// skip update if patch is empty
		if err := r.Status().Patch(ctx, helmop, statusPatch); err != nil {
			logger.Error(err, "Reconcile failed update to HelmOp status", "status", helmop.Status)
			return ctrl.Result{RequeueAfter: durations.HelmOpStatusDelay}, nil
		}
	}

	return ctrl.Result{}, nil
}

func setStatusHelm(list *fleet.BundleDeploymentList, helmop *fleet.HelmOp) error {
	// sort bundledeployments so lists in status are always in the same order
	sort.Slice(list.Items, func(i, j int) bool {
		return list.Items[i].UID < list.Items[j].UID
	})

	err := status.SetFields(list, &helmop.Status.StatusBase)
	if err != nil {
		return err
	}

	resourcestatus.SetResources(list.Items, &helmop.Status.StatusBase)

	summary.SetReadyConditions(&helmop.Status, "Bundle", helmop.Status.Summary)

	helmop.Status.Display.ReadyBundleDeployments = fmt.Sprintf("%d/%d",
		helmop.Status.Summary.Ready,
		helmop.Status.Summary.DesiredReady)

	return nil
}



================================================
FILE: internal/cmd/controller/helmops/reconciler/polling_job.go
================================================
// Copyright (c) 2021-2025 SUSE LLC

package reconciler

import (
	"context"
	"crypto/sha256"
	"fmt"
	"time"

	"github.com/Masterminds/semver/v3"
	"github.com/reugn/go-quartz/quartz"
	"golang.org/x/sync/semaphore"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	fleetevent "github.com/rancher/fleet/pkg/event"

	"github.com/rancher/wrangler/v3/pkg/condition"
	"github.com/rancher/wrangler/v3/pkg/kstatus"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	errutil "k8s.io/apimachinery/pkg/util/errors"
	"k8s.io/client-go/tools/record"
	"k8s.io/client-go/util/retry"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

var _ quartz.Job = &helmPollingJob{}

type helmPollingJob struct {
	sem    *semaphore.Weighted
	client client.Client

	namespace string
	name      string

	repo    string
	chart   string
	version string

	recorder record.EventRecorder
}

func newHelmPollingJob(
	c client.Client,
	r record.EventRecorder,
	namespace,
	name string,
	helmRef fleet.HelmOptions,
) *helmPollingJob {
	return &helmPollingJob{
		sem:      semaphore.NewWeighted(1),
		client:   c,
		recorder: r,

		namespace: namespace,
		name:      name,

		repo:    helmRef.Repo,
		chart:   helmRef.Chart,
		version: helmRef.Version,
	}
}

func (j *helmPollingJob) Execute(ctx context.Context) error {
	logger := log.FromContext(ctx)

	if !j.sem.TryAcquire(1) {
		// already running
		logger.V(1).Info("skipping polling job execution: already running")

		return nil
	}
	defer j.sem.Release(1)

	return j.pollHelm(ctx)
}

// Description returns a description for the job.
// This is needed to implement the Quartz Job interface.
func (j *helmPollingJob) Description() string {
	hasher := sha256.New()
	hasher.Write([]byte(j.repo))
	hasher.Write([]byte(j.chart))
	hasher.Write([]byte(j.version))

	chartRefHash := fmt.Sprintf("%x", hasher.Sum(nil))

	return fmt.Sprintf("helmops-polling-%s-%s-%s", j.namespace, j.name, chartRefHash)
}

func (j *helmPollingJob) pollHelm(ctx context.Context) error {
	h := &fleet.HelmOp{}
	nsName := types.NamespacedName{
		Name:      j.name,
		Namespace: j.namespace,
	}
	if err := j.client.Get(ctx, nsName, h); err != nil {
		return fmt.Errorf("could not get HelmOp resource from polling job: %w", err)
	}

	if h.Spec.Helm == nil {
		// This should not happen unless something has gone wrong in the reconciler's job management logic.
		return fmt.Errorf("helm options are unset")
	}

	// In case the version constraint has changed before the job was updated or deleted, this prevents an unwanted
	// update caused by a race between the scheduler and the reconciler.
	if _, err := semver.StrictNewVersion(h.Spec.Helm.Version); err == nil {
		return nil
	}

	// From here on, polling is considered to have been triggered.
	// Even if it fails, this timestamp will be updated in the HelmOp status.
	pollingTimestamp := time.Now().UTC()

	fail := func(origErr error, eventReason string) error {
		if eventReason != "" {
			j.recorder.Event(h, fleetevent.Warning, eventReason, origErr.Error())
		}

		return j.updateErrorStatus(ctx, h, pollingTimestamp, origErr)
	}

	version, err := getChartVersion(ctx, j.client, *h)
	if err != nil {
		return fail(err, "FailedToGetNewChartVersion")
	}

	b := &fleet.Bundle{}

	if err := j.client.Get(ctx, nsName, b); err != nil {
		return fail(fmt.Errorf("could not get bundle before patching its version: %w", err), "FailedToGetBundle")
	}

	orig := b.DeepCopy()
	b.Spec.Helm.Version = version

	if version != h.Status.Version {
		j.recorder.Event(h, fleetevent.Normal, "GotNewChartVersion", version)
	}

	patch := client.MergeFrom(orig)
	if patchData, err := patch.Data(b); err == nil && string(patchData) == "{}" && !isInErrorState(h.Status) {
		// skip update if patch is empty
		return nil
	}

	if err := j.client.Patch(ctx, b, patch); err != nil {
		return fail(fmt.Errorf("could not patch bundle to set the resolved version: %w", err), "FailedToPatchBundle")
	}

	nsn := types.NamespacedName{Name: h.Name, Namespace: h.Namespace}

	err = retry.RetryOnConflict(retry.DefaultRetry, func() error {
		t := &fleet.HelmOp{}
		if err := j.client.Get(ctx, nsn, t); err != nil {
			return fmt.Errorf("could not get HelmOp to update its status: %w", err)
		}

		t.Status.LastPollingTime = metav1.Time{Time: pollingTimestamp}
		t.Status.Version = version

		condition.Cond(fleet.HelmOpAcceptedCondition).SetStatusBool(&t.Status, true)
		condition.Cond(fleet.HelmOpPolledCondition).SetStatusBool(&t.Status, true)
		condition.Cond(fleet.HelmOpPolledCondition).Message(&t.Status, "")
		condition.Cond(fleet.HelmOpPolledCondition).Reason(&t.Status, "")
		kstatus.SetActive(&t.Status)

		statusPatch := client.MergeFrom(h)
		if patchData, err := statusPatch.Data(t); err == nil && string(patchData) == "{}" {
			// skip update if patch is empty
			return nil
		}
		return j.client.Status().Patch(ctx, t, statusPatch)
	})
	if err != nil {
		return fail(
			fmt.Errorf("could not update HelmOp status with polling timestamp: %w", err),
			"FailedToUpdateHelmOpStatus",
		)
	}

	return nil
}

// updateErrorStatus updates the provided helmOp's status to reflect the provided orgErr.
// This includes updating the helmOp's polling timestamp, if provided.
func (j *helmPollingJob) updateErrorStatus(
	ctx context.Context,
	helmOp *fleet.HelmOp,
	pollingTimestamp time.Time,
	orgErr error,
) error {
	nsn := types.NamespacedName{Name: helmOp.Name, Namespace: helmOp.Namespace}

	merr := []error{orgErr}
	err := retry.RetryOnConflict(retry.DefaultRetry, func() error {
		t := &fleet.HelmOp{}
		if err := j.client.Get(ctx, nsn, t); err != nil {
			return fmt.Errorf("could not get HelmOp to update its status: %w", err)
		}

		condition.Cond(fleet.HelmOpPolledCondition).SetError(&t.Status, "", orgErr)
		kstatus.SetError(t, orgErr.Error())

		if !pollingTimestamp.IsZero() {
			t.Status.LastPollingTime = metav1.Time{Time: pollingTimestamp}
		}

		statusPatch := client.MergeFrom(helmOp)
		if patchData, err := statusPatch.Data(t); err == nil && string(patchData) == "{}" {
			// skip update if patch is empty
			return nil
		}
		return j.client.Status().Patch(ctx, t, statusPatch)
	})
	if err != nil {
		merr = append(merr, err)
	}
	return errutil.NewAggregate(merr)
}

func isInErrorState(status fleet.HelmOpStatus) bool {
	for _, cond := range status.Conditions {
		// When an error is found we set the Reason to either Error or Stalled
		// and the Message field has the error message.
		if cond.Reason != "" && cond.Message != "" {
			return true
		}
	}

	return false
}



================================================
FILE: internal/cmd/controller/imagescan/gitcommit_job.go
================================================
// Copyright (c) 2021-2023 SUSE LLC

package imagescan

import (
	"context"
	"errors"
	"fmt"
	"html/template"
	"os"
	"path"
	"path/filepath"
	"sort"
	"strings"
	"sync"
	"time"

	gogit "github.com/go-git/go-git/v5"
	"github.com/go-git/go-git/v5/plumbing"
	"github.com/go-git/go-git/v5/plumbing/object"
	"github.com/go-git/go-git/v5/plumbing/transport"
	"github.com/go-git/go-git/v5/plumbing/transport/http"
	"github.com/go-git/go-git/v5/plumbing/transport/ssh"
	"github.com/go-logr/logr"
	"github.com/reugn/go-quartz/quartz"
	"golang.org/x/sync/semaphore"

	"github.com/rancher/fleet/internal/cmd/controller/imagescan/update"
	fleetgithub "github.com/rancher/fleet/internal/github"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"

	"github.com/rancher/wrangler/v3/pkg/condition"
	"github.com/rancher/wrangler/v3/pkg/kstatus"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	errutil "k8s.io/apimachinery/pkg/util/errors"
	"k8s.io/client-go/util/retry"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

const (
	defaultMessageTemplate = `Update from image update automation`
)

var (
	DefaultInterval = metav1.Duration{Duration: durations.DefaultImageInterval}
	lock            = sync.Mutex{}
)

var _ quartz.Job = &GitCommitJob{}

type GitCommitJob struct {
	sem    *semaphore.Weighted
	client client.Client

	namespace string
	name      string
}

func gitCommitDescription(namespace string, name string) string {
	return fmt.Sprintf("gitrepo-git-commit-%s-%s", namespace, name)
}

func GitCommitKey(namespace string, name string) *quartz.JobKey {
	return quartz.NewJobKey(gitCommitDescription(namespace, name))
}

func NewGitCommitJob(c client.Client, namespace string, name string) *GitCommitJob {
	return &GitCommitJob{
		sem:    semaphore.NewWeighted(1),
		client: c,

		namespace: namespace,
		name:      name,
	}
}

func (j *GitCommitJob) Execute(ctx context.Context) error {
	if !j.sem.TryAcquire(1) {
		// already running
		return nil
	}
	defer j.sem.Release(1)

	j.cloneAndReplace(ctx)

	return nil
}

func (j *GitCommitJob) Description() string {
	return gitCommitDescription(j.namespace, j.name)
}

func (j *GitCommitJob) cloneAndReplace(ctx context.Context) {
	logger := log.FromContext(ctx).WithName("imagescan-clone").WithValues("gitrepo", j.name, "namespace", j.namespace)
	nsn := types.NamespacedName{Namespace: j.namespace, Name: j.name}

	gitrepo := &fleet.GitRepo{}
	if err := j.client.Get(ctx, nsn, gitrepo); err != nil {
		return
	}

	images := &fleet.ImageScanList{}
	if err := j.client.List(ctx, images, client.InNamespace(j.namespace)); err != nil {
		return
	}

	if len(images.Items) == 0 {
		return
	}

	// filter all imagescans in the namespace for this repo (.spec.gitRepoName)
	scans := make([]*fleet.ImageScan, 0, len(images.Items))
	for _, image := range images.Items {
		if image.Spec.GitRepoName == j.name {
			image := image
			scans = append(scans, &image)
		}
	}

	isStalled := false
	var messages []string
	sort.Slice(scans, func(i, j int) bool {
		return scans[i].Spec.TagName < scans[j].Spec.TagName
	})
	c := condition.Cond(fleet.ImageScanScanCondition)
	for _, scan := range scans {
		if c.IsFalse(scan) {
			isStalled = true
			messages = append(messages, fmt.Sprintf("imageScan %s is not ready: %s", scan.Spec.TagName, c.GetMessage(scan)))
		}
	}
	if isStalled {
		err := errors.New(strings.Join(messages, ";"))
		logger.V(1).Info("Image scan is stalled", "error", err)
		return
	}

	if !shouldSync(gitrepo) {
		return
	}

	logger.V(1).Info("Syncing repo for image scans")

	// This lock is required to prevent conflicts while using the environment variable SSH_KNOWN_HOSTS.
	// It was added before the SSH support so there might be other potential conflicts without it.
	lock.Lock()
	defer lock.Unlock()

	// todo: maybe we should preserve the dir
	tmp, err := os.MkdirTemp("", fmt.Sprintf("%s-%s", gitrepo.Namespace, gitrepo.Name))
	if err != nil {
		err = j.updateErrorStatus(ctx, gitrepo, err)
		logger.V(1).Info("Cannot create temp dir to clone repo", "error", err)
		return
	}
	defer os.RemoveAll(tmp)

	auth, err := readAuth(ctx, logger, j.client, gitrepo)
	if err != nil {
		err = j.updateErrorStatus(ctx, gitrepo, err)
		logger.V(1).Info("Cannot create temp dir to clone repo", "error", err)
		return
	}

	// Remove SSH known_hosts tmpdir unless it was provided by the user
	if os.Getenv("SSH_KNOWN_HOSTS") != "" {
		tmpdir := filepath.Dir(os.Getenv("SSH_KNOWN_HOSTS"))
		if strings.HasPrefix(tmpdir, "/tmp/"+fmt.Sprintf("ssh-%s-%s-", gitrepo.Namespace, gitrepo.Name)) {
			defer os.RemoveAll(tmpdir)
		}
	}

	repo, err := gogit.PlainClone(tmp, false, &gogit.CloneOptions{
		URL:           gitrepo.Spec.Repo,
		Auth:          auth,
		RemoteName:    "origin",
		ReferenceName: plumbing.NewBranchReferenceName(gitrepo.Spec.Branch),
		SingleBranch:  true,
		Depth:         1,
		Progress:      nil,
		Tags:          gogit.NoTags,
	})
	if err != nil {
		err = j.updateErrorStatus(ctx, gitrepo, err)
		logger.V(1).Info("Cannot clone git repo", "error", err)
		return
	}

	// Checking if paths field is empty
	// if yes, using the default value "/"
	paths := gitrepo.Spec.Paths
	if len(paths) == 0 {
		paths = []string{"/"}
	}

	for _, path := range paths {
		updatePath := filepath.Join(tmp, path)
		if err := update.WithSetters(updatePath, updatePath, scans); err != nil {
			err = j.updateErrorStatus(ctx, gitrepo, err)
			logger.V(1).Info("Cannot update image tags in repo", "error", err)
			return
		}
	}

	commit, err := commitAllAndPush(ctx, repo, auth, *gitrepo.Spec.ImageScanCommit)
	if err != nil {
		err = j.updateErrorStatus(ctx, gitrepo, err)
		logger.V(1).Info("Cannot commit and push to repo", "error", err)
		return
	}
	if commit != "" {
		logger.Info("Created commit in repo", "repo", gitrepo.Spec.Repo, "commit", commit)
	}
	interval := gitrepo.Spec.ImageSyncInterval
	if interval == nil || interval.Seconds() == 0.0 {
		interval = &DefaultInterval
	}
	gitrepo.Status.LastSyncedImageScanTime = metav1.NewTime(time.Now())

	// update gitrepo status
	condition.Cond(fleet.ImageScanSyncCondition).SetError(&gitrepo.Status, "", nil)
	err = retry.RetryOnConflict(retry.DefaultRetry, func() error {
		t := &fleet.GitRepo{}
		err := j.client.Get(ctx, nsn, t)
		if err != nil {
			return err
		}
		t.Status = gitrepo.Status
		return j.client.Status().Update(ctx, t)
	})

	if err != nil {
		logger.Error(err, "Failed to update gitrepo status", "status", gitrepo.Status)
	}
}

func (j *GitCommitJob) updateErrorStatus(ctx context.Context, gitrepo *fleet.GitRepo, orgErr error) error {
	nsn := types.NamespacedName{Name: gitrepo.Name, Namespace: gitrepo.Namespace}

	condition.Cond(fleet.ImageScanSyncCondition).SetError(&gitrepo.Status, "", orgErr)
	kstatus.SetError(gitrepo, orgErr.Error())
	merr := []error{orgErr}
	err := retry.RetryOnConflict(retry.DefaultRetry, func() error {
		t := &fleet.GitRepo{}
		err := j.client.Get(ctx, nsn, t)
		if err != nil {
			return err
		}
		t.Status = gitrepo.Status
		return j.client.Status().Update(ctx, t)
	})
	if err != nil {
		merr = append(merr, err)
	}
	return errutil.NewAggregate(merr)
}

func shouldSync(gitrepo *fleet.GitRepo) bool {
	interval := gitrepo.Spec.ImageSyncInterval
	if interval == nil || interval.Seconds() == 0.0 {
		interval = &DefaultInterval
	}

	if time.Since(gitrepo.Status.LastSyncedImageScanTime.Time) < interval.Duration {
		return false
	}
	return true
}

func readAuth(ctx context.Context, logger logr.Logger, c client.Client, gitrepo *fleet.GitRepo) (transport.AuthMethod, error) {
	if gitrepo.Spec.ClientSecretName == "" {
		return nil, errors.New("requires git secret for write access")
	}
	secret := &corev1.Secret{}
	err := c.Get(ctx, types.NamespacedName{Namespace: gitrepo.Namespace, Name: gitrepo.Spec.ClientSecretName}, secret)
	if err != nil {
		return nil, err
	}

	switch secret.Type {
	case corev1.SecretTypeBasicAuth:
		return &http.BasicAuth{
			Username: string(secret.Data[corev1.BasicAuthUsernameKey]),
			Password: string(secret.Data[corev1.BasicAuthPasswordKey]),
		}, nil
	case corev1.SecretTypeSSHAuth:
		knownHosts := secret.Data["known_hosts"]
		if knownHosts == nil {
			logger.Info("The git secret does not have a known_hosts field, so no host key verification possible!", "secret", gitrepo.Spec.ClientSecretName)
		} else {
			err := setupKnownHosts(gitrepo, knownHosts)
			if err != nil {
				return nil, err
			}
		}

		publicKey, err := ssh.NewPublicKeys("git", secret.Data[corev1.SSHAuthPrivateKey], "")
		if err != nil {
			return nil, err
		}
		return publicKey, nil
	default:
		auth, err := fleetgithub.GetGithubAppAuthFromSecret(secret, fleetgithub.DefaultAppAuthGetter{})
		if err != nil {
			return nil, err
		}
		return auth, nil
	}
}

func setupKnownHosts(gitrepo *fleet.GitRepo, data []byte) error {
	tmpdir, err := os.MkdirTemp("", fmt.Sprintf("ssh-%s-%s-", gitrepo.Namespace, gitrepo.Name))
	if err != nil {
		return err
	}

	known := path.Join(tmpdir, "known_hosts")
	err = os.Setenv("SSH_KNOWN_HOSTS", known)
	if err != nil {
		return err
	}

	file, err := os.Create(known)
	if err != nil {
		return err
	}
	defer file.Close()

	_, err = file.Write(data)
	if err != nil {
		return err
	}

	return nil
}

func commitAllAndPush(ctx context.Context, repo *gogit.Repository, auth transport.AuthMethod, commit fleet.CommitSpec) (string, error) {
	working, err := repo.Worktree()
	if err != nil {
		return "", err
	}

	status, err := working.Status()
	if err != nil {
		return "", err
	} else if status.IsClean() {
		return "", nil
	}

	msgTmpl := commit.MessageTemplate
	if msgTmpl == "" {
		msgTmpl = defaultMessageTemplate
	}
	tmpl, err := template.New("commit message").Parse(msgTmpl)
	if err != nil {
		return "", err
	}
	buf := &strings.Builder{}
	if err := tmpl.Execute(buf, "no data! yet"); err != nil {
		return "", err
	}

	var rev plumbing.Hash
	if rev, err = working.Commit(buf.String(), &gogit.CommitOptions{
		All: true,
		Author: &object.Signature{
			Name:  commit.AuthorName,
			Email: commit.AuthorEmail,
			When:  time.Now(),
		},
	}); err != nil {
		return "", err
	}

	return rev.String(), repo.PushContext(ctx, &gogit.PushOptions{
		Auth: auth,
	})
}



================================================
FILE: internal/cmd/controller/imagescan/tagscan_job.go
================================================
// Copyright (c) 2021-2023 SUSE LLC

package imagescan

import (
	"bytes"
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"strings"
	"time"

	"github.com/Masterminds/semver/v3"
	"github.com/google/go-containerregistry/pkg/authn"
	"github.com/google/go-containerregistry/pkg/name"
	"github.com/google/go-containerregistry/pkg/v1/remote"
	"github.com/reugn/go-quartz/quartz"
	"golang.org/x/sync/semaphore"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"
	"github.com/rancher/wrangler/v3/pkg/condition"
	"github.com/rancher/wrangler/v3/pkg/kstatus"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	errutil "k8s.io/apimachinery/pkg/util/errors"
	"k8s.io/client-go/util/retry"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

const (
	// AlphabeticalOrderDesc descending order
	AlphabeticalOrderDesc = "DESC"
)

var _ quartz.Job = &TagScanJob{}

type TagScanJob struct {
	sem    *semaphore.Weighted
	client client.Client

	namespace string
	name      string
}

func tagScanDescription(namespace string, name string) string {
	return fmt.Sprintf("image-tag-scan-%s-%s", namespace, name)
}

func TagScanKey(namespace string, name string) *quartz.JobKey {
	return quartz.NewJobKey(tagScanDescription(namespace, name))
}

func NewTagScanJob(c client.Client, namespace string, name string) *TagScanJob {
	return &TagScanJob{
		sem:    semaphore.NewWeighted(1),
		client: c,

		namespace: namespace,
		name:      name,
	}
}

func (j *TagScanJob) Execute(ctx context.Context) error {
	if !j.sem.TryAcquire(1) {
		// already running
		return nil
	}
	defer j.sem.Release(1)

	j.updateImageTags(ctx)

	return nil
}

func (j *TagScanJob) Description() string {
	return tagScanDescription(j.namespace, j.name)
}

func (j *TagScanJob) updateImageTags(ctx context.Context) {
	logger := log.FromContext(ctx).WithName("imagescan-tag-scanner")
	nsn := types.NamespacedName{Namespace: j.namespace, Name: j.name}

	image := &fleet.ImageScan{}
	err := j.client.Get(ctx, nsn, image)
	if err != nil {
		return
	}

	if image.Spec.Suspend {
		return
	}

	logger = logger.WithValues("name", image.Name, "namespace", image.Namespace, "gitrepo", image.Spec.GitRepoName)

	ref, err := name.ParseReference(image.Spec.Image)
	if err != nil {
		err = j.updateErrorStatus(ctx, image, err)
		logger.V(1).Info("Failed to parse image name", "image", image.Spec.Image, "error", err)
		return
	}

	if !shouldScan(image) {
		return
	}

	canonical := ref.Context().String()
	if canonical != image.Status.CanonicalImageName {
		image.Status.CanonicalImageName = canonical
	}

	var options []remote.Option
	if image.Spec.SecretRef != nil {
		secret := &corev1.Secret{}
		err := j.client.Get(ctx, types.NamespacedName{Namespace: image.Namespace, Name: image.Spec.SecretRef.Name}, secret)
		if err != nil {
			err = j.updateErrorStatus(ctx, image, err)
			logger.Error(err, "Failed to get image secret")
			return
		}

		auth, err := authFromSecret(secret, ref.Context().RegistryStr())
		if err != nil {
			err = j.updateErrorStatus(ctx, image, err)
			logger.Error(err, "Failed to build auth info from secret")
			return
		}
		options = append(options, remote.WithAuth(auth))
	}

	tags, err := remote.List(ref.Context(), append(options, remote.WithContext(ctx))...)
	if err != nil {
		err = j.updateErrorStatus(ctx, image, err)
		logger.Error(err, "Failed to list remote tags")
		return
	}

	image.Status.LastScanTime = metav1.NewTime(time.Now())

	latestTag, err := latestTag(image.Spec.Policy, tags)
	if err != nil {
		err = j.updateErrorStatus(ctx, image, err)
		logger.Error(err, "Failed get the digest", "latestImage", image.Status.LatestImage)
		return
	}

	image.Status.LatestTag = latestTag
	image.Status.LatestImage = image.Status.CanonicalImageName + ":" + latestTag
	digest, err := getDigest(image.Status.LatestImage, options...)
	if err != nil {
		err = j.updateErrorStatus(ctx, image, err)
		logger.Error(err, "Failed get the digest", "latestImage", image.Status.LatestImage)
		return
	}
	image.Status.LatestDigest = digest

	condition.Cond(fleet.ImageScanScanCondition).SetError(&image.Status, "", nil)
	err = retry.RetryOnConflict(retry.DefaultRetry, func() error {
		t := &fleet.ImageScan{}
		err := j.client.Get(ctx, nsn, t)
		if err != nil {
			return err
		}
		t.Status = image.Status
		return j.client.Status().Update(ctx, t)
	})
	if err != nil {
		logger.Error(err, "Failed to update image scan status", "status", image.Status)
	}
}

func (j *TagScanJob) updateErrorStatus(ctx context.Context, image *fleet.ImageScan, orgErr error) error {
	nsn := types.NamespacedName{Name: image.Name, Namespace: image.Namespace}

	condition.Cond(fleet.ImageScanScanCondition).SetError(&image.Status, "", orgErr)
	kstatus.SetError(image, orgErr.Error())
	merr := []error{orgErr}
	err := retry.RetryOnConflict(retry.DefaultRetry, func() error {
		t := &fleet.ImageScan{}
		err := j.client.Get(ctx, nsn, t)
		if err != nil {
			return err
		}
		t.Status = image.Status
		err = j.client.Status().Update(ctx, t)
		return err
	})
	if err != nil {
		merr = append(merr, err)
	}
	return errutil.NewAggregate(merr)
}

func shouldScan(image *fleet.ImageScan) bool {
	if image.Status.LatestTag == "" {
		return true
	}

	interval := image.Spec.Interval
	if interval.Seconds() == 0.0 {
		interval = metav1.Duration{
			Duration: durations.DefaultImageInterval,
		}
	}
	if time.Since(image.Status.LastScanTime.Time) < interval.Duration {
		return false
	}
	return true
}

func getDigest(image string, options ...remote.Option) (string, error) {
	nameRef, err := name.ParseReference(image)
	if err != nil {
		return "", err
	}

	im, err := remote.Image(nameRef, options...)
	if err != nil {
		return "", err
	}
	digest, err := im.Digest()
	if err != nil {
		return "", err
	}
	return digest.String(), nil
}

// authFromSecret creates an Authenticator that can be given to the
// `remote` funcs, from a Kubernetes secret. If the secret doesn't
// have the right format or data, it returns an error.
func authFromSecret(secret *corev1.Secret, registry string) (authn.Authenticator, error) {
	switch secret.Type {
	case "kubernetes.io/dockerconfigjson":
		var dockerconfig struct {
			Auths map[string]authn.AuthConfig
		}
		configData := secret.Data[".dockerconfigjson"]
		if err := json.NewDecoder(bytes.NewBuffer(configData)).Decode(&dockerconfig); err != nil {
			return nil, err
		}
		auth, ok := dockerconfig.Auths[registry]
		if !ok {
			return nil, fmt.Errorf("auth for %q not found in secret %v", registry, types.NamespacedName{Name: secret.GetName(), Namespace: secret.GetNamespace()})
		}
		return authn.FromConfig(auth), nil
	default:
		return nil, fmt.Errorf("unknown secret type %q", secret.Type)
	}
}

func latestTag(policy fleet.ImagePolicyChoice, versions []string) (string, error) {
	if len(versions) == 0 {
		return "", errors.New("no tag found")
	}
	switch {
	case policy.SemVer != nil:
		return semverLatest(policy.SemVer.Range, versions)
	case policy.Alphabetical != nil:
		var des bool
		if policy.Alphabetical.Order == "" {
			des = true
		} else {
			des = strings.ToUpper(policy.Alphabetical.Order) == AlphabeticalOrderDesc
		}
		var latest string
		for _, version := range versions {
			if latest == "" {
				latest = version
				continue
			}

			if version >= latest && des {
				latest = version
			}

			if version <= latest && !des {
				latest = version
			}
		}
		return latest, nil
	default:
		return semverLatest("*", versions)
	}
}

func semverLatest(r string, versions []string) (string, error) {
	constraints, err := semver.NewConstraint(r)
	if err != nil {
		return "", err
	}
	var latestVersion *semver.Version
	for _, version := range versions {
		if ver, err := semver.NewVersion(version); err == nil {
			if latestVersion == nil || ver.GreaterThan(latestVersion) {
				if constraints.Check(ver) {
					latestVersion = ver
				}
			}
		}
	}
	if latestVersion == nil {
		return "", fmt.Errorf("no available version matching %s", r)
	}
	return latestVersion.Original(), nil
}



================================================
FILE: internal/cmd/controller/imagescan/tagscan_job_test.go
================================================
package imagescan

import (
	"testing"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

func TestLatestTag(t *testing.T) {
	var alphabeticalVersions = []string{"a", "b", "c"}

	tests := []struct {
		name, want string
		policy     fleet.ImagePolicyChoice
	}{
		{
			name: "alphabetical asc lowercase",
			policy: fleet.ImagePolicyChoice{
				Alphabetical: &fleet.AlphabeticalPolicy{Order: "asc"},
			},
			want: "a",
		},
		{
			name: "alphabetical asc uppercase",
			policy: fleet.ImagePolicyChoice{
				Alphabetical: &fleet.AlphabeticalPolicy{Order: "ASC"},
			},
			want: "a",
		},
		{
			name: "alphabetical desc lowercase",
			policy: fleet.ImagePolicyChoice{
				Alphabetical: &fleet.AlphabeticalPolicy{Order: "desc"},
			},
			want: "c",
		},
		{
			name: "alphabetical desc uppercase",
			policy: fleet.ImagePolicyChoice{
				Alphabetical: &fleet.AlphabeticalPolicy{Order: "DESC"},
			},
			want: "c",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got, err := latestTag(tt.policy, alphabeticalVersions)
			if err != nil {
				t.Fatalf("Error calling latestTag: %v", err)
			}

			if got != tt.want {
				t.Errorf("latestTag() = %v, want %v", got, tt.want)
			}
		})
	}
}



================================================
FILE: internal/cmd/controller/imagescan/update/README.md
================================================
# Credit

This package is copied from https://github.com/fluxcd/image-automation-controller so giving credit to them 


================================================
FILE: internal/cmd/controller/imagescan/update/filereader.go
================================================
/*
Copyright 2020, 2021 The Flux authors

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package update

import (
	"bytes"
	"fmt"
	"io/fs"
	"os"
	"path/filepath"

	"sigs.k8s.io/kustomize/kyaml/kio"
	"sigs.k8s.io/kustomize/kyaml/kio/kioutil"
	"sigs.k8s.io/kustomize/kyaml/yaml"
)

// ScreeningReader is a kio.Reader that includes only files that are
// pertinent to automation. In practice this means looking for a
// particular token in each file, and ignoring those files without the
// token. This avoids most problematic cases -- e.g., templates in a
// Helm chart, which won't parse as YAML -- and cheaply filters for
// only those files that need processing.
type ScreeningLocalReader struct {
	Token string
	Path  string

	// This records the relative path of each file that passed
	// screening (i.e., contained the token), but couldn't be parsed.
	ProblemFiles []string
}

// Read scans the .Path recursively for files that contain .Token, and
// parses any that do. It applies the filename annotation used by
// [`kio.LocalPackageWriter`](https://godoc.org/sigs.k8s.io/kustomize/kyaml/kio#LocalPackageWriter)
// so that the same will write files back to their original
// location. The implementation follows that of
// [LocalPackageReader.Read](https://godoc.org/sigs.k8s.io/kustomize/kyaml/kio#LocalPackageReader.Read),
// adapting lightly (mainly to leave features out).
func (r *ScreeningLocalReader) Read() ([]*yaml.RNode, error) {
	if r.Path == "" {
		return nil, fmt.Errorf("must supply path to scan for files")
	}

	root, err := filepath.Abs(r.Path)
	if err != nil {
		return nil, fmt.Errorf("path field cannot be made absolute: %w", err)
	}

	// For the filename annotation, I want a directory for filenames
	// to be relative to; but I don't know whether path is a directory
	// or file yet so this must wait until the body of the filepath.WalkDir.
	var relativePath string

	tokenbytes := []byte(r.Token)

	var result []*yaml.RNode
	err = filepath.WalkDir(root, func(p string, info fs.DirEntry, err error) error {
		if err != nil {
			return fmt.Errorf("walking path for files: %w", err)
		}

		if p == root {
			if info.IsDir() {
				relativePath = p
				return nil // keep walking
			}
			relativePath = filepath.Dir(p)
		}

		if info.IsDir() {
			return nil
		}

		if ext := filepath.Ext(p); ext != ".yaml" && ext != ".yml" {
			return nil
		}

		// To check for the token, I need the file contents. This
		// assumes the file is encoded as UTF8.
		filebytes, err := os.ReadFile(p)
		if err != nil {
			return fmt.Errorf("reading YAML file: %w", err)
		}

		if !bytes.Contains(filebytes, tokenbytes) {
			return nil
		}

		path, err := filepath.Rel(relativePath, p)
		if err != nil {
			return fmt.Errorf("relativising path: %w", err)
		}
		annotations := map[string]string{
			kioutil.PathAnnotation: path,
		}

		rdr := &kio.ByteReader{
			Reader:            bytes.NewBuffer(filebytes),
			SetAnnotations:    annotations,
			PreserveSeqIndent: true,
		}

		nodes, err := rdr.Read()
		// Having screened the file and decided it's worth examining,
		// an error at this point is most unfortunate. However, it
		// doesn't need to be the end of the matter; we can record
		// this file as problematic, and continue.
		if err != nil {
			r.ProblemFiles = append(r.ProblemFiles, path)
			//nolint:nilerr // Intentionally continue processing other files to collect all problems
			return nil
		}
		result = append(result, nodes...)
		return nil
	})

	return result, err
}



================================================
FILE: internal/cmd/controller/imagescan/update/filter.go
================================================
/*
Copyright 2020, 2021 The Flux authors

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package update

import (
	"encoding/json"

	"k8s.io/kube-openapi/pkg/validation/spec"
	"sigs.k8s.io/kustomize/kyaml/fieldmeta"
	"sigs.k8s.io/kustomize/kyaml/openapi"
	"sigs.k8s.io/kustomize/kyaml/yaml"
)

// The implementation of this filter is adapted from
// [kyaml](https://github.com/kubernetes-sigs/kustomize/blob/kyaml/v0.10.16/kyaml/setters2/set.go),
// with the following changes:
//
// - it calls its callback for each field it sets
//
// - it will set all fields referring to a setter present in the
// schema -- this is behind a flag in the kyaml implementation, but
// the only desired mode of operation here
//
// - substitutions are not supported -- they are not used for image
// updates
//
// - no validation is done on the value being set -- since the schema
// is constructed here, it's assumed the values will be appropriate
//
// - only scalar nodes are considered (i.e., no sequence replacements)
//
// - only per-field schema references (those in a comment in the YAML)
// are considered -- these are the only ones relevant to image updates

type SetAllCallback struct {
	SettersSchema *spec.Schema
	Callback      func(setter, oldValue, newValue string)
}

func (s *SetAllCallback) Filter(object *yaml.RNode) (*yaml.RNode, error) {
	return object, accept(s, object, "", s.SettersSchema)
}

// visitor is provided to accept to walk the AST.
type visitor interface {
	// visitScalar is called for each scalar field value on a resource
	// node is the scalar field value
	// path is the path to the field; path elements are separated by '.'
	visitScalar(node *yaml.RNode, path string, schema *openapi.ResourceSchema) error
}

// getSchema returns per-field OpenAPI schema for a particular node.
func getSchema(r *yaml.RNode, settersSchema *spec.Schema) *openapi.ResourceSchema {
	// get the override schema if it exists on the field
	fm := fieldmeta.FieldMeta{SettersSchema: settersSchema}
	if err := fm.Read(r); err == nil && !fm.IsEmpty() {
		// per-field schema, this is fine
		if fm.Schema.Ref.String() != "" {
			// resolve the reference
			s, err := openapi.Resolve(&fm.Schema.Ref, settersSchema)
			if err == nil && s != nil {
				fm.Schema = *s
			}
		}
		return &openapi.ResourceSchema{Schema: &fm.Schema}
	}
	return nil
}

// accept walks the AST and calls the visitor at each scalar node.
func accept(v visitor, object *yaml.RNode, p string, settersSchema *spec.Schema) error {
	switch object.YNode().Kind {
	case yaml.DocumentNode:
		// Traverse the child of the document
		return accept(v, yaml.NewRNode(object.YNode()), p, settersSchema)
	case yaml.MappingNode:
		return object.VisitFields(func(node *yaml.MapNode) error {
			// Traverse each field value
			return accept(v, node.Value, p+"."+node.Key.YNode().Value, settersSchema)
		})
	case yaml.SequenceNode:
		return object.VisitElements(func(node *yaml.RNode) error {
			// Traverse each list element
			return accept(v, node, p, settersSchema)
		})
	case yaml.ScalarNode:
		fieldSchema := getSchema(object, settersSchema)
		return v.visitScalar(object, p, fieldSchema)
	}
	return nil
}

type setter struct {
	Name  string `json:"name"`
	Value string `json:"value"`
}

type extension struct {
	Setter *setter `json:"setter,omitempty"`
}

// set applies the value from ext to field
func (s *SetAllCallback) set(field *yaml.RNode, ext *extension, sch *spec.Schema) error {
	// check full setter
	if ext.Setter == nil {
		return nil
	}

	// this has a full setter, set its value
	old := field.YNode().Value
	field.YNode().Value = ext.Setter.Value
	s.Callback(ext.Setter.Name, old, ext.Setter.Value)

	// format the node so it is quoted if it is a string. If there is
	// type information on the setter schema, we use it.
	if len(sch.Type) > 0 {
		yaml.FormatNonStringStyle(field.YNode(), *sch)
	}
	return nil
}

// visitScalar
func (s *SetAllCallback) visitScalar(object *yaml.RNode, p string, fieldSchema *openapi.ResourceSchema) error {
	if fieldSchema == nil {
		return nil
	}
	// get the openAPI for this field describing how to apply the setter
	ext, err := getExtFromSchema(fieldSchema.Schema)
	if err != nil {
		return err
	}
	if ext == nil {
		return nil
	}

	// perform a direct set of the field if it matches
	err = s.set(object, ext, fieldSchema.Schema)
	return err
}

func getExtFromSchema(schema *spec.Schema) (*extension, error) {
	cep := schema.Extensions[K8sCliExtensionKey]
	if cep == nil {
		return nil, nil
	}
	b, err := json.Marshal(cep)
	if err != nil {
		return nil, err
	}
	val := &extension{}
	if err := json.Unmarshal(b, val); err != nil {
		return nil, err
	}
	return val, nil
}



================================================
FILE: internal/cmd/controller/imagescan/update/result.go
================================================
package update

import (
	"github.com/google/go-containerregistry/pkg/name"

	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/kustomize/kyaml/yaml"
)

// ImageRef represents the image reference used to replace a field
// value in an update.
type ImageRef interface {
	// String returns a string representation of the image ref as it
	// is used in the update; e.g., "helloworld:v1.0.1"
	String() string
	// Identifier returns the tag or digest; e.g., "v1.0.1"
	Identifier() string
	// Repository returns the repository component of the ImageRef,
	// with an implied defaults, e.g., "library/helloworld"
	Repository() string
	// Registry returns the registry component of the ImageRef, e.g.,
	// "index.docker.io"
	Registry() string
	// Name gives the fully-qualified reference name, e.g.,
	// "index.docker.io/library/helloworld:v1.0.1"
	Name() string
	// Policy gives the namespaced name of the image policy that led
	// to the update.
	Policy() types.NamespacedName
}

type imageRef struct {
	name.Reference
	policy types.NamespacedName
}

// Policy gives the namespaced name of the policy that led to the
// update.
func (i imageRef) Policy() types.NamespacedName {
	return i.policy
}

// Repository gives the repository component of the image ref.
func (i imageRef) Repository() string {
	return i.Context().RepositoryStr()
}

// Registry gives the registry component of the image ref.
func (i imageRef) Registry() string {
	return i.Context().Registry.String()
}

// ObjectIdentifier holds the identifying data for a particular
// object. This won't always have a name (e.g., a kustomization.yaml).
type ObjectIdentifier struct {
	yaml.ResourceIdentifier
}

// Result reports the outcome of an automated update. It has a nested
// structure file->objects->images. Different projections (e.g., all
// the images, regardless of object) are available via methods.
type Result struct {
	Files map[string]FileResult
}

// FileResult gives the updates in a particular file.
type FileResult struct {
	Objects map[ObjectIdentifier][]ImageRef
}

// Images returns all the images that were involved in at least one
// update.
func (r Result) Images() []ImageRef {
	seen := make(map[ImageRef]struct{})
	var result []ImageRef
	for _, file := range r.Files {
		for _, images := range file.Objects {
			for _, ref := range images {
				if _, ok := seen[ref]; !ok {
					seen[ref] = struct{}{}
					result = append(result, ref)
				}
			}
		}
	}
	return result
}

// Objects returns a map of all the objects against the images updated
// within, regardless of which file they appear in.
func (r Result) Objects() map[ObjectIdentifier][]ImageRef {
	result := make(map[ObjectIdentifier][]ImageRef)
	for _, file := range r.Files {
		for res, refs := range file.Objects {
			result[res] = append(result[res], refs...)
		}
	}
	return result
}



================================================
FILE: internal/cmd/controller/imagescan/update/setters.go
================================================
package update

import (
	"fmt"

	"github.com/google/go-containerregistry/pkg/name"

	"github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"k8s.io/apimachinery/pkg/types"
	"k8s.io/apimachinery/pkg/util/sets"
	"k8s.io/kube-openapi/pkg/validation/spec"
	"sigs.k8s.io/kustomize/kyaml/fieldmeta"
	"sigs.k8s.io/kustomize/kyaml/kio"
	"sigs.k8s.io/kustomize/kyaml/kio/kioutil"
	"sigs.k8s.io/kustomize/kyaml/yaml"
)

const (
	// This is preserved from setters2
	K8sCliExtensionKey = "x-k8s-cli"
)

// Credit: https://github.com/fluxcd/image-automation-controller

const (
	// SetterShortHand is a shorthand that can be used to mark
	// setters; instead of
	// # { "$ref": "#/definitions/
	SetterShortHand = "$imagescan"
)

func init() {
	fieldmeta.SetShortHandRef(SetterShortHand)
}

// WithSetters takes all YAML files from `inpath`, updates any
// that contain an "in scope" image policy marker, and writes files it
// updated (and only those files) back to `outpath`.
func WithSetters(inpath, outpath string, scans []*v1alpha1.ImageScan) error {
	var settersSchema spec.Schema

	// collect setter defs and setters by going through all the image
	// policies available.
	result := Result{
		Files: make(map[string]FileResult),
	}
	// the OpenAPI schema is a package variable in kyaml/openapi. In
	// lieu of being able to isolate invocations (per
	// https://github.com/kubernetes-sigs/kustomize/issues/3058), I
	// serialise access to it and reset it each time.

	// construct definitions

	// the format of the definitions expected is given here:
	//     https://github.com/kubernetes-sigs/kustomize/blob/master/kyaml/setters2/doc.go
	//
	//     {
	//        "definitions": {
	//          "io.k8s.cli.setters.replicas": {
	//            "x-k8s-cli": {
	//              "setter": {
	//                "name": "replicas",
	//                "value": "4"
	//              }
	//            }
	//          }
	//        }
	//      }
	//
	// (there are consts in kyaml/fieldmeta with the
	// prefixes).
	//
	// `fieldmeta.SetShortHandRef("$imagepolicy")` makes it possible
	// to just use (e.g.,)
	//
	//     image: foo:v1 # {"$imagepolicy": "automation-ns:foo"}
	//
	// to mark the fields at which to make replacements. A colon is
	// used to separate namespace and name in the key, because a slash
	// would be interpreted as part of the $ref path.
	imageRefs := make(map[string]imageRef)
	setAllCallback := func(file, setterName string, node *yaml.RNode) {
		ref, ok := imageRefs[setterName]
		if !ok {
			return
		}

		meta, err := node.GetMeta()
		if err != nil {
			return
		}
		oid := ObjectIdentifier{meta.GetIdentifier()}

		fileres, ok := result.Files[file]
		if !ok {
			fileres = FileResult{
				Objects: make(map[ObjectIdentifier][]ImageRef),
			}
			result.Files[file] = fileres
		}
		objres := fileres.Objects[oid]
		for _, n := range objres {
			if n == ref {
				return
			}
		}
		objres = append(objres, ref)
		fileres.Objects[oid] = objres
	}

	defs := map[string]spec.Schema{}
	for _, scan := range scans {
		if scan.Status.LatestImage == "" {
			continue
		}
		// Using strict validation would mean any image that omits the
		// registry would be rejected, so that can't be used
		// here. Using _weak_ validation means that defaults will be
		// filled in. Usually this would mean the tag would end up
		// being `latest` if empty in the input; but I'm assuming here
		// that the policy won't have a tagless ref.
		image := scan.Status.LatestImage
		r, err := name.ParseReference(image, name.WeakValidation)
		if err != nil {
			return fmt.Errorf("encountered invalid image ref %q: %w", scan.Status.LatestImage, err)
		}
		ref := imageRef{
			Reference: r,
			policy: types.NamespacedName{
				Name:      scan.Name,
				Namespace: scan.Namespace,
			},
		}
		tag := ref.Identifier()
		// annoyingly, neither the library imported above, nor an
		// alternative, I found will yield the original image name;
		// this is an easy way to get it
		name := image[:len(image)-len(tag)-1]

		imageSetter := scan.Spec.TagName
		defs[fieldmeta.SetterDefinitionPrefix+imageSetter] = setterSchema(imageSetter, scan.Status.LatestImage)
		imageRefs[imageSetter] = ref

		tagSetter := imageSetter + ":tag"
		defs[fieldmeta.SetterDefinitionPrefix+tagSetter] = setterSchema(tagSetter, tag)
		imageRefs[tagSetter] = ref

		// Context().Name() gives the image repository _as supplied_
		nameSetter := imageSetter + ":name"
		defs[fieldmeta.SetterDefinitionPrefix+nameSetter] = setterSchema(nameSetter, name)
		imageRefs[nameSetter] = ref

		digestSetter := imageSetter + ":digest"
		defs[fieldmeta.SetterDefinitionPrefix+digestSetter] = setterSchema(digestSetter, fmt.Sprintf("%s@%s", scan.Status.LatestImage, scan.Status.LatestDigest))
	}

	settersSchema.Definitions = defs
	set := &SetAllCallback{
		SettersSchema: &settersSchema,
	}

	// get ready with the reader and writer
	reader := &ScreeningLocalReader{
		Path:  inpath,
		Token: fmt.Sprintf("%q", SetterShortHand),
	}
	writer := &kio.LocalPackageWriter{
		PackagePath: outpath,
	}

	pipeline := kio.Pipeline{
		Inputs:  []kio.Reader{reader},
		Outputs: []kio.Writer{writer},
		Filters: []kio.Filter{
			setAll(set, setAllCallback),
		},
	}

	return pipeline.Execute()
}

// setAll returns a kio.Filter using the supplied SetAllCallback
// (dealing with individual nodes), amd calling the given callback
// whenever a field value is changed, and returning only nodes from
// files with changed nodes. This is based on
// [`SetAll`](https://github.com/kubernetes-sigs/kustomize/blob/kyaml/v0.10.16/kyaml/setters2/set.go#L503
// from kyaml/kio.
func setAll(filter *SetAllCallback, callback func(file, setterName string, node *yaml.RNode)) kio.Filter {
	return kio.FilterFunc(
		func(nodes []*yaml.RNode) ([]*yaml.RNode, error) {
			filesToUpdate := sets.Set[string]{}
			for i := range nodes {
				path, _, err := kioutil.GetFileAnnotations(nodes[i])
				if err != nil {
					return nil, err
				}

				filter.Callback = func(setter, oldValue, newValue string) {
					if newValue != oldValue {
						callback(path, setter, nodes[i])
						filesToUpdate.Insert(path)
					}
				}
				_, err = filter.Filter(nodes[i])
				if err != nil {
					return nil, err
				}
			}

			var nodesInUpdatedFiles []*yaml.RNode
			for i := range nodes {
				path, _, err := kioutil.GetFileAnnotations(nodes[i])
				if err != nil {
					return nil, err
				}
				if filesToUpdate.Has(path) {
					nodesInUpdatedFiles = append(nodesInUpdatedFiles, nodes[i])
				}
			}
			return nodesInUpdatedFiles, nil
		})
}

func setterSchema(name, value string) spec.Schema {
	schema := spec.StringProperty()
	schema.Extensions = map[string]interface{}{}
	schema.Extensions.Add(K8sCliExtensionKey, map[string]interface{}{
		"setter": map[string]string{
			"name":  name,
			"value": value,
		},
	})
	return *schema
}



================================================
FILE: internal/cmd/controller/namespace/util.go
================================================
// Package namespace generates the name of the system registration namespace.
//
// Special namespaces in fleet:
// * system namespace: cattle-fleet-system
// * system registration namespace: cattle-fleet-clusters-system
// * cluster registration namespace or "workspace": fleet-local
// * cluster namespace: cluster-${namespace}-${cluster}-${random}

package namespace

import (
	"strings"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"
)

func GVK() schema.GroupVersionKind {
	return schema.GroupVersionKind{
		Group:   corev1.SchemeGroupVersion.Group,
		Version: corev1.SchemeGroupVersion.Version,
		Kind:    "Namespace",
	}
}

// SystemRegistrationNamespace generates the name of the system registration
// namespace from the configured system namespace, e.g.:
// cattle-fleet-system -> cattle-fleet-clusters-system
func SystemRegistrationNamespace(systemNamespace string) string {
	ns := strings.ReplaceAll(systemNamespace, "-system", "-clusters-system")
	if ns == systemNamespace {
		return systemNamespace + "-clusters-system"
	}
	return ns
}



================================================
FILE: internal/cmd/controller/options/calculate.go
================================================
// Package options merges the BundleDeploymentOptions, so that targetCustomizations take effect.
package options

import (
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"maps"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/wrangler/v3/pkg/data"
)

// DeploymentID hashes the options to a string
func DeploymentID(manifestID string, opts fleet.BundleDeploymentOptions) (string, error) {
	h := sha256.New()
	if err := json.NewEncoder(h).Encode(&opts); err != nil {
		return "", err
	}

	return manifestID + ":" + hex.EncodeToString(h.Sum(nil)), nil
}

// Merge overrides the 'base' options with the 'target customization' options, if 'custom' is present (pure function)
func Merge(base, custom fleet.BundleDeploymentOptions) fleet.BundleDeploymentOptions { //nolint: gocyclo // business logic
	result := *base.DeepCopy()
	if custom.DefaultNamespace != "" {
		result.DefaultNamespace = custom.DefaultNamespace
	} else if custom.DefaultNamespace == "-" {
		result.DefaultNamespace = ""
	}
	if custom.TargetNamespace != "" {
		result.TargetNamespace = custom.TargetNamespace
	} else if custom.TargetNamespace == "-" {
		result.TargetNamespace = ""
	}
	if custom.ServiceAccount != "" {
		result.ServiceAccount = custom.ServiceAccount
	} else if custom.ServiceAccount == "-" {
		result.ServiceAccount = ""
	}
	if custom.Helm != nil {
		if result.Helm == nil {
			result.Helm = &fleet.HelmOptions{}
		}
		if custom.Helm.TimeoutSeconds > 0 {
			result.Helm.TimeoutSeconds = custom.Helm.TimeoutSeconds
		} else if custom.Helm.TimeoutSeconds < 0 {
			result.Helm.TimeoutSeconds = 0
		}
		if result.Helm.Values == nil {
			result.Helm.Values = custom.Helm.Values
		} else if custom.Helm.Values != nil {
			result.Helm.Values.Data = data.MergeMaps(result.Helm.Values.Data, custom.Helm.Values.Data)
		}
		if result.Helm.TemplateValues == nil {
			result.Helm.TemplateValues = custom.Helm.TemplateValues
		} else if custom.Helm.TemplateValues != nil {
			maps.Copy(result.Helm.TemplateValues, custom.Helm.TemplateValues)
		}
		if custom.Helm.ValuesFrom != nil {
			result.Helm.ValuesFrom = append(result.Helm.ValuesFrom, custom.Helm.ValuesFrom...)
		}
		if custom.Helm.Repo != "" {
			result.Helm.Repo = custom.Helm.Repo
		}
		if custom.Helm.Chart != "" {
			result.Helm.Chart = custom.Helm.Chart
		}
		if custom.Helm.Version != "" {
			result.Helm.Version = custom.Helm.Version
		}
		if custom.Helm.ReleaseName != "" {
			result.Helm.ReleaseName = custom.Helm.ReleaseName
		}
		result.Helm.Force = result.Helm.Force || custom.Helm.Force
		result.Helm.Atomic = result.Helm.Atomic || custom.Helm.Atomic
		result.Helm.TakeOwnership = result.Helm.TakeOwnership || custom.Helm.TakeOwnership
		result.Helm.DisablePreProcess = result.Helm.DisablePreProcess || custom.Helm.DisablePreProcess
		result.Helm.WaitForJobs = result.Helm.WaitForJobs || custom.Helm.WaitForJobs
		result.Helm.DisableDNS = result.Helm.DisableDNS || custom.Helm.DisableDNS
	}
	if custom.Kustomize != nil {
		if result.Kustomize == nil {
			result.Kustomize = &fleet.KustomizeOptions{}
		}
		if custom.Kustomize.Dir != "" {
			result.Kustomize.Dir = custom.Kustomize.Dir
		}
	}
	if custom.Diff != nil {
		if result.Diff == nil {
			result.Diff = &fleet.DiffOptions{}
		}
		result.Diff.ComparePatches = append(result.Diff.ComparePatches, custom.Diff.ComparePatches...)
	}
	if custom.YAML != nil {
		if result.YAML == nil {
			result.YAML = &fleet.YAMLOptions{}
		}
		result.YAML.Overlays = append(result.YAML.Overlays, custom.YAML.Overlays...)
	}
	if custom.ForceSyncGeneration > 0 {
		result.ForceSyncGeneration = custom.ForceSyncGeneration
	}
	result.KeepResources = result.KeepResources || custom.KeepResources
	if custom.CorrectDrift != nil {
		result.CorrectDrift = custom.CorrectDrift
	}

	return result
}



================================================
FILE: internal/cmd/controller/quartz/trigger.go
================================================
package quartz

import (
	"fmt"
	"math/rand/v2"
	"time"

	"github.com/reugn/go-quartz/quartz"
)

// ControllerTrigger is a custom trigger, implementing the quartz.Trigger interface. This trigger is
// used to schedule jobs to be run both:
// * periodically, after the first polling interval, as would happen with Quartz's `simpleTrigger`
// * right away, without waiting for that first polling interval to elapse.
// It also adds the possibility to add a percentage of jitter to the duration of the trigger to avoid
// situations in which we have many collisions.
type ControllerTrigger struct {
	isInitRunDone bool
	jitterPercent int
	simpleTrigger *quartz.SimpleTrigger
}

func (t *ControllerTrigger) NextFireTime(prev int64) (int64, error) {
	if !t.isInitRunDone {
		t.isInitRunDone = true

		return prev, nil
	}

	simpleTriggerNext, err := t.simpleTrigger.NextFireTime(prev)
	if err != nil {
		return 0, err
	}

	return simpleTriggerNext + jitter(t.simpleTrigger.Interval, t.jitterPercent).Nanoseconds(), nil
}

func (t *ControllerTrigger) Description() string {
	return fmt.Sprintf("ControllerTrigger-%s", t.simpleTrigger.Interval)
}

func NewControllerTrigger(interval time.Duration, jitterPercent int) *ControllerTrigger {
	return &ControllerTrigger{
		jitterPercent: jitterPercent,
		simpleTrigger: quartz.NewSimpleTrigger(interval),
	}
}

// jitter returns a random jitter between 0% and +jitterPercent% of the original duration.
// jitterPercent is an integer percentage (e.g., 10 for 10%)
// and it's limited to 100 in case a higher number is entered.
func jitter(d time.Duration, jitterPercent int) time.Duration {
	if jitterPercent <= 0 {
		return 0
	}

	if jitterPercent > 100 {
		jitterPercent = 100
	}

	// Convert jitter percent to a fraction
	jitterFraction := float64(jitterPercent) / 100.0

	// Calculate maximum jitter in float64 (nanoseconds)
	maxJitter := float64(d) * jitterFraction

	// Generate a random float64 between 0 and maxJitter
	return time.Duration(rand.Float64() * maxJitter) //nolint:gosec // non-crypto usage
}



================================================
FILE: internal/cmd/controller/quartz/trigger_test.go
================================================
package quartz

import (
	"testing"
	"time"
)

func TestControllerTrigger(t *testing.T) {
	interval := 1 * time.Second
	jitterPercent := 10
	tr := NewControllerTrigger(interval, jitterPercent)

	if tr.isInitRunDone {
		t.Errorf("unexpected initial value for isInitRunDone, expected false, got true")
	}

	// First fire time should be immediate
	now := time.Now().UnixNano()
	ft, err := tr.NextFireTime(now)
	if err != nil {
		t.Errorf("unexpected error on first call to NextFireTime: %v", err)
	}

	if !tr.isInitRunDone {
		t.Errorf("isInitRunDone should be true after first call, but it's false")
	}

	if ft != now {
		t.Errorf("unexpected first fire time, expected %d, got %d", now, ft)
	}

	// Second fire time should be after the interval + jitter
	nextFt, err := tr.NextFireTime(now)
	if err != nil {
		t.Errorf("unexpected error on second call to NextFireTime: %v", err)
	}

	// The next fire time should be within the interval + jitter range.
	minNextFt := now + interval.Nanoseconds()
	maxJitter := time.Duration(float64(interval) * float64(jitterPercent) / 100.0)
	maxNextFt := minNextFt + maxJitter.Nanoseconds()
	if nextFt < minNextFt || nextFt > maxNextFt {
		t.Errorf("unexpected next fire time, expected between %d and %d, got %d", minNextFt, maxNextFt, nextFt)
	}

	// Test description
	expectedDesc := "ControllerTrigger-1s"
	if tr.Description() != expectedDesc {
		t.Errorf("unexpected description, expected %q, got %q", expectedDesc, tr.Description())
	}
}

func TestJitter(t *testing.T) {
	baseDuration := 100 * time.Second
	testCases := []struct {
		name          string
		jitterPercent int
	}{
		{
			name:          "no jitter (0%)",
			jitterPercent: 0,
		},
		{
			name:          "negative jitter (-10%)",
			jitterPercent: -10,
		},
		{
			name:          "with jitter (10%)",
			jitterPercent: 10,
		},
		{
			name:          "with jitter (50%)",
			jitterPercent: 50,
		},
		{
			name:          "with jitter (100%)",
			jitterPercent: 100,
		},
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			d := jitter(baseDuration, tc.jitterPercent)

			if tc.jitterPercent <= 0 {
				if d != 0 {
					t.Errorf("expected duration 0 with no jitter, got %v", d)
				}
			} else {
				maxJitter := time.Duration(float64(baseDuration) * float64(tc.jitterPercent) / 100.0)

				if d < 0 || d > maxJitter {
					t.Errorf("duration %v is outside the expected range [0, %v]", d, maxJitter)
				}
			}
		})
	}
}



================================================
FILE: internal/cmd/controller/reconciler/bundle_controller.go
================================================
// Copyright (c) 2021-2023 SUSE LLC

package reconciler

import (
	"context"
	"errors"
	"fmt"
	"maps"
	"reflect"
	"slices"
	"strings"
	"time"

	"github.com/Masterminds/semver/v3"
	"github.com/go-logr/logr"
	"github.com/rancher/fleet/internal/cmd/agent/deployer/kv"
	fleetutil "github.com/rancher/fleet/internal/cmd/controller/errorutil"
	"github.com/rancher/fleet/internal/cmd/controller/finalize"
	"github.com/rancher/fleet/internal/cmd/controller/summary"
	"github.com/rancher/fleet/internal/cmd/controller/target"
	"github.com/rancher/fleet/internal/experimental"
	"github.com/rancher/fleet/internal/helmvalues"
	"github.com/rancher/fleet/internal/manifest"
	"github.com/rancher/fleet/internal/metrics"
	"github.com/rancher/fleet/internal/ocistorage"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	fleetevent "github.com/rancher/fleet/pkg/event"
	"github.com/rancher/fleet/pkg/sharding"
	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	errutil "k8s.io/apimachinery/pkg/util/errors"
	"k8s.io/apimachinery/pkg/util/sets"
	"k8s.io/client-go/tools/record"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/builder"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"
)

const (
	// period after which the Bundle reconciler is re-scheduled, in order to wait for the BundleDeploymentReconciler cleanup to finish
	requeueAfterBundleDeploymentCleanup = 2 * time.Second
)

type BundleQuery interface {
	// BundlesForCluster is used to map from a cluster to bundles
	BundlesForCluster(context.Context, *fleet.Cluster) ([]*fleet.Bundle, []*fleet.Bundle, error)
}

type Store interface {
	Store(context.Context, *manifest.Manifest) error
}

type TargetBuilder interface {
	Targets(ctx context.Context, bundle *fleet.Bundle, manifestID string) ([]*target.Target, error)
}

// BundleReconciler reconciles a Bundle object
type BundleReconciler struct {
	client.Client
	Scheme   *runtime.Scheme
	Recorder record.EventRecorder

	Builder TargetBuilder
	Store   Store
	Query   BundleQuery
	ShardID string

	Workers int
}

// SetupWithManager sets up the controller with the Manager.
func (r *BundleReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&fleet.Bundle{},
			builder.WithPredicates(
				// do not trigger for bundle status changes (except for cache sync)
				predicate.Or(
					TypedResourceVersionUnchangedPredicate[client.Object]{},
					predicate.GenerationChangedPredicate{},
					predicate.AnnotationChangedPredicate{},
					predicate.LabelChangedPredicate{},
				),
			),
		).
		// Note: Maybe improve with WatchesMetadata, does it have access to labels?
		Watches(
			// Fan out from bundledeployment to bundle, this is useful to update the
			// bundle's status fields.
			&fleet.BundleDeployment{}, handler.EnqueueRequestsFromMapFunc(BundleDeploymentMapFunc(r)),
			builder.WithPredicates(bundleDeploymentStatusChangedPredicate()),
		).
		Watches(
			// Fan out from cluster to bundle, this is useful for targeting and templating.
			&fleet.Cluster{},
			handler.EnqueueRequestsFromMapFunc(func(ctx context.Context, a client.Object) []ctrl.Request {
				cluster := a.(*fleet.Cluster)
				bundlesToRefresh, _, err := r.Query.BundlesForCluster(ctx, cluster)
				if err != nil {
					return nil
				}
				requests := []ctrl.Request{}
				for _, bundle := range bundlesToRefresh {
					if !sharding.ShouldProcess(bundle, r.ShardID) {
						continue
					}
					requests = append(requests, ctrl.Request{
						NamespacedName: types.NamespacedName{
							Namespace: bundle.Namespace,
							Name:      bundle.Name,
						},
					})
				}

				return requests
			}),
			builder.WithPredicates(clusterChangedPredicate()),
		).
		WithEventFilter(sharding.FilterByShardID(r.ShardID)).
		WithOptions(controller.Options{MaxConcurrentReconciles: r.Workers}).
		Complete(r)
}

//+kubebuilder:rbac:groups=fleet.cattle.io,resources=bundles,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=bundles/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=bundles/finalizers,verbs=update

// Reconcile creates bundle deployments for a bundle
//
//nolint:gocyclo
func (r *BundleReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx).WithName("bundle")
	ctx = log.IntoContext(ctx, logger)

	bundle := &fleet.Bundle{}
	if err := r.Get(ctx, req.NamespacedName, bundle); err != nil {
		return ctrl.Result{}, client.IgnoreNotFound(err)
	}

	if bundle.Labels[fleet.RepoLabel] != "" {
		logger = logger.WithValues(
			"gitrepo", bundle.Labels[fleet.RepoLabel],
			"commit", bundle.Labels[fleet.CommitLabel],
		)
	}

	if userID := bundle.Labels[fleet.CreatedByUserIDLabel]; userID != "" {
		logger = logger.WithValues("userID", userID)
	}

	if !bundle.DeletionTimestamp.IsZero() {
		return r.handleDelete(ctx, logger, req, bundle)
	}

	bundleOrig := bundle.DeepCopy()

	if err := finalize.EnsureFinalizer(ctx, r.Client, bundle, finalize.BundleFinalizer); err != nil {
		return ctrl.Result{}, fmt.Errorf("%w, failed to add finalizer to bundle: %w", fleetutil.ErrRetryable, err)
	}

	// Migration: Remove the obsolete created-by-display-name label if it exists
	if err := r.removeDisplayNameLabel(ctx, bundle); err != nil {
		return r.computeResult(ctx, logger, bundleOrig, bundle, "failed to remove display name label", err)
	}

	logger.V(1).Info(
		"Reconciling bundle, checking targets, calculating changes, building objects",
		"generation",
		bundle.Generation,
		"observedGeneration",
		bundle.Status.ObservedGeneration,
	)

	// The values secret is optional, e.g. for non-helm type bundles.
	// This sets the values on the bundle, which is safe as we don't update bundle, just its status
	if bundle.Spec.ValuesHash != "" {
		if err := loadBundleValues(ctx, r.Client, bundle); err != nil {
			return r.computeResult(ctx, logger, bundleOrig, bundle, "failed to load values secret for bundle", err)
		}
	}

	contentsInOCI := bundle.Spec.ContentsID != "" && ocistorage.OCIIsEnabled()
	contentsInHelmChart := bundle.Spec.HelmOpOptions != nil

	// Skip bundle deployment creation if the bundle is a HelmOps bundle and the configured Helm version is still a
	// version constraint. That constraint should be resolved into a strict version by the HelmOps reconciler before bundle
	// deployments can be created.
	if contentsInHelmChart && bundle.Spec.Helm != nil && len(bundle.Spec.Helm.Version) > 0 {
		// #3953
		// There are helm repositories that list chart versions with the "v" prefix.
		// That's not recommended by Helm as the version with the prefix is not semver compliant,
		// but those repositories are still valid.
		// Delete the "v" prefix (if found) before checking for a valid semver.
		versionToCheck := strings.TrimPrefix(bundle.Spec.Helm.Version, "v")
		if _, err := semver.StrictNewVersion(versionToCheck); err != nil {
			err = fmt.Errorf("chart version cannot be deployed; check HelmOp status for more details: %w", err)

			return ctrl.Result{}, r.updateErrorStatus(ctx, bundleOrig, bundle, err)
		}
	}

	manifestID := bundle.Spec.ContentsID
	var resourcesManifest *manifest.Manifest
	if !contentsInOCI && !contentsInHelmChart {
		resourcesManifest = manifest.FromBundle(bundle)
		if bundle.Generation != bundle.Status.ObservedGeneration {
			resourcesManifest.ResetSHASum()
		}

		manifestDigest, err := resourcesManifest.SHASum()
		if err != nil {
			return ctrl.Result{},
				r.updateErrorStatus(
					ctx,
					bundleOrig,
					bundle,
					fmt.Errorf("failed to compute resources manifest SHA sum for OCI storage: %w", err),
				)
		}
		bundle.Status.ResourcesSHA256Sum = manifestDigest

		manifestID, err = resourcesManifest.ID()
		if err != nil {
			// this should never happen, since manifest.SHASum() cached the result and worked above.
			return ctrl.Result{},
				r.updateErrorStatus(
					ctx,
					bundleOrig,
					bundle,
					fmt.Errorf("failed to compute resources manifest ID for OCI storage: %w", err),
				)
		}
	}

	matchedTargets, err := r.Builder.Targets(ctx, bundle, manifestID)
	if err != nil {
		return ctrl.Result{},
			r.updateErrorStatus(
				ctx,
				bundleOrig,
				bundle,
				fmt.Errorf("targeting error: %w", err),
			)
	}

	if (!contentsInOCI && !contentsInHelmChart) && len(matchedTargets) > 0 {
		// when not using the OCI registry or helm chart we need to create a contents resource
		// so the BundleDeployments are able to access the contents to be deployed.
		// Otherwise, do not create a content resource if there are no targets.
		// `fleet apply` puts all resources into `bundle.Spec.Resources`.
		// `Store` copies all the resources into the content resource.
		// There is no pruning of unused resources. Therefore we write
		// the content resource immediately, even though
		// `BundleDeploymentOptions`, e.g. `targetCustomizations` on
		// the `helm.Chart` field, change which resources are used. The
		// agents have access to all resources and use their specific
		// set of `BundleDeploymentOptions`.
		if err := r.Store.Store(ctx, resourcesManifest); err != nil {
			return r.computeResult(ctx, logger, bundleOrig, bundle, "could not copy manifest into Content resource", err)
		}
	}
	logger = logger.WithValues("manifestID", manifestID)

	if err := resetStatus(&bundle.Status, matchedTargets); err != nil {
		err = fmt.Errorf("failed to reset bundle status from targets: %w", err)

		return ctrl.Result{}, r.updateErrorStatus(ctx, bundleOrig, bundle, err)
	}

	// this will add the defaults for a new bundledeployment. It propagates stagedOptions to options.
	if err := target.UpdatePartitions(&bundle.Status, matchedTargets); err != nil {
		err = fmt.Errorf("failed to update partitions: %w", err)

		return ctrl.Result{}, r.updateErrorStatus(ctx, bundleOrig, bundle, err)
	}

	if contentsInOCI {
		url, err := r.getOCIReference(ctx, bundle)
		if err != nil {
			return r.computeResult(ctx, logger, bundleOrig, bundle, "failed to build OCI reference", err)
		}
		bundle.Status.OCIReference = url
	}

	// ResourceKey is deprecated and no longer used by the UI.
	bundle.Status.ResourceKey = nil

	summary.SetReadyConditions(&bundle.Status, "Cluster", bundle.Status.Summary)
	bundle.Status.ObservedGeneration = bundle.Generation

	// build BundleDeployments out of targets discarding Status, replacing DependsOn with the
	// bundle's DependsOn (pure function) and replacing the labels with the bundle's labels
	merr := []error{}
	bundleDeploymentUIDs := make(sets.Set[types.UID])
	for _, target := range matchedTargets {
		if target.Deployment == nil {
			continue
		}
		if target.Deployment.Namespace == "" {
			logger.V(1).Info(
				"Skipping bundledeployment with empty namespace, waiting for agentmanagement to set cluster.status.namespace",
				"bundledeployment", target.Deployment,
			)
			continue
		}

		// NOTE we don't re-use the existing BundleDeployment, we discard annotations, status, etc.
		// and copy labels from Bundle as they might have changed.
		// However, matchedTargets target.Deployment contains existing BundleDeployments.
		bd := target.BundleDeployment()
		logger := logger.WithValues("bundledeployment", bd.Name)

		// No need to check the deletion timestamp here before adding a finalizer, since the bundle has just
		// been created.
		controllerutil.AddFinalizer(bd, finalize.BundleDeploymentFinalizer)

		bd.Spec.OCIContents = contentsInOCI
		bd.Spec.HelmChartOptions = bundle.Spec.HelmOpOptions

		valuesHash, optionsSecret, err := r.manageOptionsSecret(ctx, bd)
		if err != nil {
			return r.computeResult(ctx, logger, bundleOrig, bundle, "failed to initialize options secret", err)
		}

		// Changes in the values hash trigger a bundle deployment reconcile.
		bd.Spec.ValuesHash = valuesHash

		helmvalues.ClearOptions(bd)

		// If there's already a bundledeployment for this target, track its UID
		// before calling createBundleDeployment, which might fail. This prevents
		// cleanupOrphanedBundleDeployments from incorrectly removing this bundledeployment
		// as "orphaned". See https://github.com/rancher/fleet/issues/4144
		if target.Deployment != nil && target.Deployment.UID != "" {
			bundleDeploymentUIDs.Insert(target.Deployment.UID)
		}

		op, bd, err := r.createBundleDeployment(
			ctx,
			logger,
			bd,
			contentsInOCI,
			bundle.Spec.HelmOpOptions != nil,
			manifestID)
		if err != nil {
			// We could end up here, because we cannot add a
			// finalizer to a content resource, which has a
			// deletion timestamp.
			// Log the problem and keep trying to create the other
			// bundledeployments, but retry the whole reconcile
			// afterwards.
			merr = append(merr, fmt.Errorf("failed to create bundle deployment: %w", err))
			logger.Info(fmt.Sprintf("failed to create a bundledeployment, skipping and requeuing: %v", err))
			continue
		}
		bundleDeploymentUIDs.Insert(bd.UID)

		// At this stage, we know the UID of our bundle deployment, hence we can use it to populate the owner reference in the
		// options secret.
		// If the bundle deployment already existed and has simply been updated, the secret will already bear an owner
		// reference from its creation or latest update.
		if op == controllerutil.OperationResultCreated {
			if err := r.ensureOwnerReferences(ctx, bd, optionsSecret); err != nil {
				return r.computeResult(ctx, logger, bundleOrig, bundle, "failed to ensure owner references are set in options secret", err)
			}
		}

		if err := r.handleDownstreamObjects(ctx, bundle, bd); err != nil {
			return r.computeResult(ctx, logger, bundleOrig, bundle, "failed to clone config maps and secrets downstream", err)
		}

		if err := r.handleContentAccessSecrets(ctx, bundle, bd); err != nil {
			return r.computeResult(ctx, logger, bundleOrig, bundle, "failed to clone secrets downstream", err)
		}
	}

	// the targets configuration may have changed, leaving behind some BundleDeployments that are no longer needed
	if err := r.cleanupOrphanedBundleDeployments(ctx, bundle, bundleDeploymentUIDs); err != nil {
		logger.V(1).Error(err, "deleting orphaned bundle deployments", "bundle", bundle.GetName())
	}

	updateDisplay(&bundle.Status)
	if err := r.updateStatus(ctx, bundleOrig, bundle); err != nil {
		merr = append(merr, err)
		return ctrl.Result{}, errutil.NewAggregate(merr)
	}

	return ctrl.Result{}, errutil.NewAggregate(merr)
}

// handleDelete runs cleanup for resources associated to a Bundle, finally removing the finalizer to unblock the deletion of the object from kubernetes.
func (r *BundleReconciler) handleDelete(ctx context.Context, logger logr.Logger, req ctrl.Request, bundle *fleet.Bundle) (ctrl.Result, error) {
	if !controllerutil.ContainsFinalizer(bundle, finalize.BundleFinalizer) {
		return ctrl.Result{}, nil
	}

	bds, err := r.listBundleDeploymentsForBundle(ctx, bundle)
	if err != nil {
		return ctrl.Result{}, err
	}

	// BundleDeployment deletion happens asynchronously: mark them for deletion and requeue
	// This ensures the Bundle is kept around until all its BundleDeployments are completely deleted.
	// Both GitRepo and HelmOp status reconcilers rely on this condition, as they watch Bundles and not BundleDeployments
	if len(bds) > 0 {
		logger.V(1).Info("Bundle deleted, purging bundle deployments")
		return ctrl.Result{RequeueAfter: requeueAfterBundleDeploymentCleanup}, batchDeleteBundleDeployments(ctx, r.Client, bds)
	}

	if err := r.maybeDeleteOCIArtifact(ctx, bundle); err != nil {
		return ctrl.Result{}, err
	}

	metrics.BundleCollector.Delete(req.Name, req.Namespace)
	controllerutil.RemoveFinalizer(bundle, finalize.BundleFinalizer)
	if err := r.Update(ctx, bundle); err != nil {
		return ctrl.Result{}, err
	}

	// pro-actively delete the bundle's secret. k8s owner garbage collection will handle remaining orphans.
	if err := r.Delete(ctx, &corev1.Secret{ObjectMeta: metav1.ObjectMeta{Name: bundle.Name, Namespace: bundle.Namespace}}); err != nil && !apierrors.IsNotFound(err) {
		logger.V(1).Info("Cannot delete bundle's values secret, owner garbage collection will remove it")
	}

	return ctrl.Result{}, err
}

// removeDisplayNameLabel removes the obsolete created-by-display-name label from the bundle if it exists.
func (r *BundleReconciler) removeDisplayNameLabel(ctx context.Context, bundle *fleet.Bundle) error {
	if bundle.Labels == nil {
		return nil
	}

	const deprecatedLabel = "fleet.cattle.io/created-by-display-name"
	if _, exists := bundle.Labels[deprecatedLabel]; !exists {
		return nil
	}

	delete(bundle.Labels, deprecatedLabel)
	if err := r.Update(ctx, bundle); err != nil {
		return fmt.Errorf("%w: %w", fleetutil.ErrRetryable, err)
	}
	return nil
}

func (r *BundleReconciler) createBundleDeployment(
	ctx context.Context,
	l logr.Logger,
	bd *fleet.BundleDeployment,
	contentsInOCI bool,
	contentsInHelmChart bool,
	manifestID string,
) (controllerutil.OperationResult, *fleet.BundleDeployment, error) {
	logger := l.WithValues("deploymentID", bd.Spec.DeploymentID)

	// When content resources are stored in etcd, we need to add finalizers.
	if !contentsInOCI && !contentsInHelmChart {
		content := &fleet.Content{}
		if err := r.Get(ctx, types.NamespacedName{Name: manifestID}, content); err != nil {
			return controllerutil.OperationResultNone, nil, fmt.Errorf("failed to get content resource: %w", err)
		}

		if added := controllerutil.AddFinalizer(content, bd.Name); added {
			if err := r.Update(ctx, content); err != nil {
				return controllerutil.OperationResultNone, nil, fmt.Errorf(
					"could not add finalizer to content resource, thus cannot create/update bundledeployment: %w",
					err,
				)
			}
		}
	}

	updated := bd.DeepCopy()
	op, err := controllerutil.CreateOrUpdate(ctx, r.Client, bd, func() error {
		// When this mutation function is called by CreateOrUpdate, bd contains the
		// _old_ bundle deployment, if any.
		// The corresponding Content resource must only be deleted if it is no longer in use, ie if the
		// latest version of the bundle points to a different deployment ID.
		// An empty value for bd.Spec.DeploymentID means that we are deploying the first version of this
		// bundle, hence there are no Contents left over to purge.
		if (!bd.Spec.OCIContents || !contentsInHelmChart) &&
			bd.Spec.DeploymentID != "" &&
			bd.Spec.DeploymentID != updated.Spec.DeploymentID {
			if err := finalize.PurgeContent(ctx, r.Client, bd.Name, bd.Spec.DeploymentID); err != nil {
				logger.Error(err, "Reconcile failed to purge old content resource")
			}
		}

		// check if there's any OCI secret that can be purged
		if err := maybePurgeOCIReferenceSecret(ctx, r.Client, bd, updated); err != nil {
			logger.Error(err, "Reconcile failed to purge old OCI reference secret")
		}

		bd.Spec = updated.Spec
		bd.Labels = updated.GetLabels()

		return nil
	})
	if err != nil {
		logger.Error(err, "Reconcile failed to create or update bundledeployment", "operation", op)
		return controllerutil.OperationResultNone, nil, err
	}
	logger.Info(upper(op)+" bundledeployment", "operation", op)

	return op, bd, nil
}

// manageOptionsSecret creates a secret, or updates the existing one, containing options extracted from bd, ensuring
// that said secret is up-to-date. If no options are extracted from bd, it deletes any existing options secret.
// Returns a hash of options, a pointer to the options secret and an error, if any.
func (r *BundleReconciler) manageOptionsSecret(
	ctx context.Context,
	bd *fleet.BundleDeployment,
) (string, *corev1.Secret, error) {
	hash, options, stagedOptions, err := helmvalues.ExtractOptions(bd)
	if err != nil {
		return "", nil, fmt.Errorf("failed to extract Helm options for secret creation: %w", err)
	}

	if hash == "" {
		// No values to store, delete the secret if it exists
		if err := r.Delete(ctx, &corev1.Secret{
			ObjectMeta: metav1.ObjectMeta{Name: bd.Name, Namespace: bd.Namespace},
		}); err != nil && !apierrors.IsNotFound(err) {
			return "", nil, fmt.Errorf("%w: failed to delete options secret: %w", fleetutil.ErrRetryable, err)
		}

		return "", nil, nil
	}

	// Ensure secret is up-to-date
	secret := &corev1.Secret{
		Type: fleet.SecretTypeBundleDeploymentOptions,
		ObjectMeta: metav1.ObjectMeta{
			Name:      bd.Name,
			Namespace: bd.Namespace,
		},
	}

	if _, err := controllerutil.CreateOrUpdate(ctx, r.Client, secret, func() error {
		// Setting the owner reference on the secret at create/update time is more efficient than doing it separately
		// after the bundle deployment is updated, if the bundle deployment already exists (in which case its UID is
		// non-empty and immutable).
		if bd.GetUID() != "" {
			if err := controllerutil.SetControllerReference(bd, secret, r.Scheme); err != nil {
				return err
			}
		}

		secret.Data = map[string][]byte{
			helmvalues.ValuesKey:       options,
			helmvalues.StagedValuesKey: stagedOptions,
		}
		return nil
	}); err != nil {
		return "", nil, fmt.Errorf("%w: %w", fleetutil.ErrRetryable, err)
	}

	return hash, secret, nil
}

// ensureOwnerReferences sets bd as the owner of s, and returns any error occurring in the process.
func (r *BundleReconciler) ensureOwnerReferences(ctx context.Context, bd *fleet.BundleDeployment, s *corev1.Secret) error {
	if s == nil {
		return nil
	}

	_, err := controllerutil.CreateOrUpdate(ctx, r.Client, s, func() error {
		return controllerutil.SetControllerReference(bd, s, r.Scheme)
	})
	if err != nil {
		err = fmt.Errorf("%w: %w", fleetutil.ErrRetryable, err)
	}

	return err
}

func (r *BundleReconciler) getOCIReference(ctx context.Context, bundle *fleet.Bundle) (string, error) {
	if bundle.Spec.ContentsID == "" {
		return "", fmt.Errorf("cannot get OCI reference. Bundle's ContentsID is not set")
	}
	namespacedName := types.NamespacedName{
		Namespace: bundle.Namespace,
		Name:      bundle.Spec.ContentsID,
	}
	var ociSecret corev1.Secret
	if err := r.Get(ctx, namespacedName, &ociSecret); err != nil {
		return "", fmt.Errorf("%w: %w", fleetutil.ErrRetryable, err)
	}
	ref, ok := ociSecret.Data[ocistorage.OCISecretReference]
	if !ok {
		return "", fmt.Errorf("expected data [reference] not found in secret: %s", bundle.Spec.ContentsID)
	}
	// this is not a valid reference, it is only for display
	return fmt.Sprintf("oci://%s/%s:latest", string(ref), bundle.Spec.ContentsID), nil
}

// cloneConfigMap clones a config map, identified by the provided name and
// namespace, to the namespace of the provided bundle deployment bd. This makes
// the config map available to agents when deploying bd to downstream clusters.
func (r *BundleReconciler) cloneConfigMap(
	ctx context.Context,
	namespace string,
	name string,
	bd *fleet.BundleDeployment,
) error {
	namespacedName := types.NamespacedName{
		Namespace: namespace,
		Name:      name,
	}
	var cm corev1.ConfigMap
	if err := r.Get(ctx, namespacedName, &cm); err != nil {
		return fmt.Errorf("failed to load source config map, cannot clone into %q: %w", namespace, err)
	}
	// clone the config map, and just change the namespace so it's in the target's namespace
	targetCM := &corev1.ConfigMap{
		ObjectMeta: metav1.ObjectMeta{
			Name:      cm.Name,
			Namespace: bd.Namespace,
		},
		Data:       cm.Data,
		BinaryData: cm.BinaryData,
	}

	if err := controllerutil.SetControllerReference(bd, targetCM, r.Scheme); err != nil {
		return err
	}

	updated := targetCM.DeepCopy()
	if _, err := controllerutil.CreateOrUpdate(ctx, r.Client, targetCM, func() error {
		targetCM.Data = updated.Data
		targetCM.BinaryData = updated.BinaryData

		return nil
	}); err != nil {
		return fmt.Errorf("failed to create or update source config map %s/%s: %w", bd.Namespace, cm.Name, err)
	}

	return nil
}

// cloneSecret clones a secret, identified by the provided secretName and
// namespace, to the namespace of the provided bundle deployment bd. This makes
// the secret available to agents when deploying bd to downstream clusters.
func (r *BundleReconciler) cloneSecret(
	ctx context.Context,
	ns string,
	secretName string,
	secretType string,
	bd *fleet.BundleDeployment,
) error {
	namespacedName := types.NamespacedName{
		Namespace: ns,
		Name:      secretName,
	}
	var secret corev1.Secret
	if err := r.Get(ctx, namespacedName, &secret); err != nil {
		return fmt.Errorf("failed to load source secret, cannot clone into %q: %w", ns, err)
	}
	// clone the secret, and just change the namespace so it's in the target's namespace
	targetSecret := &corev1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:      secret.Name,
			Namespace: bd.Namespace,
		},
		Data:       secret.Data,
		StringData: secret.StringData,
	}

	if secretType != "" {
		targetSecret.Labels = map[string]string{fleet.InternalSecretLabel: "true"}
		targetSecret.Type = corev1.SecretType(secretType)
	}

	if err := controllerutil.SetControllerReference(bd, targetSecret, r.Scheme); err != nil {
		return err
	}

	updated := targetSecret.DeepCopy()
	if _, err := controllerutil.CreateOrUpdate(ctx, r.Client, targetSecret, func() error {
		targetSecret.Data = updated.Data
		targetSecret.StringData = updated.StringData

		return nil
	}); err != nil {
		return fmt.Errorf("failed to create or update source secret %s/%s: %w", bd.Namespace, secret.Name, err)
	}

	return nil
}

func (r *BundleReconciler) handleContentAccessSecrets(ctx context.Context, bundle *fleet.Bundle, bd *fleet.BundleDeployment) error {
	contentsInOCI := bundle.Spec.ContentsID != "" && ocistorage.OCIIsEnabled()
	contentsInHelmChart := bundle.Spec.HelmOpOptions != nil

	if contentsInOCI {
		if err := r.cloneSecret(
			ctx,
			bundle.Namespace,
			bundle.Spec.ContentsID,
			fleet.SecretTypeOCIStorage,
			bd,
		); err != nil {
			return fmt.Errorf(
				"%w: failed to clone secret %s/%s to downstream cluster namespace: %w",
				fleetutil.ErrRetryable,
				bundle.Namespace,
				bundle.Spec.ContentsID,
				err,
			)
		}
	}
	if contentsInHelmChart && bundle.Spec.HelmOpOptions.SecretName != "" {
		if err := r.cloneSecret(
			ctx,
			bundle.Namespace,
			bundle.Spec.HelmOpOptions.SecretName,
			fleet.SecretTypeHelmOpsAccess,
			bd,
		); err != nil {
			return fmt.Errorf(
				"%w: failed to clone secret %s/%s to downstream cluster namespace: %w",
				fleetutil.ErrRetryable,
				bundle.Namespace,
				bundle.Spec.HelmOpOptions.SecretName,
				err,
			)
		}
	}
	return nil
}

// updateErrorStatus sets the Ready condition in the bundle status and tries to update the resource.
// Setting that condition makes the error message visible in the Rancher UI.
// Upon successful update of the status, updateErrorStatus returns a TerminalError, preventing requeues.
func (r *BundleReconciler) updateErrorStatus(
	ctx context.Context,
	orig, bundle *fleet.Bundle,
	orgErr error,
) error {
	SetCondition(string(fleet.Ready), &bundle.Status, orgErr)

	if statusErr := r.updateStatus(ctx, orig, bundle); statusErr != nil {
		merr := []error{orgErr, fmt.Errorf("failed to update the status: %w", statusErr)}
		return errutil.NewAggregate(merr)
	}

	return reconcile.TerminalError(orgErr)
}

func (r *BundleReconciler) handleDownstreamObjects(ctx context.Context, bundle *fleet.Bundle, bd *fleet.BundleDeployment) error {
	if !experimental.CopyResourcesDownstreamEnabled() {
		return nil
	}

	for _, dr := range bundle.Spec.DownstreamResources {
		switch strings.ToLower(dr.Kind) {
		case "secret":
			if err := r.cloneSecret(ctx, bundle.Namespace, dr.Name, "", bd); err != nil {
				return fmt.Errorf(
					"%w: failed to copy secret %s/%s to downstream cluster namespace: %w",
					fleetutil.ErrRetryable,
					bundle.Namespace,
					dr.Name,
					err,
				)
			}
		case "configmap":
			if err := r.cloneConfigMap(ctx, bundle.Namespace, dr.Name, bd); err != nil {
				return fmt.Errorf(
					"%w: failed to copy config map %s/%s to downstream cluster namespace: %w",
					fleetutil.ErrRetryable,
					bundle.Namespace,
					dr.Name,
					err,
				)
			}
		default:
			return fmt.Errorf("unsupported kind for object to copy to downstream: %q", dr.Kind)
		}
	}

	return nil
}

// updateStatus patches the status of the bundle and collects metrics upon a successful update of
// the bundle status. It returns nil if the status update is successful, otherwise it returns an
// error.
func (r *BundleReconciler) updateStatus(ctx context.Context, orig *fleet.Bundle, bundle *fleet.Bundle) error {
	logger := log.FromContext(ctx).WithName("bundle - updateStatus")
	statusPatch := client.MergeFrom(orig)

	if patchData, err := statusPatch.Data(bundle); err == nil && string(patchData) == "{}" {
		// skip update if patch is empty
		return nil
	}
	if err := r.Status().Patch(ctx, bundle, statusPatch); err != nil {
		logger.V(1).Info("Reconcile failed update to bundle status", "status", bundle.Status, "error", err)
		return err
	}
	metrics.BundleCollector.Collect(ctx, bundle)
	return nil
}

func (r *BundleReconciler) listBundleDeploymentsForBundle(ctx context.Context, bundle *fleet.Bundle) ([]fleet.BundleDeployment, error) {
	list := &fleet.BundleDeploymentList{}
	if err := r.List(ctx, list,
		client.MatchingLabels{
			fleet.BundleLabel:          bundle.GetName(),
			fleet.BundleNamespaceLabel: bundle.GetNamespace(),
		},
	); err != nil {
		return nil, err
	}
	return list.Items, nil
}

// cleanupOrphanedBundleDeployments will delete all existing BundleDeployments which do not have a match in a provided list of UIDs
func (r *BundleReconciler) cleanupOrphanedBundleDeployments(ctx context.Context, bundle *fleet.Bundle, uidsToKeep sets.Set[types.UID]) error {
	list, err := r.listBundleDeploymentsForBundle(ctx, bundle)
	if err != nil {
		return err
	}
	toDelete := slices.DeleteFunc(list, func(bd fleet.BundleDeployment) bool {
		// don't delete BundleDeployments that are not in schedule as
		// that would uninstall the deployment in the agent
		return uidsToKeep.Has(bd.UID) || bd.Spec.OffSchedule
	})
	return batchDeleteBundleDeployments(ctx, r.Client, toDelete)
}

func (r *BundleReconciler) maybeDeleteOCIArtifact(ctx context.Context, bundle *fleet.Bundle) error {
	if bundle.Spec.ContentsID == "" {
		return nil
	}

	secretID := client.ObjectKey{Name: bundle.Spec.ContentsID, Namespace: bundle.Namespace}
	opts, err := ocistorage.ReadOptsFromSecret(ctx, r.Client, secretID)
	if err != nil {
		return err
	}
	err = ocistorage.NewOCIWrapper().DeleteManifest(ctx, opts, bundle.Spec.ContentsID)
	if err != nil {
		r.Recorder.Event(bundle, fleetevent.Warning, "FailedToDeleteOCIArtifact", fmt.Sprintf("deleting OCI artifact %q: %v", bundle.Spec.ContentsID, err.Error()))
	}

	// In case there's an error deleting from the OCI registry,
	// we return nil because otherwise the controller would retry the operation,
	// and since the registry is not a component of Fleet,
	// we don't have full control over it.
	return nil
}

// computeResult computes the controller result and error to return, depending on whether err is a retryable
// error, which will be wrapped with the provided prefix.
// If err is non-retryable, it will be propagated to the bundle status.
func (r *BundleReconciler) computeResult(
	ctx context.Context,
	logger logr.Logger,
	bundleOrig,
	bundle *fleet.Bundle,
	prefix string,
	err error,
) (ctrl.Result, error) {
	if done, res, ctrlErr := CheckRetryable(err, logger); done {
		return res, ctrlErr
	}

	err = fmt.Errorf("%s: %w", prefix, err)

	SetCondition(string(fleet.Ready), &bundle.Status, err)

	return ctrl.Result{}, r.updateErrorStatus(ctx, bundleOrig, bundle, err)
}

func batchDeleteBundleDeployments(ctx context.Context, c client.Client, list []fleet.BundleDeployment) error {
	var errs []error
	for _, bd := range list {
		if bd.DeletionTimestamp != nil {
			// already being deleted
			continue
		}
		// Mark the object for deletion. The BundleDeployment reconciler will react to that calling PurgeContent and finally removing the finalizer
		if err := c.Delete(ctx, &bd); client.IgnoreNotFound(err) != nil {
			errs = append(errs, err)
		}

		// k8s ownership garbage collection can take a long time, so we explicitly delete the secrets here.
		// GC will delete any remaining orphaned secrets, no need to add an error.
		_ = c.Delete(ctx, &corev1.Secret{ObjectMeta: metav1.ObjectMeta{Name: bd.Name, Namespace: bd.Namespace}})
	}

	return errors.Join(errs...)
}

// clusterChangedPredicate filters cluster events that relate to bundldeployment creation.
func clusterChangedPredicate() predicate.Funcs {
	return predicate.Funcs{
		CreateFunc: func(e event.CreateEvent) bool {
			return true
		},
		UpdateFunc: func(e event.UpdateEvent) bool {
			n := e.ObjectNew.(*fleet.Cluster)
			o := e.ObjectOld.(*fleet.Cluster)
			// cluster deletion will eventually trigger a delete event
			if n == nil || !n.DeletionTimestamp.IsZero() {
				return true
			}
			// labels and annotations are used for templating and targeting
			if !maps.Equal(n.Labels, o.Labels) {
				return true
			}
			if !maps.Equal(n.Annotations, o.Annotations) {
				return true
			}
			// spec templateValues is used in templating
			if !reflect.DeepEqual(n.Spec, o.Spec) {
				return true
			}
			// this namespace contains the bundledeployments
			if n.Status.Namespace != o.Status.Namespace {
				return true
			}
			// this namespace indicates the agent is running
			if n.Status.Agent.Namespace != o.Status.Agent.Namespace {
				return true
			}

			if n.Status.Scheduled != o.Status.Scheduled {
				return true
			}

			if n.Status.ActiveSchedule != o.Status.ActiveSchedule {
				return true
			}

			return false
		},
		DeleteFunc: func(e event.DeleteEvent) bool {
			return true
		},
	}
}

// loadBundleValues loads the values from the secret and sets them in the bundle spec
func loadBundleValues(ctx context.Context, c client.Client, bundle *fleet.Bundle) error {
	secret := &corev1.Secret{}
	if err := c.Get(ctx, types.NamespacedName{Name: bundle.Name, Namespace: bundle.Namespace}, secret); apierrors.IsNotFound(err) {
		return fmt.Errorf("%w, retrying to get values secret for bundle: %w", fleetutil.ErrRetryable, err)
	} else if err != nil {
		return fmt.Errorf("failed to get values secret for bundle: %w", err)
	}

	hash, err := helmvalues.HashValuesSecret(secret.Data)
	if err != nil {
		return fmt.Errorf("failed to hash values secret %q: %w", secret.Name, err)
	}

	if bundle.Spec.ValuesHash != hash {
		return fmt.Errorf("%w, retrying since bundle values secret has changed, expected hash %q, calculated %q", fleetutil.ErrRetryable, bundle.Spec.ValuesHash, hash)
	}

	if err := helmvalues.SetValues(bundle, secret.Data); err != nil {
		return fmt.Errorf("failed to set values from secret %q: %w", secret.Name, err)
	}

	return nil
}

// maybePurgeOCIReferenceSecret deletes an outdated OCI reference secret if necessary, i.e. if a bundle has been updated
// and either of the following applies:
// * the old bundle used OCI storage while the new one does not anymore
// * both old and new bundles use OCI storage, but the deployment ID has changed between the old bundles and the new one.
func maybePurgeOCIReferenceSecret(ctx context.Context, c client.Client, old, new *fleet.BundleDeployment) error {
	if !old.Spec.OCIContents || old.Spec.DeploymentID == "" {
		return nil
	}

	if !new.Spec.OCIContents || (old.Spec.DeploymentID != new.Spec.DeploymentID) {
		id, _ := kv.Split(old.Spec.DeploymentID, ":")
		var secret corev1.Secret
		secretID := client.ObjectKey{Name: id, Namespace: old.Namespace}
		if err := c.Get(ctx, secretID, &secret); err != nil {
			if !apierrors.IsNotFound(err) {
				return err
			}
		} else {
			if err := c.Delete(ctx, &secret); err != nil {
				return err
			}
		}
	}

	return nil
}

func upper(op controllerutil.OperationResult) string {
	switch op {
	case controllerutil.OperationResultNone:
		return "Unchanged"
	case controllerutil.OperationResultCreated:
		return "Created"
	case controllerutil.OperationResultUpdated:
		return "Updated"
	case controllerutil.OperationResultUpdatedStatus:
		return "Updated"
	case controllerutil.OperationResultUpdatedStatusOnly:
		return "Updated"
	default:
		return "Unknown"
	}
}

// BundleDeploymentMapFunc returns a function that maps a BundleDeployment to a Bundle.
func BundleDeploymentMapFunc(r *BundleReconciler) func(ctx context.Context, a client.Object) []reconcile.Request {
	return func(ctx context.Context, a client.Object) []reconcile.Request {
		bd, ok := a.(*fleet.BundleDeployment)
		if !ok {
			return nil
		}

		if !sharding.ShouldProcess(bd, r.ShardID) {
			return nil
		}
		labels := bd.GetLabels()
		if labels == nil {
			return nil
		}

		ns, name := target.BundleFromDeployment(labels)
		if ns != "" && name != "" {
			return []reconcile.Request{{
				NamespacedName: types.NamespacedName{
					Namespace: ns,
					Name:      name,
				},
			}}
		}

		return nil
	}
}



================================================
FILE: internal/cmd/controller/reconciler/bundle_controller_test.go
================================================
//go:generate mockgen --build_flags=--mod=mod -destination=../../../mocks/target_builder_mock.go -package=mocks github.com/rancher/fleet/internal/cmd/controller/reconciler TargetBuilder,Store
package reconciler_test

import (
	"context"
	"errors"
	"fmt"
	"net/http"
	"os"
	"strings"
	"testing"

	"github.com/google/go-cmp/cmp"
	"github.com/rancher/fleet/internal/cmd/controller/errorutil"
	"github.com/rancher/fleet/internal/cmd/controller/finalize"
	"github.com/rancher/fleet/internal/cmd/controller/reconciler"
	"github.com/rancher/fleet/internal/cmd/controller/target"
	"github.com/rancher/fleet/internal/mocks"
	fleetv1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/sharding"
	"github.com/rancher/wrangler/v3/pkg/genericcondition"

	"go.uber.org/mock/gomock"

	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	k8serrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/apimachinery/pkg/util/intstr"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"
)

func TestReconcile_FinalizerUpdateError(t *testing.T) {
	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	scheme := runtime.NewScheme()
	utilruntime.Must(batchv1.AddToScheme(scheme))

	bundle := fleetv1.Bundle{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "my-bundle",
			Namespace: "default",
		},
		Spec: fleetv1.BundleSpec{
			ValuesHash: "foo", // non-empty
		},
	}

	namespacedName := types.NamespacedName{Name: bundle.Name, Namespace: bundle.Namespace}

	mockClient := mocks.NewMockK8sClient(mockCtrl)

	mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&fleetv1.Bundle{}), gomock.Any()).DoAndReturn(
		func(ctx context.Context, req types.NamespacedName, b *fleetv1.Bundle, opts ...interface{}) error {
			b.Name = bundle.Name
			b.Namespace = bundle.Namespace
			// no finalizer

			b.Spec = bundle.Spec

			return nil
		},
	)

	mockClient.EXPECT().Update(gomock.Any(), gomock.AssignableToTypeOf(&fleetv1.Bundle{}), gomock.Any()).
		Return(errors.New("something went wrong"))

	expectedErrorMsg := "failed to add finalizer to bundle: something went wrong"

	// Not expecting any status update

	recorderMock := mocks.NewMockEventRecorder(mockCtrl)

	r := reconciler.BundleReconciler{
		Client:   mockClient,
		Scheme:   scheme,
		Recorder: recorderMock,
	}

	ctx := context.TODO()
	_, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
	if err == nil {
		t.Fatalf("expected an error, got nil")
	}

	if errors.Is(err, reconcile.TerminalError(nil)) {
		t.Fatalf("expected non-terminal error, got %v", err)
	}

	if !strings.Contains(err.Error(), expectedErrorMsg) {
		t.Errorf("unexpected error: %v", err)
	}
}

func TestReconcile_HelmValuesLoadError(t *testing.T) {
	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	scheme := runtime.NewScheme()
	utilruntime.Must(batchv1.AddToScheme(scheme))

	bundle := fleetv1.Bundle{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "my-bundle",
			Namespace: "default",
		},
		Spec: fleetv1.BundleSpec{
			ValuesHash: "foo", // non-empty
		},
	}

	namespacedName := types.NamespacedName{Name: bundle.Name, Namespace: bundle.Namespace}

	mockClient := mocks.NewMockK8sClient(mockCtrl)
	expectGetWithFinalizer(mockClient, bundle)

	mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{}), gomock.Any()).
		Return(errors.New("something went wrong"))

	expectedErrorMsg := "failed to load values secret for bundle:"

	statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
	mockClient.EXPECT().Status().Return(statusClient).Times(1)

	expectStatusPatch(t, statusClient, expectedErrorMsg)

	recorderMock := mocks.NewMockEventRecorder(mockCtrl)

	r := reconciler.BundleReconciler{
		Client:   mockClient,
		Scheme:   scheme,
		Recorder: recorderMock,
	}

	ctx := context.TODO()
	rs, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
	if !errors.Is(err, reconcile.TerminalError(nil)) {
		t.Errorf("expected terminal error, got: %v", err)
	}

	if rs.RequeueAfter != 0 {
		t.Errorf("expected no retries, with zero RequeueAfter in result")
	}
}

func TestReconcile_HelmVersionResolutionError(t *testing.T) {
	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	scheme := runtime.NewScheme()
	utilruntime.Must(batchv1.AddToScheme(scheme))

	bundle := fleetv1.Bundle{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "my-bundle",
			Namespace: "default",
		},
		Spec: fleetv1.BundleSpec{
			ContentsID: "foo", // non-empty
			BundleDeploymentOptions: fleetv1.BundleDeploymentOptions{
				Helm: &fleetv1.HelmOptions{
					Version: "0.1.x", // non-empty, non-strict version
				},
			},
			HelmOpOptions: &fleetv1.BundleHelmOptions{},
		},
	}

	namespacedName := types.NamespacedName{Name: bundle.Name, Namespace: bundle.Namespace}

	mockClient := mocks.NewMockK8sClient(mockCtrl)
	expectGetWithFinalizer(mockClient, bundle)

	expectedErrorMsg := "chart version cannot be deployed; check HelmOp status for more details:"

	statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
	mockClient.EXPECT().Status().Return(statusClient).Times(1)

	expectStatusPatch(t, statusClient, expectedErrorMsg)

	recorderMock := mocks.NewMockEventRecorder(mockCtrl)

	r := reconciler.BundleReconciler{
		Client:   mockClient,
		Scheme:   scheme,
		Recorder: recorderMock,
	}

	ctx := context.TODO()
	rs, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
	if !errors.Is(err, reconcile.TerminalError(nil)) {
		t.Errorf("expected terminal error, got: %v", err)
	}

	if rs.RequeueAfter != 0 {
		t.Errorf("expected no retries, with zero RequeueAfter in result")
	}
}

func TestReconcile_TargetsBuildingError(t *testing.T) {
	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	scheme := runtime.NewScheme()
	utilruntime.Must(batchv1.AddToScheme(scheme))

	bundle := fleetv1.Bundle{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "my-bundle",
			Namespace: "default",
		},
	}

	namespacedName := types.NamespacedName{Name: bundle.Name, Namespace: bundle.Namespace}

	mockClient := mocks.NewMockK8sClient(mockCtrl)
	expectGetWithFinalizer(mockClient, bundle)

	expectedErrorMsg := "targeting error: something went wrong"

	statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
	mockClient.EXPECT().Status().Return(statusClient).Times(1)

	expectStatusPatch(t, statusClient, expectedErrorMsg)

	recorderMock := mocks.NewMockEventRecorder(mockCtrl)

	targetBuilderMock := mocks.NewMockTargetBuilder(mockCtrl)
	targetBuilderMock.EXPECT().Targets(gomock.Any(), gomock.Any(), gomock.Any()).Return(nil, errors.New("something went wrong"))

	r := reconciler.BundleReconciler{
		Client:   mockClient,
		Scheme:   scheme,
		Recorder: recorderMock,
		Builder:  targetBuilderMock,
	}

	ctx := context.TODO()
	rs, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
	if !errors.Is(err, reconcile.TerminalError(nil)) {
		t.Errorf("expected terminal error, got: %v", err)
	}

	if rs.RequeueAfter != 0 {
		t.Errorf("expected no retries, with zero RequeueAfter in result")
	}
}

func TestReconcile_StatusResetFromTargetsError(t *testing.T) {
	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	scheme := runtime.NewScheme()
	utilruntime.Must(batchv1.AddToScheme(scheme))

	bundle := fleetv1.Bundle{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "my-bundle",
			Namespace: "default",
		},
		Spec: fleetv1.BundleSpec{
			RolloutStrategy: &fleetv1.RolloutStrategy{
				MaxUnavailable: &intstr.IntOrString{Type: intstr.String, StrVal: "foo"}, // will fail to parse as number or percentage
			},
		},
	}

	namespacedName := types.NamespacedName{Name: bundle.Name, Namespace: bundle.Namespace}

	mockClient := mocks.NewMockK8sClient(mockCtrl)
	expectGetWithFinalizer(mockClient, bundle)

	expectedErrorMsg := "failed to reset bundle status from targets: invalid maxUnavailable"

	statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
	mockClient.EXPECT().Status().Return(statusClient).Times(1)

	expectStatusPatch(t, statusClient, expectedErrorMsg)

	recorderMock := mocks.NewMockEventRecorder(mockCtrl)

	matchedTargets := []*target.Target{
		{
			Bundle: &bundle,
			Cluster: &fleetv1.Cluster{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "my-ns",
					Name:      "my-cluster",
				},
			},
			Deployment: &fleetv1.BundleDeployment{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "my-bd", // non-empty
				},
			},
			DeploymentID: "foo",
		},
	}
	targetBuilderMock := mocks.NewMockTargetBuilder(mockCtrl)
	targetBuilderMock.EXPECT().Targets(gomock.Any(), gomock.Any(), gomock.Any()).Return(matchedTargets, nil)

	storeMock := mocks.NewMockStore(mockCtrl)
	storeMock.EXPECT().Store(gomock.Any(), gomock.Any()).Return(nil)

	r := reconciler.BundleReconciler{
		Client:   mockClient,
		Scheme:   scheme,
		Recorder: recorderMock,
		Builder:  targetBuilderMock,
		Store:    storeMock,
	}

	ctx := context.TODO()
	rs, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
	if !errors.Is(err, reconcile.TerminalError(nil)) {
		t.Errorf("expected terminal error, got: %v", err)
	}

	if rs.RequeueAfter != 0 {
		t.Errorf("expected no retries, with zero RequeueAfter in result")
	}
}

func TestReconcile_ManifestStorageError(t *testing.T) {
	cases := []struct {
		name                      string
		storeErr                  error
		expectedStatusPatchErrMsg string
	}{
		{
			name:                      "non-retryable error",
			storeErr:                  errors.New("something went wrong"),
			expectedStatusPatchErrMsg: "could not copy manifest into Content resource: something went wrong",
		},
		{
			name:     "retryable error",
			storeErr: fmt.Errorf("%w: %w", errorutil.ErrRetryable, errors.New("something went wrong")),
			// no expected reconcile error (requeue set instead)
		},
	}

	for _, c := range cases {
		t.Run(c.name, func(t *testing.T) {
			mockCtrl := gomock.NewController(t)
			defer mockCtrl.Finish()
			scheme := runtime.NewScheme()
			utilruntime.Must(batchv1.AddToScheme(scheme))

			bundle := fleetv1.Bundle{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "my-bundle",
					Namespace: "default",
				},
			}

			namespacedName := types.NamespacedName{Name: bundle.Name, Namespace: bundle.Namespace}

			mockClient := mocks.NewMockK8sClient(mockCtrl)
			expectGetWithFinalizer(mockClient, bundle)

			if c.expectedStatusPatchErrMsg != "" {
				statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
				mockClient.EXPECT().Status().Return(statusClient).Times(1)

				expectStatusPatch(t, statusClient, c.expectedStatusPatchErrMsg)
			}

			recorderMock := mocks.NewMockEventRecorder(mockCtrl)

			matchedTargets := []*target.Target{{DeploymentID: "foo"}} // just needs to be non-empty
			targetBuilderMock := mocks.NewMockTargetBuilder(mockCtrl)
			targetBuilderMock.EXPECT().Targets(gomock.Any(), gomock.Any(), gomock.Any()).Return(matchedTargets, nil)

			storeMock := mocks.NewMockStore(mockCtrl)
			storeMock.EXPECT().Store(gomock.Any(), gomock.Any()).Return(c.storeErr)

			r := reconciler.BundleReconciler{
				Client:   mockClient,
				Scheme:   scheme,
				Recorder: recorderMock,
				Builder:  targetBuilderMock,
				Store:    storeMock,
			}

			ctx := context.TODO()
			rs, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})

			if c.expectedStatusPatchErrMsg != "" && !errors.Is(err, reconcile.TerminalError(nil)) {
				t.Errorf("expected terminal error, got: %v", err)
			} else if c.expectedStatusPatchErrMsg == "" && err != nil {
				t.Errorf("unexpected error: %v", err)
			}

			if c.expectedStatusPatchErrMsg == "" && rs.RequeueAfter == 0 {
				t.Errorf("expected non-zero RequeueAfter in result")
			}
		})
	}
}

func TestReconcile_OptionsSecretCreateUpdateError(t *testing.T) {
	cases := []struct {
		name        string
		secretCalls func(*mocks.MockK8sClient)
	}{
		{
			"create",
			func(mc *mocks.MockK8sClient) {
				// Get + Create (CreateOrUpdate) of new options secret
				mc.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{}), gomock.Any()).
					Return(&k8serrors.StatusError{ErrStatus: metav1.Status{Code: http.StatusNotFound}})
				mc.EXPECT().Create(gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{}), gomock.Any()).
					Return(errors.New("something went wrong"))
			},
		},
		{
			"update",
			func(mc *mocks.MockK8sClient) {
				// Get + Update (CreateOrUpdate) of existing options secret
				mc.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{}), gomock.Any()).
					Return(nil)
				mc.EXPECT().Update(gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{}), gomock.Any()).
					Return(errors.New("something went wrong"))
			},
		},
	}

	for _, c := range cases {
		t.Run(c.name, func(t *testing.T) {
			mockCtrl := gomock.NewController(t)
			defer mockCtrl.Finish()
			scheme := runtime.NewScheme()
			utilruntime.Must(batchv1.AddToScheme(scheme))

			bundle := fleetv1.Bundle{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "my-bundle",
					Namespace: "default",
				},
				Spec: fleetv1.BundleSpec{
					RolloutStrategy: nil,
				},
			}

			namespacedName := types.NamespacedName{Name: bundle.Name, Namespace: bundle.Namespace}

			mockClient := mocks.NewMockK8sClient(mockCtrl)
			expectGetWithFinalizer(mockClient, bundle)

			// No content creation/update expected, as options secret management happens before creating the BD

			c.secretCalls(mockClient)

			// No expected status update (retryable error)

			recorderMock := mocks.NewMockEventRecorder(mockCtrl)

			matchedTargets := []*target.Target{
				{
					Bundle: &bundle,
					Cluster: &fleetv1.Cluster{
						ObjectMeta: metav1.ObjectMeta{
							Namespace: "my-ns",
							Name:      "my-cluster",
						},
					},
					Deployment: &fleetv1.BundleDeployment{
						ObjectMeta: metav1.ObjectMeta{
							Namespace: "my-bd", // non-empty
						},
						Spec: fleetv1.BundleDeploymentSpec{
							DeploymentID: "newdeploymentID", // allows the deployment to be updated from staged.
							Options: fleetv1.BundleDeploymentOptions{
								Helm: &fleetv1.HelmOptions{
									Values: &fleetv1.GenericMap{
										Data: map[string]interface{}{"foo": "bar"}, // non-empty, to generate a non-empty hash and force secret creation/update
									},
								},
							},
						},
					},
				},
			}
			targetBuilderMock := mocks.NewMockTargetBuilder(mockCtrl)
			targetBuilderMock.EXPECT().Targets(gomock.Any(), gomock.Any(), gomock.Any()).Return(matchedTargets, nil)

			storeMock := mocks.NewMockStore(mockCtrl)
			storeMock.EXPECT().Store(gomock.Any(), gomock.Any()).Return(nil)

			r := reconciler.BundleReconciler{
				Client:   mockClient,
				Scheme:   scheme,
				Recorder: recorderMock,
				Builder:  targetBuilderMock,
				Store:    storeMock,
			}

			ctx := context.TODO()
			rs, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})

			if err != nil {
				t.Errorf("unexpected error: %v", err)
			}

			if rs.RequeueAfter == 0 {
				t.Errorf("expected non-zero RequeueAfter in result")
			}
		})
	}
}

func TestReconcile_OptionsSecretDeletionError(t *testing.T) {
	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	scheme := runtime.NewScheme()
	utilruntime.Must(batchv1.AddToScheme(scheme))

	bundle := fleetv1.Bundle{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "my-bundle",
			Namespace: "default",
		},
		Spec: fleetv1.BundleSpec{
			RolloutStrategy: nil,
		},
	}

	namespacedName := types.NamespacedName{Name: bundle.Name, Namespace: bundle.Namespace}

	mockClient := mocks.NewMockK8sClient(mockCtrl)
	expectGetWithFinalizer(mockClient, bundle)

	// No content creation/update expected, as options secret management happens before creating the BD

	mockClient.EXPECT().Delete(gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{}), gomock.Any()).
		Return(errors.New("something went wrong"))

	// No expected status update (retryable error)

	recorderMock := mocks.NewMockEventRecorder(mockCtrl)

	matchedTargets := []*target.Target{
		{
			Bundle: &bundle,
			Cluster: &fleetv1.Cluster{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "my-ns",
					Name:      "my-cluster",
				},
			},
			Deployment: &fleetv1.BundleDeployment{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "my-bd", // non-empty
				},
			},
			DeploymentID: "foo",
		},
	}
	targetBuilderMock := mocks.NewMockTargetBuilder(mockCtrl)
	targetBuilderMock.EXPECT().Targets(gomock.Any(), gomock.Any(), gomock.Any()).Return(matchedTargets, nil)

	storeMock := mocks.NewMockStore(mockCtrl)
	storeMock.EXPECT().Store(gomock.Any(), gomock.Any()).Return(nil)

	r := reconciler.BundleReconciler{
		Client:   mockClient,
		Scheme:   scheme,
		Recorder: recorderMock,
		Builder:  targetBuilderMock,
		Store:    storeMock,
	}

	ctx := context.TODO()
	rs, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})

	if err != nil {
		t.Errorf("unexpected error: %v", err)
	}

	if rs.RequeueAfter == 0 {
		t.Errorf("expected non-zero RequeueAfter in result")
	}
}

func TestReconcile_OCIReferenceSecretResolutionError(t *testing.T) {
	cases := []struct {
		name               string
		secretGet          func(ctx context.Context, req types.NamespacedName, s *corev1.Secret, opts ...interface{}) error
		expectStatusUpdate bool
		expectedErrMsg     string
	}{
		{
			name: "non-retryable error",
			secretGet: func(ctx context.Context, req types.NamespacedName, s *corev1.Secret, opts ...interface{}) error {
				// Necessary reference field is missing → non-retryable
				return nil
			},
			expectStatusUpdate: true,
			expectedErrMsg:     "failed to build OCI reference: expected data [reference] not found in secret",
		},
		{
			name: "retryable error",
			secretGet: func(ctx context.Context, req types.NamespacedName, s *corev1.Secret, opts ...interface{}) error {
				return errors.New("something went wrong")
			},
			// no expected reconcile error (requeue set instead)
		},
	}

	for _, c := range cases {
		t.Run(c.name, func(t *testing.T) {
			mockCtrl := gomock.NewController(t)
			defer mockCtrl.Finish()
			scheme := runtime.NewScheme()
			utilruntime.Must(batchv1.AddToScheme(scheme))

			bundle := fleetv1.Bundle{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "my-bundle",
					Namespace: "default",
				},
				Spec: fleetv1.BundleSpec{
					RolloutStrategy: nil,
					ContentsID:      "foo", // non-empty, to force OCI storage secret lookup
				},
			}

			namespacedName := types.NamespacedName{Name: bundle.Name, Namespace: bundle.Namespace}

			mockClient := mocks.NewMockK8sClient(mockCtrl)
			expectGetWithFinalizer(mockClient, bundle)

			// OCI reference secret
			mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{}), gomock.Any()).
				DoAndReturn(c.secretGet)

			if c.expectStatusUpdate {
				statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
				mockClient.EXPECT().Status().Return(statusClient).Times(1)

				expectStatusPatch(t, statusClient, c.expectedErrMsg)
			}

			recorderMock := mocks.NewMockEventRecorder(mockCtrl)

			matchedTargets := []*target.Target{
				{
					Bundle: &bundle,
					Cluster: &fleetv1.Cluster{
						ObjectMeta: metav1.ObjectMeta{
							Namespace: "my-ns",
							Name:      "my-cluster",
						},
					},
					Deployment: &fleetv1.BundleDeployment{
						ObjectMeta: metav1.ObjectMeta{
							Namespace: "my-bd", // non-empty
						},
					},
					DeploymentID: "foo",
				},
			}
			targetBuilderMock := mocks.NewMockTargetBuilder(mockCtrl)
			targetBuilderMock.EXPECT().Targets(gomock.Any(), gomock.Any(), gomock.Any()).Return(matchedTargets, nil)

			r := reconciler.BundleReconciler{
				Client:   mockClient,
				Scheme:   scheme,
				Recorder: recorderMock,
				Builder:  targetBuilderMock,
			}

			ctx := context.TODO()
			rs, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})

			if c.expectStatusUpdate && !errors.Is(err, reconcile.TerminalError(nil)) {
				t.Errorf("expected terminal error, got: %v", err)
			} else if !c.expectStatusUpdate && err != nil {
				t.Errorf("unexpected error: %v", err)
			}

			if c.expectedErrMsg == "" && rs.RequeueAfter == 0 {
				t.Errorf("expected non-zero RequeueAfter in result")
			}
		})
	}
}

func TestReconcile_DownstreamObjectsHandlingError(t *testing.T) {
	envVar := "EXPERIMENTAL_COPY_RESOURCES_DOWNSTREAM"
	bkp := os.Getenv(envVar)
	defer func() {
		os.Setenv(envVar, bkp)
	}()

	os.Setenv(envVar, "true")

	cases := []struct {
		name                        string
		downstreamResources         []fleetv1.DownstreamResource
		downstreamResourcesGetCalls func(mc *mocks.MockK8sClient)
		expectedErrorMsg            string
		expectRetries               bool
	}{
		{
			name: "secret not found",
			downstreamResources: []fleetv1.DownstreamResource{
				{
					Kind: "Secret",
					Name: "my-top-secret",
				},
				// will not be processed
				{
					Kind: "ConfigMap",
					Name: "my-configmap",
				},
			},
			downstreamResourcesGetCalls: func(mc *mocks.MockK8sClient) {
				mc.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{}), gomock.Any()).
					Return(errors.New("something went wrong"))

			},
			expectedErrorMsg: `failed to clone config maps and secrets downstream: failed to copy secret`,
			expectRetries:    true,
		},
		{
			name: "config map not found",
			downstreamResources: []fleetv1.DownstreamResource{
				{
					Kind: "Secret",
					Name: "my-top-secret",
				},
				{
					Kind: "ConfigMap",
					Name: "my-configmap",
				},
			},
			downstreamResourcesGetCalls: func(mc *mocks.MockK8sClient) {
				// Getting the source secret
				mc.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{}), gomock.Any()).
					Return(nil)

				// Checking if the destination secret exists
				mc.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{}), gomock.Any()).
					Return(&k8serrors.StatusError{ErrStatus: metav1.Status{Code: http.StatusNotFound}})

				mc.EXPECT().Create(gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{})).Return(nil)

				mc.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&corev1.ConfigMap{}), gomock.Any()).
					Return(errors.New("something went wrong"))

			},
			expectedErrorMsg: `failed to clone config maps and secrets downstream: failed to copy config map`,
			expectRetries:    true,
		},
		{
			name: "unsupported resource",
			downstreamResources: []fleetv1.DownstreamResource{
				{
					Kind: "SomethingElse",
					Name: "what",
				},
			},
			downstreamResourcesGetCalls: func(mc *mocks.MockK8sClient) {},
			expectedErrorMsg:            `failed to clone config maps and secrets downstream: unsupported kind for object to copy to downstream`,
		},
	}

	for _, c := range cases {
		t.Run(c.name, func(t *testing.T) {
			mockCtrl := gomock.NewController(t)
			defer mockCtrl.Finish()
			scheme := runtime.NewScheme()
			utilruntime.Must(batchv1.AddToScheme(scheme))
			utilruntime.Must(fleetv1.AddToScheme(scheme))

			bundle := fleetv1.Bundle{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "my-bundle",
					Namespace: "default",
				},
				Spec: fleetv1.BundleSpec{
					RolloutStrategy: nil,
					BundleDeploymentOptions: fleetv1.BundleDeploymentOptions{
						DownstreamResources: c.downstreamResources,
					},
				},
			}

			namespacedName := types.NamespacedName{Name: bundle.Name, Namespace: bundle.Namespace}

			mockClient := mocks.NewMockK8sClient(mockCtrl)
			expectGetWithFinalizer(mockClient, bundle)

			expectContentCreationAndUpdate(mockClient)

			// Options secret: deletion attempt in case it exists, as the bundle deployment's values hash is empty
			mockClient.EXPECT().Delete(gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{}), gomock.Any()).
				Return(nil)

			mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&fleetv1.BundleDeployment{}), gomock.Any()).
				Return(nil)

			c.downstreamResourcesGetCalls(mockClient)

			if !c.expectRetries {
				statusClient := mocks.NewMockSubResourceWriter(mockCtrl)
				mockClient.EXPECT().Status().Return(statusClient).Times(1)

				expectStatusPatch(t, statusClient, c.expectedErrorMsg)
			}

			recorderMock := mocks.NewMockEventRecorder(mockCtrl)

			matchedTargets := []*target.Target{
				{
					Bundle: &bundle,
					Cluster: &fleetv1.Cluster{
						ObjectMeta: metav1.ObjectMeta{
							Namespace: "my-ns",
							Name:      "my-cluster",
						},
					},
					Deployment: &fleetv1.BundleDeployment{
						ObjectMeta: metav1.ObjectMeta{
							Namespace: "my-bd", // non-empty
						},
					},
					DeploymentID: "foo",
				},
			}
			targetBuilderMock := mocks.NewMockTargetBuilder(mockCtrl)
			targetBuilderMock.EXPECT().Targets(gomock.Any(), gomock.Any(), gomock.Any()).Return(matchedTargets, nil)

			storeMock := mocks.NewMockStore(mockCtrl)
			storeMock.EXPECT().Store(gomock.Any(), gomock.Any()).Return(nil)

			r := reconciler.BundleReconciler{
				Client:   mockClient,
				Scheme:   scheme,
				Recorder: recorderMock,
				Builder:  targetBuilderMock,
				Store:    storeMock,
			}

			ctx := context.TODO()
			rs, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
			if c.expectRetries {
				if err != nil {
					t.Errorf("expected nil error, got: %v", err)
				}

				if rs.RequeueAfter == 0 {
					t.Errorf("expected non-zero RequeueAfter in result")
				}
			} else {
				if !errors.Is(err, reconcile.TerminalError(nil)) {
					t.Errorf("expected terminal error, got: %v", err)
				}

				if !strings.Contains(err.Error(), c.expectedErrorMsg) {
					t.Errorf("unexpected error: %v", err)
				}
			}
		})
	}
}

func TestReconcile_AccessSecretsHandlingError(t *testing.T) {
	mockCtrl := gomock.NewController(t)
	defer mockCtrl.Finish()
	scheme := runtime.NewScheme()
	utilruntime.Must(batchv1.AddToScheme(scheme))

	bundle := fleetv1.Bundle{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "my-bundle",
			Namespace: "default",
		},
		Spec: fleetv1.BundleSpec{
			RolloutStrategy: nil,
			ContentsID:      "foo", // non-empty, to force OCI storage secret cloning
		},
	}

	namespacedName := types.NamespacedName{Name: bundle.Name, Namespace: bundle.Namespace}

	mockClient := mocks.NewMockK8sClient(mockCtrl)
	expectGetWithFinalizer(mockClient, bundle)

	mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&fleetv1.BundleDeployment{}), gomock.Any()).
		Return(nil)

	// Options secret: deletion attempt in case it exists, as the bundle deployment's values hash is empty
	mockClient.EXPECT().Delete(gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{}), gomock.Any()).
		Return(nil)

	// OCI contents secret
	mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{}), gomock.Any()).
		DoAndReturn(
			func(ctx context.Context, req types.NamespacedName, s *corev1.Secret, opts ...interface{}) error {
				s.Data = map[string][]byte{
					"reference": []byte("foo"), // key exists
				}

				return nil
			},
		)

	// get OCI storage secret before cloning
	mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&corev1.Secret{}), gomock.Any()).
		Return(errors.New("something went wrong"))

	// No status update expected (errors which may happen while cloning secrets are all retryable, except for
	// framework internals)

	recorderMock := mocks.NewMockEventRecorder(mockCtrl)

	matchedTargets := []*target.Target{
		{
			Bundle: &bundle,
			Cluster: &fleetv1.Cluster{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "my-ns",
					Name:      "my-cluster",
				},
			},
			Deployment: &fleetv1.BundleDeployment{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "my-bd", // non-empty
				},
			},
			DeploymentID: "foo",
		},
	}
	targetBuilderMock := mocks.NewMockTargetBuilder(mockCtrl)
	targetBuilderMock.EXPECT().Targets(gomock.Any(), gomock.Any(), gomock.Any()).Return(matchedTargets, nil)

	r := reconciler.BundleReconciler{
		Client:   mockClient,
		Scheme:   scheme,
		Recorder: recorderMock,
		Builder:  targetBuilderMock,
	}

	ctx := context.TODO()
	rs, err := r.Reconcile(ctx, ctrl.Request{NamespacedName: namespacedName})
	if err != nil {
		t.Errorf("unexpected error: %v", err)
	}

	if rs.RequeueAfter == 0 {
		t.Errorf("expected non-zero RequeueAfter in result")
	}
}

func expectStatusPatch(t *testing.T, sClient *mocks.MockSubResourceWriter, errMsg string) {
	t.Helper()
	sClient.EXPECT().Patch(gomock.Any(), gomock.AssignableToTypeOf(&fleetv1.Bundle{}), gomock.Any()).Do(
		func(ctx context.Context, b *fleetv1.Bundle, p client.Patch, opts ...interface{}) {
			cond, found := getBundleReadyCondition(b)
			if !found {
				t.Errorf("expecting Condition %s to be found", fleetv1.BundleConditionReady)
			}
			if !strings.Contains(cond.Message, errMsg) {
				t.Errorf("expecting condition message containing [%s], got [%s]", errMsg, cond.Message)
			}
			if cond.Type != fleetv1.BundleConditionReady {
				t.Errorf("expecting condition type [%s], got [%s]", fleetv1.BundleConditionReady, cond.Type)
			}
			if cond.Status != "False" {
				t.Errorf("expecting condition Status [False], got [%s]", cond.Type)
			}
		},
	).Times(1)
}

func getBundleReadyCondition(b *fleetv1.Bundle) (genericcondition.GenericCondition, bool) {
	for _, cond := range b.Status.Conditions {
		if cond.Type == fleetv1.BundleConditionReady {
			return cond, true
		}
	}
	return genericcondition.GenericCondition{}, false
}

func TestBundleDeploymentMapFunc(t *testing.T) {
	r := &reconciler.BundleReconciler{ShardID: "test-shard"}
	mapFunc := reconciler.BundleDeploymentMapFunc(r)

	testCases := []struct {
		name     string
		obj      client.Object
		expected []reconcile.Request
	}{
		{
			name: "Matching Shard ID",
			obj: &fleetv1.BundleDeployment{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "bd-1",
					Namespace: "cluster-ns",
					Labels: map[string]string{
						fleetv1.BundleLabel:          "my-bundle",
						fleetv1.BundleNamespaceLabel: "fleet-ns",
						sharding.ShardingRefLabel:    "test-shard",
					},
				},
			},
			expected: []reconcile.Request{
				{NamespacedName: types.NamespacedName{Namespace: "fleet-ns", Name: "my-bundle"}},
			},
		},
		{
			name: "Non-matching Shard ID",
			obj: &fleetv1.BundleDeployment{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "bd-2",
					Namespace: "cluster-ns",
					Labels: map[string]string{
						fleetv1.BundleLabel:          "my-bundle",
						fleetv1.BundleNamespaceLabel: "fleet-ns",
						sharding.ShardingRefLabel:    "other-shard",
					},
				},
			},
			expected: nil,
		},
		{
			name: "Default Shard, Object has no shard label",
			obj: &fleetv1.BundleDeployment{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "bd-3",
					Namespace: "cluster-ns",
					Labels: map[string]string{
						fleetv1.BundleLabel:          "my-bundle",
						fleetv1.BundleNamespaceLabel: "fleet-ns",
					},
				},
			},
			expected: nil, // default shard is "", not "test-shard"
		},
		{
			name: "Missing bundle labels",
			obj: &fleetv1.BundleDeployment{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "bd-4",
					Namespace: "cluster-ns",
					Labels: map[string]string{
						sharding.ShardingRefLabel: "test-shard",
					},
				},
			},
			expected: nil,
		},
		{
			name: "Nil labels",
			obj: &fleetv1.BundleDeployment{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "bd-5",
					Namespace: "cluster-ns",
				},
			},
			expected: nil,
		},
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			reqs := mapFunc(context.Background(), tc.obj)
			if diff := cmp.Diff(tc.expected, reqs); diff != "" {
				t.Errorf("mismatch (-want +got):\n%s", diff)
			}
		})
	}

	t.Run("Default Shard ID", func(t *testing.T) {
		r := &reconciler.BundleReconciler{ShardID: ""}
		mapFunc := reconciler.BundleDeploymentMapFunc(r)

		bd := &fleetv1.BundleDeployment{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "bd-default",
				Namespace: "cluster-ns",
				Labels: map[string]string{
					fleetv1.BundleLabel:          "my-bundle",
					fleetv1.BundleNamespaceLabel: "fleet-ns",
				},
			},
		}

		expected := []reconcile.Request{
			{NamespacedName: types.NamespacedName{Namespace: "fleet-ns", Name: "my-bundle"}},
		}

		reqs := mapFunc(context.Background(), bd)
		if diff := cmp.Diff(expected, reqs); diff != "" {
			t.Errorf("mismatch (-want +got):\n%s", diff)
		}
	})
}

func expectGetWithFinalizer(mockCli *mocks.MockK8sClient, bundle fleetv1.Bundle) {
	mockCli.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&fleetv1.Bundle{}), gomock.Any()).DoAndReturn(
		func(ctx context.Context, req types.NamespacedName, b *fleetv1.Bundle, opts ...interface{}) error {
			b.Name = bundle.Name
			b.Namespace = bundle.Namespace
			controllerutil.AddFinalizer(b, finalize.BundleFinalizer)

			b.Spec = bundle.Spec

			return nil
		},
	)
}

func expectContentCreationAndUpdate(mockCli *mocks.MockK8sClient) {
	// Get content and update it, adding a finalizer, from createBundleDeployment
	mockCli.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.AssignableToTypeOf(&fleetv1.Content{}), gomock.Any()).
		Return(nil)

	mockCli.EXPECT().Update(gomock.Any(), gomock.AssignableToTypeOf(&fleetv1.Content{}), gomock.Any()).Return(nil)
}



================================================
FILE: internal/cmd/controller/reconciler/bundle_status.go
================================================
package reconciler

import (
	"fmt"

	"github.com/rancher/fleet/internal/cmd/controller/summary"
	"github.com/rancher/fleet/internal/cmd/controller/target"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

const (
	maxNew = 50
)

func resetStatus(status *fleet.BundleStatus, allTargets []*target.Target) (err error) {
	status.MaxNew = maxNew
	status.Summary = fleet.BundleSummary{}
	status.PartitionStatus = nil
	status.Unavailable = 0
	status.NewlyCreated = 0
	status.Summary = target.Summary(allTargets)
	status.Unavailable = target.Unavailable(allTargets)
	status.MaxUnavailable, err = target.MaxUnavailable(allTargets)
	return err
}

func updateDisplay(status *fleet.BundleStatus) {
	status.Display.ReadyClusters = fmt.Sprintf("%d/%d",
		status.Summary.Ready,
		status.Summary.DesiredReady)
	status.Display.State = string(summary.GetSummaryState(status.Summary))
}



================================================
FILE: internal/cmd/controller/reconciler/bundledeployment_controller.go
================================================
// Copyright (c) 2021-2023 SUSE LLC

package reconciler

import (
	"context"
	"fmt"
	"reflect"

	"github.com/rancher/fleet/internal/cmd/controller/finalize"
	"github.com/rancher/fleet/internal/cmd/controller/summary"
	"github.com/rancher/fleet/internal/metrics"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"
	"github.com/rancher/fleet/pkg/sharding"
	"github.com/rancher/wrangler/v3/pkg/genericcondition"

	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/client-go/util/retry"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/builder"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
)

// BundleDeploymentReconciler reconciles a BundleDeployment object
type BundleDeploymentReconciler struct {
	client.Client
	Scheme  *runtime.Scheme
	ShardID string

	Workers int
}

// SetupWithManager sets up the controller with the Manager.
func (r *BundleDeploymentReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&fleet.BundleDeployment{}, builder.WithPredicates(
			bundleDeploymentStatusChangedPredicate(),
		)).
		WithEventFilter(sharding.FilterByShardID(r.ShardID)).
		WithOptions(controller.Options{MaxConcurrentReconciles: r.Workers}).
		Complete(r)
}

//+kubebuilder:rbac:groups=fleet.cattle.io,resources=bundledeployments,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=bundledeployments/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=bundledeployments/finalizers,verbs=update

// Reconcile is part of the main kubernetes reconciliation loop which aims to
// move the current state of the cluster closer to the desired state.
func (r *BundleDeploymentReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx).WithName("bundledeployment")

	bd := &fleet.BundleDeployment{}
	err := r.Get(ctx, req.NamespacedName, bd)

	if err != nil {
		metrics.BundleDeploymentCollector.Delete(req.Name, req.Namespace)
		return ctrl.Result{}, client.IgnoreNotFound(err)
	}

	// Enrich logger with userID if present
	if userID := bd.Labels[fleet.CreatedByUserIDLabel]; userID != "" {
		logger = logger.WithValues("userID", userID)
	}
	logger.V(1).Info("Reconciling bundledeployment")

	// The bundle reconciler takes care of adding the finalizer when creating a bundle deployment
	if !bd.DeletionTimestamp.IsZero() {
		if controllerutil.ContainsFinalizer(bd, finalize.BundleDeploymentFinalizer) {
			if err := finalize.PurgeContent(ctx, r.Client, bd.Name, bd.Spec.DeploymentID); err != nil {
				return ctrl.Result{}, err
			}
			err = retry.RetryOnConflict(retry.DefaultRetry, func() error {
				err := r.Get(ctx, req.NamespacedName, bd)
				if err != nil {
					return client.IgnoreNotFound(err)
				}

				controllerutil.RemoveFinalizer(bd, finalize.BundleDeploymentFinalizer)

				return r.Update(ctx, bd)
			})
			if err != nil {
				return ctrl.Result{}, fmt.Errorf("failed to remove finalizer from bundledeployment %s: %w", bd.Name, err)
			}
		}

		return ctrl.Result{}, nil
	}

	// increased log level, this triggers a lot
	logger.V(4).Info("Reconciling bundledeployment, updating display status field", "oldDisplay", bd.Status.Display)

	orig := bd.DeepCopy()

	var (
		deployed, monitored string
	)

	for _, cond := range bd.Status.Conditions {
		switch cond.Type {
		case "Deployed":
			deployed = conditionToMessage(cond)
		case "Monitored":
			monitored = conditionToMessage(cond)
		}
	}

	bd.Status.Display = fleet.BundleDeploymentDisplay{
		Deployed:  deployed,
		Monitored: monitored,
		State:     string(summary.GetDeploymentState(bd)),
	}

	// final update to bd
	statusPatch := client.MergeFrom(orig)
	if patchData, err := statusPatch.Data(bd); err == nil && string(patchData) == "{}" {
		// skip update if patch is empty
		return ctrl.Result{}, nil
	}
	if err := r.Status().Patch(ctx, bd, statusPatch); client.IgnoreNotFound(err) != nil {
		logger.V(1).Info("Reconcile failed update to bundledeployment status, requeuing", "status", bd.Status, "error", err)
		// Use explicit requeue timing instead of error backoff for predictable retry behavior
		// Status patch failures are often transient (conflicts) and don't need exponential backoff
		//nolint:nilerr // Intentionally using fixed requeue interval instead of error backoff
		return ctrl.Result{RequeueAfter: durations.DefaultRequeueAfter}, nil
	}

	metrics.BundleDeploymentCollector.Collect(ctx, bd)

	return ctrl.Result{}, nil
}

// bundleDeploymentStatusChangedPredicate returns true if the bundledeployment
// status has changed, or the bundledeployment was created
func bundleDeploymentStatusChangedPredicate() predicate.Funcs {
	return predicate.Funcs{
		CreateFunc: func(e event.CreateEvent) bool { return true },
		UpdateFunc: func(e event.UpdateEvent) bool {
			n, nOK := e.ObjectNew.(*fleet.BundleDeployment)
			o, oOK := e.ObjectOld.(*fleet.BundleDeployment)
			if !nOK || !oOK {
				return false
			}
			return !n.DeletionTimestamp.IsZero() || !reflect.DeepEqual(n.Status, o.Status)
		},
		GenericFunc: func(e event.GenericEvent) bool { return false },
	}
}

func conditionToMessage(cond genericcondition.GenericCondition) string {
	if cond.Reason == "Error" {
		return "Error: " + cond.Message
	}
	return string(cond.Status)
}



================================================
FILE: internal/cmd/controller/reconciler/bundledeployment_controller_test.go
================================================
package reconciler

import (
	"testing"

	fleetv1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/event"
)

func TestBundleDeploymentStatusChangedPredicate(t *testing.T) {
	p := bundleDeploymentStatusChangedPredicate()

	t.Run("Create", func(t *testing.T) {
		e := event.CreateEvent{Object: &fleetv1.BundleDeployment{}}
		if !p.Create(e) {
			t.Error("expected true for create event")
		}
	})

	t.Run("Delete", func(t *testing.T) {
		// Delete is not defined, so it should be false
		e := event.DeleteEvent{Object: &fleetv1.BundleDeployment{}}
		if !p.Delete(e) {
			t.Error("expected true for delete event")
		}
	})

	t.Run("Generic", func(t *testing.T) {
		// Generic is not defined, so it should be false
		e := event.GenericEvent{Object: &fleetv1.BundleDeployment{}}
		if p.Generic(e) {
			t.Error("expected false for generic event")
		}
	})

	t.Run("Update", func(t *testing.T) {
		oldBD := &fleetv1.BundleDeployment{
			Status: fleetv1.BundleDeploymentStatus{Ready: false},
		}
		newBD := oldBD.DeepCopy()

		// No change
		e := event.UpdateEvent{ObjectOld: oldBD, ObjectNew: newBD}
		if p.Update(e) {
			t.Error("should be false when status is identical")
		}

		// Status changed
		newBD.Status.Ready = true
		e = event.UpdateEvent{ObjectOld: oldBD, ObjectNew: newBD}
		if !p.Update(e) {
			t.Error("should be true when status changes")
		}

		// Deletion timestamp added
		newBD = oldBD.DeepCopy()
		now := metav1.Now()
		newBD.DeletionTimestamp = &now
		e = event.UpdateEvent{ObjectOld: oldBD, ObjectNew: newBD}
		if !p.Update(e) {
			t.Error("should be true when deletion timestamp is set")
		}

		// Nil objects
		e = event.UpdateEvent{ObjectOld: nil, ObjectNew: newBD}
		if p.Update(e) {
			t.Error("should be false for nil old object")
		}
		e = event.UpdateEvent{ObjectOld: oldBD, ObjectNew: nil}
		if p.Update(e) {
			t.Error("should be false for nil new object")
		}
	})
}



================================================
FILE: internal/cmd/controller/reconciler/cluster_controller.go
================================================
// Copyright (c) 2021-2023 SUSE LLC

package reconciler

import (
	"context"
	"fmt"
	"reflect"
	"slices"
	"sort"
	"time"

	"github.com/rancher/fleet/internal/cmd/controller/finalize"
	"github.com/rancher/fleet/internal/cmd/controller/summary"
	"github.com/rancher/fleet/internal/metrics"
	"github.com/rancher/fleet/internal/resourcestatus"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/durations"
	"github.com/rancher/fleet/pkg/sharding"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"

	fleetutil "github.com/rancher/fleet/internal/cmd/controller/errorutil"
	"github.com/rancher/wrangler/v3/pkg/condition"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/equality"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	errutil "k8s.io/apimachinery/pkg/util/errors"
	"k8s.io/apimachinery/pkg/util/wait"
	"k8s.io/client-go/util/retry"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/builder"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
)

var LongRetry = wait.Backoff{
	Steps:    5,
	Duration: 5 * time.Second,
	Factor:   1.0,
	Jitter:   0.1,
}

// ClusterReconciler reconciles a Cluster object
type ClusterReconciler struct {
	client.Client
	Scheme *runtime.Scheme

	Query   BundleQuery
	ShardID string

	Workers int
}

// SetupWithManager sets up the controller with the Manager.
func (r *ClusterReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&fleet.Cluster{}).
		// Watch bundledeployments so we can update the status fields
		Watches(
			&fleet.BundleDeployment{},
			handler.EnqueueRequestsFromMapFunc(r.mapBundleDeploymentToCluster),
			builder.WithPredicates(predicate.Funcs{
				CreateFunc: func(e event.CreateEvent) bool {
					return true
				},
				// Triggering on every update would run into an
				// endless loop with the agentmanagement
				// cluster controller.
				// We still need to update often enough to keep the
				// status fields up to date.
				UpdateFunc: func(e event.UpdateEvent) bool {
					n := e.ObjectNew.(*fleet.BundleDeployment)
					o := e.ObjectOld.(*fleet.BundleDeployment)
					if n == nil || o == nil {
						return false
					}
					if !reflect.DeepEqual(n.Spec, o.Spec) {
						return true
					}
					if n.Status.AppliedDeploymentID != o.Status.AppliedDeploymentID {
						return true
					}
					if n.Status.Ready != o.Status.Ready {
						return true
					}
					return false
				},
				DeleteFunc: func(e event.DeleteEvent) bool {
					o := e.Object.(*fleet.BundleDeployment)
					if o == nil || o.Status.AppliedDeploymentID == "" {
						return false
					}
					return true
				},
			}),
		).
		WithEventFilter(sharding.FilterByShardID(r.ShardID)).
		WithOptions(controller.Options{MaxConcurrentReconciles: r.Workers}).
		Complete(r)
}

func indexByNamespacedName[T metav1.Object](list []T) map[types.NamespacedName]T {
	res := make(map[types.NamespacedName]T, len(list))
	for _, obj := range list {
		res[types.NamespacedName{Namespace: obj.GetNamespace(), Name: obj.GetName()}] = obj
	}
	return res
}

//+kubebuilder:rbac:groups=fleet.cattle.io,resources=clusters,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=clusters/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=clusters/finalizers,verbs=update

// Reconcile is part of the main kubernetes reconciliation loop which aims to
// move the current state of the cluster closer to the desired state.
func (r *ClusterReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx).WithName("cluster")

	cluster := &fleet.Cluster{}
	err := r.Get(ctx, req.NamespacedName, cluster)
	if err != nil {
		return ctrl.Result{}, client.IgnoreNotFound(err)
	}

	if !cluster.DeletionTimestamp.IsZero() {
		return r.handleDelete(ctx, cluster)
	}

	if err := finalize.EnsureFinalizer(ctx, r.Client, cluster, finalize.ClusterFinalizer); err != nil {
		return ctrl.Result{}, fmt.Errorf("%w, failed to add finalizer to cluster: %w", fleetutil.ErrRetryable, err)
	}

	if cluster.Status.Namespace == "" {
		// wait for the cluster's namespace to be created by agentmanagement
		return ctrl.Result{
			RequeueAfter: durations.ClusterRegisterDelay,
		}, nil
	}

	// increased log level, this triggers a lot
	logger.V(4).Info("Reconciling cluster, cleaning old bundledeployments and updating status", "oldDisplay", cluster.Status.Display)

	bundleDeploymentList := &fleet.BundleDeploymentList{}
	if err = r.List(ctx, bundleDeploymentList, client.InNamespace(cluster.Status.Namespace)); err != nil {
		return ctrl.Result{}, r.updateErrorStatus(ctx, req.NamespacedName, cluster.Status, err)
	}

	// Clean up old bundledeployments
	_, cleanup, err := r.Query.BundlesForCluster(ctx, cluster)
	if err != nil {
		return ctrl.Result{}, r.updateErrorStatus(ctx, req.NamespacedName, cluster.Status, err)
	}
	toDeleteBundles := indexByNamespacedName(cleanup)

	// Delete BundleDeployments for Bundles being removed while getting a filtered items list
	bundleDeployments := slices.DeleteFunc(bundleDeploymentList.Items, func(bd fleet.BundleDeployment) bool {
		bundleNamespace := bd.Labels[fleet.BundleNamespaceLabel]
		bundleName := bd.Labels[fleet.BundleLabel]
		if _, ok := toDeleteBundles[types.NamespacedName{Namespace: bundleNamespace, Name: bundleName}]; ok {
			logger.V(1).Info("cleaning up bundleDeployment not matching the cluster", "bundledeployment", bd)
			if err := r.Delete(ctx, &bd); err != nil {
				logger.V(1).Info("deleting bundleDeployment returned an error", "error", err)
			}
			return true
		}
		return false
	})

	// Count the number of gitrepos, helmOps, bundledeployment and deployed resources for this cluster
	cluster.Status.DesiredReadyGitRepos = 0
	cluster.Status.ReadyGitRepos = 0
	cluster.Status.DesiredReadyHelmOps = 0
	cluster.Status.ReadyHelmOps = 0
	cluster.Status.ResourceCounts = fleet.ResourceCounts{}
	cluster.Status.Summary = fleet.BundleSummary{}

	sort.Slice(bundleDeployments, func(i, j int) bool {
		return bundleDeployments[i].Name < bundleDeployments[j].Name
	})

	resourcestatus.SetClusterResources(bundleDeployments, cluster)

	gitRepos := map[types.NamespacedName]bool{}
	helmOps := map[types.NamespacedName]bool{}
	for _, bd := range bundleDeployments {
		state := summary.GetDeploymentState(&bd)
		summary.IncrementState(&cluster.Status.Summary, bd.Name, state, summary.MessageFromDeployment(&bd), bd.Status.ModifiedStatus, bd.Status.NonReadyStatus)
		cluster.Status.Summary.DesiredReady++

		ns := bd.Labels[fleet.BundleNamespaceLabel]

		repoName := bd.Labels[fleet.RepoLabel]
		helmOpName := bd.Labels[fleet.HelmOpLabel]

		if ns == "" {
			continue
		}

		switch { // if both labels are set (which should never happen), the bundle deployment will be counted as gitOps
		case repoName != "":
			// a gitrepo is ready if its bundledeployments are ready, take previous state into account
			repoKey := types.NamespacedName{Namespace: ns, Name: repoName}
			gitRepos[repoKey] = (state == fleet.Ready) || gitRepos[repoKey]
		case helmOpName != "":
			// a helmOp is ready if its bundledeployments are ready, take previous state into account
			helmOpKey := types.NamespacedName{Namespace: ns, Name: helmOpName}
			helmOps[helmOpKey] = (state == fleet.Ready) || helmOps[helmOpKey]
		}
	}

	// a cluster is ready if all its gitrepos are ready and the resources are ready too
	allReady := true
	for repo, ready := range gitRepos {
		gitrepo := &fleet.GitRepo{}
		if err := r.Get(ctx, repo, gitrepo); err == nil {
			cluster.Status.DesiredReadyGitRepos++
			if ready {
				cluster.Status.ReadyGitRepos++
			} else {
				allReady = false
			}
		}
	}

	for key, ready := range helmOps {
		helmOp := &fleet.HelmOp{}
		if err := r.Get(ctx, key, helmOp); err == nil {
			cluster.Status.DesiredReadyHelmOps++
			if ready {
				cluster.Status.ReadyHelmOps++
			} else {
				allReady = false
			}
		}
	}

	summary.SetReadyConditions(&cluster.Status, "Bundle", cluster.Status.Summary)

	// Calculate display status fields
	cluster.Status.Display.ReadyBundles = fmt.Sprintf("%d/%d",
		cluster.Status.Summary.Ready,
		cluster.Status.Summary.DesiredReady)

	var state fleet.BundleState
	for _, nonReady := range cluster.Status.Summary.NonReadyResources {
		if fleet.StateRank[nonReady.State] > fleet.StateRank[state] {
			state = nonReady.State
		}
	}

	cluster.Status.Display.State = string(state)
	if cluster.Status.Agent.LastSeen.IsZero() {
		cluster.Status.Display.State = "WaitCheckIn"
	}

	r.setCondition(&cluster.Status, nil)

	err = r.updateStatus(ctx, req.NamespacedName, cluster.Status)
	if err != nil {
		logger.V(1).Info("Reconcile failed final update to cluster status", "status", cluster.Status, "error", err)
	} else {
		metrics.ClusterCollector.Collect(ctx, cluster)
	}

	if allReady && cluster.Status.ResourceCounts.Ready != cluster.Status.ResourceCounts.DesiredReady {
		logger.V(1).Info("Cluster is not ready, because not all gitrepos/helmOps are ready",
			"namespace", cluster.Namespace,
			"name", cluster.Name,
			"ready", cluster.Status.ResourceCounts.Ready,
			"desiredReady", cluster.Status.ResourceCounts.DesiredReady,
		)

		// Counts from gitrepo are out of sync with bundleDeployment state, retry in a number of seconds.
		return ctrl.Result{
			RequeueAfter: durations.ClusterRegisterDelay,
		}, nil
	}

	return ctrl.Result{}, err
}

// setCondition sets the condition and updates the timestamp, if the condition changed
func (r *ClusterReconciler) setCondition(status *fleet.ClusterStatus, err error) {
	cond := condition.Cond(fleet.ClusterConditionProcessed)
	origStatus := status.DeepCopy()
	cond.SetError(status, "", fleetutil.IgnoreConflict(err))
	if !equality.Semantic.DeepEqual(origStatus, status) {
		cond.LastUpdated(status, time.Now().UTC().Format(time.RFC3339))
	}
}

func (r *ClusterReconciler) updateErrorStatus(ctx context.Context, req types.NamespacedName, status fleet.ClusterStatus, orgErr error) error {
	r.setCondition(&status, orgErr)
	if statusErr := r.updateStatus(ctx, req, status); statusErr != nil {
		merr := []error{orgErr, fmt.Errorf("failed to update the status: %w", statusErr)}
		return errutil.NewAggregate(merr)
	}
	return orgErr
}

func (r *ClusterReconciler) updateStatus(ctx context.Context, req types.NamespacedName, status fleet.ClusterStatus) error {
	return retry.RetryOnConflict(retry.DefaultRetry, func() error {
		t := &fleet.Cluster{}
		err := r.Get(ctx, req, t)
		if err != nil {
			return err
		}
		t.Status = status
		return r.Status().Update(ctx, t)
	})
}

func (r *ClusterReconciler) mapBundleDeploymentToCluster(ctx context.Context, a client.Object) []ctrl.Request {
	clusterNS := &corev1.Namespace{}
	err := r.Get(ctx, types.NamespacedName{Name: a.GetNamespace()}, clusterNS)
	if err != nil {
		return nil
	}

	ns := clusterNS.Annotations[fleet.ClusterNamespaceAnnotation]
	name := clusterNS.Annotations[fleet.ClusterAnnotation]
	if ns == "" || name == "" {
		return nil
	}

	log.FromContext(ctx).WithName("cluster-handler").V(1).Info("Enqueueing cluster for bundledeployment",
		"cluster", name,
		"bundledeployment", a.(*fleet.BundleDeployment).Name,
		"clusterNamespace", clusterNS.Name,
		"clusterRegistrationNamespace", ns)

	return []ctrl.Request{{
		NamespacedName: types.NamespacedName{
			Namespace: ns,
			Name:      name,
		},
	}}
}

// handleDelete runs cleanup the namespace associated to a Cluster,
// finally removing the finalizer to unblock the deletion of the object from kubernetes.
func (r *ClusterReconciler) handleDelete(ctx context.Context, cluster *fleet.Cluster) (ctrl.Result, error) {
	if !controllerutil.ContainsFinalizer(cluster, finalize.ClusterFinalizer) {

		return ctrl.Result{}, nil
	}

	if cluster.Status.Namespace != "" {
		// pro-actively delete the cluster's namespace.
		ns := &corev1.Namespace{
			ObjectMeta: metav1.ObjectMeta{
				Name: cluster.Status.Namespace,
			},
		}
		if err := r.Delete(ctx, ns); err != nil && !apierrors.IsNotFound(err) {

			return ctrl.Result{}, err
		}
	}

	metrics.ClusterCollector.Delete(cluster.Name, cluster.Namespace)
	controllerutil.RemoveFinalizer(cluster, finalize.ClusterFinalizer)

	return ctrl.Result{}, r.Update(ctx, cluster)
}



================================================
FILE: internal/cmd/controller/reconciler/cluster_controller_test.go
================================================
package reconciler

import (
	"context"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
	"github.com/rancher/fleet/internal/cmd/controller/finalize"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/kubernetes/scheme"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/client/fake"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"
)

type FakeQuery struct {
}

// BundlesForCluster returns empty list, so no cleanup is needed
func (q *FakeQuery) BundlesForCluster(context.Context, *fleet.Cluster) ([]*fleet.Bundle, []*fleet.Bundle, error) {
	return nil, nil, nil
}

var _ = Describe("ClusterReconciler", func() {
	var (
		ctx        context.Context
		reconciler *ClusterReconciler
		k8sclient  client.Client
		cluster    *fleet.Cluster
		req        reconcile.Request
		sch        *runtime.Scheme
	)

	BeforeEach(func() {
		ctx = context.Background()
		sch = scheme.Scheme
		Expect(fleet.AddToScheme(sch)).To(Succeed())
		Expect(corev1.AddToScheme(sch)).To(Succeed())

		cluster = &fleet.Cluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "fleet-local",
			},
			Status: fleet.ClusterStatus{
				Namespace: "cluster-test-cluster-somehash",
			},
		}

		req = reconcile.Request{
			NamespacedName: types.NamespacedName{
				Name:      "test-cluster",
				Namespace: "fleet-local",
			},
		}
	})

	JustBeforeEach(func() {
		if k8sclient == nil {
			k8sclient = fake.NewClientBuilder().
				WithScheme(sch).
				WithObjects(cluster).
				WithStatusSubresource(&fleet.Cluster{}).
				Build()
		}

		reconciler = &ClusterReconciler{
			Client: k8sclient,
			Scheme: sch,
			Query:  &FakeQuery{},
		}
	})

	AfterEach(func() {
		k8sclient = nil
	})

	Context("Reconcile finalizer", func() {
		It("should add a finalizer to a new cluster", func() {
			_, err := reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			updatedCluster := &fleet.Cluster{}
			err = k8sclient.Get(ctx, req.NamespacedName, updatedCluster)
			Expect(err).NotTo(HaveOccurred())
			Expect(updatedCluster.Finalizers).To(ContainElement(finalize.ClusterFinalizer))
		})
	})

	Context("Reconcile deletion", func() {
		var clusterNamespace *corev1.Namespace

		BeforeEach(func() {
			cluster.Finalizers = []string{finalize.ClusterFinalizer}
			now := metav1.NewTime(time.Now())
			cluster.DeletionTimestamp = &now

			clusterNamespace = &corev1.Namespace{
				ObjectMeta: metav1.ObjectMeta{
					Name: cluster.Status.Namespace,
				},
			}
		})

		JustBeforeEach(func() {
			k8sclient = fake.NewClientBuilder().
				WithScheme(sch).
				WithObjects(cluster, clusterNamespace).
				WithStatusSubresource(&fleet.Cluster{}).
				Build()

			reconciler.Client = k8sclient
		})

		It("should delete the cluster namespace and remove the finalizer", func() {
			_, err := reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			// Check finalizer is removed
			err = k8sclient.Get(ctx, req.NamespacedName, cluster)
			Expect(err).To(HaveOccurred())
			Expect(errors.IsNotFound(err)).To(BeTrue(), "cluster should be gone as finalizer is removed")

			// Check namespace is deleted
			ns := &corev1.Namespace{}
			err = k8sclient.Get(ctx, client.ObjectKey{Name: clusterNamespace.Name}, ns)
			Expect(err).To(HaveOccurred())
			Expect(errors.IsNotFound(err)).To(BeTrue(), "cluster namespace should be deleted")
		})

		It("should remove the finalizer when cluster namespace is not set", func() {
			// Remove cluster namespace before test
			cluster.Status.Namespace = ""
			k8sclient = fake.NewClientBuilder().
				WithScheme(sch).
				WithObjects(cluster).
				WithStatusSubresource(&fleet.Cluster{}).
				Build()
			reconciler.Client = k8sclient

			_, err := reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			// Check finalizer is removed
			err = k8sclient.Get(ctx, req.NamespacedName, cluster)
			Expect(err).To(HaveOccurred())
			Expect(errors.IsNotFound(err)).To(BeTrue(), "cluster should be gone as finalizer is removed")
		})

		It("should remove the finalizer even if the namespace is already gone", func() {
			// Delete the namespace before the test
			Expect(k8sclient.Delete(ctx, clusterNamespace)).To(Succeed())

			_, err := reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			// Check finalizer is removed
			err = k8sclient.Get(ctx, req.NamespacedName, cluster)
			Expect(err).To(HaveOccurred())
			Expect(errors.IsNotFound(err)).To(BeTrue(), "cluster should be gone as finalizer is removed")
		})
	})
})



================================================
FILE: internal/cmd/controller/reconciler/clustergroup_controller.go
================================================
// Copyright (c) 2021-2023 SUSE LLC

package reconciler

import (
	"context"
	"fmt"
	"reflect"
	"sort"
	"strings"
	"time"

	fleetutil "github.com/rancher/fleet/internal/cmd/controller/errorutil"
	"github.com/rancher/fleet/internal/cmd/controller/summary"
	"github.com/rancher/fleet/internal/metrics"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/sharding"
	"github.com/rancher/wrangler/v3/pkg/condition"

	"k8s.io/apimachinery/pkg/api/equality"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	errutil "k8s.io/apimachinery/pkg/util/errors"
	"k8s.io/client-go/util/retry"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/builder"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
)

// ClusterGroupReconciler reconciles a ClusterGroup object
type ClusterGroupReconciler struct {
	client.Client
	Scheme  *runtime.Scheme
	ShardID string

	Workers int
}

const MaxReportedNonReadyClusters = 10

// SetupWithManager sets up the controller with the Manager.
func (r *ClusterGroupReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&fleet.ClusterGroup{}, builder.WithPredicates(
			// only trigger on status changes, create
			predicate.Funcs{
				CreateFunc: func(e event.CreateEvent) bool {
					return true
				},
				UpdateFunc: func(e event.UpdateEvent) bool {
					n := e.ObjectNew.(*fleet.ClusterGroup)
					o := e.ObjectOld.(*fleet.ClusterGroup)
					if n == nil || o == nil {
						return false
					}
					return !reflect.DeepEqual(n.Status, o.Status)
				},
			},
		)).
		Watches(
			// Fan out from cluster to clustergroup
			&fleet.Cluster{},
			handler.EnqueueRequestsFromMapFunc(r.mapClusterToClusterGroup),
		).
		WithEventFilter(sharding.FilterByShardID(r.ShardID)).
		WithOptions(controller.Options{MaxConcurrentReconciles: r.Workers}).
		Complete(r)
}

//+kubebuilder:rbac:groups=fleet.cattle.io,resources=clustergroups,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=clustergroups/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=clustergroups/finalizers,verbs=update

// Reconcile is part of the main kubernetes reconciliation loop which aims to
// move the current state of the cluster closer to the desired state.
func (r *ClusterGroupReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx).WithName("clustergroup")

	group := &fleet.ClusterGroup{}
	err := r.Get(ctx, req.NamespacedName, group)
	if err != nil {
		metrics.ClusterGroupCollector.Delete(req.Name, req.Namespace)
		return ctrl.Result{}, client.IgnoreNotFound(err)
	}
	logger.V(1).Info("Reconciling clustergroup, updating summary and display status field", "oldDisplay", group.Status.Display)

	clusters := &fleet.ClusterList{}
	if group.Spec.Selector != nil {
		selector, err := metav1.LabelSelectorAsSelector(group.Spec.Selector)
		if err != nil {
			logger.Error(err, "Failed to parse selector", "selector", group.Spec.Selector)
			return ctrl.Result{}, r.updateErrorStatus(ctx, req.NamespacedName, group.Status, err)
		}

		err = r.List(ctx, clusters, client.InNamespace(req.Namespace), client.MatchingLabelsSelector{Selector: selector})
		if err != nil {
			return ctrl.Result{}, r.updateErrorStatus(ctx, req.NamespacedName, group.Status, err)
		}
	}

	// update summary
	group.Status.Summary = fleet.BundleSummary{}
	group.Status.ResourceCounts = fleet.ResourceCounts{}
	group.Status.ClusterCount = 0
	group.Status.NonReadyClusterCount = 0
	group.Status.NonReadyClusters = nil

	sort.Slice(clusters.Items, func(i, j int) bool {
		return clusters.Items[i].Name < clusters.Items[j].Name
	})

	for _, cluster := range clusters.Items {
		summary.IncrementResourceCounts(&group.Status.ResourceCounts, cluster.Status.ResourceCounts)
		summary.Increment(&group.Status.Summary, cluster.Status.Summary)
		group.Status.ClusterCount++
		if !summary.IsReady(cluster.Status.Summary) {
			group.Status.NonReadyClusterCount++
			if len(group.Status.NonReadyClusters) < MaxReportedNonReadyClusters {
				group.Status.NonReadyClusters = append(group.Status.NonReadyClusters, cluster.Name)
			}
		}
	}

	summary.SetReadyConditions(&group.Status, "Bundle", group.Status.Summary)

	// update display
	group.Status.Display.ReadyBundles = fmt.Sprintf("%d/%d", group.Status.Summary.Ready, group.Status.Summary.DesiredReady)
	group.Status.Display.ReadyClusters = fmt.Sprintf("%d/%d", group.Status.ClusterCount-group.Status.NonReadyClusterCount, group.Status.ClusterCount)
	if len(group.Status.NonReadyClusters) > 0 {
		group.Status.Display.ReadyClusters += " (" + strings.Join(group.Status.NonReadyClusters, ",") + ")"
	}

	var state fleet.BundleState
	for _, nonReady := range group.Status.Summary.NonReadyResources {
		if fleet.StateRank[nonReady.State] > fleet.StateRank[state] {
			state = nonReady.State
		}
	}

	group.Status.Display.State = string(state)

	r.setCondition(&group.Status, nil)

	err = r.updateStatus(ctx, req.NamespacedName, group.Status)
	if err != nil {
		logger.V(1).Info("Reconcile failed final update to cluster group status", "status", group.Status, "error", err)
	} else {
		metrics.ClusterGroupCollector.Collect(ctx, group)
	}

	return ctrl.Result{}, err
}

// setCondition sets the condition and updates the timestamp, if the condition changed
func (r *ClusterGroupReconciler) setCondition(status *fleet.ClusterGroupStatus, err error) {
	cond := condition.Cond(fleet.ClusterGroupConditionProcessed)
	origStatus := status.DeepCopy()
	cond.SetError(status, "", fleetutil.IgnoreConflict(err))
	if !equality.Semantic.DeepEqual(origStatus, status) {
		cond.LastUpdated(status, time.Now().UTC().Format(time.RFC3339))
	}
}

func (r *ClusterGroupReconciler) updateErrorStatus(ctx context.Context, req types.NamespacedName, status fleet.ClusterGroupStatus, orgErr error) error {
	r.setCondition(&status, orgErr)
	if statusErr := r.updateStatus(ctx, req, status); statusErr != nil {
		merr := []error{orgErr, fmt.Errorf("failed to update the status: %w", statusErr)}
		return errutil.NewAggregate(merr)
	}
	return orgErr
}

func (r *ClusterGroupReconciler) updateStatus(ctx context.Context, req types.NamespacedName, status fleet.ClusterGroupStatus) error {
	return retry.RetryOnConflict(retry.DefaultRetry, func() error {
		t := &fleet.ClusterGroup{}
		err := r.Get(ctx, req, t)
		if err != nil {
			return err
		}
		t.Status = status
		return r.Status().Update(ctx, t)
	})
}

func (r *ClusterGroupReconciler) mapClusterToClusterGroup(ctx context.Context, a client.Object) []ctrl.Request {
	ns := a.GetNamespace()
	logger := log.FromContext(ctx).WithName("clustergroup-cluster-handler").WithValues("namespace", ns)
	cluster := a.(*fleet.Cluster)

	cgs := &fleet.ClusterGroupList{}
	err := r.List(ctx, cgs, client.InNamespace(ns))
	if err != nil {
		logger.Error(err, "Failed to list cluster groups in namespace")
		return nil
	}

	// avoid log message if no clustergroups are found, no fan-out needed
	if len(cgs.Items) == 0 {
		return nil
	}
	logger.Info("Cluster changed, enqueue matching cluster groups", "name", cluster.GetName())

	requests := []ctrl.Request{}
	for _, cg := range cgs.Items {
		if cg.Spec.Selector == nil {
			// clustergroup does not match any clusters
			continue
		}

		sel, err := metav1.LabelSelectorAsSelector(cg.Spec.Selector)
		if err != nil {
			logger.Error(err, "invalid selector on clustergroup", "selector", sel, "name", cg.GetName())
			continue
		}

		if sel.Matches(labels.Set(cluster.Labels)) {
			requests = append(requests, ctrl.Request{
				NamespacedName: types.NamespacedName{
					Namespace: ns,
					Name:      cg.Name,
				},
			})
			// only need to enqueue this cluster group once, no need to check cluster count
			continue
		}

		// if cluster is removed from CG, need to reconcile if ClusterCount doesnt match
		clusters := &fleet.ClusterList{}
		err = r.List(ctx, clusters, client.InNamespace(ns), &client.ListOptions{LabelSelector: sel})
		if err != nil {
			// non-fatal error, just log and continue
			logger.Error(err, "error fetching clusters in clustergroup", "name", cg.GetName())
		}
		if cg.Status.ClusterCount != len(clusters.Items) {
			requests = append(requests, ctrl.Request{
				NamespacedName: types.NamespacedName{
					Namespace: ns,
					Name:      cg.Name,
				},
			})
		}
	}

	return requests
}



================================================
FILE: internal/cmd/controller/reconciler/config_controller.go
================================================
// Package config reads the initial global configuration.
package reconciler

import (
	"context"

	"github.com/rancher/fleet/internal/config"
	"github.com/rancher/fleet/pkg/sharding"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
)

// ConfigReconciler reconciles the Fleet config object, by
// reloading the config on change.
type ConfigReconciler struct {
	client.Client
	Scheme *runtime.Scheme

	SystemNamespace string
	ShardID         string
}

// Load the config from the configmap and set it in the config package.
func Load(ctx context.Context, c client.Reader, namespace string) error {
	cm := &corev1.ConfigMap{}
	err := c.Get(ctx, types.NamespacedName{Namespace: namespace, Name: config.ManagerConfigName}, cm)
	// use an empty config if the configmap is not found
	if client.IgnoreNotFound(err) != nil {
		return err
	}

	cfg, err := config.ReadConfig(cm)
	if err != nil {
		return err
	}

	config.Set(cfg)
	return nil
}

// SetupWithManager sets up the controller with the Manager.
func (r *ConfigReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		// TODO Maybe we can limit this Watch to the system namespace?
		For(&corev1.ConfigMap{}).
		WithEventFilter(
			// we do not trigger for status changes
			predicate.And(
				sharding.FilterByShardID(r.ShardID),
				predicate.NewPredicateFuncs(func(object client.Object) bool {
					return object.GetNamespace() == r.SystemNamespace &&
						object.GetName() == config.ManagerConfigName
				}),
				predicate.Or(
					// Note: These predicates prevent cache
					// syncPeriod from triggering reconcile, since
					// cache sync is an Update event.
					predicate.ResourceVersionChangedPredicate{},
					predicate.GenerationChangedPredicate{},
					predicate.AnnotationChangedPredicate{},
					predicate.LabelChangedPredicate{},
				))).
		Complete(r)
}

// Reconcile is part of the main kubernetes reconciliation loop which aims to
// move the current state of the cluster closer to the desired state.
func (r *ConfigReconciler) Reconcile(ctx context.Context, _ ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx).WithName("config")
	ctx = log.IntoContext(ctx, logger)

	cm := &corev1.ConfigMap{}
	err := r.Get(ctx, types.NamespacedName{Namespace: r.SystemNamespace, Name: config.ManagerConfigName}, cm)
	if client.IgnoreNotFound(err) != nil {
		return ctrl.Result{}, err
	}
	logger.V(1).Info("Reconciling config configmap, loading config")

	cfg, err := config.ReadConfig(cm)
	if err != nil {
		return ctrl.Result{}, err
	}

	config.Set(cfg)

	return ctrl.Result{}, nil
}



================================================
FILE: internal/cmd/controller/reconciler/cronduration_job.go
================================================
package reconciler

import (
	"context"
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"slices"
	"time"

	fleetutil "github.com/rancher/fleet/internal/cmd/controller/errorutil"
	"github.com/rancher/fleet/internal/cmd/controller/target"
	"github.com/rancher/fleet/internal/cmd/controller/target/matcher"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/reugn/go-quartz/quartz"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

type CronDurationJob struct {
	Started          bool
	Schedule         *fleet.Schedule
	Scheduler        quartz.Scheduler
	Matcher          *matcher.ScheduleMatch
	MatchingClusters []string
	Location         *time.Location
	client           client.Client
	hash             string
	key              *quartz.JobKey
}

// newCronDurationJob constructs a new CronDurationJob.
// It also verifies the validity and correctness of the schedule and duration data.
// Internally, it assigns Location = Local if no location was specified in the schedule,
// and from that point onward, any time-related calculations are performed using this location.
func newCronDurationJob(ctx context.Context, schedule *fleet.Schedule, scheduler quartz.Scheduler, c client.Client) (*CronDurationJob, error) {
	locationStr := schedule.Spec.Location
	if locationStr == "" {
		locationStr = "Local"
	}
	location, err := time.LoadLocation(locationStr)
	if err != nil {
		return nil, err
	}

	if err := checkScheduleAndDuration(schedule, location); err != nil {
		return nil, err
	}

	hash, err := getScheduleJobHash(schedule)
	if err != nil {
		return nil, err
	}

	matcher, err := matcher.NewScheduleMatch(schedule)
	if err != nil {
		return nil, err
	}

	matchingClusters, err := matchingClusters(ctx, matcher, c, schedule.Namespace)
	if err != nil {
		return nil, err
	}

	return &CronDurationJob{
		Schedule:         schedule,
		Scheduler:        scheduler,
		Matcher:          matcher,
		MatchingClusters: matchingClusters,
		Location:         location,
		client:           c,
		hash:             hash,
		key:              scheduleKey(schedule),
	}, nil
}

// Execute implements the quartz.Job interface function to run a scheduled job.
func (c *CronDurationJob) Execute(ctx context.Context) error {
	if c.Started {
		// If the job has already started, this execution is for the "stop" action,
		// which was scheduled to run after the specified duration.
		return c.executeStop(ctx)
	}
	return c.executeStart(ctx)
}

// Description implements the quartz.Job interface function to describe a scheduled job.
func (c *CronDurationJob) Description() string {
	return fmt.Sprintf("CronDurationJob-%s", c.hash)
}

// scheduleKey builds a quartz.JobKey for the given fleet Schedule
func scheduleKey(schedule *fleet.Schedule) *quartz.JobKey {
	return quartz.NewJobKey(fmt.Sprintf("schedule-%s/%s", schedule.Namespace, schedule.Name))
}

// getScheduleJobHash returns a unique key to identify the given schedule.
// The key is a hash of the json representation of the schedule.
func getScheduleJobHash(sched *fleet.Schedule) (string, error) {
	jsonBytes, err := json.Marshal(sched.Spec)
	if err != nil {
		return "", err
	}

	hash := sha256.Sum256(jsonBytes)
	return hex.EncodeToString(hash[:]), nil
}

func (c *CronDurationJob) durationToNextStart() (time.Duration, error) {
	cronTrigger, err := quartz.NewCronTriggerWithLoc(c.Schedule.Spec.Schedule, c.Location)
	if err != nil {
		return 0, err
	}
	now := quartz.NowNano()
	nextFireTime, err := cronTrigger.NextFireTime(now)
	if err != nil {
		return 0, err
	}

	return time.Duration(nextFireTime - now), nil
}

// checkScheduleAndDuration verifies that the given schedule start time and the duration are feasible.
// If the duration is longer than 2 consecutive triggers it is considered as non valid.
func checkScheduleAndDuration(schedule *fleet.Schedule, location *time.Location) error {
	trigger, err := quartz.NewCronTriggerWithLoc(schedule.Spec.Schedule, location)
	if err != nil {
		return err
	}
	now := quartz.NowNano()
	firstFireTime, err := trigger.NextFireTime(now)
	if err != nil {
		return err
	}
	firstFireTimeSecs := firstFireTime / 1_000_000_000

	secondFireTime, err := trigger.NextFireTime(firstFireTime)
	if err != nil {
		return err
	}
	secondFireTimeSecs := secondFireTime / 1_000_000_000

	if int64(schedule.Spec.Duration.Seconds()) >= (secondFireTimeSecs - firstFireTimeSecs) {
		// we also consider an error when duration is equal to the next time
		// the job should trigger because we could incur race conditions.
		return fmt.Errorf("duration is too long and overlaps with the next execution time")
	}

	return nil
}

func (c *CronDurationJob) scheduleStopJob() error {
	return c.Scheduler.ScheduleJob(
		quartz.NewJobDetailWithOptions(
			c,
			c.key,
			&quartz.JobDetailOptions{
				Replace: true,
			},
		),
		quartz.NewRunOnceTrigger(c.Schedule.Spec.Duration.Duration),
	)
}

func (c *CronDurationJob) scheduleJob(ctx context.Context) error {
	return c.rescheduleJob(ctx)
}

func (c *CronDurationJob) updateJob(ctx context.Context) error {
	return c.rescheduleJob(ctx)
}

func (c *CronDurationJob) rescheduleJob(ctx context.Context) error {
	next, err := c.durationToNextStart()
	if err != nil {
		return err
	}

	if err := c.Scheduler.ScheduleJob(
		quartz.NewJobDetailWithOptions(
			c,
			c.key,
			&quartz.JobDetailOptions{
				Replace: true,
			},
		),
		quartz.NewRunOnceTrigger(next),
	); err != nil {
		return err
	}

	now := time.Now()
	status := fleet.ScheduleStatus{
		Active: false,
		NextStartTime: metav1.Time{
			Time: now.In(c.Location).Add(next),
		},
		MatchingClusters: c.MatchingClusters,
	}

	return setScheduleStatus(ctx, c.client, c.Schedule, status)
}

func (c *CronDurationJob) executeStart(ctx context.Context) error {
	c.Started = true

	// Recalculate matching clusters at execution time. This ensures that any
	// changes to cluster labels that occurred since the last reconciliation
	// are included. The controller's watchers only trigger reconciles for
	// clusters that are already part of a schedule.
	clusters, err := matchingClusters(ctx, c.Matcher, c.client, c.Schedule.Namespace)
	if err != nil {
		return err
	}

	// Sets Scheduled to false for all clusters that previously matched but no longer do.
	for _, cluster := range c.MatchingClusters {
		if !slices.Contains(clusters, cluster) {
			// this cluster is no longer targeted
			if err := setClusterScheduled(ctx, c.client, cluster, c.Schedule.Namespace, false); err != nil {
				return err
			}
		}
	}

	// Sets ActiveSchedule to true for all matching clusters.
	for _, cluster := range clusters {
		if err := setClusterActiveSchedule(ctx, c.client, cluster, c.Schedule.Namespace, true); err != nil {
			return err
		}
	}
	c.MatchingClusters = clusters

	// Update the status of the Schedule resource
	if err := setScheduleActive(ctx, c.client, c.Schedule, true); err != nil {
		return err
	}

	// Schedules the Stop call (now + duration)
	return c.scheduleStopJob()
}

func (c *CronDurationJob) executeStop(ctx context.Context) error {
	c.Started = false

	// Sets ActiveSchedule to false for all matching clusters.
	// This action disables the creation of BundleDeployments on the clusters.
	for _, cluster := range c.MatchingClusters {
		if err := setClusterActiveSchedule(ctx, c.client, cluster, c.Schedule.Namespace, false); err != nil {
			return err
		}
	}

	// Update the status of the Schedule resource
	if err := setScheduleActive(ctx, c.client, c.Schedule, false); err != nil {
		return err
	}

	// Schedules again the job.
	return c.scheduleJob(ctx)
}

// matchingClusters returns the list of clusters that match the given Schedule at this moment.
func matchingClusters(ctx context.Context, matcher *matcher.ScheduleMatch, c client.Client, namespace string) ([]string, error) {
	clusters := &fleet.ClusterList{}
	if err := c.List(ctx, clusters, client.InNamespace(namespace)); err != nil {
		return nil, fmt.Errorf("%w, listing clusters: %w", fleetutil.ErrRetryable, err)
	}
	var clusterNames []string
	for _, cluster := range clusters.Items {
		cgs, err := target.ClusterGroupsForCluster(ctx, c, &cluster)
		if err != nil {
			return nil, fmt.Errorf("%w, getting cluster groups from clusters: %w", fleetutil.ErrRetryable, err)
		}

		if matcher.MatchCluster(cluster.Name, target.ClusterGroupsToLabelMap(cgs), cluster.Labels) {
			clusterNames = append(clusterNames, cluster.Name)
		}
	}

	return clusterNames, nil
}

// ClusterScheduledMatcher implements the quarts.Matcher interface to match for
// Scheduled clusters.
type ClusterScheduledMatcher struct {
	name      string
	namespace string
}

func NewClusterScheduledMatcher(namespace, name string) *ClusterScheduledMatcher {
	return &ClusterScheduledMatcher{
		namespace: namespace,
		name:      name,
	}
}

// IsMatch implements the quartz.Matcher interface and returns true if the cluster stored
// in the matcher is found in any of the matching clusters of the given job.
// Returns false otherwise.
func (n *ClusterScheduledMatcher) IsMatch(job quartz.ScheduledJob) bool {
	cronDurationJob, ok := job.JobDetail().Job().(*CronDurationJob)
	if !ok {
		return false
	}

	if cronDurationJob.Schedule.Namespace != n.namespace {
		return false
	}
	return slices.Contains(cronDurationJob.MatchingClusters, n.name)
}

// getClusterScheduleKeys returns the keys of the scheduled jobs that reference the given cluster.
func getClusterScheduleKeys(scheduler quartz.Scheduler, cluster, namespace string) ([]*quartz.JobKey, error) {
	return scheduler.GetJobKeys(NewClusterScheduledMatcher(namespace, cluster))
}



================================================
FILE: internal/cmd/controller/reconciler/cronduration_job_test.go
================================================
package reconciler

import (
	"context"
	"fmt"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
	"github.com/reugn/go-quartz/quartz"

	"github.com/rancher/fleet/internal/cmd/controller/target/matcher"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/kubernetes/scheme"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/client/fake"
)

var _ = Describe("CronDurationJob", func() {
	var (
		ctx                          context.Context
		k8sclient                    client.Client
		scheduler                    quartz.Scheduler
		schedule                     *fleet.Schedule
		cluster, cluster2            *fleet.Cluster
		clusterGroup1, clusterGroup2 *fleet.ClusterGroup
	)

	BeforeEach(func() {
		ctx = context.Background()
		Expect(fleet.AddToScheme(scheme.Scheme)).To(Succeed())

		schedule = &fleet.Schedule{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-schedule",
				Namespace: "default",
			},
			Spec: fleet.ScheduleSpec{
				Schedule: "0 */1 * * * *", // Every minute
				Duration: metav1.Duration{Duration: 30 * time.Second},
				Targets: fleet.ScheduleTargets{
					Clusters: []fleet.ScheduleTarget{
						fleet.ScheduleTarget{
							ClusterSelector: &metav1.LabelSelector{
								MatchLabels: map[string]string{"env": "test"},
							},
						},
					},
				},
			},
		}

		cluster = &fleet.Cluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
				Labels: map[string]string{
					"env":  "test",
					"cg":   "cluster-group1",
					"type": "cluster",
				},
			},
		}

		cluster2 = &fleet.Cluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster-2",
				Namespace: "default",
				Labels: map[string]string{
					"env":  "prod",
					"cg":   "cluster-group2",
					"type": "cluster",
				},
			},
		}

		clusterGroup1 = &fleet.ClusterGroup{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "cluster-group-1",
				Namespace: "default",
				Labels: map[string]string{
					"cglabel": "cluster-group1-label",
					"type":    "cluster-group",
				},
			},
			Spec: fleet.ClusterGroupSpec{
				Selector: &metav1.LabelSelector{
					MatchLabels: map[string]string{"cg": "cluster-group1"},
				},
			},
		}

		clusterGroup2 = &fleet.ClusterGroup{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "cluster-group-2",
				Namespace: "default",
				Labels: map[string]string{
					"cglabel": "cluster-group2-label",
					"type":    "cluster-group",
				},
			},
			Spec: fleet.ClusterGroupSpec{
				Selector: &metav1.LabelSelector{
					MatchLabels: map[string]string{"cg": "cluster-group2"},
				},
			},
		}

		k8sclient = fake.NewClientBuilder().
			WithScheme(scheme.Scheme).
			WithObjects(schedule, cluster, cluster2, clusterGroup1, clusterGroup2).
			WithStatusSubresource(&fleet.Schedule{}, &fleet.Cluster{}).
			Build()

		var err error
		scheduler, err = quartz.NewStdScheduler()
		Expect(err).NotTo(HaveOccurred())
		scheduler.Start(ctx)
	})

	AfterEach(func() {
		scheduler.Stop()
		_ = scheduler.Clear()
	})

	Context("newCronDurationJob", func() {
		It("should create a new job successfully", func() {
			job, err := newCronDurationJob(ctx, schedule, scheduler, k8sclient)
			Expect(err).NotTo(HaveOccurred())
			Expect(job).NotTo(BeNil())
			Expect(job.MatchingClusters).To(ConsistOf("test-cluster"))
			Expect(job.Description()).To(ContainSubstring("CronDurationJob-"))
		})

		It("should fail with an unknown time zone location", func() {
			schedule.Spec.Location = "Invalid/Location"
			_, err := newCronDurationJob(ctx, schedule, scheduler, k8sclient)
			Expect(err).To(HaveOccurred())
			Expect(err.Error()).To(Equal("unknown time zone Invalid/Location"))
		})

		It("should fail with invalid schedule (duration too long)", func() {
			schedule.Spec.Duration = metav1.Duration{Duration: 61 * time.Second}
			_, err := newCronDurationJob(ctx, schedule, scheduler, k8sclient)
			Expect(err).To(HaveOccurred())
			Expect(err.Error()).To(ContainSubstring("duration is too long"))
		})

		It("should find no matching clusters", func() {
			cluster.Labels = map[string]string{"env": "prod"}
			k8sclient = fake.NewClientBuilder().
				WithScheme(scheme.Scheme).
				WithObjects(schedule, cluster).
				Build()

			job, err := newCronDurationJob(ctx, schedule, scheduler, k8sclient)
			Expect(err).NotTo(HaveOccurred())
			Expect(job.MatchingClusters).To(BeEmpty())
		})
	})

	Context("checkScheduleAndDuration", func() {
		It("should pass for valid duration", func() {
			err := checkScheduleAndDuration(schedule, time.Local)
			Expect(err).NotTo(HaveOccurred())
		})

		It("should fail for duration equal to schedule interval", func() {
			schedule.Spec.Duration = metav1.Duration{Duration: 1 * time.Minute}
			err := checkScheduleAndDuration(schedule, time.Local)
			Expect(err).To(HaveOccurred())
			Expect(err.Error()).To(ContainSubstring("duration is too long"))
		})

		It("should fail for duration longer than schedule interval", func() {
			schedule.Spec.Duration = metav1.Duration{Duration: 2 * time.Minute}
			err := checkScheduleAndDuration(schedule, time.Local)
			Expect(err).To(HaveOccurred())
			Expect(err.Error()).To(ContainSubstring("duration is too long"))
		})
	})

	Context("Job Execution", func() {
		var job *CronDurationJob

		BeforeEach(func() {
			var err error
			job, err = newCronDurationJob(ctx, schedule, scheduler, k8sclient)
			Expect(err).NotTo(HaveOccurred())
		})

		It("should execute start and stop logic", func() {
			// Manually trigger start
			err := job.executeStart(ctx)
			Expect(err).NotTo(HaveOccurred())

			// Verify cluster status after start
			Eventually(func() bool {
				updatedCluster := &fleet.Cluster{}
				err := k8sclient.Get(ctx, types.NamespacedName{Name: "test-cluster", Namespace: "default"}, updatedCluster)
				Expect(err).NotTo(HaveOccurred())
				return updatedCluster.Status.ActiveSchedule
			}).Should(BeTrue())

			// Verify schedule status after start
			updatedSchedule := &fleet.Schedule{}
			err = k8sclient.Get(ctx, types.NamespacedName{Name: "test-schedule", Namespace: "default"}, updatedSchedule)
			Expect(err).NotTo(HaveOccurred())
			Expect(updatedSchedule.Status.Active).To(BeTrue())

			// Manually trigger stop
			err = job.executeStop(ctx)
			Expect(err).NotTo(HaveOccurred())

			// Verify cluster status after stop
			Eventually(func() bool {
				updatedCluster := &fleet.Cluster{}
				err := k8sclient.Get(ctx, types.NamespacedName{Name: "test-cluster", Namespace: "default"}, updatedCluster)
				Expect(err).NotTo(HaveOccurred())
				return !updatedCluster.Status.ActiveSchedule
			}).Should(BeTrue())

			// Verify schedule status after stop
			err = k8sclient.Get(ctx, types.NamespacedName{Name: "test-schedule", Namespace: "default"}, updatedSchedule)
			Expect(err).NotTo(HaveOccurred())
			Expect(updatedSchedule.Status.Active).To(BeFalse())
		})

		It("should handle cluster label changes between start executions", func() {
			// Initial state: cluster matches
			Expect(job.MatchingClusters).To(ConsistOf("test-cluster"))

			// Manually trigger start
			err := job.executeStart(ctx)
			Expect(err).NotTo(HaveOccurred())

			// Verify cluster is active
			updatedCluster := &fleet.Cluster{}
			Eventually(func() bool {
				err := k8sclient.Get(ctx, types.NamespacedName{Name: "test-cluster", Namespace: "default"}, updatedCluster)
				Expect(err).NotTo(HaveOccurred())
				return updatedCluster.Status.ActiveSchedule
			}).Should(BeTrue())

			// Change cluster label so it no longer matches
			updatedCluster.Labels = map[string]string{"env": "prod"}
			Expect(k8sclient.Update(ctx, updatedCluster)).To(Succeed())

			// Manually trigger stop
			err = job.executeStop(ctx)
			Expect(err).NotTo(HaveOccurred())

			// Manually trigger start again
			err = job.executeStart(ctx)
			Expect(err).NotTo(HaveOccurred())

			// Verify cluster is no longer scheduled
			Eventually(func() bool {
				err := k8sclient.Get(ctx, types.NamespacedName{Name: "test-cluster", Namespace: "default"}, updatedCluster)
				Expect(err).NotTo(HaveOccurred())
				return !updatedCluster.Status.Scheduled
			}).Should(BeTrue())
		})

		It("should handle new matching clusters that appear before execution", func() {
			// Initial state: only 'test-cluster' matches
			Expect(job.MatchingClusters).To(ConsistOf("test-cluster"))

			// Create a new cluster that does not match yet
			newCluster := &fleet.Cluster{
				ObjectMeta: metav1.ObjectMeta{Name: "new-cluster", Namespace: "default", Labels: map[string]string{"env": "prod"}},
			}
			Expect(k8sclient.Create(ctx, newCluster)).To(Succeed())

			// Update the job's internal list of matching clusters
			job.MatchingClusters, _ = matchingClusters(ctx, job.Matcher, k8sclient, "default")
			Expect(job.MatchingClusters).To(ConsistOf("test-cluster"))

			// Now, change the label of the new cluster so it matches
			newCluster.Labels["env"] = "test"
			Expect(k8sclient.Update(ctx, newCluster)).To(Succeed())

			// Manually trigger start. It should pick up the new cluster.
			err := job.executeStart(ctx)
			Expect(err).NotTo(HaveOccurred())

			// Verify both clusters are now active
			Eventually(func() bool {
				updatedNewCluster := &fleet.Cluster{}
				err := k8sclient.Get(ctx, client.ObjectKeyFromObject(newCluster), updatedNewCluster)
				Expect(err).NotTo(HaveOccurred())
				return updatedNewCluster.Status.ActiveSchedule
			}).Should(BeTrue())
		})
	})

	Context("scheduleJob", func() {
		It("should schedule a job and update status", func() {
			// set the schedule time to now plus 10 seconds so we can check
			// later knowing the expected value.
			scheduleTime := time.Now().Add(10 * time.Second)
			schedule.Spec.Schedule = fmt.Sprintf("%d %d %d * * *", scheduleTime.Second(), scheduleTime.Minute(), scheduleTime.Hour())
			job, err := newCronDurationJob(ctx, schedule, scheduler, k8sclient)
			Expect(err).NotTo(HaveOccurred())

			err = job.scheduleJob(ctx)
			Expect(err).NotTo(HaveOccurred())

			// Check if job is scheduled
			scheduledJob, err := scheduler.GetScheduledJob(job.key)
			Expect(err).NotTo(HaveOccurred())
			Expect(scheduledJob).NotTo(BeNil())

			// Check schedule status
			updatedSchedule := &fleet.Schedule{}
			err = k8sclient.Get(ctx, types.NamespacedName{Name: "test-schedule", Namespace: "default"}, updatedSchedule)
			Expect(err).NotTo(HaveOccurred())
			Expect(updatedSchedule.Status.Active).To(BeFalse())

			Expect(updatedSchedule.Status.NextStartTime.Time.Second()).To(Equal(scheduleTime.Second()))
			Expect(updatedSchedule.Status.NextStartTime.Time.Minute()).To(Equal(scheduleTime.Minute()))
			Expect(updatedSchedule.Status.NextStartTime.Time.Hour()).To(Equal(scheduleTime.Hour()))
			Expect(updatedSchedule.Status.MatchingClusters).To(Equal(job.MatchingClusters))
		})
	})

	Context("updateJob", func() {
		It("should update an existing job", func() {
			// set the schedule time to now plus 10 seconds so we can check
			// later knowing the expected value.
			scheduleTime := time.Now().Add(10 * time.Second)
			schedule.Spec.Schedule = fmt.Sprintf("%d %d %d * * *", scheduleTime.Second(), scheduleTime.Minute(), scheduleTime.Hour())
			job, err := newCronDurationJob(ctx, schedule, scheduler, k8sclient)
			Expect(err).NotTo(HaveOccurred())

			err = job.scheduleJob(ctx)
			Expect(err).NotTo(HaveOccurred())

			// Modify schedule to force an update
			newScheduleTime := time.Now().Add(30 * time.Second)
			schedule.Spec.Schedule = fmt.Sprintf("%d %d %d * * *", newScheduleTime.Second(), newScheduleTime.Minute(), newScheduleTime.Hour())
			updatedJob, err := newCronDurationJob(ctx, schedule, scheduler, k8sclient)
			Expect(err).NotTo(HaveOccurred())

			err = updatedJob.updateJob(ctx)
			Expect(err).NotTo(HaveOccurred())

			scheduledJob, err := scheduler.GetScheduledJob(job.key)
			Expect(err).NotTo(HaveOccurred())
			cronDurationJob, ok := scheduledJob.JobDetail().Job().(*CronDurationJob)
			Expect(ok).To(BeTrue())
			Expect(cronDurationJob.Schedule).To(Equal(schedule))

			// Check schedule status
			updatedSchedule := &fleet.Schedule{}
			err = k8sclient.Get(ctx, types.NamespacedName{Name: "test-schedule", Namespace: "default"}, updatedSchedule)
			Expect(err).NotTo(HaveOccurred())
			Expect(updatedSchedule.Status.Active).To(BeFalse())

			Expect(updatedSchedule.Status.NextStartTime.Time.Second()).To(Equal(newScheduleTime.Second()))
			Expect(updatedSchedule.Status.NextStartTime.Time.Minute()).To(Equal(newScheduleTime.Minute()))
			Expect(updatedSchedule.Status.NextStartTime.Time.Hour()).To(Equal(newScheduleTime.Hour()))
			Expect(updatedSchedule.Status.MatchingClusters).To(Equal(job.MatchingClusters))
		})
	})

	Context("ClusterScheduledMatcher", func() {
		It("should match a job if the cluster is in MatchingClusters", func() {
			job, err := newCronDurationJob(ctx, schedule, scheduler, k8sclient)
			Expect(err).NotTo(HaveOccurred())
			err = job.scheduleJob(ctx)
			Expect(err).NotTo(HaveOccurred())

			matcher := NewClusterScheduledMatcher("default", "test-cluster")
			keys, err := scheduler.GetJobKeys(matcher)
			Expect(err).NotTo(HaveOccurred())
			Expect(keys).To(HaveLen(1))
			Expect(keys[0].String()).To(Equal(job.key.String()))
		})

		It("should not match a job if the cluster is not in MatchingClusters", func() {
			job, err := newCronDurationJob(ctx, schedule, scheduler, k8sclient)
			Expect(err).NotTo(HaveOccurred())
			err = job.scheduleJob(ctx)
			Expect(err).NotTo(HaveOccurred())

			matcher := NewClusterScheduledMatcher("default", "another-cluster")
			keys, err := scheduler.GetJobKeys(matcher)
			Expect(err).NotTo(HaveOccurred())
			Expect(keys).To(BeEmpty())
		})

		It("should not match a job if the namespace is different", func() {
			job, err := newCronDurationJob(ctx, schedule, scheduler, k8sclient)
			Expect(err).NotTo(HaveOccurred())
			err = job.scheduleJob(ctx)
			Expect(err).NotTo(HaveOccurred())

			matcher := NewClusterScheduledMatcher("other-ns", "test-cluster")
			keys, err := scheduler.GetJobKeys(matcher)
			Expect(err).NotTo(HaveOccurred())
			Expect(keys).To(BeEmpty())
		})
	})

	Context("getClusterScheduleKeys", func() {
		It("should return keys for scheduled jobs matching a cluster", func() {
			job, err := newCronDurationJob(ctx, schedule, scheduler, k8sclient)
			Expect(err).NotTo(HaveOccurred())
			err = job.scheduleJob(ctx)
			Expect(err).NotTo(HaveOccurred())

			keys, err := getClusterScheduleKeys(scheduler, "test-cluster", "default")
			Expect(err).NotTo(HaveOccurred())
			Expect(keys).To(HaveLen(1))
			Expect(keys[0].String()).To(Equal(job.key.String()))
		})
	})

	Context("getScheduleJobHash", func() {
		It("should return a consistent hash", func() {
			schedule1 := schedule.DeepCopy()
			schedule2 := schedule.DeepCopy()

			hash1, err := getScheduleJobHash(schedule1)
			Expect(err).NotTo(HaveOccurred())

			hash2, err := getScheduleJobHash(schedule2)
			Expect(err).NotTo(HaveOccurred())

			Expect(hash1).To(Equal(hash2))

			// check that changing any single item in the spec changes the hash
			schedule2.Spec.Duration.Duration = 10 * time.Second
			hash3, err := getScheduleJobHash(schedule2)
			Expect(err).NotTo(HaveOccurred())
			Expect(hash1).NotTo(Equal(hash3))

			schedule2 = schedule.DeepCopy()
			schedule2.Spec.Schedule = "0 */10 * * * *"
			hash4, err := getScheduleJobHash(schedule2)
			Expect(err).NotTo(HaveOccurred())
			Expect(hash1).NotTo(Equal(hash4))

			schedule2 = schedule.DeepCopy()
			schedule2.Spec.Location = "Europe/Paris"
			hash5, err := getScheduleJobHash(schedule2)
			Expect(err).NotTo(HaveOccurred())
			Expect(hash1).NotTo(Equal(hash5))

			schedule2 = schedule.DeepCopy()
			schedule2.Spec.Targets.Clusters[0].ClusterSelector.MatchLabels["env"] = "prod"
			hash6, err := getScheduleJobHash(schedule2)
			Expect(err).NotTo(HaveOccurred())
			Expect(hash1).NotTo(Equal(hash6))

			schedule2 = schedule.DeepCopy()
			schedule2.Spec.Targets.Clusters[0].ClusterName = "test"
			hash7, err := getScheduleJobHash(schedule2)
			Expect(err).NotTo(HaveOccurred())
			Expect(hash1).NotTo(Equal(hash7))

			// check that changing metadata does not change the hash
			schedule2 = schedule.DeepCopy()
			schedule2.Labels = map[string]string{"new": "label"}
			hash8, err := getScheduleJobHash(schedule2)
			Expect(err).NotTo(HaveOccurred())
			Expect(hash1).To(Equal(hash8))

		})
	})

	Context("matchingClusters", func() {
		It("should return matching clusters when schedule uses cluster label selector", func() {
			matcher, err := matcher.NewScheduleMatch(schedule)
			Expect(err).NotTo(HaveOccurred())

			clusters, err := matchingClusters(ctx, matcher, k8sclient, "default")
			Expect(err).NotTo(HaveOccurred())
			Expect(clusters).To(ConsistOf("test-cluster"))
		})

		It("should return matching clusters when schedule uses cluster name", func() {
			schedule.Spec.Targets.Clusters[0].ClusterName = "test-cluster"
			schedule.Spec.Targets.Clusters[0].ClusterSelector = nil
			matcher, err := matcher.NewScheduleMatch(schedule)
			Expect(err).NotTo(HaveOccurred())

			clusters, err := matchingClusters(ctx, matcher, k8sclient, "default")
			Expect(err).NotTo(HaveOccurred())
			Expect(clusters).To(ConsistOf("test-cluster"))
		})

		It("should return matching clusters when schedule uses cluster group name", func() {
			schedule.Spec.Targets.Clusters[0].ClusterSelector = nil
			schedule.Spec.Targets.Clusters[0].ClusterGroup = "cluster-group-1"
			matcher, err := matcher.NewScheduleMatch(schedule)
			Expect(err).NotTo(HaveOccurred())

			clusters, err := matchingClusters(ctx, matcher, k8sclient, "default")
			Expect(err).NotTo(HaveOccurred())
			Expect(clusters).To(ConsistOf("test-cluster"))
		})

		It("should return matching clusters when schedule uses cluster group selector", func() {
			schedule.Spec.Targets.Clusters[0].ClusterSelector = nil
			schedule.Spec.Targets.Clusters[0].ClusterGroupSelector = &metav1.LabelSelector{
				MatchLabels: map[string]string{"cglabel": "cluster-group1-label"},
			}
			matcher, err := matcher.NewScheduleMatch(schedule)
			Expect(err).NotTo(HaveOccurred())

			clusters, err := matchingClusters(ctx, matcher, k8sclient, "default")
			Expect(err).NotTo(HaveOccurred())
			Expect(clusters).To(ConsistOf("test-cluster"))
		})

		It("should return both clusters when schedule uses label selector that matches both", func() {
			schedule.Spec.Targets.Clusters[0].ClusterSelector.MatchLabels = map[string]string{"type": "cluster"}
			matcher, err := matcher.NewScheduleMatch(schedule)
			Expect(err).NotTo(HaveOccurred())

			clusters, err := matchingClusters(ctx, matcher, k8sclient, "default")
			Expect(err).NotTo(HaveOccurred())
			Expect(clusters).To(ConsistOf("test-cluster", "test-cluster-2"))
		})

		It("should return both clusters when schedule uses cluster group selector that matches both", func() {
			schedule.Spec.Targets.Clusters[0].ClusterSelector.MatchLabels = nil
			schedule.Spec.Targets.Clusters[0].ClusterGroupSelector = &metav1.LabelSelector{
				MatchLabels: map[string]string{"type": "cluster-group"},
			}
			matcher, err := matcher.NewScheduleMatch(schedule)
			Expect(err).NotTo(HaveOccurred())

			clusters, err := matchingClusters(ctx, matcher, k8sclient, "default")
			Expect(err).NotTo(HaveOccurred())
			Expect(clusters).To(ConsistOf("test-cluster", "test-cluster-2"))
		})
	})
})



================================================
FILE: internal/cmd/controller/reconciler/error_handling.go
================================================
package reconciler

import (
	"errors"
	"time"

	"github.com/go-logr/logr"
	"k8s.io/apimachinery/pkg/api/equality"
	ctrl "sigs.k8s.io/controller-runtime"

	fleetutil "github.com/rancher/fleet/internal/cmd/controller/errorutil"
	"github.com/rancher/fleet/pkg/durations"

	"github.com/rancher/wrangler/v3/pkg/condition"
)

type copyable[T any] interface {
	DeepCopy() T
}

// CheckRetryable checks if err is retryable; if so, it returns `true`, along with a controller result triggering a
// requeue after the default duration.
// If err is non-retryable, it simply returns `false` with empty/nil values.
func CheckRetryable(err error, logger logr.Logger) (bool, ctrl.Result, error) {
	if errors.Is(err, fleetutil.ErrRetryable) {
		logger.Info(err.Error())
		return true, ctrl.Result{RequeueAfter: durations.DefaultRequeueAfter}, nil
	}

	return false, ctrl.Result{}, nil
}

// SetCondition sets the condition and updates the timestamp, if the condition changed
func SetCondition[T any](cond string, s copyable[T], err error) {
	c := condition.Cond(cond)
	origStatus := s.DeepCopy()

	c.SetError(s, "", fleetutil.IgnoreConflict(err))

	if !equality.Semantic.DeepEqual(origStatus, s) {
		c.LastUpdated(s, time.Now().UTC().Format(time.RFC3339))
	}
}



================================================
FILE: internal/cmd/controller/reconciler/imagescan_controller.go
================================================
// Copyright (c) 2021-2023 SUSE LLC

package reconciler

import (
	"context"

	"github.com/reugn/go-quartz/quartz"

	"github.com/rancher/fleet/internal/cmd/controller/imagescan"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/sharding"

	apierrors "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/runtime"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
)

// ImageScanReconciler reconciles a ImageScan object
type ImageScanReconciler struct {
	client.Client
	Scheme *runtime.Scheme

	Scheduler quartz.Scheduler
	ShardID   string

	Workers int
}

// SetupWithManager sets up the controller with the Manager.
func (r *ImageScanReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&fleet.ImageScan{}).
		WithEventFilter(
			// we do not trigger for status changes
			predicate.And(
				sharding.FilterByShardID(r.ShardID),
				predicate.Or(
					// Note: These predicates prevent cache
					// syncPeriod from triggering reconcile, since
					// cache sync is an Update event.
					predicate.GenerationChangedPredicate{},
					predicate.AnnotationChangedPredicate{},
					predicate.LabelChangedPredicate{},
				),
			)).
		WithOptions(controller.Options{MaxConcurrentReconciles: r.Workers}).
		Complete(r)
}

//+kubebuilder:rbac:groups=fleet.cattle.io,resources=clusters,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=clusters/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=clusters/finalizers,verbs=update

// Reconcile is part of the main kubernetes reconciliation loop which aims to
// move the current state of the cluster closer to the desired state.
func (r *ImageScanReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx).WithName("imagescan")

	image := &fleet.ImageScan{}
	err := r.Get(ctx, req.NamespacedName, image)
	if apierrors.IsNotFound(err) {
		logger.V(4).Info("Deleting ImageScan jobs")
		_ = r.Scheduler.DeleteJob(imagescan.TagScanKey(req.Namespace, req.Name))
		return ctrl.Result{}, nil
	} else if err != nil {
		return ctrl.Result{}, client.IgnoreNotFound(err)
	}
	logger.V(1).Info("Reconciling imagescan, clean up and schedule jobs")

	if image.Spec.Suspend {
		_ = r.Scheduler.DeleteJob(imagescan.TagScanKey(req.Namespace, req.Name))
		return ctrl.Result{}, nil
	}

	interval := image.Spec.Interval
	if interval.Seconds() == 0.0 {
		interval = imagescan.DefaultInterval
	}

	// Make sure no duplicate jobs are scheduled. DeleteJob might return an
	// error if the job does not exist, which we ignore.
	tagScanKey := imagescan.TagScanKey(req.Namespace, req.Name)
	_ = r.Scheduler.DeleteJob(tagScanKey)

	err = r.Scheduler.ScheduleJob(
		quartz.NewJobDetail(
			imagescan.NewTagScanJob(r.Client, req.Namespace, req.Name),
			tagScanKey),
		quartz.NewSimpleTrigger(interval.Duration),
	)
	if err != nil {
		logger.Error(err, "Failed to schedule imagescan tagscan job")
		return ctrl.Result{}, err
	}

	gitrepo := &fleet.GitRepo{}
	err = r.Get(ctx, client.ObjectKey{Namespace: image.Namespace, Name: image.Spec.GitRepoName}, gitrepo)
	if err != nil {
		return ctrl.Result{}, client.IgnoreNotFound(err)
	}

	gitCommitKey := imagescan.GitCommitKey(gitrepo.Namespace, gitrepo.Name)
	_ = r.Scheduler.DeleteJob(gitCommitKey)
	err = r.Scheduler.ScheduleJob(
		quartz.NewJobDetail(
			imagescan.NewGitCommitJob(r.Client, gitrepo.Namespace, gitrepo.Name),
			gitCommitKey),
		quartz.NewSimpleTrigger(interval.Duration),
	)
	if err != nil {
		logger.Error(err, "Failed to schedule gitrepo gitcommit job")
		return ctrl.Result{}, err
	}

	return ctrl.Result{}, nil
}



================================================
FILE: internal/cmd/controller/reconciler/predicate.go
================================================
package reconciler

import (
	"reflect"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
)

// TypedResourceVersionUnchangedPredicate implements a update predicate to
// allow syncPeriod to trigger the reconciler
type TypedResourceVersionUnchangedPredicate[T metav1.Object] struct {
	predicate.TypedFuncs[T]
}

func isNil(arg any) bool {
	if v := reflect.ValueOf(arg); !v.IsValid() || ((v.Kind() == reflect.Ptr ||
		v.Kind() == reflect.Interface ||
		v.Kind() == reflect.Slice ||
		v.Kind() == reflect.Map ||
		v.Kind() == reflect.Chan ||
		v.Kind() == reflect.Func) && v.IsNil()) {
		return true
	}
	return false
}

func (TypedResourceVersionUnchangedPredicate[T]) Create(e event.CreateEvent) bool {
	return false
}

func (TypedResourceVersionUnchangedPredicate[T]) Delete(e event.DeleteEvent) bool {
	return false
}

// Update implements default UpdateEvent filter for validating resource version change.
func (TypedResourceVersionUnchangedPredicate[T]) Update(e event.TypedUpdateEvent[T]) bool {
	if isNil(e.ObjectOld) {
		return false
	}
	if isNil(e.ObjectNew) {
		return false
	}

	return e.ObjectNew.GetResourceVersion() == e.ObjectOld.GetResourceVersion()
}

func (TypedResourceVersionUnchangedPredicate[T]) Generic(e event.GenericEvent) bool {
	return false
}



================================================
FILE: internal/cmd/controller/reconciler/schedule_controller.go
================================================
package reconciler

import (
	"context"
	"errors"
	"fmt"
	"slices"
	"time"

	fleetutil "github.com/rancher/fleet/internal/cmd/controller/errorutil"
	"github.com/rancher/fleet/internal/cmd/controller/finalize"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/sharding"
	"github.com/rancher/wrangler/v3/pkg/condition"
	"github.com/reugn/go-quartz/quartz"

	"k8s.io/apimachinery/pkg/api/equality"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/tools/record"
	"k8s.io/client-go/util/retry"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/builder"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
)

// ScheduleReconciler reconciles a Schedule object
type ScheduleReconciler struct {
	client.Client
	Scheme    *runtime.Scheme
	Recorder  record.EventRecorder
	ShardID   string
	Workers   int
	Scheduler quartz.Scheduler
}

//+kubebuilder:rbac:groups=fleet.cattle.io,resources=schedules,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=schedules/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=schedules/finalizers,verbs=update
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=clusters,verbs=get;list;watch;update;patch
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=clusters/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=fleet.cattle.io,resources=clustergroups,verbs=get;list;watch

// SetupWithManager sets up the controller with the Manager.
func (r *ScheduleReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&fleet.Schedule{},
			builder.WithPredicates(
				predicate.GenerationChangedPredicate{},
			),
		).
		Watches(
			&fleet.Cluster{},
			handler.EnqueueRequestsFromMapFunc(r.mapClustersToSchedules),
			builder.WithPredicates(clusterChangedPredicate()),
		).
		WithEventFilter(sharding.FilterByShardID(r.ShardID)).
		WithOptions(controller.Options{MaxConcurrentReconciles: r.Workers}).
		Complete(r)
}

func (r *ScheduleReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	logger := log.FromContext(ctx).WithName("schedule")

	schedule := &fleet.Schedule{}
	if err := r.Get(ctx, req.NamespacedName, schedule); err != nil {
		return ctrl.Result{}, client.IgnoreNotFound(err)
	}

	if !schedule.DeletionTimestamp.IsZero() {
		return r.handleDelete(ctx, schedule)
	}

	if err := finalize.EnsureFinalizer(ctx, r.Client, schedule, finalize.ScheduleFinalizer); err != nil {
		return ctrl.Result{}, err
	}

	logger.Info("Reconciling Schedule")

	if err := r.handleSchedule(ctx, schedule); err != nil {
		// If the error is retryable (e.g., a transient k8s client error),
		// return it to trigger a requeue by the controller runtime.
		if errors.Is(err, fleetutil.ErrRetryable) {
			return ctrl.Result{}, err
		}

		setScheduleReadyCondition(&schedule.Status, err)
		return ctrl.Result{}, r.Client.Status().Update(ctx, schedule)
	}

	return ctrl.Result{}, nil
}

func (r *ScheduleReconciler) handleSchedule(ctx context.Context, s *fleet.Schedule) error {
	k := scheduleKey(s)
	schedJob, err := r.Scheduler.GetScheduledJob(k)
	if err != nil && !errors.Is(err, quartz.ErrJobNotFound) {
		return fmt.Errorf("an unknown error occurred when looking for a schedule job: %w", err)
	}

	if errors.Is(err, quartz.ErrJobNotFound) {
		return scheduleNewCronDurationJob(ctx, s, r.Scheduler, r.Client)
	}

	// the job already exists, check if an update is needed
	newJob, err := newCronDurationJob(ctx, s, r.Scheduler, r.Client)
	if err != nil {
		return err
	}
	existingJob := schedJob.JobDetail().Job()
	existingCronJob, ok := existingJob.(*CronDurationJob)
	if !ok {
		return fmt.Errorf("unexpected job found for key: %s", k.String())
	}

	if jobNeedsUpdate(newJob, existingCronJob) {
		if err := newJob.updateJob(ctx); err != nil {
			return err
		}
		return updateScheduledClusters(
			ctx,
			r.Scheduler,
			r.Client,
			newJob.MatchingClusters,
			existingCronJob.MatchingClusters,
			s.Namespace,
		)
	}
	return nil
}

func (r *ScheduleReconciler) handleDelete(ctx context.Context, schedule *fleet.Schedule) (ctrl.Result, error) {
	if !controllerutil.ContainsFinalizer(schedule, finalize.ScheduleFinalizer) {
		return ctrl.Result{}, nil
	}

	if err := deleteSchedule(ctx, schedule, r.Scheduler); err != nil {
		return ctrl.Result{}, err
	}

	controllerutil.RemoveFinalizer(schedule, finalize.ScheduleFinalizer)
	if err := r.Update(ctx, schedule); err != nil {
		return ctrl.Result{}, err
	}

	return ctrl.Result{}, nil
}

// mapClustersToSchedules is a mapping function used to trigger a reconciliation of Schedules
// when a targeted Cluster changes. It finds all schedules that target the cluster
// and enqueues a reconcile request for each of them.
func (r *ScheduleReconciler) mapClustersToSchedules(ctx context.Context, a client.Object) []ctrl.Request {
	ns := a.GetNamespace()
	logger := log.FromContext(ctx).WithName("cluster-scheduler-handler").WithValues("namespace", ns)
	cluster := a.(*fleet.Cluster)

	// check if the cluster is scheduled
	schedules, err := getClusterSchedules(r.Scheduler, cluster.Name, cluster.Namespace)
	if err != nil {
		logger.Error(err, "Failed to get cluster schedules")
		return nil
	}
	requests := []ctrl.Request{}
	for _, schedule := range schedules {
		requests = append(requests, ctrl.Request{
			NamespacedName: types.NamespacedName{
				Namespace: ns,
				Name:      schedule.Name,
			},
		})
	}

	return requests
}

// jobNeedsUpdate returns true if there is a discrepancy between newJob and existingJob:
// * either in their descriptions, indicating that the parent schedule for those jobs has been updated
// * or in clusters matched by the jobs, which may result from updates to the clusters themselves,
// or creation/deletion of clusters since the existingJob was created.
func jobNeedsUpdate(newJob, existingJob *CronDurationJob) bool {
	return newJob.Description() != existingJob.Description() ||
		!slices.Equal(newJob.MatchingClusters, existingJob.MatchingClusters)
}

func scheduleNewCronDurationJob(ctx context.Context, s *fleet.Schedule, scheduler quartz.Scheduler, c client.Client) error {
	job, err := newCronDurationJob(ctx, s, scheduler, c)
	if err != nil {
		return err
	}

	if err := job.scheduleJob(ctx); err != nil {
		return err
	}

	return setClustersScheduled(ctx, c, job.MatchingClusters, s.Namespace, true)
}

func deleteSchedule(ctx context.Context, s *fleet.Schedule, scheduler quartz.Scheduler) error {
	k := scheduleKey(s)
	schedJob, err := scheduler.GetScheduledJob(k)
	if err != nil && !errors.Is(err, quartz.ErrJobNotFound) {
		return fmt.Errorf("an unknown error occurred when trying to delete a schedule job: %w", err)
	}
	if errors.Is(err, quartz.ErrJobNotFound) {
		return nil
	}

	cronDurationJob, ok := schedJob.JobDetail().Job().(*CronDurationJob)
	if !ok {
		return fmt.Errorf("found an unexpected job type for key: %s", k.String())
	}

	if err := scheduler.DeleteJob(k); err != nil {
		return err
	}

	// get the list of clusters that are no longer in any schedule
	noLongerScheduled, err := getClustersNotScheduled(scheduler, cronDurationJob.MatchingClusters, s.Namespace)
	if err != nil {
		return err
	}

	// set the Scheduled property of those not scheduled to false
	return setClustersScheduled(ctx, cronDurationJob.client, noLongerScheduled, s.Namespace, false)
}

func setClusterActiveSchedule(ctx context.Context, c client.Client, name, namespace string, active bool) error {
	key := client.ObjectKey{Name: name, Namespace: namespace}
	cluster := &fleet.Cluster{}
	if err := c.Get(ctx, key, cluster); err != nil {
		return fmt.Errorf("%w, getting cluster: %w", fleetutil.ErrRetryable, err)
	}

	// if the values are already the expected ones, avoid the update
	if cluster.Status.Scheduled && cluster.Status.ActiveSchedule == active {
		return nil
	}
	old := cluster.DeepCopy()
	cluster.Status.ActiveSchedule = active
	cluster.Status.Scheduled = true

	return updateClusterStatus(ctx, c, old, cluster)
}

func setClusterScheduled(ctx context.Context, c client.Client, name, namespace string, scheduled bool) error {
	key := client.ObjectKey{Name: name, Namespace: namespace}
	cluster := &fleet.Cluster{}
	if err := c.Get(ctx, key, cluster); err != nil {
		return fmt.Errorf("%w, getting cluster: %w", fleetutil.ErrRetryable, err)
	}

	// if the values are already the expected ones, avoid the update
	if cluster.Status.Scheduled == scheduled && !cluster.Status.ActiveSchedule {
		return nil
	}

	old := cluster.DeepCopy()
	cluster.Status.Scheduled = scheduled

	// This function is called either because we're updating a
	// Schedule or because we're creating it.
	// In both cases ActiveSchedule should be false as a Schedule
	// always begins in OffSchedule mode until the first start call is executed.
	cluster.Status.ActiveSchedule = false

	return updateClusterStatus(ctx, c, old, cluster)
}

func setScheduleActive(ctx context.Context, c client.Client, schedule *fleet.Schedule, active bool) error {
	// if the value is already the expected one, avoid the update
	if schedule.Status.Active == active {
		return nil
	}
	old := schedule.DeepCopy()
	schedule.Status.Active = active

	return updateScheduleStatus(ctx, c, old, schedule)
}

func setScheduleStatus(ctx context.Context, c client.Client, schedule *fleet.Schedule, status fleet.ScheduleStatus) error {
	old := schedule.DeepCopy()
	schedule.Status = status

	return updateScheduleStatus(ctx, c, old, schedule)
}

// setScheduleReadyCondition sets the Ready condition on the status and updates the timestamp if the condition has changed.
func setScheduleReadyCondition(status *fleet.ScheduleStatus, err error) {
	if status == nil {
		status = &fleet.ScheduleStatus{}
	}
	cond := condition.Cond(fleet.Ready)
	origStatus := status.DeepCopy()
	cond.SetError(status, "", err)
	if !equality.Semantic.DeepEqual(origStatus, status) {
		cond.LastUpdated(status, time.Now().UTC().Format(time.RFC3339))
	}
}

func updateClusterStatus(ctx context.Context, c client.Client, old *fleet.Cluster, new *fleet.Cluster) error {
	nsn := types.NamespacedName{Name: new.Name, Namespace: new.Namespace}
	if err := retry.RetryOnConflict(retry.DefaultRetry, func() error {
		cluster := &fleet.Cluster{}
		if err := c.Get(ctx, nsn, cluster); err != nil {
			return fmt.Errorf("could not get Cluster to update its status: %w", err)
		}
		cluster.Status.Scheduled = new.Status.Scheduled
		cluster.Status.ActiveSchedule = new.Status.ActiveSchedule
		statusPatch := client.MergeFrom(old)
		if patchData, err := statusPatch.Data(cluster); err == nil && string(patchData) == "{}" {
			// skip update if patch is empty
			return nil
		}
		return c.Status().Patch(ctx, cluster, statusPatch)
	}); err != nil {
		return fmt.Errorf("%w, updating cluster status: %w", fleetutil.ErrRetryable, err)
	}
	return nil
}

func updateScheduleStatus(ctx context.Context, c client.Client, old *fleet.Schedule, new *fleet.Schedule) error {
	nsn := types.NamespacedName{Name: new.Name, Namespace: new.Namespace}
	return retry.RetryOnConflict(retry.DefaultRetry, func() error {
		schedule := &fleet.Schedule{}
		if err := c.Get(ctx, nsn, schedule); err != nil {
			return fmt.Errorf("could not get Schedule to update its status: %w", err)
		}

		schedule.Status = new.Status

		statusPatch := client.MergeFrom(old)
		if patchData, err := statusPatch.Data(schedule); err == nil && string(patchData) == "{}" {
			return nil
		}
		return c.Status().Patch(ctx, schedule, statusPatch)
	})
}

// isClusterScheduled returns true if the given cluster is part of
// any scheduled job as a matching cluster.
func isClusterScheduled(scheduler quartz.Scheduler, cluster, namespace string) (bool, error) {
	keys, err := getClusterScheduleKeys(scheduler, cluster, namespace)
	if err != nil {
		return false, err
	}

	return len(keys) != 0, nil
}

// getClusterSchedules returns all the fleet Schedules in which the given cluster is found as a matching target.
func getClusterSchedules(scheduler quartz.Scheduler, cluster, namespace string) ([]*fleet.Schedule, error) {
	keys, err := getClusterScheduleKeys(scheduler, cluster, namespace)
	if err != nil {
		return nil, err
	}

	schedules := []*fleet.Schedule{}
	for _, key := range keys {
		job, err := scheduler.GetScheduledJob(key)
		if err != nil {
			return nil, err
		}
		cronDurationJob, ok := job.JobDetail().Job().(*CronDurationJob)
		if !ok {
			return nil, fmt.Errorf("unexpected job type for key: %s", key.String())
		}
		schedules = append(schedules, cronDurationJob.Schedule)
	}

	return schedules, nil
}

// getClustersNotScheduled returns the list of the given clusters
// that are not part of any scheduled job.
func getClustersNotScheduled(scheduler quartz.Scheduler, clusters []string, namespace string) ([]string, error) {
	notScheduled := []string{}
	for _, cluster := range clusters {
		scheduled, err := isClusterScheduled(scheduler, cluster, namespace)
		if err != nil {
			return nil, err
		}
		if !scheduled {
			notScheduled = append(notScheduled, cluster)
		}
	}

	return notScheduled, nil
}

func setClustersScheduled(ctx context.Context, c client.Client, clusters []string, namespace string, scheduled bool) error {
	for _, cluster := range clusters {
		if err := setClusterScheduled(ctx, c, cluster, namespace, scheduled); err != nil {
			return err
		}
	}

	return nil
}

func updateScheduledClusters(ctx context.Context, scheduler quartz.Scheduler, c client.Client, clustersNew []string, clustersOld []string, namespace string) error {
	for _, cluster := range clustersNew {
		if err := setClusterScheduled(ctx, c, cluster, namespace, true); err != nil {
			return err
		}
	}

	// now check for clusters that are flagged as scheduled and should no longer be flagged because
	// they are no longer targeted by any schedule
	for _, cluster := range clustersOld {
		if !slices.Contains(clustersNew, cluster) {
			targeted, err := isClusterScheduled(scheduler, cluster, namespace)
			if err != nil {
				return err
			}
			if !targeted {
				if err := setClusterScheduled(ctx, c, cluster, namespace, false); err != nil {
					return err
				}
			}
		}
	}
	return nil
}



================================================
FILE: internal/cmd/controller/reconciler/schedule_controller_test.go
================================================
package reconciler

import (
	"context"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
	"github.com/rancher/fleet/internal/cmd/controller/finalize"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/wrangler/v3/pkg/condition"
	"github.com/reugn/go-quartz/quartz"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/kubernetes/scheme"
	"k8s.io/client-go/tools/record"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/client/fake"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"
)

type expected struct {
	scheduledJob         bool
	statusScheduled      bool
	statusActiveSchedule bool
}

var _ = Describe("ScheduleReconciler", func() {
	var (
		ctx        context.Context
		reconciler *ScheduleReconciler
		k8sclient  client.Client
		scheduler  quartz.Scheduler
		schedule   *fleet.Schedule
		cluster    *fleet.Cluster
		req        reconcile.Request
	)

	BeforeEach(func() {
		ctx = context.Background()
		Expect(fleet.AddToScheme(scheme.Scheme)).To(Succeed())

		schedule = &fleet.Schedule{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-schedule",
				Namespace: "default",
			},
			Spec: fleet.ScheduleSpec{
				Schedule: "0 */1 * * * *", // Every minute
				Duration: metav1.Duration{Duration: 30 * time.Second},
				Targets: fleet.ScheduleTargets{
					Clusters: []fleet.ScheduleTarget{
						{
							ClusterSelector: &metav1.LabelSelector{
								MatchLabels: map[string]string{"env": "test"},
							},
						},
					},
				},
			},
		}

		cluster = &fleet.Cluster{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "test-cluster",
				Namespace: "default",
				Labels:    map[string]string{"env": "test"},
			},
		}

		req = reconcile.Request{
			NamespacedName: types.NamespacedName{
				Name:      "test-schedule",
				Namespace: "default",
			},
		}

		var err error
		scheduler, err = quartz.NewStdScheduler()
		Expect(err).NotTo(HaveOccurred())
		scheduler.Start(ctx)
	})

	JustBeforeEach(func() {
		if k8sclient == nil {
			k8sclient = fake.NewClientBuilder().
				WithScheme(scheme.Scheme).
				WithObjects(schedule, cluster).
				WithStatusSubresource(&fleet.Schedule{}, &fleet.Cluster{}).
				Build()
		}

		reconciler = &ScheduleReconciler{
			Client:    k8sclient,
			Scheme:    scheme.Scheme,
			Scheduler: scheduler,
			Recorder:  record.NewFakeRecorder(10),
		}
	})

	AfterEach(func() {
		scheduler.Stop()
		_ = scheduler.Clear()
		k8sclient = nil
	})

	Context("Reconcile", func() {
		It("should add a finalizer, schedule a new job, and update the cluster status", func() {
			_, err := reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			// Check finalizer
			updatedSchedule := &fleet.Schedule{}
			err = k8sclient.Get(ctx, req.NamespacedName, updatedSchedule)
			Expect(err).NotTo(HaveOccurred())
			Expect(updatedSchedule.Finalizers).To(ContainElement(finalize.ScheduleFinalizer))

			// Check job in scheduler
			jobKey := scheduleKey(schedule)
			_, err = scheduler.GetScheduledJob(jobKey)
			Expect(err).NotTo(HaveOccurred())

			// Check cluster status
			updatedCluster := &fleet.Cluster{}
			err = k8sclient.Get(ctx, client.ObjectKeyFromObject(cluster), updatedCluster)
			Expect(err).NotTo(HaveOccurred())
			Expect(updatedCluster.Status.Scheduled).To(BeTrue())
			Expect(updatedCluster.Status.ActiveSchedule).To(BeFalse())
		})

		It("should remove the job and finalizer on schedule deletion", func() {
			// First, reconcile to create the job and add finalizer
			_, err := reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			// Now, delete the schedule
			err = k8sclient.Get(ctx, req.NamespacedName, schedule)
			Expect(err).NotTo(HaveOccurred())

			err = k8sclient.Delete(ctx, schedule)
			Expect(err).NotTo(HaveOccurred())

			// Reconcile again to handle deletion
			_, err = reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			// Check job is deleted from scheduler
			jobKey := scheduleKey(schedule)
			_, err = scheduler.GetScheduledJob(jobKey)
			Expect(err).To(HaveOccurred())
			Expect(err).To(MatchError(quartz.ErrJobNotFound))

			// Check finalizer is removed
			err = k8sclient.Get(ctx, req.NamespacedName, schedule)
			Expect(err).To(HaveOccurred()) // Should be gone as finalizer is removed
			Expect(errors.IsNotFound(err)).To(BeTrue())

			// Check cluster status
			updatedCluster := &fleet.Cluster{}
			err = k8sclient.Get(ctx, client.ObjectKeyFromObject(cluster), updatedCluster)
			Expect(err).NotTo(HaveOccurred())
			Expect(updatedCluster.Status.Scheduled).To(BeFalse())
			Expect(updatedCluster.Status.ActiveSchedule).To(BeFalse())
		})

		It("should update the scheduled job when the schedule's spec changes", func() {
			// First reconcile
			_, err := reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			// Get the first job's description
			jobKey := scheduleKey(schedule)
			scheduledJob, err := scheduler.GetScheduledJob(jobKey)
			Expect(err).NotTo(HaveOccurred())
			originalDescription := scheduledJob.JobDetail().Job().Description()

			// Update schedule spec
			err = k8sclient.Get(ctx, req.NamespacedName, schedule)
			Expect(err).NotTo(HaveOccurred())
			schedule.Spec.Schedule = "0 */2 * * * *" // Change schedule
			Expect(k8sclient.Update(ctx, schedule)).To(Succeed())

			// Reconcile again
			_, err = reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			// Check job is updated
			updatedScheduledJob, err := scheduler.GetScheduledJob(jobKey)
			Expect(err).NotTo(HaveOccurred())
			newDescription := updatedScheduledJob.JobDetail().Job().Description()
			Expect(newDescription).NotTo(Equal(originalDescription))
		})

		It("should update the cluster's scheduled status when its labels no longer match", func() {
			// Initial reconcile
			_, err := reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			// Cluster is scheduled
			updatedCluster := &fleet.Cluster{}
			err = k8sclient.Get(ctx, client.ObjectKeyFromObject(cluster), updatedCluster)
			Expect(err).NotTo(HaveOccurred())
			Expect(updatedCluster.Status.Scheduled).To(BeTrue())

			// Change cluster label so it no longer matches
			updatedCluster.Labels = map[string]string{"env": "prod"}
			Expect(k8sclient.Update(ctx, updatedCluster)).To(Succeed())

			// Reconcile schedule again
			_, err = reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			// Check cluster is no longer scheduled
			err = k8sclient.Get(ctx, client.ObjectKeyFromObject(cluster), updatedCluster)
			Expect(err).NotTo(HaveOccurred())
			Expect(updatedCluster.Status.Scheduled).To(BeFalse())
		})

		It("should set ready condition to false on error", func() {
			// Use an invalid schedule
			schedule.Spec.Schedule = "invalid cron"
			k8sclient = fake.NewClientBuilder().
				WithScheme(scheme.Scheme).
				WithObjects(schedule, cluster).
				WithStatusSubresource(&fleet.Schedule{}, &fleet.Cluster{}).
				Build()
			reconciler.Client = k8sclient

			_, err := reconciler.Reconcile(ctx, req)
			Expect(err).ToNot(HaveOccurred())

			updatedSchedule := &fleet.Schedule{}
			err = k8sclient.Get(ctx, req.NamespacedName, updatedSchedule)
			Expect(err).NotTo(HaveOccurred())

			readyCond := condition.Cond(fleet.Ready)
			Expect(readyCond.IsTrue(updatedSchedule)).To(BeFalse())
			Expect(readyCond.GetMessage(updatedSchedule)).To(Equal("parse cron expression: invalid expression length"))
		})
	})

	Context("mapClustersToSchedules", func() {
		It("should enqueue schedules that target a changed cluster", func() {
			// Reconcile to create the job
			_, err := reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			// Trigger the map function
			requests := reconciler.mapClustersToSchedules(ctx, cluster)
			Expect(requests).To(HaveLen(1))
			Expect(requests[0].NamespacedName).To(Equal(req.NamespacedName))
		})

		It("should not enqueue schedules that do not target a changed cluster", func() {
			// Reconcile to create the job
			_, err := reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			// Create a cluster that doesn't match
			otherCluster := &fleet.Cluster{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "other-cluster",
					Namespace: "default",
					Labels:    map[string]string{"env": "prod"},
				},
			}

			// Trigger the map function
			requests := reconciler.mapClustersToSchedules(ctx, otherCluster)
			Expect(requests).To(BeEmpty())
		})
	})

	Context("updateScheduledClusters", func() {
		It("should update the clusters matching in the scheduled jobs and also the Status.Scheduled flag", func() {
			cluster2 := &fleet.Cluster{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "test-cluster2",
					Namespace: "default",
					Labels:    map[string]string{"env": "test", "foo": "bar"},
				},
			}

			cluster3 := &fleet.Cluster{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "test-cluster3",
					Namespace: "default",
					Labels:    map[string]string{"env": "test"},
				},
			}

			cluster4 := &fleet.Cluster{
				ObjectMeta: metav1.ObjectMeta{
					Name:      "test-cluster4",
					Namespace: "default",
					Labels:    map[string]string{"foo": "bar"},
				},
			}

			k8sclient = fake.NewClientBuilder().
				WithScheme(scheme.Scheme).
				WithObjects(schedule, cluster, cluster2, cluster3, cluster4).
				WithStatusSubresource(&fleet.Schedule{}, &fleet.Cluster{}).
				Build()

			reconciler.Client = k8sclient

			// initial reconcile
			_, err := reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			// clusters 1, 2 and 3 should be scheduled in the quartz.Scheduler and also
			// be flagged as Scheduled
			checkState(scheduler, k8sclient, "test-cluster", "default",
				expected{scheduledJob: true, statusScheduled: true, statusActiveSchedule: false})
			checkState(scheduler, k8sclient, "test-cluster2", "default",
				expected{scheduledJob: true, statusScheduled: true, statusActiveSchedule: false})
			checkState(scheduler, k8sclient, "test-cluster3", "default",
				expected{scheduledJob: true, statusScheduled: true, statusActiveSchedule: false})
			checkState(scheduler, k8sclient, "test-cluster4", "default",
				expected{scheduledJob: false, statusScheduled: false, statusActiveSchedule: false})

			// force the start of the schedule (so it sets .Status.ActiveSchedule=true)
			jobKey := scheduleKey(schedule)
			job, err := scheduler.GetScheduledJob(jobKey)
			Expect(err).NotTo(HaveOccurred())

			cronDurationJob, ok := job.JobDetail().Job().(*CronDurationJob)
			Expect(ok).To(BeTrue())

			// Manually trigger start
			err = cronDurationJob.executeStart(ctx)
			Expect(err).NotTo(HaveOccurred())

			// check now that the clusters have the expected values, specially Status.ActiveSchedule
			checkState(scheduler, k8sclient, "test-cluster", "default",
				expected{scheduledJob: true, statusScheduled: true, statusActiveSchedule: true})
			checkState(scheduler, k8sclient, "test-cluster2", "default",
				expected{scheduledJob: true, statusScheduled: true, statusActiveSchedule: true})
			checkState(scheduler, k8sclient, "test-cluster3", "default",
				expected{scheduledJob: true, statusScheduled: true, statusActiveSchedule: true})
			checkState(scheduler, k8sclient, "test-cluster4", "default",
				expected{scheduledJob: false, statusScheduled: false, statusActiveSchedule: false})

			// update the schedule, now it only looks for the label foo=bar
			scheduleUpdated := &fleet.Schedule{}
			Expect(k8sclient.Get(ctx, req.NamespacedName, scheduleUpdated)).To(Succeed())

			scheduleUpdated.Spec.Targets.Clusters = []fleet.ScheduleTarget{
				{
					ClusterSelector: &metav1.LabelSelector{
						MatchLabels: map[string]string{"foo": "bar"},
					},
				},
			}
			Expect(k8sclient.Update(ctx, scheduleUpdated)).To(Succeed())

			_, err = reconciler.Reconcile(ctx, req)
			Expect(err).NotTo(HaveOccurred())

			// cluster 2 and 4 should be still targeted.
			checkState(scheduler, k8sclient, "test-cluster", "default",
				expected{scheduledJob: false, statusScheduled: false, statusActiveSchedule: false})
			// cluster 2 had Status.ActiveSchedule set to true, but because we updated the Schedule
			// it should be back to false.
			checkState(scheduler, k8sclient, "test-cluster2", "default",
				expected{scheduledJob: true, statusScheduled: true, statusActiveSchedule: false})
			checkState(scheduler, k8sclient, "test-cluster3", "default",
				expected{scheduledJob: false, statusScheduled: false, statusActiveSchedule: false})
			checkState(scheduler, k8sclient, "test-cluster4", "default",
				expected{scheduledJob: true, statusScheduled: true, statusActiveSchedule: false})
		})
	})
})

//nolint:unparam // namespace is always default, for now. That may change.
func checkState(
	scheduler quartz.Scheduler,
	k8sclient client.Client,
	cluster, namespace string,
	expectedState expected) {
	isScheduled, err := isClusterScheduled(scheduler, cluster, namespace)
	Expect(err).NotTo(HaveOccurred())
	Expect(isScheduled).To(Equal(expectedState.scheduledJob))

	key := client.ObjectKey{Name: cluster, Namespace: namespace}
	clusterObj := &fleet.Cluster{}
	err = k8sclient.Get(context.Background(), key, clusterObj)
	Expect(err).NotTo(HaveOccurred())
	Expect(clusterObj.Status.Scheduled).To(Equal(expectedState.statusScheduled))
	Expect(clusterObj.Status.ActiveSchedule).To(Equal(expectedState.statusActiveSchedule))
}



================================================
FILE: internal/cmd/controller/reconciler/suite_test.go
================================================
package reconciler

import (
	"testing"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

func TestControllers(t *testing.T) {
	RegisterFailHandler(Fail)
	RunSpecs(t, "Fleet Controllers Suite")
}



================================================
FILE: internal/cmd/controller/status/status.go
================================================
package status

import (
	"reflect"

	"github.com/rancher/fleet/internal/cmd/controller/summary"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
)

// BundleStatusChangedPredicate returns true if the bundle
// status has changed, or the bundle was created
func BundleStatusChangedPredicate() predicate.TypedFuncs[*fleet.Bundle] {
	return predicate.TypedFuncs[*fleet.Bundle]{
		CreateFunc: func(e event.TypedCreateEvent[*fleet.Bundle]) bool {
			return true
		},
		UpdateFunc: func(e event.TypedUpdateEvent[*fleet.Bundle]) bool {
			return !reflect.DeepEqual(e.ObjectNew.Status, e.ObjectOld.Status)
		},
		DeleteFunc: func(e event.TypedDeleteEvent[*fleet.Bundle]) bool {
			return true
		},
	}
}

// setFields sets bundledeployment related status fields:
// Summary, ReadyClusters, DesiredReadyClusters, Display.State, Display.Message, Display.Error
func SetFields(list *fleet.BundleDeploymentList, status *fleet.StatusBase) error {
	var (
		maxState   fleet.BundleState
		message    string
		count      = map[client.ObjectKey]int{}
		readyCount = map[client.ObjectKey]int{}
	)

	status.Summary = fleet.BundleSummary{}

	for _, bd := range list.Items {
		state := summary.GetDeploymentState(&bd)
		summary.IncrementState(&status.Summary, bd.Name, state, summary.MessageFromDeployment(&bd), bd.Status.ModifiedStatus, bd.Status.NonReadyStatus)
		status.Summary.DesiredReady++
		if fleet.StateRank[state] > fleet.StateRank[maxState] {
			maxState = state
			message = summary.MessageFromDeployment(&bd)
		}

		// gather status per cluster
		// try to avoid old bundle deployments, which might be missing the labels
		if bd.Labels == nil {
			// this should not happen
			continue
		}

		name := bd.Labels[fleet.ClusterLabel]
		namespace := bd.Labels[fleet.ClusterNamespaceLabel]
		if name == "" || namespace == "" {
			// this should not happen
			continue
		}

		key := client.ObjectKey{Name: name, Namespace: namespace}
		count[key]++
		if state == fleet.Ready {
			readyCount[key]++
		}
	}

	// unique number of clusters from bundledeployments
	status.DesiredReadyClusters = len(count)

	// number of clusters where all deployments are ready
	readyClusters := 0
	for key, n := range readyCount {
		if count[key] == n {
			readyClusters++
		}
	}
	status.ReadyClusters = readyClusters

	if maxState == fleet.Ready {
		maxState = ""
		message = ""
	}

	status.Display.State = string(maxState)
	status.Display.Message = message
	status.Display.Error = len(message) > 0

	return nil
}



================================================
FILE: internal/cmd/controller/summary/summary.go
================================================
// Package summary provides a summary of a bundle's, gitrepo's or cluster's state.
package summary

import (
	"fmt"
	"reflect"
	"sort"
	"strings"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"github.com/rancher/wrangler/v3/pkg/condition"
	"github.com/rancher/wrangler/v3/pkg/genericcondition"
)

// IncrementState increments counters in the BundleSummary. We store up to 10 non ready resources in the summary, with the bundldedeployment's state.
func IncrementState(summary *fleet.BundleSummary, name string, state fleet.BundleState, message string, modified []fleet.ModifiedStatus, nonReady []fleet.NonReadyStatus) {
	switch state {
	case fleet.Modified:
		summary.Modified++
	case fleet.Pending:
		summary.Pending++
	case fleet.WaitApplied:
		summary.WaitApplied++
	case fleet.ErrApplied:
		summary.ErrApplied++
	case fleet.NotReady:
		summary.NotReady++
	case fleet.OutOfSync:
		summary.OutOfSync++
	case fleet.Ready:
		summary.Ready++
	}
	if name != "" && state != fleet.Ready {
		if len(summary.NonReadyResources) < 10 {
			summary.NonReadyResources = append(summary.NonReadyResources, fleet.NonReadyResource{
				Name:           name,
				State:          state,
				Message:        message,
				ModifiedStatus: modified,
				NonReadyStatus: nonReady,
			})
		}
	}
}

func IsReady(summary fleet.BundleSummary) bool {
	return summary.DesiredReady == summary.Ready
}

func Increment(left *fleet.BundleSummary, right fleet.BundleSummary) {
	left.NotReady += right.NotReady
	left.WaitApplied += right.WaitApplied
	left.ErrApplied += right.ErrApplied
	left.OutOfSync += right.OutOfSync
	left.Modified += right.Modified
	left.Ready += right.Ready
	left.Pending += right.Pending
	left.DesiredReady += right.DesiredReady
	if len(left.NonReadyResources) < 10 {
		left.NonReadyResources = append(left.NonReadyResources, right.NonReadyResources...)
	}
}

func IncrementResourceCounts(left *fleet.ResourceCounts, right fleet.ResourceCounts) {
	left.Ready += right.Ready
	left.DesiredReady += right.DesiredReady
	left.WaitApplied += right.WaitApplied
	left.Modified += right.Modified
	left.Orphaned += right.Orphaned
	left.Missing += right.Missing
	left.Unknown += right.Unknown
	left.NotReady += right.NotReady
}

// GetSummaryState returns the summary state of a bundle. The returned value is
// empty if the bundle has no non-ready resources.
func GetSummaryState(summary fleet.BundleSummary) fleet.BundleState {
	var state fleet.BundleState
	for _, nonReady := range summary.NonReadyResources {
		if fleet.StateRank[nonReady.State] > fleet.StateRank[state] {
			state = nonReady.State
		}
	}
	return state
}

// GetDeploymentState calculates a fleet.BundleState from bundleDeployment (pure function)
func GetDeploymentState(bundleDeployment *fleet.BundleDeployment) fleet.BundleState {
	switch {
	case bundleDeployment.Status.AppliedDeploymentID != bundleDeployment.Spec.DeploymentID:
		if condition.Cond(fleet.BundleDeploymentConditionDeployed).IsFalse(bundleDeployment) {
			return fleet.ErrApplied
		}
		return fleet.WaitApplied
	case !bundleDeployment.Status.Ready:
		return fleet.NotReady
	case bundleDeployment.Spec.DeploymentID != bundleDeployment.Spec.StagedDeploymentID:
		return fleet.OutOfSync
	case !bundleDeployment.Status.NonModified:
		return fleet.Modified
	default:
		return fleet.Ready
	}
}

// SetReadyConditions expects a status object as obj and updates its ready conditions according to summary
// as per ReadyMessage
func SetReadyConditions(obj interface{}, referencedKind string, summary fleet.BundleSummary) {
	if reflect.ValueOf(obj).Kind() != reflect.Ptr {
		panic("obj passed must be a pointer")
	}
	c := condition.Cond("Ready")
	msg := ReadyMessage(summary, referencedKind)
	c.SetStatusBool(obj, len(msg) == 0)
	c.Message(obj, msg)
}

func MessageFromCondition(conditionType string, conds []genericcondition.GenericCondition) string {
	for _, cond := range conds {
		if cond.Type == conditionType {
			return cond.Message
		}
	}
	return ""
}

// MessageFromDeployment returns a relevant message from the deployment conditions (pure function)
func MessageFromDeployment(deployment *fleet.BundleDeployment) string {
	if deployment == nil {
		return ""
	}
	message := MessageFromCondition("Deployed", deployment.Status.Conditions)
	if message == "" {
		message = MessageFromCondition("Installed", deployment.Status.Conditions)
	}
	if message == "" {
		message = MessageFromCondition("Monitored", deployment.Status.Conditions)
	}
	return message
}

func ReadyMessage(summary fleet.BundleSummary, referencedKind string) string {
	var messages []string
	for msg, count := range map[fleet.BundleState]int{
		fleet.OutOfSync:   summary.OutOfSync,
		fleet.NotReady:    summary.NotReady,
		fleet.WaitApplied: summary.WaitApplied,
		fleet.ErrApplied:  summary.ErrApplied,
		fleet.Pending:     summary.Pending,
		fleet.Modified:    summary.Modified,
	} {
		if count <= 0 {
			continue
		}
		for _, v := range summary.NonReadyResources {
			name := v.Name
			if v.State == msg {
				if len(v.Message) == 0 {
					messages = append(messages, fmt.Sprintf("%s(%d) [%s %s]", msg, count, referencedKind, name))
				} else {
					messages = append(messages, fmt.Sprintf("%s(%d) [%s %s: %s]", msg, count, referencedKind, name, v.Message))
				}
				for i, m := range v.ModifiedStatus {
					if i > 3 {
						break
					}
					messages = append(messages, m.String())
				}
				for i, m := range v.NonReadyStatus {
					if i > 3 {
						break
					}
					messages = append(messages, m.String())
				}
				break
			}
		}
	}

	sort.Strings(messages)
	return strings.Join(messages, "; ")
}



================================================
FILE: internal/cmd/controller/summary/summary_test.go
================================================
package summary_test

import (
	"testing"

	"github.com/rancher/fleet/internal/cmd/controller/summary"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

func TestGetSummaryState(t *testing.T) {
	// It is supposed to return an empty string if there are no non-ready
	// resources, independent of the state of the bundle.
	s := fleet.BundleSummary{
		Modified:     1,
		Pending:      2,
		WaitApplied:  3,
		ErrApplied:   4,
		NotReady:     5,
		OutOfSync:    6,
		Ready:        7,
		DesiredReady: 8,
	}
	bundleState := summary.GetSummaryState(s)
	if string(bundleState) != "" {
		t.Errorf("Expected empty string, got %s", bundleState)
	}

	// It is supposed to return "Modified" if there is a non-ready resource in
	// state Modified.
	s.NonReadyResources = []fleet.NonReadyResource{
		{
			Name:  "test",
			State: fleet.Modified,
		},
	}
	bundleState = summary.GetSummaryState(s)
	if bundleState != fleet.Modified {
		t.Errorf("Expected Modified, got %s", bundleState)
	}

	// It is supposed to return the highest priority state if there are multiple
	// non-ready resources. Rank depends on v1alpha1.StateRank.
	// ErrApplied:  7,
	// WaitApplied: 6,
	// Modified:    5,
	// OutOfSync:   4,
	// Pending:     3,
	// NotReady:    2,
	// Ready:       1,
	s.NonReadyResources = []fleet.NonReadyResource{
		{
			Name:  "test",
			State: fleet.Pending,
		},
		{
			Name:  "test",
			State: fleet.WaitApplied,
		},
	}
	bundleState = summary.GetSummaryState(s)
	if bundleState != fleet.WaitApplied {
		t.Errorf("Expected WaitApplied, got %s", bundleState)
	}
}



================================================
FILE: internal/config/config.go
================================================
// Package config implements the config for the fleet controller and agent
package config

import (
	"context"
	"encoding/json"
	"sync"
	"time"

	corev1 "github.com/rancher/wrangler/v3/pkg/generated/controllers/core/v1"
	v1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/yaml"

	"github.com/rancher/fleet/pkg/version"
)

const (
	ManagerConfigName        = "fleet-controller"
	AgentConfigName          = "fleet-agent"
	AgentBootstrapConfigName = "fleet-agent-bootstrap"
	AgentTLSModeStrict       = "strict"
	AgentTLSModeSystemStore  = "system-store"
	Key                      = "config"
	// DefaultNamespace is the default for the system namespace, which
	// contains the controller and agent
	DefaultNamespace       = "cattle-fleet-system"
	LegacyDefaultNamespace = "fleet-system"
	// ImportTokenSecretValuesKey is the key in the import token secret,
	// which contains the values for cluster registration.
	ImportTokenSecretValuesKey = "values"
	// KubeConfigSecretValueKey is the key in the kubeconfig secret, which
	// contains the kubeconfig for the downstream cluster.
	KubeConfigSecretValueKey = "value"
	// APIServerURLKey is the key which contains the API server URL of the
	// upstream server. It is used in the controller config, the kubeconfig
	// secret of a cluster, the cluster registration secret "import-NAME"
	// and the fleet-agent-bootstrap secret.
	APIServerURLKey = "apiServerURL"
	// APIServerCAKey is the key which contains the CA of the upstream
	// server.
	APIServerCAKey = "apiServerCA"

	// Default secret name for git credentials, used as a fallback if no secret is referenced by an app.
	DefaultGitCredentialsSecretName = "gitcredential" //nolint:gosec // this is a resource name

	// Default secret name for oci storage,
	// used as a fallback if no secret is specified by the user in the GitRepo.
	DefaultOCIStorageSecretName = "ocistorage"
)

var (
	DefaultManagerImage     = "rancher/fleet" + ":" + version.Version
	DefaultAgentImage       = "rancher/fleet-agent" + ":" + version.Version
	DefaultGitClientTimeout = metav1.Duration{Duration: 30 * time.Second}

	config       *Config
	callbacks    = map[int]func(*Config) error{}
	callbackID   int
	callbackLock sync.Mutex
)

// Config is the config for the fleet controller and agent. Each use slightly
// different fields from this struct. It is stored as JSON in configmaps under
// the 'config' key.
type Config struct {
	// AgentImage defaults to rancher/fleet-agent:version if empty, can include a prefixed SystemDefaultRegistry
	AgentImage           string `json:"agentImage,omitempty"`
	AgentImagePullPolicy string `json:"agentImagePullPolicy,omitempty"`

	// SystemDefaultRegistry used by Rancher when constructing the
	// agentImage string, it's in the config so fleet can remove it if a
	// private repo url prefix is specified on the agent's cluster resource
	SystemDefaultRegistry string `json:"systemDefaultRegistry,omitempty"`

	// AgentCheckinInterval determines how often agents update their clusters status, defaults to 15m
	AgentCheckinInterval metav1.Duration `json:"agentCheckinInterval,omitempty"`

	// ManageAgent if present and set to false, no bundles will be created to manage agents
	ManageAgent *bool `json:"manageAgent,omitempty"`

	// Labels are copied to the cluster registration resource. In detail:
	// fleet-controller will copy the labels to the fleet-agent's config,
	// fleet-agent copies the labels to the cluster registration resource,
	// when fleet-controller accepts the registration, the labels are
	// copied to the cluster resource.
	// +optional
	Labels map[string]string `json:"labels,omitempty"`

	// ClientID of the cluster to associate with. Used by the agent only.
	// +optional
	ClientID string `json:"clientID,omitempty"`

	// APIServerURL is the URL of the fleet-controller's k8s API server. It
	// can be empty, if the value is provided in the cluster's kubeconfig
	// secret instead. The value is copied into the fleet-agent-bootstrap
	// secret on the downstream cluster.
	// +optional
	APIServerURL string `json:"apiServerURL,omitempty"`

	// APIServerCA is the CA bundle used to connect to the
	// fleet-controllers k8s API server. It can be empty, if the value is
	// provided in the cluster's kubeconfig secret instead. The value is
	// copied into the fleet-agent-bootstrap secret on the downstream
	// cluster.
	// +optional
	APIServerCA []byte `json:"apiServerCA,omitempty"`

	Bootstrap Bootstrap `json:"bootstrap,omitempty"`

	// IgnoreClusterRegistrationLabels if set to true, the labels on the cluster registration resource will not be copied to the cluster resource.
	IgnoreClusterRegistrationLabels bool `json:"ignoreClusterRegistrationLabels,omitempty"`

	// AgentTLSMode supports two values: `system-store` and `strict`. If set to `system-store`, instructs the agent
	// to trust CA bundles from the operating system's store. If set to `strict`, then the agent shall only connect
	// to a server which uses the exact CA configured when creating/updating the agent.
	AgentTLSMode string `json:"agentTLSMode,omitempty"`

	// The amount of time to wait for a response from the server before
	// canceling the request.  Used to retrieve the latest commit of configured
	// git repositories. A non-existent value or 0 will result in a timeout of
	// 30 seconds.
	GitClientTimeout metav1.Duration `json:"gitClientTimeout,omitempty"`

	// GarbageCollectionInterval determines how often agents clean up obsolete Helm releases.
	GarbageCollectionInterval metav1.Duration `json:"garbageCollectionInterval,omitempty"`

	// AgentWorkers specifies the maximum number of workers for each agent reconciler.
	AgentWorkers AgentWorkers `json:"agentWorkers,omitempty"`
}

type AgentWorkers struct {
	BundleDeployment string `json:"bundledeployment,omitempty"`
	Drift            string `json:"drift,omitempty"`
}

type Bootstrap struct {
	Namespace      string `json:"namespace,omitempty"`
	AgentNamespace string `json:"agentNamespace,omitempty"`
	// Repo to add at install time that will deploy to the local cluster. This allows
	// one to fully bootstrap fleet, its configuration and all its downstream clusters
	// in one shot.
	Repo   string `json:"repo,omitempty"`
	Secret string `json:"secret,omitempty"` // gitrepo.ClientSecretName for agent from repo
	Paths  string `json:"paths,omitempty"`
	Branch string `json:"branch,omitempty"`
}

// OnChange is used by agentmanagement to react to config changes. The callback is triggered by 'Set' via
// the config controller during startup and when the configmap changes.
func OnChange(ctx context.Context, f func(*Config) error) {
	callbackLock.Lock()
	defer callbackLock.Unlock()

	callbackID++
	id := callbackID
	callbacks[id] = f

	go func() {
		<-ctx.Done()
		callbackLock.Lock()
		delete(callbacks, id)
		callbackLock.Unlock()
	}()
}

// Set doesn't trigger the callbacks, use SetAndTrigger for that. Set is used
// by controller-runtime controllers.
func Set(cfg *Config) {
	config = cfg
}

// SetAndTrigger sets the config and triggers the callbacks. It is used by the
// agentmanagement wrangler controllers.
func SetAndTrigger(cfg *Config) error {
	callbackLock.Lock()
	defer callbackLock.Unlock()

	config = cfg
	for _, f := range callbacks {
		if err := f(cfg); err != nil {
			return err
		}
	}
	return nil
}

func Get() *Config {
	if config == nil {
		panic("config.Get() called before Set()")
	}
	return config
}

func Exists(_ context.Context, namespace, name string, configMaps corev1.ConfigMapClient) (bool, error) {
	_, err := configMaps.Get(namespace, name, metav1.GetOptions{})
	if apierrors.IsNotFound(err) {
		return false, nil
	} else if err != nil {
		return false, err
	}
	return true, nil
}

func Lookup(_ context.Context, namespace, name string, configMaps corev1.ConfigMapClient) (*Config, error) {
	cm, err := configMaps.Get(namespace, name, metav1.GetOptions{})
	if apierrors.IsNotFound(err) {
		cm = &v1.ConfigMap{}
	} else if err != nil {
		return nil, err
	}

	return ReadConfig(cm)
}

func DefaultConfig() *Config {
	return &Config{
		AgentImage:       DefaultAgentImage,
		GitClientTimeout: DefaultGitClientTimeout,
	}
}

func ReadConfig(cm *v1.ConfigMap) (*Config, error) {
	cfg := DefaultConfig()
	data := cm.Data[Key]
	if len(data) == 0 {
		return cfg, nil
	}

	err := yaml.Unmarshal([]byte(data), &cfg)
	return cfg, err
}

func ToConfigMap(namespace, name string, cfg *Config) (*v1.ConfigMap, error) {
	bytes, err := json.Marshal(cfg)
	if err != nil {
		return nil, err
	}

	return &v1.ConfigMap{
		ObjectMeta: metav1.ObjectMeta{
			Name:      name,
			Namespace: namespace,
		},
		Data: map[string]string{
			Key: string(bytes),
		},
	}, nil
}



================================================
FILE: internal/config/config_test.go
================================================
package config_test

import (
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"

	v1 "k8s.io/api/core/v1"

	"github.com/rancher/fleet/internal/config"
)

var _ = Describe("Config", func() {
	When("not having set a value for gitClientTimeout", func() {
		It("should return the default value", func() {
			cfg, err := config.ReadConfig(&v1.ConfigMap{Data: map[string]string{}})
			Expect(err).ToNot(HaveOccurred())
			Expect(cfg.GitClientTimeout.Duration).To(Equal(30 * time.Second))
		})
	})
	When("having set a value for gitClientTimeout", func() {
		It("should return the set value", func() {
			jsonConfig := `{"gitClientTimeout": "20s"}`
			cfg, err := config.ReadConfig(&v1.ConfigMap{
				Data: map[string]string{
					"config": jsonConfig,
				},
			})
			Expect(err).ToNot(HaveOccurred())
			Expect(cfg.GitClientTimeout.Duration).To(Equal(20 * time.Second))
		})
	})
})



================================================
FILE: internal/config/overrides.go
================================================
package config

import (
	"os"

	"github.com/sirupsen/logrus"
)

const (
	envVarSSLCertFile = "SSL_CERT_FILE"
	envVarSSLCertDir  = "SSL_CERT_DIR"
)

// BypassSystemCAStore is used to bypass the OS trust store in agents through env vars, see
// https://pkg.go.dev/crypto/x509#SystemCertPool for more info.
// We set values to paths belonging to the root filesystem, which is read-only, to prevent tampering.
// Eventually, this should not be necessary, if/when we find a way to set client-go's API Config to achieve similar
// effects.
// Note: this will not work on Windows nor Mac OS. Agents are expected to run on Linux nodes.
// Returns a function allowing the bypass to be undone.
func BypassSystemCAStore() func() {
	certFileBkp := os.Getenv(envVarSSLCertFile)
	certDirBkp := os.Getenv(envVarSSLCertDir)

	if err := os.Setenv(envVarSSLCertFile, "/dev/null"); err != nil {
		logrus.Errorf("failed to set env var %s: %s", envVarSSLCertFile, err.Error())
	}

	if err := os.Setenv(envVarSSLCertDir, "/dev/null"); err != nil {
		logrus.Errorf("failed to set env var %s: %s", envVarSSLCertDir, err.Error())
	}

	return func() {
		if err := os.Setenv(envVarSSLCertFile, certFileBkp); err != nil {
			logrus.Errorf("failed to restore env var %s: %s", envVarSSLCertFile, err.Error())
		}

		if err := os.Setenv(envVarSSLCertDir, certDirBkp); err != nil {
			logrus.Errorf("failed to restore env var %s: %s", envVarSSLCertDir, err.Error())
		}
	}
}



================================================
FILE: internal/config/suite_test.go
================================================
package config_test

import (
	"testing"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

const (
	timeout = 30 * time.Second
)

func TestFleet(t *testing.T) {
	RegisterFailHandler(Fail)
	RunSpecs(t, "Config Suite")
}

var _ = BeforeSuite(func() {
	SetDefaultEventuallyTimeout(timeout)
})



================================================
FILE: internal/content/helpers.go
================================================
package content

import (
	"bytes"
	"compress/gzip"
	"encoding/base64"
	"io"
	"strings"
)

func GUnzip(content []byte) ([]byte, error) {
	r, err := gzip.NewReader(bytes.NewBuffer(content))
	if err != nil {
		return nil, err
	}
	return io.ReadAll(r)
}

func Base64GZ(data []byte) (string, error) {
	gz, err := Gzip(data)
	if err != nil {
		return "", err
	}
	return base64.StdEncoding.EncodeToString(gz), nil
}

func Decode(content, encoding string) ([]byte, error) {
	var data []byte

	if encoding == "base64" || strings.HasPrefix(encoding, "base64+") {
		d, err := base64.StdEncoding.DecodeString(content)
		if err != nil {
			return nil, err
		}
		data = d
		encoding = strings.TrimPrefix(encoding, "base64")
		encoding = strings.TrimPrefix(encoding, "+")
	} else {
		data = []byte(content)
	}

	if encoding == "gz" {
		return GUnzip(data)
	}

	return data, nil
}

func Gzip(data []byte) ([]byte, error) {
	buf := &bytes.Buffer{}
	w := gzip.NewWriter(buf)
	defer w.Close()

	if _, err := w.Write(data); err != nil {
		return nil, err
	}
	if err := w.Close(); err != nil {
		return nil, err
	}
	return buf.Bytes(), nil
}



================================================
FILE: internal/experimental/experimental.go
================================================
package experimental

import (
	"os"
	"strconv"
)

const (
	CopyResourcesDownstreamFlag = "EXPERIMENTAL_COPY_RESOURCES_DOWNSTREAM"
	SchedulesFlag               = "EXPERIMENTAL_SCHEDULES"
)

// CopyResourcesDownstreamEnabled returns true if the EXPERIMENTAL_COPY_RESOURCES_DOWNSTREAM env variable
// is set to true; returns false otherwise.
func CopyResourcesDownstreamEnabled() bool {
	value, err := strconv.ParseBool(os.Getenv(CopyResourcesDownstreamFlag))
	return err == nil && value
}

// SchedulesEnabled returns true if the EXPERIMENTAL_SCHEDULES env variable is set to true
// returns false otherwise
func SchedulesEnabled() bool {
	value, err := strconv.ParseBool(os.Getenv(SchedulesFlag))
	return err == nil && value
}



================================================
FILE: internal/fleetyaml/fleetyaml.go
================================================
// Package fleetyaml provides utilities for working with fleet.yaml files,
// which are the central yaml files for bundles.
package fleetyaml

import (
	"os"
	"path/filepath"
	"strings"
)

const (
	fleetYaml         = "fleet.yaml"
	fallbackFleetYaml = "fleet.yml"
)

func FoundFleetYamlInDirectory(baseDir string) bool {
	if _, err := os.Stat(GetFleetYamlPath(baseDir, false)); err != nil {
		if _, err := os.Stat(GetFleetYamlPath(baseDir, true)); err != nil {
			return false
		}
	}
	return true
}

func GetFleetYamlPath(baseDir string, useFallbackFileExtension bool) string {
	if useFallbackFileExtension {
		return filepath.Join(baseDir, fallbackFleetYaml)
	}
	return filepath.Join(baseDir, fleetYaml)
}

func IsFleetYaml(fileName string) bool {
	if fileName == fleetYaml || fileName == fallbackFleetYaml {
		return true
	}
	return false
}

func IsFleetYamlSuffix(filePath string) bool {
	return strings.HasSuffix(filePath, "/"+fleetYaml) || strings.HasSuffix(filePath, "/"+fallbackFleetYaml)
}



================================================
FILE: internal/fleetyaml/fleetyaml_test.go
================================================
//go:build !windows

package fleetyaml

import (
	"path/filepath"
	"testing"

	"github.com/stretchr/testify/assert"
)

func TestBundleYaml(t *testing.T) {
	a := assert.New(t)
	for _, path := range []string{"/foo", "foo", "/foo/", "foo/", "../foo/bar"} {

		// Test both the primary extension and the fallback extension.
		for _, fullPath := range []string{GetFleetYamlPath(path, false), GetFleetYamlPath(path, true)} {
			a.True(IsFleetYaml(filepath.Base(fullPath)))
			a.True(IsFleetYamlSuffix(fullPath))
		}
	}

	// Test expected failure payloads.
	for _, fullPath := range []string{"fleet.yaaaaaaaaaml", "", ".", "weakmonkey.yaml", "../fleet.yaaaaml"} {
		a.False(IsFleetYaml(filepath.Base(fullPath)))
		a.False(IsFleetYamlSuffix(fullPath))
	}
}



================================================
FILE: internal/github/app.go
================================================
package github

import (
	"context"
	"crypto/x509"
	"encoding/pem"
	"fmt"
	"net/http"

	"github.com/bradleyfalzon/ghinstallation/v2"
)

type GitHubApp struct {
	appID, installID int64
	pem              []byte
}

// NewApp creates a new GitHubApp instance with the provided app ID, installation ID,
// and private key, and returns a pointer to it.
func NewApp(appID, installID int64, pem []byte) *GitHubApp {
	return &GitHubApp{appID: appID, installID: installID, pem: pem}
}

// GetToken retrieves a GitHub App installation token using the provided app ID,
// installation ID, and private key (PEM format). It returns the token as a string
// or an error if the process fails.
func (app *GitHubApp) GetToken(ctx context.Context) (string, error) {
	err := app.checkIfPrivateKeyIsValid()
	if err != nil {
		return "", err
	}

	tr := http.DefaultTransport
	itr, err := ghinstallation.New(tr, app.appID, app.installID, app.pem)
	if err != nil {
		return "", err
	}

	token, err := itr.Token(ctx)
	if err != nil {
		return "", err
	}

	return token, nil
}

func (app *GitHubApp) checkIfPrivateKeyIsValid() error {
	blk, _ := pem.Decode(app.pem)
	if blk == nil {
		return fmt.Errorf("githubapp: pem decode failed for app %d", app.appID)
	}
	if blk.Type != "RSA PRIVATE KEY" {
		return fmt.Errorf("githubapp: unsupported key type %q for app %d", blk.Type, app.appID)
	}
	if _, err := x509.ParsePKCS1PrivateKey(blk.Bytes); err != nil {
		return fmt.Errorf("githubapp: invalid RSA key for app %d: %w", app.appID, err)
	}
	return nil
}



================================================
FILE: internal/github/app_test.go
================================================
package github

import (
	"context"
	"io"
	"net/http"
	"strings"
	"sync"
	"testing"
)

const (
	validRSA = `-----BEGIN RSA PRIVATE KEY-----
MIICXQIBAAKBgQC1ZuFGlFeAFqeS6p04QsliOXG3NH1/lQC4UMXdQ0F73ciYBPKq
iQZcoyOu8a2Hsi5HvxDqR1rreTAkJ37C3ErrmKcE1CUJwxBVqkgE17Fzw63QBu0X
0OVtaUarG8Pd9HuKbXPK8HXFTEh6F5hoqmzCmG7cRHmagBeh1SqZm1awzQIDAQAB
AoGAChHZ84cMjGm1h6xKafMbJr61l0vso4Zr8c9aDHxNSEj5d6beqaTNm5rawj1c
Oqojc4whrj+jxmqFx5wBp2N/LRi7GhpPco4wy8gg2t/OjmcR+jTRJgT1x1Co9W58
U+O5c001YFTNoa1UUUBweqye/sX/k5GBCUt0V2G839Cn+8ECQQD2K2eZcyUeeBHT
/YhGAq++mmfVEkzMY7U+G59oeF038zXX+wtMwoKmC9/LHwVPWpnzL/oMu3zZqv4a
jzCOAdZpAkEAvKVas8KUctHUBvDoU6hq9bVyIZMZZnlBfysuFEeJLU8efp/n4KRO
93EyhcXe2FmOC/VzGbkiQobmAqVvIwTixQJBAIKYZE20GG0hpdOhHTqHElU79PnE
y5ljDDP204rI0Ctui5IZTNVcG5ObmQ5ZVqfSmPm66hz3GjUf0c6lSE0ODIECQHB0
silO6We5JggtPJICaCCpVawmIJIx3pWMjB+StXfJHoilknkb+ecQF+ofFsUqPb9r
Rn4jGwVFnYAeVq4tj3ECQQCyeMeCprz5AQ8HSd16Asd3zhv7N7olpb4XMIP6YZXy
udiSlDctMM/X3ZM2JN5M1rtAJ2WR3ZQtmWbOjZAbG2Eq
-----END RSA PRIVATE KEY-----`
	invalidRSA = `-----BEGIN RSA PRIVATE KEY-----
AQIDBA==
-----END RSA PRIVATE KEY-----`
	notRSA = `-----BEGIN PRIVATE KEY-----
MIGHAgEAMBMGByqGSM49AgEGCCqGSM49AwEHBG0wawIBAQQg3rAS658JOtxkOQ4L
7n8EebUpsbeV9Kx/iFGXwxjHPUOhRANCAAQCidzm5b6x5dXdMuq3b7sL52FdqkWx
ytV/UsL9lo9CSv5UTTAnRAjZkyFjDO3cieDA322H+5VQKI7moiKsfz6p
-----END PRIVATE KEY-----`
)

type fakeRT struct {
	mu    sync.Mutex
	calls int
}

func (f *fakeRT) RoundTrip(_ *http.Request) (*http.Response, error) {
	f.mu.Lock()
	f.calls++
	f.mu.Unlock()

	body := `{"token":"abc123","expires_at":"2100-01-01T00:00:00Z"}`
	return &http.Response{
		StatusCode: http.StatusCreated,
		Body:       io.NopCloser(strings.NewReader(body)),
		Header:     make(http.Header),
	}, nil
}

func TestGitHubApp_GetToken_Success(t *testing.T) {
	orig := http.DefaultTransport
	stub := &fakeRT{}
	http.DefaultTransport = stub
	t.Cleanup(func() { http.DefaultTransport = orig })

	app := NewApp(123, 456, []byte(validRSA))

	token, err := app.GetToken(context.Background())
	if err != nil {
		t.Fatalf("GetToken returned error: %v", err)
	}
	if token != "abc123" {
		t.Fatalf("unexpected token %q (want %q)", token, "abc123")
	}
	if stub.calls != 1 {
		t.Fatalf("expected exactly one outbound HTTP request, got %d", stub.calls)
	}
}

func TestGitHubApp_GetToken_InvalidPEM(t *testing.T) {
	app := NewApp(123, 456, []byte("definitely-not-a-PEM-block"))

	_, err := app.GetToken(context.Background())
	if err == nil {
		t.Fatalf("expected error for invalid PEM, got nil")
	}
	const want = "pem decode failed for app"
	if !strings.Contains(err.Error(), want) {
		t.Fatalf("error %q does not contain %q", err, want)
	}
}

func TestGitHubApp_GetToken_NotRSA(t *testing.T) {
	app := NewApp(123, 456, []byte(notRSA))

	_, err := app.GetToken(context.Background())
	if err == nil {
		t.Fatalf("expected error for not RSA PEM, got nil")
	}
	const want = "unsupported key type"
	if !strings.Contains(err.Error(), want) {
		t.Fatalf("error %q does not contain %q", err, want)
	}
}

func TestGitHubApp_GetToken_InvalidRSA(t *testing.T) {
	app := NewApp(123, 456, []byte(invalidRSA))

	_, err := app.GetToken(context.Background())
	if err == nil {
		t.Fatalf("expected error for not RSA PEM, got nil")
	}
	const want = "invalid RSA key for app"
	if !strings.Contains(err.Error(), want) {
		t.Fatalf("error %q does not contain %q", err, want)
	}
}



================================================
FILE: internal/github/auth.go
================================================
package github

import (
	"context"
	"errors"
	"fmt"
	"strconv"

	httpgit "github.com/go-git/go-git/v5/plumbing/transport/http"
	corev1 "k8s.io/api/core/v1"
)

const (
	GitHubAppAuthInstallationIDKey = "github_app_installation_id"
	GitHubAppAuthIDKey             = "github_app_id"
	GitHubAppAuthPrivateKeyKey     = "github_app_private_key"
)

var ErrNotGithubAppSecret = errors.New("not a GitHub App secret")

type AppAuthGetter interface {
	Get(appID, insID int64, pem []byte) (*httpgit.BasicAuth, error)
}

type DefaultAppAuthGetter struct{}

func (DefaultAppAuthGetter) Get(appID, insID int64, pem []byte) (*httpgit.BasicAuth, error) {
	tok, err := NewApp(appID, insID, pem).GetToken(context.Background())
	if err != nil {
		return nil, fmt.Errorf("could not authenticate as GitHub App installation: %w", err)
	}
	// See https://docs.github.com/en/apps/creating-github-apps/authenticating-with-a-github-app/authenticating-as-a-github-app-installation#about-authentication-as-a-github-app-installation for reference
	return &httpgit.BasicAuth{
		Username: "x-access-token",
		Password: tok,
	}, nil
}

func GetGithubAppAuthFromSecret(creds *corev1.Secret, getter AppAuthGetter) (*httpgit.BasicAuth, error) {
	idBytes, okID := creds.Data[GitHubAppAuthIDKey]
	insBytes, okIns := creds.Data[GitHubAppAuthInstallationIDKey]
	pemBytes, okPem := creds.Data[GitHubAppAuthPrivateKeyKey]
	if !okID || !okIns || !okPem {
		return nil, ErrNotGithubAppSecret
	}

	appID, err := strconv.ParseInt(string(idBytes), 10, 64)
	if err != nil {
		return nil, fmt.Errorf("github-app id is not numeric: %w", err)
	}
	insID, err := strconv.ParseInt(string(insBytes), 10, 64)
	if err != nil {
		return nil, fmt.Errorf("github-app installation id is not numeric: %w", err)
	}

	auth, err := getter.Get(appID, insID, pemBytes)
	if err != nil {
		return nil, err
	}
	return auth, nil
}



================================================
FILE: internal/github/auth_test.go
================================================
package github

import (
	"fmt"
	"reflect"
	"testing"

	httpgit "github.com/go-git/go-git/v5/plumbing/transport/http"
	corev1 "k8s.io/api/core/v1"
)

type fakeGetter struct {
	auth *httpgit.BasicAuth
	err  error
}

func (f fakeGetter) Get(appID, instID int64, pem []byte) (*httpgit.BasicAuth, error) {
	return f.auth, f.err
}

func TestGetGithubAppAuthFromSecret(t *testing.T) {
	validAuth := &httpgit.BasicAuth{Username: "x-access-token", Password: "token"}

	tests := []struct {
		name     string
		secret   *corev1.Secret
		getter   AppAuthGetter
		wantAuth *httpgit.BasicAuth
		wantErr  bool
	}{
		{
			name: "missing some keys",
			secret: &corev1.Secret{
				Data: map[string][]byte{
					GitHubAppAuthIDKey: []byte("123"),
				},
			},
			getter:   fakeGetter{err: fmt.Errorf("not a GitHub App secret")},
			wantAuth: nil,
			wantErr:  true,
		},
		{
			name: "all keys present – success path",
			secret: &corev1.Secret{
				Data: map[string][]byte{
					GitHubAppAuthIDKey:             []byte("123"),
					GitHubAppAuthInstallationIDKey: []byte("456"),
					GitHubAppAuthPrivateKeyKey:     []byte("my-pem"),
				},
			},
			getter:   fakeGetter{auth: validAuth},
			wantAuth: validAuth,
			wantErr:  false,
		},
		{
			name: "all keys present – GetGitHubAppAuth returns error",
			secret: &corev1.Secret{
				Data: map[string][]byte{
					GitHubAppAuthIDKey:             []byte("123"),
					GitHubAppAuthInstallationIDKey: []byte("456"),
					GitHubAppAuthPrivateKeyKey:     []byte("my-pem"),
				},
			},
			getter:   fakeGetter{err: fmt.Errorf("token fetch failed")},
			wantAuth: nil,
			wantErr:  true,
		},
		{
			name: "non‑numeric app id",
			secret: &corev1.Secret{
				Data: map[string][]byte{
					GitHubAppAuthIDKey:             []byte("abc"),
					GitHubAppAuthInstallationIDKey: []byte("456"),
					GitHubAppAuthPrivateKeyKey:     []byte("my-pem"),
				},
			},
			getter:   fakeGetter{}, // never called
			wantAuth: nil,
			wantErr:  true,
		},
		{
			name: "non‑numeric installation id",
			secret: &corev1.Secret{
				Data: map[string][]byte{
					GitHubAppAuthIDKey:             []byte("123"),
					GitHubAppAuthInstallationIDKey: []byte("xyz"),
					GitHubAppAuthPrivateKeyKey:     []byte("my-pem"),
				},
			},
			getter:   fakeGetter{}, // never called
			wantAuth: nil,
			wantErr:  true,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			gotAuth, err := GetGithubAppAuthFromSecret(tt.secret, tt.getter)

			if (err != nil) != tt.wantErr {
				t.Fatalf("error mismatch: got %v, wantErr %v", err, tt.wantErr)
			}
			if !reflect.DeepEqual(gotAuth, tt.wantAuth) {
				t.Fatalf("auth mismatch: got %+v, want %+v", gotAuth, tt.wantAuth)
			}
		})
	}
}



================================================
FILE: internal/github/helpers.go
================================================
package github

import (
	corev1 "k8s.io/api/core/v1"
)

const (
	GithubAppIDKey             = "github_app_id"
	GithubAppInstallationIDKey = "github_app_installation_id"
	GithubAppPrivateKeyKey     = "github_app_private_key"
)

// HasGitHubAppKeys checks if the provided Kubernetes secret contains the necessary keys
// for a GitHub App: app ID, installation ID, and private key.
func HasGitHubAppKeys(secret *corev1.Secret) bool {
	if secret == nil {
		return false
	}

	id, hasID := secret.Data[GithubAppIDKey]
	installationID, hasInstallationID := secret.Data[GithubAppInstallationIDKey]
	privateKey, hasPrivateKey := secret.Data[GithubAppPrivateKeyKey]

	return hasID && len(id) > 0 &&
		hasInstallationID && len(installationID) > 0 &&
		hasPrivateKey && len(privateKey) > 0
}



================================================
FILE: internal/github/helpers_test.go
================================================
package github

import (
	"testing"

	corev1 "k8s.io/api/core/v1"
)

func TestHasGitHubAppKeys(t *testing.T) {
	tests := []struct {
		name   string
		secret *corev1.Secret
		want   bool
	}{
		{
			name:   "nil secret",
			secret: nil,
			want:   false,
		},
		{
			name:   "empty secret",
			secret: &corev1.Secret{},
			want:   false,
		},
		{
			name: "only app id",
			secret: &corev1.Secret{
				Data: map[string][]byte{
					GithubAppIDKey: []byte("1"),
				},
			},
			want: false,
		},
		{
			name: "missing installation id",
			secret: &corev1.Secret{
				Data: map[string][]byte{
					GithubAppIDKey:         []byte("1"),
					GithubAppPrivateKeyKey: []byte("priv"),
				},
			},
			want: false,
		},
		{
			name: "all keys present",
			secret: &corev1.Secret{
				Data: map[string][]byte{
					GithubAppIDKey:             []byte("1"),
					GithubAppInstallationIDKey: []byte("123"),
					GithubAppPrivateKeyKey:     []byte("priv"),
				},
			},
			want: true,
		},
		{
			name: "all keys present but app id empty",
			secret: &corev1.Secret{
				Data: map[string][]byte{
					GithubAppIDKey:             []byte(""),
					GithubAppInstallationIDKey: []byte("123"),
					GithubAppPrivateKeyKey:     []byte("priv"),
				},
			},
			want: false,
		},
		{
			name: "all keys present but installation id empty",
			secret: &corev1.Secret{
				Data: map[string][]byte{
					GithubAppIDKey:             []byte("1"),
					GithubAppInstallationIDKey: []byte(""),
					GithubAppPrivateKeyKey:     []byte("priv"),
				},
			},
			want: false,
		},
		{
			name: "all keys present but private key empty",
			secret: &corev1.Secret{
				Data: map[string][]byte{
					GithubAppIDKey:             []byte("1"),
					GithubAppInstallationIDKey: []byte("123"),
					GithubAppPrivateKeyKey:     []byte(""),
				},
			},
			want: false,
		},
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			got := HasGitHubAppKeys(tc.secret)
			if got != tc.want {
				t.Errorf("HasGitHubAppKeys() = %v, want %v", got, tc.want)
			}
		})
	}
}



================================================
FILE: internal/helmdeployer/capabilities.go
================================================
/*
Copyright The Helm Authors.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

	http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/
package helmdeployer

import (
	"context"
	"errors"
	"path"

	"helm.sh/helm/v4/pkg/action"
	"helm.sh/helm/v4/pkg/chart/common"

	"k8s.io/client-go/discovery"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

// capabilities builds a Capabilities from discovery information.
func getCapabilities(ctx context.Context, c *action.Configuration) (*common.Capabilities, error) {
	if c.Capabilities != nil {
		return c.Capabilities, nil
	}
	dc, err := c.RESTClientGetter.ToDiscoveryClient()
	if err != nil {
		return nil, errors.New("could not get Kubernetes discovery client")
	}
	// force a discovery cache invalidation to always fetch the latest server version/capabilities.
	dc.Invalidate()
	kubeVersion, err := dc.ServerVersion()
	if err != nil {
		return nil, errors.New("could not get server version from Kubernetes")
	}
	// Issue #6361:
	// Client-Go emits an error when an API service is registered but unimplemented.
	// We trap that error here and print a warning. But since the discovery client continues
	// building the API object, it is correctly populated with all valid APIs.
	// See https://github.com/kubernetes/kubernetes/issues/72051#issuecomment-521157642
	apiVersions, err := getVersionSet(dc)
	if err != nil {
		if discovery.IsGroupDiscoveryFailedError(err) {
			logger := log.FromContext(ctx).WithName("helm-capabilities")
			logger.Error(err, "The Kubernetes server has an orphaned API service")
			logger.Info("To fix this, run: kubectl delete apiservice <service-name>")
		} else {
			return nil, errors.New("could not get apiVersions from Kubernetes")
		}
	}

	c.Capabilities = common.DefaultCapabilities.Copy()
	c.Capabilities.APIVersions = apiVersions
	c.Capabilities.KubeVersion = common.KubeVersion{
		Version: kubeVersion.GitVersion,
		Major:   kubeVersion.Major,
		Minor:   kubeVersion.Minor,
	}
	return c.Capabilities, nil
}

// getVersionSet retrieves the set of available Kubernetes API versions and resources
// from the discovery client. It tolerates GroupDiscoveryFailedErrors which occur when
// some API groups are unavailable.
func getVersionSet(client discovery.ServerResourcesInterface) (common.VersionSet, error) {
	groups, resources, err := client.ServerGroupsAndResources()
	if err != nil && !discovery.IsGroupDiscoveryFailedError(err) {
		return common.DefaultVersionSet, errors.New("could not get apiVersions from Kubernetes")
	}

	// FIXME: The Kubernetes test fixture for cli appears to always return nil
	// for calls to Discovery().ServerGroupsAndResources(). So in this case, we
	// return the default API list. This is also a safe value to return in any
	// other odd-ball case.
	if len(groups) == 0 && len(resources) == 0 {
		return common.DefaultVersionSet, nil
	}

	versionMap := make(map[string]interface{})
	versions := []string{}

	// Extract the groups
	for _, g := range groups {
		for _, gv := range g.Versions {
			versionMap[gv.GroupVersion] = struct{}{}
		}
	}

	// Extract the resources
	var id string
	var ok bool
	for _, r := range resources {
		for _, rl := range r.APIResources {

			// A Kind at a GroupVersion can show up more than once. We only want
			// it displayed once in the final output.
			id = path.Join(r.GroupVersion, rl.Kind)
			if _, ok = versionMap[id]; !ok {
				versionMap[id] = struct{}{}
			}
		}
	}

	// Convert to a form that NewVersionSet can use
	for k := range versionMap {
		versions = append(versions, k)
	}

	return versions, nil
}



================================================
FILE: internal/helmdeployer/delete.go
================================================
package helmdeployer

import (
	"context"
	"errors"
	"fmt"
	"strings"

	"github.com/go-logr/logr"
	"helm.sh/helm/v4/pkg/action"
	"helm.sh/helm/v4/pkg/kube"
	releasev1 "helm.sh/helm/v4/pkg/release/v1"
	"helm.sh/helm/v4/pkg/storage/driver"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/kv"
	"github.com/rancher/fleet/internal/experimental"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	corev1 "k8s.io/api/core/v1"
	errutil "k8s.io/apimachinery/pkg/util/errors"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

// DeleteRelease deletes the release for the DeployedBundle.
func (h *Helm) DeleteRelease(ctx context.Context, deployment DeployedBundle) error {
	return h.deleteByRelease(ctx, deployment.BundleID, deployment.ReleaseName, deployment.KeepResources)
}

// Delete the release for the given bundleID. The bundleID is the name of the
// bundledeployment.
func (h *Helm) Delete(ctx context.Context, bundleID string) error {
	releaseName := ""
	keepResources := false
	deployments, err := h.ListDeployments(h.NewListAction())
	if err != nil {
		return err
	}
	for _, deployment := range deployments {
		if deployment.BundleID == bundleID {
			releaseName = deployment.ReleaseName
			keepResources = deployment.KeepResources
			break
		}
	}
	if releaseName == "" {
		// Never found anything to delete
		return nil
	}
	return h.deleteByRelease(ctx, bundleID, releaseName, keepResources)
}

func (h *Helm) deleteByRelease(ctx context.Context, bundleID, releaseName string, keepResources bool) error {
	logger := log.FromContext(ctx).WithName("delete-by-release").WithValues("releaseName", releaseName, "keepResources", keepResources)
	releaseNamespace, releaseName := kv.Split(releaseName, "/")
	rels, err := listReleases(h.globalCfg.Releases, func(r *releasev1.Release) bool {
		return r.Namespace == releaseNamespace &&
			r.Name == releaseName &&
			r.Chart.Metadata.Annotations[BundleIDAnnotation] == bundleID &&
			r.Chart.Metadata.Annotations[AgentNamespaceAnnotation] == h.agentNamespace
	})
	if err != nil {
		return err
	}
	if len(rels) == 0 {
		return nil
	}

	var (
		serviceAccountName string
	)
	for _, rel := range rels {
		serviceAccountName = rel.Chart.Metadata.Annotations[ServiceAccountNameAnnotation]
		if serviceAccountName != "" {
			break
		}
	}

	cfg, err := h.getCfg(ctx, releaseNamespace, serviceAccountName)
	if err != nil {
		return err
	}

	if strings.HasPrefix(bundleID, "fleet-agent") {
		// Never uninstall the fleet-agent, just "forget" it
		return deleteHistory(cfg, logger, bundleID)
	}

	if keepResources {
		// don't delete resources, just delete the helm release secrets
		return deleteHistory(cfg, logger, bundleID)
	}

	u := action.NewUninstall(cfg)
	// WaitStrategy must be set in Helm v4 to avoid "unknown wait strategy" error
	// HookOnlyStrategy is the default behavior (equivalent to not waiting)
	u.WaitStrategy = kube.HookOnlyStrategy
	if _, err := u.Run(releaseName); err != nil {
		return fmt.Errorf("failed to delete release %s: %w", releaseName, err)
	}

	return deleteResourcesCopiedFromUpstream(ctx, h.client, bundleID)
}

func (h *Helm) delete(ctx context.Context, bundleID string, options fleet.BundleDeploymentOptions, dryRun bool) error {
	logger := log.FromContext(ctx).WithName("helm-deployer").WithName("delete").WithValues("dryRun", dryRun)
	timeout, _, releaseName := h.getOpts(bundleID, options)

	r, err := getLastRelease(h.globalCfg.Releases, releaseName)
	if err != nil {
		// If the release doesn't exist, there's nothing to delete
		if errors.Is(err, driver.ErrReleaseNotFound) || errors.Is(err, driver.ErrNoDeployedReleases) {
			return nil
		}
		return err
	}

	if r.Chart.Metadata.Annotations[BundleIDAnnotation] != bundleID {
		rels, err := getReleaseHistory(h.globalCfg.Releases, releaseName)
		if err != nil {
			// If we can't get the history, treat it as not found
			if errors.Is(err, driver.ErrReleaseNotFound) || errors.Is(err, driver.ErrNoDeployedReleases) {
				return nil
			}
			return err
		}
		r = nil
		for _, rel := range rels {
			if rel.Chart.Metadata.Annotations[BundleIDAnnotation] == bundleID {
				r = rel
				break
			}
		}
		if r == nil {
			return fmt.Errorf("failed to find helm release to delete for %s", bundleID)
		}
	}

	serviceAccountName := r.Chart.Metadata.Annotations[ServiceAccountNameAnnotation]
	cfg, err := h.getCfg(ctx, r.Namespace, serviceAccountName)
	if err != nil {
		return err
	}

	if strings.HasPrefix(bundleID, "fleet-agent") {
		// Never uninstall the fleet-agent, just "forget" it
		return deleteHistory(cfg, logger, bundleID)
	}

	u := action.NewUninstall(cfg)
	// WaitStrategy must be set in Helm v4 to avoid "unknown wait strategy" error
	// HookOnlyStrategy is the default behavior (equivalent to not waiting)
	u.WaitStrategy = kube.HookOnlyStrategy
	u.DryRun = dryRun
	u.Timeout = timeout

	if !dryRun {
		logger.Info("Helm: Uninstalling")
	}
	_, err = u.Run(releaseName)
	return err
}

func deleteHistory(cfg *action.Configuration, logger logr.Logger, bundleID string) error {
	releases, err := listReleases(cfg.Releases, func(r *releasev1.Release) bool {
		return r.Name == bundleID && r.Chart.Metadata.Annotations[BundleIDAnnotation] == bundleID
	})
	if err != nil {
		return err
	}
	for _, release := range releases {
		logger.Info("Helm: Deleting release", "releaseVersion", release.Version)
		if _, err := cfg.Releases.Delete(release.Name, release.Version); err != nil {
			return err
		}
	}
	return nil
}

// deleteResourcesCopiedFromUpstream deletes resources referenced through a bundle's `DownstreamResources`
// field, and copied from downstream.
func deleteResourcesCopiedFromUpstream(ctx context.Context, c client.Client, bdName string) error {
	if !experimental.CopyResourcesDownstreamEnabled() {
		return nil
	}

	var merr []error

	// No information is available about a deleted bundle deployment beside its name and namespace;
	// in particular, we do not know where its resources copied from the upstream cluster, if any, might live, so we
	// cannot delete them by name and namespace; instead, we need to resort to labels.
	opts := client.MatchingLabels{
		fleet.BundleDeploymentOwnershipLabel: bdName,
	}

	secrets := corev1.SecretList{}

	// XXX: should we log instead of erroring?
	if err := c.List(ctx, &secrets, opts); err != nil {
		merr = append(merr, fmt.Errorf("failed to list copied secrets from upstream to delete from outdated bundle: %w", err))
	}

	for _, s := range secrets.Items {
		if err := c.Delete(ctx, &s); err != nil {
			merr = append(merr, fmt.Errorf("failed to delete outdated secrets copied from downstream: %w", err))
		}
	}

	cms := corev1.ConfigMapList{}

	if err := c.List(ctx, &cms, opts); err != nil {
		return fmt.Errorf("failed to list copied configmaps from upstream to delete from outdated bundle: %w", err)
	}
	for _, cm := range cms.Items {
		if err := c.Delete(ctx, &cm); err != nil {
			merr = append(merr, fmt.Errorf("failed to delete outdated configmaps copied from downstream: %w", err))
		}
	}

	return errutil.NewAggregate(merr)
}



================================================
FILE: internal/helmdeployer/deployer.go
================================================
package helmdeployer

import (
	"context"
	"log/slog"
	"time"

	"github.com/go-logr/logr"
	"github.com/pkg/errors"
	"github.com/rancher/fleet/internal/helmdeployer/helmcache"
	"helm.sh/helm/v4/pkg/action"
	"helm.sh/helm/v4/pkg/kube"
	"helm.sh/helm/v4/pkg/storage"
	"helm.sh/helm/v4/pkg/storage/driver"

	"github.com/rancher/fleet/internal/names"
	"github.com/rancher/fleet/internal/namespaces"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/cli-runtime/pkg/genericclioptions"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

const (
	BundleIDAnnotation           = "fleet.cattle.io/bundle-id"
	CommitAnnotation             = "fleet.cattle.io/commit"
	AgentNamespaceAnnotation     = "fleet.cattle.io/agent-namespace"
	ServiceAccountNameAnnotation = "fleet.cattle.io/service-account"
	DefaultServiceAccount        = "fleet-default"
	KeepResourcesAnnotation      = "fleet.cattle.io/keep-resources"
	HelmUpgradeInterruptedError  = "another operation (install/upgrade/rollback) is in progress"
	MaxHelmHistory               = 2
)

var (
	ErrNoRelease    = errors.New("failed to find release")
	ErrNoResourceID = errors.New("no resource ID available")
	DefaultKey      = "values.yaml"
)

type Helm struct {
	client         client.Client
	agentNamespace string
	getter         genericclioptions.RESTClientGetter
	globalCfg      *action.Configuration
	// useGlobalCfg is only used by Template
	useGlobalCfg     bool
	template         bool
	defaultNamespace string
	labelPrefix      string
	labelSuffix      string
}

// Resources contains information from a helm release
type Resources struct {
	// DefaultNamespace is the namespace of the helm release
	DefaultNamespace string           `json:"defaultNamespace,omitempty"`
	Objects          []runtime.Object `json:"objects,omitempty"`
}

// DeployedBundle is the link between a bundledeployment and a helm release
type DeployedBundle struct {
	// BundleID is the bundledeployment.Name
	BundleID string
	// ReleaseName is actually in the form "namespace/release name"
	ReleaseName string
	// KeepResources indicate if resources should be kept when deleting a GitRepo or Bundle
	KeepResources bool
}

// New returns a new helm deployer
// * agentNamespace is the system namespace, which is the namespace the agent is running in, e.g. cattle-fleet-system
func New(agentNamespace, defaultNamespace, labelPrefix, labelSuffix string) *Helm {
	return &Helm{
		agentNamespace:   agentNamespace,
		defaultNamespace: defaultNamespace,
		labelPrefix:      labelPrefix,
		labelSuffix:      labelSuffix,
	}
}

func (h *Helm) Setup(ctx context.Context, client client.Client, getter genericclioptions.RESTClientGetter) error {
	h.client = client
	h.getter = getter

	cfg, err := h.createCfg(ctx, "")
	if err != nil {
		return err
	}
	h.globalCfg = cfg

	return nil
}

func (h *Helm) getOpts(bundleID string, options fleet.BundleDeploymentOptions) (time.Duration, string, string) {
	if options.Helm == nil {
		options.Helm = &fleet.HelmOptions{}
	}

	var timeout time.Duration
	if options.Helm.TimeoutSeconds > 0 {
		timeout = time.Second * time.Duration(options.Helm.TimeoutSeconds)
	}

	ns := namespaces.GetDeploymentNS(h.defaultNamespace, options)

	if options.Helm != nil && options.Helm.ReleaseName != "" {
		// JSON schema validation makes sure that the option is valid
		return timeout, ns, options.Helm.ReleaseName
	}

	// releaseName has a limit of 53 in helm https://github.com/helm/helm/blob/main/pkg/action/install.go#L58
	// fleet apply already produces valid names, but we need to make sure
	// that bundles from other sources are valid
	return timeout, ns, names.HelmReleaseName(bundleID)
}

func (h *Helm) getCfg(ctx context.Context, namespace, serviceAccountName string) (*action.Configuration, error) {
	var getter = h.getter

	if h.useGlobalCfg {
		return h.globalCfg, nil
	}

	serviceAccountNamespace, serviceAccountName, err := h.getServiceAccount(ctx, serviceAccountName)
	if err != nil {
		return nil, err
	}

	if serviceAccountName != "" {
		getter, err = newImpersonatingGetter(serviceAccountNamespace, serviceAccountName, h.getter)
		if err != nil {
			return nil, err
		}
	}

	kClient := kube.New(getter)
	kClient.Namespace = namespace

	cfg, err := h.createCfg(ctx, namespace)
	if err != nil {
		return nil, err
	}
	cfg.Releases.MaxHistory = MaxHelmHistory
	cfg.KubeClient = kClient

	cfg.Capabilities, _ = getCapabilities(ctx, cfg)

	return cfg, nil
}

func (h *Helm) createCfg(ctx context.Context, namespace string) (*action.Configuration, error) {
	// Create a logger handler for Helm SDK components.
	// This uses Fleet's controller-runtime logger (which uses logr/zapr) and adapts it to slog.
	// The logger level is set to V(1) to match the verbosity level used in Helm v3.
	logger := log.FromContext(ctx).WithName("helmSDK")
	handler := slog.NewTextHandler(&logrWriter{logger: logger}, &slog.HandlerOptions{
		Level: slog.LevelDebug,
	})

	kc := kube.New(h.getter)
	kc.SetLogger(handler)
	clientSet, err := kc.Factory.KubernetesClientSet()
	if err != nil {
		return nil, err
	}
	d := driver.NewSecrets(helmcache.NewSecretClient(h.client, clientSet, namespace))
	d.SetLogger(handler)
	store := storage.Init(d)
	store.MaxHistory = MaxHelmHistory

	cfg := &action.Configuration{
		RESTClientGetter: h.getter,
		Releases:         store,
		KubeClient:       kc,
	}
	cfg.SetLogger(handler)

	return cfg, nil
}

// logrWriter adapts a logr.Logger to io.Writer interface for slog.TextHandler.
// This allows Helm v4's slog-based logging to write through Fleet's controller-runtime logger.
type logrWriter struct {
	logger logr.Logger
}

func (w *logrWriter) Write(p []byte) (n int, err error) {
	// Log at V(1) level to match the verbosity used in the original Helm v3 integration
	w.logger.V(1).Info(string(p))
	return len(p), nil
}



================================================
FILE: internal/helmdeployer/history.go
================================================
package helmdeployer

import (
	"bytes"
	"errors"
	"fmt"
	"strconv"

	"helm.sh/helm/v4/pkg/action"
	releasev1 "helm.sh/helm/v4/pkg/release/v1"
	"helm.sh/helm/v4/pkg/storage/driver"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/klog/v2"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/kv"

	"github.com/rancher/wrangler/v3/pkg/yaml"
)

func (h *Helm) EnsureInstalled(bundleID, resourcesID string) (bool, error) {
	releaseName, version, namespace, err := getReleaseNameVersionAndNamespace(bundleID, resourcesID)
	if err != nil {
		return false, err
	}

	if _, err := h.getRelease(releaseName, namespace, version); errors.Is(err, ErrNoRelease) {
		return false, nil
	} else if err != nil {
		return false, err
	}
	return true, nil
}

// Resources returns the resources from the helm release history
func (h *Helm) Resources(bundleID, resourcesID string) (*Resources, error) {
	releaseName, version, namespace, err := getReleaseNameVersionAndNamespace(bundleID, resourcesID)
	if err != nil {
		return &Resources{}, err
	}

	release, err := h.getRelease(releaseName, namespace, version)
	if errors.Is(err, ErrNoRelease) {
		return &Resources{}, nil
	} else if err != nil {
		return nil, err
	}

	resources := &Resources{DefaultNamespace: release.Namespace}
	resources.Objects, err = ReleaseToObjects(release)
	return resources, err
}

func (h *Helm) ResourcesFromPreviousReleaseVersion(bundleID, resourcesID string) (*Resources, error) {
	releaseName, version, namespace, err := getReleaseNameVersionAndNamespace(bundleID, resourcesID)
	if err != nil {
		return &Resources{}, err
	}

	release, err := h.getRelease(releaseName, namespace, version-1)
	if errors.Is(err, ErrNoRelease) {
		return &Resources{}, nil
	} else if err != nil {
		return nil, err
	}

	resources := &Resources{DefaultNamespace: release.Namespace}
	resources.Objects, err = ReleaseToObjects(release)
	return resources, err
}

func getReleaseNameVersionAndNamespace(bundleID, resourcesID string) (string, int, string, error) {
	// When a bundle is installed a resourcesID is generated. If there is no
	// resourcesID then there isn't anything to lookup.
	if resourcesID == "" {
		return "", 0, "", ErrNoResourceID
	}
	namespace, name := kv.Split(resourcesID, "/")
	releaseName, versionStr := kv.Split(name, ":")
	version, _ := strconv.Atoi(versionStr)

	if releaseName == "" {
		releaseName = bundleID
	}

	return releaseName, version, namespace, nil
}

func (h *Helm) getRelease(releaseName, namespace string, version int) (*releasev1.Release, error) {
	hist := action.NewHistory(h.globalCfg)

	releases, err := hist.Run(releaseName)
	if errors.Is(err, driver.ErrReleaseNotFound) {
		return nil, ErrNoRelease
	} else if err != nil {
		return nil, err
	}

	for _, releaser := range releases {
		release, err := releaserToV1Release(releaser)
		if err != nil {
			klog.V(1).InfoS("Skipping release entry with unsupported type during history lookup",
				"error", err, "releaseName", releaseName, "namespace", namespace)
			continue
		}
		if release.Name == releaseName && release.Version == version && release.Namespace == namespace {
			return release, nil
		}
	}

	return nil, ErrNoRelease
}

// ReleaseToResourceID converts a Helm release to Fleet's resource ID format.
// The resource ID uniquely identifies a release by namespace, name, and version.
func ReleaseToResourceID(release *releasev1.Release) string {
	return fmt.Sprintf("%s/%s:%d", release.Namespace, release.Name, release.Version)
}

// ReleaseToObjects parses the manifest from a Helm release and converts it to Kubernetes runtime objects.
func ReleaseToObjects(release *releasev1.Release) ([]runtime.Object, error) {
	var err error

	objs, err := yaml.ToObjects(bytes.NewBufferString(release.Manifest))
	return objs, err
}



================================================
FILE: internal/helmdeployer/impersonate.go
================================================
package helmdeployer

import (
	"context"
	"fmt"

	corev1 "k8s.io/api/core/v1"
	apierror "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/cli-runtime/pkg/genericclioptions"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	clientcmdapi "k8s.io/client-go/tools/clientcmd/api"
)

// getServiceAccount is called with an empty name, unless the user specified a
// service account in their git repo.
//
// If the service account passed to this func is empty, it will default to
// "fleet-default". It will then check for existence and return the default
// account or an empty namespace and name.
// Returning an empty name, will make the agent use its own service account,
// which is cluster admin.
//
// If a specific account was passed in and it was not found, it will return an error.
func (h *Helm) getServiceAccount(ctx context.Context, name string) (string, string, error) {
	currentName := name
	if currentName == "" {
		currentName = DefaultServiceAccount
	}
	sa := &corev1.ServiceAccount{}
	ns := getServiceAccountNamespace(h.agentNamespace, h.defaultNamespace)
	err := h.client.Get(ctx, types.NamespacedName{Namespace: ns, Name: currentName}, sa)
	if apierror.IsNotFound(err) && name == "" {
		// if we can't find the fleet-default service account, but none
		// was asked for, use the pods service account instead
		return "", "", nil
	} else if err != nil {
		// we failed to find an explicitly asked for service account
		return "", "", fmt.Errorf("looking up service account %s/%s: %w", h.agentNamespace, currentName, err)
	}

	return h.agentNamespace, currentName, nil
}

type impersonatingGetter struct {
	genericclioptions.RESTClientGetter

	config     clientcmd.ClientConfig
	restConfig *rest.Config
}

func newImpersonatingGetter(namespace, name string, getter genericclioptions.RESTClientGetter) (genericclioptions.RESTClientGetter, error) {
	config := clientcmd.NewDefaultClientConfig(impersonationConfig(namespace, name), &clientcmd.ConfigOverrides{})

	restConfig, err := config.ClientConfig()
	if err != nil {
		return nil, err
	}
	restConfig.QPS = -1
	restConfig.RateLimiter = nil

	return &impersonatingGetter{
		RESTClientGetter: getter,
		config:           config,
		restConfig:       restConfig,
	}, nil
}

func (i *impersonatingGetter) ToRESTConfig() (*rest.Config, error) {
	return i.restConfig, nil
}

func (i *impersonatingGetter) ToRawKubeConfigLoader() clientcmd.ClientConfig {
	return i.config
}

func impersonationConfig(namespace, name string) clientcmdapi.Config {
	return clientcmdapi.Config{
		Clusters: map[string]*clientcmdapi.Cluster{
			"cluster": {
				Server:               "https://kubernetes.default",
				CertificateAuthority: "/run/secrets/kubernetes.io/serviceaccount/ca.crt",
			},
		},
		AuthInfos: map[string]*clientcmdapi.AuthInfo{
			"user": {
				TokenFile:   "/run/secrets/kubernetes.io/serviceaccount/token",
				Impersonate: fmt.Sprintf("system:serviceaccount:%s:%s", namespace, name),
			},
		},
		Contexts: map[string]*clientcmdapi.Context{
			"default": {
				Cluster:  "cluster",
				AuthInfo: "user",
			},
		},
		CurrentContext: "default",
	}
}

func getServiceAccountNamespace(agentNamespace, defaultNamespace string) string {
	if agentNamespace == "" {
		// Since ServiceAccount is a namespaced resource,
		// the client rejects the Get request if the agent namespace is not provided with the error:
		// "an empty namespace may not be set when a resource name is provided".
		// Use the default namespace to avoid this.
		// Note: agent namespace is unset in some integration tests.
		return defaultNamespace
	}

	return agentNamespace
}



================================================
FILE: internal/helmdeployer/install.go
================================================
package helmdeployer

import (
	"bytes"
	"context"
	"errors"
	"fmt"
	"strconv"
	"time"

	"helm.sh/helm/v4/pkg/action"
	chartv2 "helm.sh/helm/v4/pkg/chart/v2"
	"helm.sh/helm/v4/pkg/chart/v2/loader"
	"helm.sh/helm/v4/pkg/kube"
	releasecommon "helm.sh/helm/v4/pkg/release/common"
	releasev1 "helm.sh/helm/v4/pkg/release/v1"
	"helm.sh/helm/v4/pkg/storage/driver"

	"github.com/rancher/fleet/internal/helmdeployer/render"
	"github.com/rancher/fleet/internal/manifest"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/apimachinery/pkg/util/yaml"
	"sigs.k8s.io/controller-runtime/pkg/log"
)

type dryRunConfig struct {
	DryRun       bool
	DryRunOption string
}

// Deploy deploys an unpacked content resource with helm. bundleID is the name of the bundledeployment.
func (h *Helm) Deploy(ctx context.Context, bundleID string, manifest *manifest.Manifest, options fleet.BundleDeploymentOptions) (*releasev1.Release, error) {
	if options.Helm == nil {
		options.Helm = &fleet.HelmOptions{}
	}
	if options.Kustomize == nil {
		options.Kustomize = &fleet.KustomizeOptions{}
	}

	tar, err := render.HelmChart(bundleID, manifest, options)
	if err != nil {
		return nil, err
	}

	chart, err := loader.LoadArchive(tar)
	if err != nil {
		return nil, err
	}

	if chart.Metadata.Annotations == nil {
		chart.Metadata.Annotations = map[string]string{}
	}
	chart.Metadata.Annotations[ServiceAccountNameAnnotation] = options.ServiceAccount
	chart.Metadata.Annotations[BundleIDAnnotation] = bundleID
	chart.Metadata.Annotations[AgentNamespaceAnnotation] = h.agentNamespace
	chart.Metadata.Annotations[KeepResourcesAnnotation] = strconv.FormatBool(options.KeepResources)

	if manifest.Commit != "" {
		chart.Metadata.Annotations[CommitAnnotation] = manifest.Commit
	}

	if release, err := h.install(ctx, bundleID, manifest, chart, options, getDryRunConfig(chart, true)); err != nil {
		return nil, err
	} else if h.template {
		return release, nil
	}

	return h.install(ctx, bundleID, manifest, chart, options, getDryRunConfig(chart, false))
}

// install runs helm install or upgrade and supports dry running the action. Will run helm rollback in case of a failed upgrade.
func (h *Helm) install(ctx context.Context, bundleID string, manifest *manifest.Manifest, chart *chartv2.Chart, options fleet.BundleDeploymentOptions, dryRunCfg dryRunConfig) (*releasev1.Release, error) {
	logger := log.FromContext(ctx).WithName("helm-deployer").WithName("install").WithValues("commit", manifest.Commit, "dryRun", dryRunCfg.DryRun)
	timeout, defaultNamespace, releaseName := h.getOpts(bundleID, options)

	values, err := h.getValues(ctx, options, defaultNamespace)
	if err != nil {
		return nil, err
	}

	cfg, err := h.getCfg(ctx, defaultNamespace, options.ServiceAccount)
	if err != nil {
		return nil, err
	}

	uninstall, err := h.mustUninstall(cfg, releaseName)
	if err != nil {
		return nil, err
	}

	if uninstall {
		logger.Info("Uninstalling helm release first")
		if err := h.delete(ctx, bundleID, options, dryRunCfg.DryRun); err != nil {
			return nil, err
		}
		if dryRunCfg.DryRun {
			// In dry run mode, we've validated that uninstall is needed but can't proceed
			// with install/upgrade since the old release conceptually still exists.
			// Returning (nil, nil) indicates successful dry run completion with no release object.
			return nil, nil
		}
	}

	install, err := h.mustInstall(cfg, releaseName)
	if err != nil {
		return nil, err
	}

	pr, err := h.createPostRenderer(cfg, bundleID, manifest, chart, options)
	if err != nil {
		return nil, err
	}

	if install {
		return h.runInstall(ctx, cfg, chart, values, releaseName, defaultNamespace, timeout, options, pr, dryRunCfg)
	}

	return h.runUpgrade(ctx, cfg, chart, values, releaseName, defaultNamespace, timeout, options, pr, dryRunCfg)
}

// createPostRenderer creates a post-renderer for Helm charts that handles label/annotation
// transformations and CRD deletion policies based on Fleet bundle deployment options.
func (h *Helm) createPostRenderer(cfg *action.Configuration, bundleID string, manifest *manifest.Manifest, chart *chartv2.Chart, options fleet.BundleDeploymentOptions) (*postRender, error) {
	pr := &postRender{
		labelPrefix: h.labelPrefix,
		labelSuffix: h.labelSuffix,
		bundleID:    bundleID,
		manifest:    manifest,
		opts:        options,
		chart:       chart,
	}

	if !h.useGlobalCfg {
		mapper, err := cfg.RESTClientGetter.ToRESTMapper()
		if err != nil {
			return nil, err
		}
		pr.mapper = mapper
	}

	return pr, nil
}

// runInstall executes a Helm install operation with the provided configuration and values.
// It creates an Install action, configures it, and runs the installation.
func (h *Helm) runInstall(
	ctx context.Context,
	cfg *action.Configuration,
	chart *chartv2.Chart,
	values map[string]interface{},
	releaseName string,
	namespace string,
	timeout time.Duration,
	options fleet.BundleDeploymentOptions,
	pr *postRender,
	dryRunCfg dryRunConfig,
) (*releasev1.Release, error) {
	logger := log.FromContext(ctx)
	u := action.NewInstall(cfg)

	h.configureInstallAction(u, cfg, releaseName, namespace, timeout, options, pr, dryRunCfg)

	if !dryRunCfg.DryRun {
		logger.Info("Installing helm release")
	}

	rel, err := u.Run(chart, values)
	if err != nil {
		return nil, err
	}

	return assertRelease(rel)
}

// configureDryRunStrategy sets the DryRunStrategy based on template mode and dryRunConfig.
// Template mode requires DryRunClient to render without cluster interaction.
// If DryRunOption is "server", use DryRunServer to allow lookup functions to query the cluster.
// Otherwise, use DryRunClient for client-only dry run or DryRunNone for actual execution.
func (h *Helm) configureDryRunStrategy(dryRunCfg dryRunConfig) action.DryRunStrategy {
	if h.template {
		return action.DryRunClient
	} else if dryRunCfg.DryRun {
		if dryRunCfg.DryRunOption == "server" {
			return action.DryRunServer
		}
		return action.DryRunClient
	}
	return action.DryRunNone
}

// configureInstallAction configures a Helm Install action with Fleet-specific options,
// including timeout, wait strategies, and dry-run configuration.
func (h *Helm) configureInstallAction(u *action.Install, cfg *action.Configuration, releaseName, namespace string, timeout time.Duration, options fleet.BundleDeploymentOptions, pr *postRender, dryRunCfg dryRunConfig) {
	if cfg.Capabilities != nil {
		if cfg.Capabilities.KubeVersion.Version != "" {
			u.KubeVersion = &cfg.Capabilities.KubeVersion
		}
		if cfg.Capabilities.APIVersions != nil {
			u.APIVersions = cfg.Capabilities.APIVersions
		}
	}
	u.TakeOwnership = options.Helm.TakeOwnership
	// Disable server-side apply when taking ownership to avoid managedFields validation errors.
	// When adopting existing resources, they have managedFields populated by Kubernetes,
	// but server-side apply requires managedFields to be nil. Using client-side apply (three-way merge) instead.
	if u.TakeOwnership {
		u.ServerSideApply = false
	}
	u.EnableDNS = !options.Helm.DisableDNS
	u.Replace = true
	u.RollbackOnFailure = options.Helm.Atomic
	u.ReleaseName = releaseName
	u.CreateNamespace = true
	u.Namespace = namespace
	u.Timeout = timeout
	u.DryRunStrategy = h.configureDryRunStrategy(dryRunCfg)
	u.SkipSchemaValidation = options.Helm.SkipSchemaValidation
	u.PostRenderer = pr
	u.WaitForJobs = options.Helm.WaitForJobs
	// When timeout is set, use StatusWatcherStrategy to wait for resources.
	// Otherwise use HookOnlyStrategy (the default, equivalent to not waiting).
	if u.Timeout > 0 {
		u.WaitStrategy = kube.StatusWatcherStrategy
	} else {
		u.WaitStrategy = kube.HookOnlyStrategy
	}
}

// runUpgrade executes a Helm upgrade operation with the provided configuration and values.
// It creates an Upgrade action, configures it, and runs the upgrade with automatic rollback
// retry logic if the upgrade is interrupted.
func (h *Helm) runUpgrade(
	ctx context.Context,
	cfg *action.Configuration,
	chart *chartv2.Chart,
	values map[string]interface{},
	releaseName string,
	namespace string,
	timeout time.Duration,
	options fleet.BundleDeploymentOptions,
	pr *postRender,
	dryRunCfg dryRunConfig,
) (*releasev1.Release, error) {
	logger := log.FromContext(ctx)
	u := action.NewUpgrade(cfg)

	h.configureUpgradeAction(u, namespace, timeout, options, pr, dryRunCfg)

	if !dryRunCfg.DryRun {
		logger.Info("Upgrading helm release")
	}

	rel, err := u.Run(releaseName, chart, values)
	if err != nil && err.Error() == HelmUpgradeInterruptedError {
		return h.retryUpgradeAfterRollback(ctx, cfg, u, releaseName, chart, values)
	}
	if err != nil {
		return nil, err
	}

	return assertRelease(rel)
}

// configureUpgradeAction configures a Helm Upgrade action with Fleet-specific options,
// including timeout, wait strategies, and drift correction settings.
func (h *Helm) configureUpgradeAction(u *action.Upgrade, namespace string, timeout time.Duration, options fleet.BundleDeploymentOptions, pr *postRender, dryRunCfg dryRunConfig) {
	u.TakeOwnership = true
	u.EnableDNS = !options.Helm.DisableDNS
	u.ForceReplace = options.Helm.Force
	if options.CorrectDrift != nil {
		u.ForceReplace = u.ForceReplace || options.CorrectDrift.Force
	}
	// When using ForceReplace, must disable ServerSideApply.
	// ForceReplace and ServerSideApply cannot be used together in Helm v4.
	// Set to "false" (not "auto") to explicitly disable server-side apply.
	// Otherwise use "auto" to respect the previous release's apply method.
	if u.ForceReplace {
		u.ServerSideApply = "false"
	} else {
		u.ServerSideApply = "auto"
	}
	u.RollbackOnFailure = options.Helm.Atomic
	u.MaxHistory = options.Helm.MaxHistory
	if u.MaxHistory == 0 {
		u.MaxHistory = MaxHelmHistory
	}
	u.Namespace = namespace
	u.Timeout = timeout
	u.DryRunStrategy = h.configureDryRunStrategy(dryRunCfg)
	u.SkipSchemaValidation = options.Helm.SkipSchemaValidation
	u.DisableOpenAPIValidation = h.template || dryRunCfg.DryRun
	u.PostRenderer = pr
	u.WaitForJobs = options.Helm.WaitForJobs
	// When timeout is set, use StatusWatcherStrategy to wait for resources.
	// Otherwise use HookOnlyStrategy (the default, equivalent to not waiting).
	if u.Timeout > 0 {
		u.WaitStrategy = kube.StatusWatcherStrategy
	} else {
		u.WaitStrategy = kube.HookOnlyStrategy
	}
}

// retryUpgradeAfterRollback handles the case where a Helm upgrade is interrupted and retries
// the upgrade after performing a rollback. This addresses the "another operation is in progress" error.
func (h *Helm) retryUpgradeAfterRollback(ctx context.Context, cfg *action.Configuration, u *action.Upgrade, releaseName string, chart *chartv2.Chart, values map[string]interface{}) (*releasev1.Release, error) {
	logger := log.FromContext(ctx)
	logger.Info("Helm doing a rollback", "error", HelmUpgradeInterruptedError)

	r := action.NewRollback(cfg)
	err := r.Run(releaseName)
	if err != nil {
		return nil, err
	}

	logger.V(1).Info("Retrying upgrade after rollback")
	rel, err := u.Run(releaseName, chart, values)
	if err != nil {
		return nil, err
	}

	return assertRelease(rel)
}

// assertRelease converts a Helm release interface to a concrete *releasev1.Release type.
func assertRelease(rel interface{}) (*releasev1.Release, error) {
	if v1Rel, ok := rel.(*releasev1.Release); ok {
		return v1Rel, nil
	}
	return nil, fmt.Errorf("unexpected release type: %T", rel)
}

func (h *Helm) mustUninstall(cfg *action.Configuration, releaseName string) (bool, error) {
	r, err := getLastRelease(cfg.Releases, releaseName)
	if err != nil {
		// If the release doesn't exist, there's nothing to uninstall
		if errors.Is(err, driver.ErrReleaseNotFound) || errors.Is(err, driver.ErrNoDeployedReleases) {
			return false, nil
		}
		return false, err
	}
	return r.Info.Status == releasecommon.StatusUninstalling || r.Info.Status == releasecommon.StatusPendingInstall, nil
}

// mustInstall checks if a fresh install is required by verifying if there is no deployed release.
// Returns true if no deployed release exists for the given release name.
func (h *Helm) mustInstall(cfg *action.Configuration, releaseName string) (bool, error) {
	_, err := cfg.Releases.Deployed(releaseName)
	if err != nil && errors.Is(err, driver.ErrNoDeployedReleases) {
		return true, nil
	}
	return false, err
}

func (h *Helm) getValues(ctx context.Context, options fleet.BundleDeploymentOptions, defaultNamespace string) (map[string]interface{}, error) {
	if options.Helm == nil {
		return nil, nil
	}

	var values map[string]interface{}
	if options.Helm.Values != nil {
		values = options.Helm.Values.Data
	}

	// avoid the possibility of returning a nil map
	if values == nil {
		values = map[string]interface{}{}
	}
	// do not run this when using template
	if !h.template {
		for _, valuesFrom := range options.Helm.ValuesFrom {
			var tempValues map[string]interface{}
			if valuesFrom.ConfigMapKeyRef != nil {
				name := valuesFrom.ConfigMapKeyRef.Name
				namespace := valuesFrom.ConfigMapKeyRef.Namespace
				if namespace == "" {
					namespace = defaultNamespace
				}
				key := valuesFrom.ConfigMapKeyRef.Key
				if key == "" {
					key = DefaultKey
				}
				configMap := &corev1.ConfigMap{}
				err := h.client.Get(ctx, types.NamespacedName{Name: name, Namespace: namespace}, configMap)
				if err != nil {
					return nil, err
				}
				tempValues, err = valuesFromConfigMap(name, namespace, key, configMap)
				if err != nil {
					return nil, err
				}
			}
			if tempValues != nil {
				values = mergeValues(values, tempValues)
				tempValues = nil
			}

			// merge secret last to be compatible with fleet <= 0.6.0
			if valuesFrom.SecretKeyRef != nil {
				name := valuesFrom.SecretKeyRef.Name
				namespace := valuesFrom.SecretKeyRef.Namespace
				if namespace == "" {
					namespace = defaultNamespace
				}
				key := valuesFrom.SecretKeyRef.Key
				if key == "" {
					key = DefaultKey
				}
				secret := &corev1.Secret{}
				err := h.client.Get(ctx, types.NamespacedName{Namespace: namespace, Name: name}, secret)
				if err != nil {
					return nil, err
				}
				tempValues, err = valuesFromSecret(name, namespace, key, secret)
				if err != nil {
					return nil, err
				}
			}
			if tempValues != nil {
				values = mergeValues(values, tempValues)
			}
		}
	}

	return values, nil
}

func valuesFromSecret(name, namespace, key string, secret *corev1.Secret) (map[string]interface{}, error) {
	var m map[string]interface{}
	if secret == nil {
		return m, nil
	}

	values, ok := secret.Data[key]
	if !ok {
		return nil, fmt.Errorf("key %s is missing from secret %s/%s, can't use it in valuesFrom", key, namespace, name)
	}
	if err := yaml.NewYAMLToJSONDecoder(bytes.NewBuffer(values)).Decode(&m); err != nil {
		return nil, err
	}
	return m, nil
}

func valuesFromConfigMap(name, namespace, key string, configMap *corev1.ConfigMap) (map[string]interface{}, error) {
	var m map[string]interface{}
	if configMap == nil {
		return m, nil
	}

	values, ok := configMap.Data[key]
	if !ok {
		return nil, fmt.Errorf("key %s is missing from configmap %s/%s, can't use it in valuesFrom", key, namespace, name)
	}
	if err := yaml.NewYAMLToJSONDecoder(bytes.NewBufferString(values)).Decode(&m); err != nil {
		return nil, err
	}
	return m, nil
}

func mergeMaps(base, other map[string]string) map[string]string {
	result := map[string]string{}
	for k, v := range base {
		result[k] = v
	}
	for k, v := range other {
		result[k] = v
	}
	return result
}

// mergeValues merges source and destination map, preferring values over maps
// from the source values. This is slightly adapted from:
// https://github.com/helm/helm/blob/2332b480c9cb70a0d8a85247992d6155fbe82416/cmd/helm/install.go#L359
func mergeValues(dest, src map[string]interface{}) map[string]interface{} {
	for k, v := range src {
		// If the key doesn't exist already, then just set the key to that value
		if _, exists := dest[k]; !exists {
			// new key
			dest[k] = v
			continue
		}
		nextMap, ok := v.(map[string]interface{})
		// If it isn't another map, overwrite the value
		if !ok {
			// new key is not a map, overwrite existing key as we prefer values over maps
			dest[k] = v
			continue
		}
		// Edge case: If the key exists in the destination, but isn't a map
		destMap, isMap := dest[k].(map[string]interface{})
		// If the source map has a map for this key, prefer it
		if !isMap {
			dest[k] = v
			continue
		}
		// If we got to this point, it is a map in both, so merge them
		dest[k] = mergeValues(destMap, nextMap)
	}
	return dest
}

// getDryRunConfig determines the dry-run configuration based on whether the chart
// uses the Helm "lookup" function.
// If the chart contains the "lookup" function, DryRunOption is set to "server"
// to allow the lookup function to interact with the Kubernetes API during a dry-run.
// Otherwise, DryRunOption remains empty, implying a client-side dry-run.
func getDryRunConfig(chart *chartv2.Chart, dryRun bool) dryRunConfig {
	cfg := dryRunConfig{DryRun: dryRun}
	if dryRun && hasLookupFunction(chart) {
		cfg.DryRunOption = "server"
	}

	return cfg
}



================================================
FILE: internal/helmdeployer/install_test.go
================================================
package helmdeployer

import (
	"fmt"
	"runtime"
	"testing"
	"time"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"helm.sh/helm/v4/pkg/action"
	"helm.sh/helm/v4/pkg/kube"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

func TestValuesFrom(t *testing.T) {
	a := assert.New(t)
	r := require.New(t)
	key := "values.yaml"
	newline := "\n"
	if runtime.GOOS == "windows" {
		newline = "\r\n"
	}

	configMapPayload := fmt.Sprintf("replication: \"true\"%sreplicas: \"2\"%sserviceType: NodePort", newline, newline)
	secretPayload := fmt.Sprintf("replication: \"false\"%sreplicas: \"3\"%sserviceType: NodePort%sfoo: bar", newline, newline, newline)
	totalValues := map[string]interface{}{"beforeMerge": "value"}
	expected := map[string]interface{}{
		"beforeMerge": "value",
		"replicas":    "2",
		"replication": "true",
		"serviceType": "NodePort",
		"foo":         "bar",
	}

	configMapName := "configmap-name"
	configMapNamespace := "configmap-namespace"
	configMapValues, err := valuesFromConfigMap(configMapName, configMapNamespace, key, &corev1.ConfigMap{
		ObjectMeta: metav1.ObjectMeta{
			Name:      configMapName,
			Namespace: configMapNamespace,
		},
		Data: map[string]string{
			key: configMapPayload,
		},
	})
	r.NoError(err)

	secretName := "secret-name"
	secretNamespace := "secret-namespace"
	secretValues, err := valuesFromSecret(secretName, secretNamespace, key, &corev1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:      secretName,
			Namespace: secretNamespace,
		},
		Data: map[string][]byte{
			key: []byte(secretPayload),
		},
	})
	r.NoError(err)

	totalValues = mergeValues(totalValues, secretValues)
	totalValues = mergeValues(totalValues, configMapValues)
	a.Equal(expected, totalValues)
}

func TestAtomicMapsToRollbackOnFailure(t *testing.T) {
	a := assert.New(t)
	h := &Helm{}

	tests := []struct {
		name     string
		atomic   bool
		expected bool
	}{
		{
			name:     "atomic true maps to RollbackOnFailure true",
			atomic:   true,
			expected: true,
		},
		{
			name:     "atomic false maps to RollbackOnFailure false",
			atomic:   false,
			expected: false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			installAction := &action.Install{}
			h.configureInstallAction(
				installAction,
				&action.Configuration{},
				"test-release",
				"test-namespace",
				time.Duration(0),
				fleet.BundleDeploymentOptions{
					Helm: &fleet.HelmOptions{
						Atomic: tt.atomic,
					},
				},
				nil,
				dryRunConfig{DryRun: false},
			)
			a.Equal(tt.expected, installAction.RollbackOnFailure, "Install: Atomic should map to RollbackOnFailure")

			upgradeAction := &action.Upgrade{}
			h.configureUpgradeAction(
				upgradeAction,
				"test-namespace",
				time.Duration(0),
				fleet.BundleDeploymentOptions{
					Helm: &fleet.HelmOptions{
						Atomic: tt.atomic,
					},
				},
				nil,
				dryRunConfig{DryRun: false},
			)
			a.Equal(tt.expected, upgradeAction.RollbackOnFailure, "Upgrade: Atomic should map to RollbackOnFailure")
		})
	}
}

func TestForceMapsToForceReplace(t *testing.T) {
	a := assert.New(t)
	h := &Helm{}

	tests := []struct {
		name     string
		force    bool
		expected bool
	}{
		{
			name:     "force true maps to ForceReplace true",
			force:    true,
			expected: true,
		},
		{
			name:     "force false maps to ForceReplace false",
			force:    false,
			expected: false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			upgradeAction := &action.Upgrade{}
			h.configureUpgradeAction(
				upgradeAction,
				"test-namespace",
				time.Duration(0),
				fleet.BundleDeploymentOptions{
					Helm: &fleet.HelmOptions{
						Force: tt.force,
					},
				},
				nil,
				dryRunConfig{DryRun: false},
			)
			a.Equal(tt.expected, upgradeAction.ForceReplace, "Force should map to ForceReplace")
		})
	}
}

func TestDryRunStrategyMapping(t *testing.T) {
	a := assert.New(t)

	tests := []struct {
		name            string
		template        bool
		dryRun          bool
		dryRunOption    string
		expectedInstall action.DryRunStrategy
		expectedUpgrade action.DryRunStrategy
		description     string
	}{
		// Normal execution cases
		{
			name:            "no dry run and no template mode",
			template:        false,
			dryRun:          false,
			dryRunOption:    "",
			expectedInstall: action.DryRunNone,
			expectedUpgrade: action.DryRunNone,
			description:     "Normal execution without dry run",
		},
		{
			name:            "client dry run without template mode",
			template:        false,
			dryRun:          true,
			dryRunOption:    "",
			expectedInstall: action.DryRunClient,
			expectedUpgrade: action.DryRunClient,
			description:     "Client-side dry run for validation",
		},
		{
			name:            "server dry run without template mode",
			template:        false,
			dryRun:          true,
			dryRunOption:    "server",
			expectedInstall: action.DryRunServer,
			expectedUpgrade: action.DryRunServer,
			description:     "Server-side dry run to enable lookup functions",
		},
		// Template mode cases (always uses DryRunClient)
		{
			name:            "template mode without dry run",
			template:        true,
			dryRun:          false,
			dryRunOption:    "",
			expectedInstall: action.DryRunClient,
			expectedUpgrade: action.DryRunClient,
			description:     "Template mode always uses DryRunClient to prevent cluster interaction",
		},
		{
			name:            "template mode overrides server dry run",
			template:        true,
			dryRun:          true,
			dryRunOption:    "server",
			expectedInstall: action.DryRunClient,
			expectedUpgrade: action.DryRunClient,
			description:     "Template mode takes precedence over server dry run configuration",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			h := &Helm{
				template: tt.template,
			}

			// Test Install action
			installAction := &action.Install{}
			h.configureInstallAction(
				installAction,
				&action.Configuration{},
				"test-release",
				"test-namespace",
				time.Duration(0),
				fleet.BundleDeploymentOptions{
					Helm: &fleet.HelmOptions{},
				},
				nil,
				dryRunConfig{
					DryRun:       tt.dryRun,
					DryRunOption: tt.dryRunOption,
				},
			)
			a.Equal(tt.expectedInstall, installAction.DryRunStrategy,
				"Install: %s - expected DryRunStrategy=%v, got %v",
				tt.description, tt.expectedInstall, installAction.DryRunStrategy)

			// Test Upgrade action
			upgradeAction := &action.Upgrade{}
			h.configureUpgradeAction(
				upgradeAction,
				"test-namespace",
				time.Duration(0),
				fleet.BundleDeploymentOptions{
					Helm: &fleet.HelmOptions{},
				},
				nil,
				dryRunConfig{
					DryRun:       tt.dryRun,
					DryRunOption: tt.dryRunOption,
				},
			)
			a.Equal(tt.expectedUpgrade, upgradeAction.DryRunStrategy,
				"Upgrade: %s - expected DryRunStrategy=%v, got %v",
				tt.description, tt.expectedUpgrade, upgradeAction.DryRunStrategy)
		})
	}
}

func TestWaitStrategyConfiguration(t *testing.T) {
	a := assert.New(t)
	h := &Helm{}

	tests := []struct {
		name            string
		timeout         time.Duration
		expectedInstall kube.WaitStrategy
		expectedUpgrade kube.WaitStrategy
	}{
		{
			name:            "no timeout uses HookOnlyStrategy",
			timeout:         0,
			expectedInstall: kube.HookOnlyStrategy,
			expectedUpgrade: kube.HookOnlyStrategy,
		},
		{
			name:            "with timeout uses StatusWatcherStrategy",
			timeout:         5 * time.Minute,
			expectedInstall: kube.StatusWatcherStrategy,
			expectedUpgrade: kube.StatusWatcherStrategy,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			installAction := &action.Install{}
			h.configureInstallAction(
				installAction,
				&action.Configuration{},
				"test-release",
				"test-namespace",
				tt.timeout,
				fleet.BundleDeploymentOptions{
					Helm: &fleet.HelmOptions{},
				},
				nil,
				dryRunConfig{DryRun: false},
			)
			a.Equal(tt.expectedInstall, installAction.WaitStrategy, "Install: WaitStrategy should be set based on timeout")

			upgradeAction := &action.Upgrade{}
			h.configureUpgradeAction(
				upgradeAction,
				"test-namespace",
				tt.timeout,
				fleet.BundleDeploymentOptions{
					Helm: &fleet.HelmOptions{},
				},
				nil,
				dryRunConfig{DryRun: false},
			)
			a.Equal(tt.expectedUpgrade, upgradeAction.WaitStrategy, "Upgrade: WaitStrategy should be set based on timeout")
		})
	}
}

func TestServerSideApplyConfiguration(t *testing.T) {
	a := assert.New(t)
	h := &Helm{}

	t.Run("Install with TakeOwnership disables ServerSideApply", func(t *testing.T) {
		installAction := &action.Install{}
		h.configureInstallAction(
			installAction,
			&action.Configuration{},
			"test-release",
			"test-namespace",
			time.Duration(0),
			fleet.BundleDeploymentOptions{
				Helm: &fleet.HelmOptions{
					TakeOwnership: true,
				},
			},
			nil,
			dryRunConfig{DryRun: false},
		)
		a.False(installAction.ServerSideApply, "ServerSideApply should be false when TakeOwnership is true")
	})

	t.Run("Install without TakeOwnership keeps ServerSideApply default", func(t *testing.T) {
		installAction := &action.Install{}
		h.configureInstallAction(
			installAction,
			&action.Configuration{},
			"test-release",
			"test-namespace",
			time.Duration(0),
			fleet.BundleDeploymentOptions{
				Helm: &fleet.HelmOptions{
					TakeOwnership: false,
				},
			},
			nil,
			dryRunConfig{DryRun: false},
		)
		// When TakeOwnership is false, ServerSideApply remains at its zero value (false)
		// but it's not explicitly set, so Helm would use its default behavior
		a.False(installAction.ServerSideApply)
	})

	t.Run("Upgrade uses auto mode for ServerSideApply", func(t *testing.T) {
		upgradeAction := &action.Upgrade{}
		h.configureUpgradeAction(
			upgradeAction,
			"test-namespace",
			time.Duration(0),
			fleet.BundleDeploymentOptions{
				Helm: &fleet.HelmOptions{},
			},
			nil,
			dryRunConfig{DryRun: false},
		)
		a.Equal("auto", upgradeAction.ServerSideApply, "Upgrade should use 'auto' mode for ServerSideApply")
	})
}

func TestCorrectDriftForceOption(t *testing.T) {
	a := assert.New(t)
	h := &Helm{}

	t.Run("CorrectDrift.Force is combined with Helm.Force", func(t *testing.T) {
		tests := []struct {
			name          string
			helmForce     bool
			driftForce    bool
			expectedForce bool
		}{
			{
				name:          "both false",
				helmForce:     false,
				driftForce:    false,
				expectedForce: false,
			},
			{
				name:          "helm true, drift false",
				helmForce:     true,
				driftForce:    false,
				expectedForce: true,
			},
			{
				name:          "helm false, drift true",
				helmForce:     false,
				driftForce:    true,
				expectedForce: true,
			},
			{
				name:          "both true",
				helmForce:     true,
				driftForce:    true,
				expectedForce: true,
			},
		}

		for _, tt := range tests {
			t.Run(tt.name, func(t *testing.T) {
				upgradeAction := &action.Upgrade{}
				h.configureUpgradeAction(
					upgradeAction,
					"test-namespace",
					time.Duration(0),
					fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							Force: tt.helmForce,
						},
						CorrectDrift: &fleet.CorrectDrift{
							Force: tt.driftForce,
						},
					},
					nil,
					dryRunConfig{DryRun: false},
				)
				a.Equal(tt.expectedForce, upgradeAction.ForceReplace, "ForceReplace should combine Helm.Force and CorrectDrift.Force")
			})
		}
	})
}



================================================
FILE: internal/helmdeployer/list.go
================================================
package helmdeployer

import (
	"strconv"

	"helm.sh/helm/v4/pkg/action"
	releasev1 "helm.sh/helm/v4/pkg/release/v1"
)

type ListAction interface {
	Run() ([]*releasev1.Release, error)
}

type listWrapper struct {
	list *action.List
}

func (lw *listWrapper) Run() ([]*releasev1.Release, error) {
	releasers, err := lw.list.Run()
	if err != nil {
		return nil, err
	}

	return releaseListToV1List(releasers)
}

func (h *Helm) NewListAction() ListAction {
	list := action.NewList(h.globalCfg)
	list.All = true
	return &listWrapper{list: list}
}

// ListDeployments returns a list of deployedBundles by listing all helm releases via
// helm's storage driver (secrets)
// It only returns deployedBundles for helm releases which have the
// "fleet.cattle.io/bundle-id" annotation.
func (h *Helm) ListDeployments(list ListAction) ([]DeployedBundle, error) {
	releases, err := list.Run()
	if err != nil {
		return nil, err
	}

	var (
		result []DeployedBundle
	)

	for _, release := range releases {
		// skip releases that don't have the bundleID annotation
		d := release.Chart.Metadata.Annotations[BundleIDAnnotation]
		if d == "" {
			continue
		}
		ns := release.Chart.Metadata.Annotations[AgentNamespaceAnnotation]
		// skip releases that don't have the agentNamespace annotation
		if ns == "" {
			continue
		}
		// skip releases from other agents
		if ns != h.agentNamespace {
			continue
		}
		// ignore error as keepResources should be false if annotation not found
		keepResources, _ := strconv.ParseBool(release.Chart.Metadata.Annotations[KeepResourcesAnnotation])
		result = append(result, DeployedBundle{
			BundleID:      d,
			ReleaseName:   release.Namespace + "/" + release.Name,
			KeepResources: keepResources,
		})
	}

	return result, nil
}



================================================
FILE: internal/helmdeployer/list_test.go
================================================
package helmdeployer_test

import (
	"testing"

	"github.com/rancher/fleet/internal/helmdeployer"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	chartv2 "helm.sh/helm/v4/pkg/chart/v2"
	releasev1 "helm.sh/helm/v4/pkg/release/v1"
)

type fakeList struct {
	releases []*releasev1.Release
}

func (f fakeList) Run() ([]*releasev1.Release, error) {
	return f.releases, nil
}

func newRelease(name string, namespace string, annotations map[string]string) *releasev1.Release {
	return &releasev1.Release{
		Name:      name,
		Namespace: namespace,
		Chart: &chartv2.Chart{
			Metadata: &chartv2.Metadata{
				Annotations: annotations,
			},
		},
	}
}

func TestListDeployments(t *testing.T) {
	r := assert.New(t)

	const (
		bundleIDAnnotation = "fleet.cattle.io/bundle-id"
		agentNSAnnotation  = "fleet.cattle.io/agent-namespace"
	)

	h := helmdeployer.New("cattle-fleet-test", "", "", "")

	tests := map[string]struct {
		releases             []*releasev1.Release
		expectedBundleIDs    []string
		expectedReleaseNames []string
	}{
		"no chart has fleet annotations": {
			releases: []*releasev1.Release{
				newRelease("test0", "any", map[string]string{}),
				newRelease("test1", "any", map[string]string{
					bundleIDAnnotation: "any",
					agentNSAnnotation:  "any",
				}),
			},
			expectedBundleIDs:    []string{},
			expectedReleaseNames: []string{},
		},
		"finds charts with fleet annotations": {
			releases: []*releasev1.Release{
				newRelease("test1", "any", nil),
				newRelease("test2", "namespace", map[string]string{
					bundleIDAnnotation: "testID",
					agentNSAnnotation:  "cattle-fleet-test",
				}),
				newRelease("test3", "cattle-fleet-namespace", map[string]string{
					bundleIDAnnotation: "test3-id",
					agentNSAnnotation:  "cattle-fleet-test",
				}),
			},
			expectedBundleIDs:    []string{"testID", "test3-id"},
			expectedReleaseNames: []string{"namespace/test2", "cattle-fleet-namespace/test3"},
		},
		"only finds own charts": {
			releases: []*releasev1.Release{
				newRelease("test2", "cattle-fleet-test", map[string]string{
					bundleIDAnnotation: "any",
					agentNSAnnotation:  "cattle-fleet-SYSTEM",
				}),
			},
			expectedBundleIDs:    []string{},
			expectedReleaseNames: []string{},
		},
	}

	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			listAction := &fakeList{releases: test.releases}
			result, err := h.ListDeployments(listAction)
			require.NoError(t, err)

			r.Len(result, len(test.expectedBundleIDs))
			for _, deployedBundle := range result {
				r.Contains(test.expectedBundleIDs, deployedBundle.BundleID)
				r.Contains(test.expectedReleaseNames, deployedBundle.ReleaseName)
			}
		})
	}
}



================================================
FILE: internal/helmdeployer/lookup.go
================================================
package helmdeployer

import (
	"reflect"
	"strings"
	"text/template"
	"text/template/parse"

	chartv2 "helm.sh/helm/v4/pkg/chart/v2"
)

// hasLookupFunction checks if any template in the given Helm chart
// calls the "lookup" function. It parses the templates to ensure it's a function
// call and not just the word "lookup" in text or comments.
func hasLookupFunction(ch *chartv2.Chart) bool {
	for _, tpl := range ch.Templates {
		// Parse the template into an AST.
		t, err := template.New(
			tpl.Name,
		).Option(
			"missingkey=zero",
		).Funcs(
			map[string]interface{}{"lookup": func() error { return nil }},
		).Parse(string(tpl.Data))
		if err != nil {
			// Some templates might not parse correctly if they depend on values
			// that aren't available. We can safely ignore these errors and continue,
			// as a parse error means we couldn't definitively find a valid 'lookup' call.
			continue
		}

		// Walk all parse trees in this template and look for lookup invocations.
		if t.Tree != nil && t.Root != nil {
			if containsLookup(t.Root) {
				return true
			}
		}
	}

	return false
}

// containsLookup recursively checks whether a parse.Node (and its children)
// contains a call to the "lookup" function.
func containsLookup(node parse.Node) bool { //nolint:gocyclo // recursive logic
	if nodeIsNil(node) {
		return false
	}

	// Quick textual pre-check. If the node's string representation does not
	// contain the word "lookup", there's no need to traverse it deeply.
	// This avoids unnecessary recursion for nodes that clearly don't reference
	// the lookup function. If the textual representation contains
	// "lookup", fall through to the detailed inspection below.
	if !nodeExprContainsLookup(node) {
		return false
	}

	switch node.Type() {
	case parse.NodeAction:
		if n, ok := node.(*parse.ActionNode); ok && n != nil {
			return containsLookup(n.Pipe)
		}
		return false
	case parse.NodeIf:
		if n, ok := node.(*parse.IfNode); ok && n != nil {
			// check if any of the sub-nodes contain lookup
			return containsLookup(n.ElseList) || containsLookup(n.Pipe) || containsLookup(n.List)
		}
		return false
	case parse.NodeList:
		if n, ok := node.(*parse.ListNode); ok && n != nil {
			for _, subNode := range n.Nodes {
				if containsLookup(subNode) {
					return true
				}
			}
		}
		return false
	case parse.NodeRange:
		if n, ok := node.(*parse.RangeNode); ok && n != nil {
			// check if any of the sub-nodes contain lookup
			return containsLookup(n.ElseList) || containsLookup(n.Pipe) || containsLookup(n.List)
		}
		return false
	case parse.NodeTemplate:
		if n, ok := node.(*parse.TemplateNode); ok && n != nil {
			return containsLookup(n.Pipe)
		}
		return false
	case parse.NodeWith:
		if n, ok := node.(*parse.WithNode); ok && n != nil {
			// check if any of the sub-nodes contain lookup
			return containsLookup(n.Pipe) || containsLookup(n.List) || containsLookup(n.ElseList)
		}
		return false
	case parse.NodePipe:
		if n, ok := node.(*parse.PipeNode); ok && n != nil {
			for _, cmd := range n.Cmds {
				if containsLookup(cmd) {
					return true
				}
			}
		}
		return false
	case parse.NodeCommand:
		if n, ok := node.(*parse.CommandNode); ok && n != nil {
			for i, arg := range n.Args {
				// The first argument of a command node is usually the function name.
				if i == 0 {
					ident, ok := arg.(*parse.IdentifierNode)
					if ok && ident != nil && ident.Ident == "lookup" {
						return true
					}
				}
				// Recurse into arguments to find nested lookups, e.g., {{ template "foo" (lookup ...) }}
				if containsLookup(arg) {
					return true
				}
			}
		}
		return false
	case parse.NodeChain:
		if n, ok := node.(*parse.ChainNode); ok && n != nil {
			// Covers cases like (lookup ...).items where the lookup is part of a chained expression.
			if n.Node != nil {
				return containsLookup(n.Node)
			}
		}
		return false
	default:
		return false
	}
}

// nodeExprContainsLookup returns true when the node has a textual
// expression (via String()) and that textual representation contains the
// substring "lookup". This is a cheap pre-check to avoid deep traversal for
// nodes that don't reference the lookup function at all.
func nodeExprContainsLookup(node parse.Node) bool {
	if nodeIsNil(node) {
		return false
	}
	s := node.String()

	return strings.Contains(s, "lookup")
}

func nodeIsNil(node parse.Node) bool {
	if node == nil {
		return true
	}
	rv := reflect.ValueOf(node)
	if rv.Kind() == reflect.Ptr && rv.IsNil() {
		return true
	}
	return false
}



================================================
FILE: internal/helmdeployer/lookup_test.go
================================================
package helmdeployer

import (
	"testing"

	"github.com/stretchr/testify/assert"
	"helm.sh/helm/v4/pkg/chart/common"
	chartv2 "helm.sh/helm/v4/pkg/chart/v2"
)

func TestHasLookupFunction(t *testing.T) {
	testCases := []struct {
		name           string
		templates      []*common.File
		expectedResult bool
	}{
		{
			name:           "Chart with no templates",
			templates:      []*common.File{},
			expectedResult: false,
		},
		{
			name: "Template without lookup",
			templates: []*common.File{
				{Name: "templates/deployment.yaml", Data: []byte(`apiVersion: apps/v1\nkind: Deployment`)},
			},
			expectedResult: false,
		},
		{
			name: "Template with simple lookup",
			templates: []*common.File{
				{Name: "templates/service.yaml", Data: []byte(`{{ lookup "v1" "Service" "default" "kubernetes" }}`)},
			},
			expectedResult: true,
		},
		{
			name: "Template with 'lookup' as text",
			templates: []*common.File{
				{Name: "templates/configmap.yaml", Data: []byte(`data:\n  key: "some lookup value"`)},
			},
			expectedResult: false,
		},
		{
			name: "Template with 'lookup' in a comment",
			templates: []*common.File{
				{Name: "templates/configmap.yaml", Data: []byte(`{{- /* This is a lookup function */ -}}`)},
			},
			expectedResult: false,
		},
		{
			name: "Template with lookup in an 'if' block",
			templates: []*common.File{
				{Name: "templates/if.yaml", Data: []byte(`{{ if .Values.enabled }}{{ lookup "v1" "Pod" "default" "mypod" }}{{ end }}`)},
			},
			expectedResult: true,
		},
		{
			name: "Template with lookup in an 'if' condition",
			templates: []*common.File{
				{Name: "templates/if.yaml", Data: []byte(`{{ if lookup "v1" "ConfigMap" "default" "my-cm" }}found{{ end }}`)},
			},
			expectedResult: true,
		},
		{
			name: "Template with lookup in a 'range' block",
			templates: []*common.File{
				{Name: "templates/range.yaml", Data: []byte(`{{ range .Values.items }}{{ lookup "v1" "Secret" .Release.Namespace .Name }}{{ end }}`)},
			},
			expectedResult: true,
		},
		{
			name: "Template with lookup in a 'with' block",
			templates: []*common.File{
				{Name: "templates/with.yaml", Data: []byte(`{{ with .Values.service }}{{ lookup "v1" "Service" .Namespace .Name }}{{ end }}`)},
			},
			expectedResult: true,
		},
		{
			name: "Template with nested lookup",
			templates: []*common.File{
				{Name: "templates/nested.yaml", Data: []byte(`{{ $cm := lookup "v1" "ConfigMap" "default" "my-cm" }}{{ if $cm }}{{ lookup "v1" "Secret" "default" "my-secret" }}{{ end }}`)},
			},
			expectedResult: true,
		},
		{
			name: "Template with lookup used in chained expression",
			templates: []*common.File{
				{Name: "templates/chained.yaml", Data: []byte(`{{- range $index, $pod := (lookup "v1" "Pod" "kube-system" "").items }}- {{ $pod.metadata.name }}{{- end }}`)},
			},
			expectedResult: true,
		},
		{
			name: "Template with invalid syntax (should be ignored)",
			templates: []*common.File{
				{Name: "templates/invalid.yaml", Data: []byte(`{{ .Values.name }`)},
				{Name: "templates/valid.yaml", Data: []byte(`data: "valid"`)},
			},
			expectedResult: false,
		},
		{
			name: "Template with invalid syntax and another with lookup",
			templates: []*common.File{
				{Name: "templates/invalid.yaml", Data: []byte(`{{ .Values.name }`)},
				{Name: "templates/valid_with_lookup.yaml", Data: []byte(`{{ lookup "v1" "Service" "default" "kubernetes" }}`)},
			},
			expectedResult: true,
		},
		{
			name: "Template with lookup as an argument",
			templates: []*common.File{
				{Name: "templates/arg.yaml", Data: []byte(`{{- define "myTpl" }}{{ . }}{{ end -}}{{ template "myTpl" (lookup "v1" "Pod" "default" "my-pod") }}`)},
			},
			expectedResult: true,
		},
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			assert := assert.New(t)

			chart := &chartv2.Chart{
				Metadata: &chartv2.Metadata{
					Name:    "test-chart",
					Version: "0.1.0",
				},
				Templates: tc.templates,
			}

			assert.Equal(tc.expectedResult, hasLookupFunction(chart))
		})
	}
}



================================================
FILE: internal/helmdeployer/postrender.go
================================================
package helmdeployer

import (
	"bytes"
	"fmt"
	"strings"

	"helm.sh/helm/v4/pkg/kube"

	chartv2 "helm.sh/helm/v4/pkg/chart/v2"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/desiredset"
	"github.com/rancher/fleet/internal/helmdeployer/kustomize"
	"github.com/rancher/fleet/internal/helmdeployer/rawyaml"
	"github.com/rancher/fleet/internal/manifest"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"github.com/rancher/wrangler/v3/pkg/yaml"

	"k8s.io/apimachinery/pkg/api/meta"
)

const CRDKind = "CustomResourceDefinition"

type postRender struct {
	labelPrefix string
	labelSuffix string
	bundleID    string
	manifest    *manifest.Manifest
	chart       *chartv2.Chart
	mapper      meta.RESTMapper
	opts        fleet.BundleDeploymentOptions
}

func (p *postRender) Run(renderedManifests *bytes.Buffer) (modifiedManifests *bytes.Buffer, err error) {
	data := renderedManifests.Bytes()

	objs, err := yaml.ToObjects(bytes.NewBuffer(data))
	if err != nil {
		return nil, err
	}

	if len(objs) == 0 {
		data = nil
	}

	// Kustomize applies some restrictions fleet does not have, like a regular expression, which checks for valid file
	// names. If no instructions for kustomize are found in the manifests, then kustomize shouldn't be called at all
	// to prevent causing issues with these restrictions.
	kustomizable := false
	for _, resource := range p.manifest.Resources {
		if strings.HasSuffix(resource.Name, "kustomization.yaml") ||
			strings.HasSuffix(resource.Name, "kustomization.yml") ||
			strings.HasSuffix(resource.Name, "Kustomization") {
			kustomizable = true
			break
		}
	}
	if kustomizable {
		newObjs, processed, err := kustomize.Process(p.manifest, data, p.opts.Kustomize.Dir)
		if err != nil {
			return nil, err
		}
		if processed {
			objs = newObjs
		}
	}

	yamlObjs, err := rawyaml.ToObjects(p.chart)
	if err != nil {
		return nil, err
	}
	objs = append(objs, yamlObjs...)

	setID := desiredset.GetSetID(p.bundleID, p.labelPrefix, p.labelSuffix)
	labels, annotations, err := desiredset.GetLabelsAndAnnotations(setID)
	if err != nil {
		return nil, err
	}

	for _, obj := range objs {
		m, err := meta.Accessor(obj)
		if err != nil {
			return nil, err
		}
		objAnnotations := mergeMaps(m.GetAnnotations(), annotations)
		if !p.opts.DeleteCRDResources &&
			obj.GetObjectKind().GroupVersionKind().Kind == CRDKind {
			objAnnotations[kube.ResourcePolicyAnno] = kube.KeepPolicy
		}
		m.SetLabels(mergeMaps(m.GetLabels(), labels))
		m.SetAnnotations(objAnnotations)

		if p.opts.TargetNamespace != "" {
			if p.mapper != nil {
				gvk := obj.GetObjectKind().GroupVersionKind()
				mapping, err := p.mapper.RESTMapping(gvk.GroupKind(), gvk.Version)
				if err != nil {
					return nil, err
				}
				if mapping.Scope.Name() == meta.RESTScopeNameRoot {
					apiVersion, kind := gvk.ToAPIVersionAndKind()
					return nil, fmt.Errorf("invalid cluster scoped object [name=%s kind=%v apiVersion=%s] found. "+
						"Your config uses targetNamespace or namespace and thus forbids cluster-scoped resources. "+
						"If you do not intend to disallow cluster scoped resources, you could switch to defaultNamespace",
						m.GetName(),
						kind, apiVersion)
				}
			}
			m.SetNamespace(p.opts.TargetNamespace)
		}
	}

	data, err = yaml.ToBytes(objs)
	return bytes.NewBuffer(data), err
}



================================================
FILE: internal/helmdeployer/postrender_test.go
================================================
package helmdeployer

import (
	"bytes"
	"testing"

	"github.com/rancher/fleet/internal/manifest"
	"github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/wrangler/v3/pkg/yaml"

	"github.com/google/go-cmp/cmp"
	chartv2 "helm.sh/helm/v4/pkg/chart/v2"
	"helm.sh/helm/v4/pkg/kube"
	corev1 "k8s.io/api/core/v1"
	apiextensionsv1 "k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1"
	"k8s.io/apimachinery/pkg/api/meta"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	kruntime "k8s.io/apimachinery/pkg/runtime"
)

func TestPostRenderer_Run_DeleteCRDs(t *testing.T) {
	tests := map[string]struct {
		obj                 kruntime.Object
		opts                v1alpha1.BundleDeploymentOptions
		expectedAnnotations map[string]string
	}{
		"default (no DeleteCRDResources specified)": {
			obj: &apiextensionsv1.CustomResourceDefinition{
				TypeMeta: metav1.TypeMeta{
					Kind:       CRDKind,
					APIVersion: "apiextensions.k8s.io/v1",
				},
			},
			opts: v1alpha1.BundleDeploymentOptions{},
			expectedAnnotations: map[string]string{
				kube.ResourcePolicyAnno:      kube.KeepPolicy,
				"objectset.rio.cattle.io/id": "-",
			},
		},
		"DeleteCRDResources set to true": {
			obj: &apiextensionsv1.CustomResourceDefinition{
				TypeMeta: metav1.TypeMeta{
					Kind:       CRDKind,
					APIVersion: "apiextensions.k8s.io/v1",
				},
			},
			opts: v1alpha1.BundleDeploymentOptions{
				DeleteCRDResources: true,
			},
			expectedAnnotations: map[string]string{
				"objectset.rio.cattle.io/id": "-",
			},
		},
		"DeleteCRDResources set to false": {
			obj: &apiextensionsv1.CustomResourceDefinition{
				TypeMeta: metav1.TypeMeta{
					Kind:       CRDKind,
					APIVersion: "apiextensions.k8s.io/v1",
				},
			},
			opts: v1alpha1.BundleDeploymentOptions{
				DeleteCRDResources: false,
			},
			expectedAnnotations: map[string]string{
				kube.ResourcePolicyAnno:      kube.KeepPolicy,
				"objectset.rio.cattle.io/id": "-",
			},
		},
		"Annotation not added for non CRDs resources": {
			obj: &corev1.Pod{
				TypeMeta: metav1.TypeMeta{
					Kind: "Pod",
				},
			},
			opts: v1alpha1.BundleDeploymentOptions{
				DeleteCRDResources: false,
			},
			expectedAnnotations: map[string]string{
				"objectset.rio.cattle.io/id": "-",
			},
		},
	}

	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			data, err := yaml.ToBytes([]kruntime.Object{test.obj})
			if err != nil {
				t.Errorf("unexpected error %v", err)
			}
			renderedManifests := bytes.NewBuffer(data)

			pr := postRender{
				manifest: &manifest.Manifest{
					Resources: []v1alpha1.BundleResource{},
				},
				chart: &chartv2.Chart{},
				opts:  test.opts,
			}
			postRenderedManifests, err := pr.Run(renderedManifests)
			if err != nil {
				t.Errorf("unexpected error %v", err)
			}

			data = postRenderedManifests.Bytes()
			objs, err := yaml.ToObjects(bytes.NewBuffer(data))
			if err != nil {
				t.Errorf("unexpected error %v", err)
			}

			m, err := meta.Accessor(objs[0])
			if err != nil {
				t.Errorf("unexpected error %v", err)
			}
			if !cmp.Equal(m.GetAnnotations(), test.expectedAnnotations) {
				t.Errorf("expected %s, got %s", test.expectedAnnotations, m.GetAnnotations())
			}
		})
	}

	t.Run("Multiple resources, only add to CRDs", func(t *testing.T) {
		crd := &apiextensionsv1.CustomResourceDefinition{
			TypeMeta: metav1.TypeMeta{
				Kind:       CRDKind,
				APIVersion: "apiextensions.k8s.io/v1",
			},
		}
		pod := &corev1.Pod{
			TypeMeta: metav1.TypeMeta{
				Kind: "Pod",
			},
		}

		data, err := yaml.ToBytes([]kruntime.Object{crd, pod})
		if err != nil {
			t.Errorf("unexpected error %v", err)
		}
		renderedManifests := bytes.NewBuffer(data)

		pr := postRender{
			manifest: &manifest.Manifest{
				Resources: []v1alpha1.BundleResource{},
			},
			chart: &chartv2.Chart{},
			opts: v1alpha1.BundleDeploymentOptions{
				DeleteCRDResources: false,
			},
		}
		postRenderedManifests, err := pr.Run(renderedManifests)
		if err != nil {
			t.Errorf("unexpected error %v", err)
		}

		data = postRenderedManifests.Bytes()
		objs, err := yaml.ToObjects(bytes.NewBuffer(data))
		if err != nil {
			t.Errorf("unexpected error %v", err)
		}

		for _, obj := range objs {
			m, err := meta.Accessor(obj)
			if err != nil {
				t.Errorf("unexpected error %v", err)
			}

			annotations := m.GetAnnotations()
			kind := obj.GetObjectKind().GroupVersionKind().Kind
			if kind == CRDKind {
				if val, ok := annotations[kube.ResourcePolicyAnno]; !ok || val != kube.KeepPolicy {
					t.Errorf("expected %s, got %s", kube.KeepPolicy, annotations[kube.ResourcePolicyAnno])
				}
			} else {
				if val, ok := annotations[kube.ResourcePolicyAnno]; ok {
					t.Errorf("unexpected annotation on %s, got %s: %s", kind, kube.ResourcePolicyAnno, val)
				}
			}
		}
	})

}



================================================
FILE: internal/helmdeployer/release_conversion.go
================================================
package helmdeployer

import (
	"fmt"

	"helm.sh/helm/v4/pkg/release"
	releasev1 "helm.sh/helm/v4/pkg/release/v1"
	"helm.sh/helm/v4/pkg/storage"
)

// releaserToV1Release converts a release.Releaser interface to a concrete v1.Release type.
// This follows the same pattern used in Helm v4's own codebase for handling versioned releases.
func releaserToV1Release(rel release.Releaser) (*releasev1.Release, error) {
	switch r := rel.(type) {
	case releasev1.Release:
		return &r, nil
	case *releasev1.Release:
		return r, nil
	case nil:
		return nil, nil
	default:
		return nil, fmt.Errorf("unsupported release type: %T", rel)
	}
}

// releaseListToV1List converts a slice of release.Releaser interfaces to v1.Release pointers.
func releaseListToV1List(ls []release.Releaser) ([]*releasev1.Release, error) {
	rls := make([]*releasev1.Release, 0, len(ls))
	for _, val := range ls {
		rel, err := releaserToV1Release(val)
		if err != nil {
			return nil, err
		}
		rls = append(rls, rel)
	}
	return rls, nil
}

// listReleases queries storage with a filter function and returns v1.Release pointers.
func listReleases(store *storage.Storage, filter func(*releasev1.Release) bool) ([]*releasev1.Release, error) {
	releasers, err := store.List(func(r release.Releaser) bool {
		if v1Rel, ok := r.(*releasev1.Release); ok {
			return filter(v1Rel)
		}
		return false
	})
	if err != nil {
		return nil, err
	}

	return releaseListToV1List(releasers)
}

// getReleaseHistory retrieves the history for a release name.
func getReleaseHistory(store *storage.Storage, name string) ([]*releasev1.Release, error) {
	releasers, err := store.History(name)
	if err != nil {
		return nil, err
	}

	return releaseListToV1List(releasers)
}

// getLastRelease retrieves the most recent release for a name.
func getLastRelease(store *storage.Storage, name string) (*releasev1.Release, error) {
	releaser, err := store.Last(name)
	if err != nil {
		return nil, err
	}

	return releaserToV1Release(releaser)
}



================================================
FILE: internal/helmdeployer/release_conversion_test.go
================================================
package helmdeployer

import (
	"testing"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"helm.sh/helm/v4/pkg/release"
	releasev1 "helm.sh/helm/v4/pkg/release/v1"
)

func TestReleaserToV1Release(t *testing.T) {
	tests := []struct {
		name        string
		input       release.Releaser
		wantErr     bool
		expectedNil bool
	}{
		{
			name: "converts pointer to v1.Release successfully",
			input: &releasev1.Release{
				Name:    "test-release",
				Version: 1,
			},
			wantErr:     false,
			expectedNil: false,
		},
		{
			name: "converts value v1.Release successfully",
			input: releasev1.Release{
				Name:    "test-release-value",
				Version: 2,
			},
			wantErr:     false,
			expectedNil: false,
		},
		{
			name:        "handles nil input",
			input:       nil,
			wantErr:     false,
			expectedNil: true,
		},
		{
			name:    "returns error for unsupported type",
			input:   "not-a-release",
			wantErr: true,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			result, err := releaserToV1Release(tt.input)

			if tt.wantErr {
				require.Error(t, err)
				assert.Nil(t, result)
				assert.Contains(t, err.Error(), "unsupported release type")
				return
			}

			require.NoError(t, err)

			if tt.expectedNil {
				assert.Nil(t, result)
			} else {
				require.NotNil(t, result)
				// Verify the conversion preserved the data
				switch r := tt.input.(type) {
				case *releasev1.Release:
					assert.Equal(t, r.Name, result.Name)
					assert.Equal(t, r.Version, result.Version)
				case releasev1.Release:
					assert.Equal(t, r.Name, result.Name)
					assert.Equal(t, r.Version, result.Version)
				}
			}
		})
	}
}

func TestReleaseListToV1List(t *testing.T) {
	tests := []struct {
		name    string
		input   []release.Releaser
		wantErr bool
		wantLen int
	}{
		{
			name: "converts list successfully",
			input: []release.Releaser{
				&releasev1.Release{Name: "release-1", Version: 1},
				&releasev1.Release{Name: "release-2", Version: 2},
				&releasev1.Release{Name: "release-3", Version: 3},
			},
			wantErr: false,
			wantLen: 3,
		},
		{
			name:    "handles empty list",
			input:   []release.Releaser{},
			wantErr: false,
			wantLen: 0,
		},
		{
			name: "handles nil in list",
			input: []release.Releaser{
				&releasev1.Release{Name: "release-1", Version: 1},
				nil,
				&releasev1.Release{Name: "release-2", Version: 2},
			},
			wantErr: false,
			wantLen: 3,
		},
		{
			name: "returns error for unsupported type in list",
			input: []release.Releaser{
				&releasev1.Release{Name: "release-1", Version: 1},
				"invalid-type",
			},
			wantErr: true,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			result, err := releaseListToV1List(tt.input)

			if tt.wantErr {
				require.Error(t, err)
				assert.Nil(t, result)
				return
			}

			require.NoError(t, err)
			assert.Len(t, result, tt.wantLen)

			// Verify data preservation for non-nil releases
			for i, r := range tt.input {
				if r != nil {
					if rel, ok := r.(*releasev1.Release); ok {
						assert.Equal(t, rel.Name, result[i].Name)
						assert.Equal(t, rel.Version, result[i].Version)
					}
				}
			}
		})
	}
}

// Note: Tests for listReleases, getReleaseHistory, and getLastRelease are covered
// by the existing tests in delete.go and other helmdeployer tests, as they require
// full release initialization with charts, metadata, and info objects.



================================================
FILE: internal/helmdeployer/rollback.go
================================================
package helmdeployer

import (
	"context"
	"strings"

	"github.com/pkg/errors"
	"helm.sh/helm/v4/pkg/action"
	"helm.sh/helm/v4/pkg/kube"
	releasecommon "helm.sh/helm/v4/pkg/release/common"
	releasev1 "helm.sh/helm/v4/pkg/release/v1"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"sigs.k8s.io/controller-runtime/pkg/log"
)

// RemoveExternalChanges removes changes made outside of fleet using Helm rollback.
func (h *Helm) RemoveExternalChanges(ctx context.Context, bd *fleet.BundleDeployment) (string, error) {
	log.FromContext(ctx).WithName("remove-external-changes").Info("Drift correction: rollback")

	_, defaultNamespace, releaseName := h.getOpts(bd.Name, bd.Spec.Options)
	cfg, err := h.getCfg(ctx, defaultNamespace, bd.Spec.Options.ServiceAccount)
	if err != nil {
		return "", err
	}
	currentRelease, err := getLastRelease(cfg.Releases, releaseName)
	if err != nil {
		return "", err
	}

	r := action.NewRollback(cfg)
	// When using ForceReplace, must disable ServerSideApply
	// ForceReplace and ServerSideApply cannot be used together in Helm v4
	// Set to "false" (not "auto" or empty string) to explicitly disable server-side apply
	if bd.Spec.CorrectDrift.Force {
		r.ServerSideApply = "false"
		r.ForceReplace = true
	} else {
		// For non-force mode, use "auto" to preserve the apply method from the original release
		r.ServerSideApply = "auto"
	}
	// WaitStrategy must be set in Helm v4 to avoid "unknown wait strategy" error
	// HookOnlyStrategy is the default behavior (equivalent to not waiting)
	r.WaitStrategy = kube.HookOnlyStrategy
	r.Version = currentRelease.Version
	r.MaxHistory = bd.Spec.Options.Helm.MaxHistory
	if r.MaxHistory == 0 {
		r.MaxHistory = MaxHelmHistory
	}
	if err := r.Run(releaseName); err != nil {
		if bd.Spec.CorrectDrift.KeepFailHistory {
			return "", err
		}
		return "", removeFailedRollback(cfg, currentRelease, err)
	}
	release, err := getLastRelease(cfg.Releases, releaseName)
	if err != nil {
		return "", err
	}
	return ReleaseToResourceID(release), nil
}

func removeFailedRollback(cfg *action.Configuration, currentRelease *releasev1.Release, err error) error {
	failedRelease, errRel := getLastRelease(cfg.Releases, currentRelease.Name)
	if errRel != nil {
		return errors.Wrap(err, errRel.Error())
	}
	if failedRelease.Version == currentRelease.Version+1 &&
		failedRelease.Info.Status == releasecommon.StatusFailed &&
		strings.HasPrefix(failedRelease.Info.Description, "Rollback") {
		_, errDel := cfg.Releases.Delete(failedRelease.Name, failedRelease.Version)
		if errDel != nil {
			return errors.Wrap(err, errDel.Error())
		}
		errUpdate := cfg.Releases.Update(currentRelease)
		if errUpdate != nil {
			return errors.Wrap(err, errUpdate.Error())
		}
	}

	return err
}



================================================
FILE: internal/helmdeployer/template.go
================================================
package helmdeployer

import (
	"context"
	"fmt"
	"io"

	"github.com/Masterminds/semver/v3"
	"github.com/rancher/fleet/internal/manifest"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"helm.sh/helm/v4/pkg/action"
	"helm.sh/helm/v4/pkg/chart/common"
	kubefake "helm.sh/helm/v4/pkg/kube/fake"
	releasev1 "helm.sh/helm/v4/pkg/release/v1"
	"helm.sh/helm/v4/pkg/storage"
	"helm.sh/helm/v4/pkg/storage/driver"
)

var (
	defaultKubernetesVersion = "v1.25.0"
)

// Template runs helm template and returns the resources as a list of objects, without applying them.
func Template(ctx context.Context, bundleID string, manifest *manifest.Manifest, options fleet.BundleDeploymentOptions, kubeVersionString string) (*releasev1.Release, error) {
	h := &Helm{
		globalCfg:    &action.Configuration{},
		useGlobalCfg: true,
		template:     true,
	}

	mem := driver.NewMemory()
	mem.SetNamespace("default")
	// Template operations use a discard logger since they don't interact with a real cluster
	mem.SetLogger(nil) // nil sets discard handler in Helm v4
	kubeVersionToUse := defaultKubernetesVersion
	if kubeVersionString != "" {
		kubeVersionToUse = kubeVersionString
	}
	kubeVersion, err := semver.NewVersion(kubeVersionToUse)
	if err != nil {
		return nil, fmt.Errorf("invalid kubeVersion: %s", kubeVersionToUse)
	}
	h.globalCfg.Capabilities = common.DefaultCapabilities.Copy()
	h.globalCfg.Capabilities.KubeVersion = common.KubeVersion{
		Version: kubeVersion.String(),
		Major:   fmt.Sprint(kubeVersion.Major()),
		Minor:   fmt.Sprint(kubeVersion.Minor()),
	}
	h.globalCfg.KubeClient = &kubefake.PrintingKubeClient{Out: io.Discard}
	h.globalCfg.Releases = storage.Init(mem)
	// Template operations don't need logging since they're just rendering
	h.globalCfg.SetLogger(nil) // nil sets discard handler in Helm v4

	return h.Deploy(ctx, bundleID, manifest, options)
}



================================================
FILE: internal/helmdeployer/helmcache/secret.go
================================================
package helmcache

import (
	"context"

	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/apimachinery/pkg/watch"
	applycorev1 "k8s.io/client-go/applyconfigurations/core/v1"
	"k8s.io/client-go/kubernetes"
	corev1 "k8s.io/client-go/kubernetes/typed/core/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// SecretClient implements methods to handle secrets. Get and list will be retrieved from the wrangler cache, the other calls
// will be make to the Kubernetes API server.
type SecretClient struct {
	cache     client.Client
	client    kubernetes.Interface
	namespace string
}

var _ corev1.SecretInterface = &SecretClient{}

func NewSecretClient(cache client.Client, client kubernetes.Interface, namespace string) *SecretClient {
	return &SecretClient{cache, client, namespace}
}

// Create creates a secret using a k8s client that calls the Kubernetes API server
func (s *SecretClient) Create(ctx context.Context, secret *v1.Secret, opts metav1.CreateOptions) (*v1.Secret, error) {
	return s.client.CoreV1().Secrets(s.namespace).Create(ctx, secret, opts)
}

// Update updates a secret using a k8s client that calls the Kubernetes API server
func (s *SecretClient) Update(ctx context.Context, secret *v1.Secret, opts metav1.UpdateOptions) (*v1.Secret, error) {
	return s.client.CoreV1().Secrets(s.namespace).Update(ctx, secret, opts)
}

// Delete deletes a secret using a k8s client that calls the Kubernetes API server
func (s *SecretClient) Delete(ctx context.Context, name string, opts metav1.DeleteOptions) error {
	return s.client.CoreV1().Secrets(s.namespace).Delete(ctx, name, opts)
}

// DeleteCollection deletes a secret collection using a k8s client that calls the Kubernetes API server
func (s *SecretClient) DeleteCollection(ctx context.Context, opts metav1.DeleteOptions, listOpts metav1.ListOptions) error {
	return s.client.CoreV1().Secrets(s.namespace).DeleteCollection(ctx, opts, listOpts)
}

// Get gets a secret from the cache.
func (s *SecretClient) Get(ctx context.Context, name string, _ metav1.GetOptions) (*v1.Secret, error) {
	secret := &v1.Secret{}
	err := s.cache.Get(ctx, types.NamespacedName{Namespace: s.namespace, Name: name}, secret)
	return secret, err
}

// List lists secrets from the cache.
func (s *SecretClient) List(ctx context.Context, opts metav1.ListOptions) (*v1.SecretList, error) {
	labels, err := labels.Parse(opts.LabelSelector)
	if err != nil {
		return nil, err
	}
	secrets := v1.SecretList{}
	err = s.cache.List(ctx, &secrets, &client.ListOptions{
		Namespace:     s.namespace,
		LabelSelector: labels,
	})
	if err != nil {
		return nil, err
	}

	return &secrets, nil
}

// Watch watches a secret using a k8s client that calls the Kubernetes API server
func (s *SecretClient) Watch(ctx context.Context, opts metav1.ListOptions) (watch.Interface, error) {
	return s.client.CoreV1().Secrets(s.namespace).Watch(ctx, opts)
}

// Patch patches a secret using a k8s client that calls the Kubernetes API server
func (s *SecretClient) Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions, subresources ...string) (*v1.Secret, error) {
	return s.client.CoreV1().Secrets(s.namespace).Patch(ctx, name, pt, data, opts, subresources...)
}

// Apply applies a secret using a k8s client that calls the Kubernetes API server
func (s *SecretClient) Apply(ctx context.Context, secretConfiguration *applycorev1.SecretApplyConfiguration, opts metav1.ApplyOptions) (*v1.Secret, error) {
	return s.client.CoreV1().Secrets(s.namespace).Apply(ctx, secretConfiguration, opts)
}



================================================
FILE: internal/helmdeployer/helmcache/secret_test.go
================================================
package helmcache

import (
	"context"
	"testing"

	"github.com/google/go-cmp/cmp"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	applycorev1 "k8s.io/client-go/applyconfigurations/core/v1"
	v1 "k8s.io/client-go/applyconfigurations/meta/v1"
	k8sfake "k8s.io/client-go/kubernetes/fake"
	clientgoscheme "k8s.io/client-go/kubernetes/scheme"
	"sigs.k8s.io/controller-runtime/pkg/client/fake"
)

const (
	secretName      = "test"
	secretNamespace = "test-ns"
)

var defaultSecret = corev1.Secret{
	ObjectMeta: metav1.ObjectMeta{
		Name:      secretName,
		Namespace: secretNamespace,
	},
}

func TestGet(t *testing.T) {
	scheme := runtime.NewScheme()
	utilruntime.Must(clientgoscheme.AddToScheme(scheme))

	secret := defaultSecret
	cache := fake.NewClientBuilder().WithScheme(scheme).WithObjects(&secret).Build()
	secretClient := NewSecretClient(cache, nil, secretNamespace)

	secretGot, err := secretClient.Get(context.TODO(), secretName, metav1.GetOptions{})
	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
	if !cmp.Equal(secret.ObjectMeta, secretGot.ObjectMeta) {
		t.Errorf("expected secret meta %#v, got %#v", secret.ObjectMeta, secretGot.ObjectMeta)
	}
	if !cmp.Equal(secret.Data, secretGot.Data) {
		t.Errorf("expected secret data %#v, got %#v", secret.Data, secretGot.Data)
	}
}

func TestList(t *testing.T) {
	scheme := runtime.NewScheme()
	utilruntime.Must(clientgoscheme.AddToScheme(scheme))

	secret := defaultSecret
	secret.Labels = map[string]string{"foo": "bar"}
	cache := fake.NewClientBuilder().WithScheme(scheme).WithObjects(&secret).Build()
	secretClient := NewSecretClient(cache, nil, secretNamespace)

	secretList, err := secretClient.List(context.TODO(), metav1.ListOptions{LabelSelector: "foo=bar"})
	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
	if len(secretList.Items) != 1 {
		t.Errorf("expected secret list to have 1 element, got %v", len(secretList.Items))
	}
	if !cmp.Equal(secret.ObjectMeta, secretList.Items[0].ObjectMeta) {
		t.Errorf("expected secret meta %#v, got %#v", secret, secretList.Items[0])
	}
	if !cmp.Equal(secret.Data, secretList.Items[0].Data) {
		t.Errorf("expected secret data %#v, got %#v", secret, secretList.Items[0])
	}
}

func TestCreate(t *testing.T) {
	client := k8sfake.NewSimpleClientset()
	secretClient := NewSecretClient(nil, client, secretNamespace)
	secret := defaultSecret
	secretCreated, err := secretClient.Create(context.TODO(), &secret, metav1.CreateOptions{})

	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
	if !cmp.Equal(&secret, secretCreated) {
		t.Errorf("expected secret %v, got %v", secret, secretCreated)
	}
}

func TestUpdate(t *testing.T) {
	secretUpdate := corev1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:      secretName,
			Namespace: secretNamespace,
		},
		Data: map[string][]byte{"test": []byte("data")},
	}
	secret := defaultSecret
	client := k8sfake.NewSimpleClientset(&secret)
	secretClient := NewSecretClient(nil, client, secretNamespace)
	secretUpdated, err := secretClient.Update(context.TODO(), &secretUpdate, metav1.UpdateOptions{})

	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
	if !cmp.Equal(&secretUpdate, secretUpdated) {
		t.Errorf("expected secret %v, got %v", secretUpdate, secretUpdated)
	}
}

func TestDelete(t *testing.T) {
	secret := defaultSecret
	client := k8sfake.NewSimpleClientset(&secret)
	secretClient := NewSecretClient(nil, client, secretNamespace)
	err := secretClient.Delete(context.TODO(), secretName, metav1.DeleteOptions{})

	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
}

func TestDeleteCollection(t *testing.T) {
	secret := defaultSecret
	client := k8sfake.NewSimpleClientset(&secret)
	secretClient := NewSecretClient(nil, client, secretNamespace)
	err := secretClient.DeleteCollection(context.TODO(), metav1.DeleteOptions{}, metav1.ListOptions{FieldSelector: "name=" + secretName})

	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
}

func TestWatch(t *testing.T) {
	secret := defaultSecret
	client := k8sfake.NewSimpleClientset(&secret)
	secretClient := NewSecretClient(nil, client, secretNamespace)
	watch, err := secretClient.Watch(context.TODO(), metav1.ListOptions{FieldSelector: "name=" + secretName})

	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
	if watch == nil {
		t.Errorf("watch should not be nil")
	}
}

func TestPatch(t *testing.T) {
	secretPatch := corev1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:      secretName,
			Namespace: secretNamespace,
		},
		Data: map[string][]byte{"test": []byte("content")},
	}
	secret := defaultSecret
	client := k8sfake.NewSimpleClientset(&secret)
	secretClient := NewSecretClient(nil, client, secretNamespace)
	patch := []byte(`{"data":{"test":"Y29udGVudA=="}}`) // "content", base64-encoded
	secretPatched, err := secretClient.Patch(context.TODO(), secretName, types.StrategicMergePatchType, patch, metav1.PatchOptions{})

	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
	if !cmp.Equal(&secretPatch, secretPatched) {
		t.Errorf("expected secret %v, got %v", secretPatch, secretPatched)
	}
}

func TestApply(t *testing.T) {
	secretApply := corev1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:      secretName,
			Namespace: secretNamespace,
		},
		Data: map[string][]byte{"test": []byte("content")},
	}
	secret := defaultSecret
	client := k8sfake.NewSimpleClientset(&secret)
	secretClient := NewSecretClient(nil, client, secretNamespace)
	secretName := "test"
	secretApplied, err := secretClient.Apply(context.TODO(), &applycorev1.SecretApplyConfiguration{
		ObjectMetaApplyConfiguration: &v1.ObjectMetaApplyConfiguration{
			Name: &secretName,
		},
		Data: map[string][]byte{"test": []byte("content")},
	}, metav1.ApplyOptions{})

	if err != nil {
		t.Errorf("unexpected error %v", err)
	}
	if !cmp.Equal(&secretApply, secretApplied) {
		t.Errorf("expected secret %v, got %v", secretApply, secretApplied)
	}
}



================================================
FILE: internal/helmdeployer/kustomize/kustomize.go
================================================
package kustomize

import (
	"path/filepath"

	"github.com/rancher/fleet/internal/cmd/agent/deployer/data/convert"
	"github.com/rancher/fleet/internal/content"
	"github.com/rancher/fleet/internal/manifest"

	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
	"sigs.k8s.io/kustomize/api/krusty"
	"sigs.k8s.io/kustomize/api/types"
	"sigs.k8s.io/kustomize/kyaml/filesys"
	"sigs.k8s.io/yaml"
)

const (
	KustomizeYAML = "kustomization.yaml"
	ManifestsYAML = "fleet-manifests.yaml"
)

func Process(m *manifest.Manifest, content []byte, dir string) ([]runtime.Object, bool, error) {
	if dir == "" {
		dir = "."
	}

	fs, err := toFilesystem(m, dir, content)
	if err != nil {
		return nil, false, err
	}

	d := filepath.Join(dir, KustomizeYAML)
	if !fs.Exists(d) {
		return nil, false, nil
	}

	if len(content) > 0 {
		if err := modifyKustomize(fs, dir); err != nil {
			return nil, false, err
		}
	}

	objs, err := kustomize(fs, dir)
	return objs, true, err
}

func containsString(slice []string, item string) bool {
	for _, j := range slice {
		if j == item {
			return true
		}
	}
	return false
}

func modifyKustomize(f filesys.FileSystem, dir string) error {
	file := filepath.Join(dir, KustomizeYAML)
	fileBytes, err := f.ReadFile(file)
	if err != nil {
		return err
	}

	data := map[string]interface{}{}
	if err := yaml.Unmarshal(fileBytes, &data); err != nil {
		return err
	}

	resources := convert.ToStringSlice(data["resources"])
	if containsString(resources, ManifestsYAML) {
		return nil
	}

	data["resources"] = append([]string{ManifestsYAML}, resources...)
	fileBytes, err = yaml.Marshal(data)
	if err != nil {
		return err
	}

	return f.WriteFile(file, fileBytes)
}

func toFilesystem(m *manifest.Manifest, dir string, manifestsContent []byte) (filesys.FileSystem, error) {
	f := filesys.MakeEmptyDirInMemory()
	for _, resource := range m.Resources {
		if resource.Name == "" {
			continue
		}
		data, err := content.Decode(resource.Content, resource.Encoding)
		if err != nil {
			return nil, err
		}
		if _, err := f.AddFile(resource.Name, data); err != nil {
			return nil, err
		}
	}

	_, err := f.AddFile(filepath.Join(dir, ManifestsYAML), manifestsContent)
	return f, err
}

func kustomize(fs filesys.FileSystem, dir string) (result []runtime.Object, err error) {
	pcfg := types.DisabledPluginConfig()
	kust := krusty.MakeKustomizer(&krusty.Options{
		LoadRestrictions: types.LoadRestrictionsRootOnly,
		PluginConfig:     pcfg,
	})
	resMap, err := kust.Run(fs, dir)
	if err != nil {
		return nil, err
	}
	for _, m := range resMap.Resources() {
		mm, err := m.Map()
		if err != nil {
			return nil, err
		}
		result = append(result, &unstructured.Unstructured{
			Object: mm,
		})
	}
	return
}



================================================
FILE: internal/helmdeployer/rawyaml/resources.go
================================================
package rawyaml

import (
	"bytes"
	"strings"

	chartv2 "helm.sh/helm/v4/pkg/chart/v2"

	"github.com/rancher/wrangler/v3/pkg/yaml"
	"k8s.io/apimachinery/pkg/runtime"
)

const (
	YAMLPrefix    = "chart/raw-yaml/"
	inChartPrefix = "raw-yaml/"
)

func ToObjects(c *chartv2.Chart) (result []runtime.Object, _ error) {
	for _, resource := range c.Files {
		if !strings.HasPrefix(resource.Name, inChartPrefix) {
			continue
		}
		objs, err := yaml.ToObjects(bytes.NewBuffer(resource.Data))
		if err != nil {
			if runtime.IsMissingKind(err) {
				continue
			}
			return nil, err
		}
		for _, obj := range objs {
			apiVersion, kind := obj.GetObjectKind().GroupVersionKind().ToAPIVersionAndKind()
			if apiVersion == "" || kind == "" {
				continue
			}
			result = append(result, obj)
		}
	}

	return result, nil
}



================================================
FILE: internal/helmdeployer/render/helm.go
================================================
package render

import (
	"io"
	"path/filepath"
	"strings"

	chartv2 "helm.sh/helm/v4/pkg/chart/v2"

	"github.com/rancher/fleet/internal/bundlereader"
	"github.com/rancher/fleet/internal/fleetyaml"
	"github.com/rancher/fleet/internal/helmdeployer/rawyaml"
	"github.com/rancher/fleet/internal/helmdeployer/render/patch"
	"github.com/rancher/fleet/internal/manifest"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"github.com/rancher/wrangler/v3/pkg/kv"

	"sigs.k8s.io/yaml"
)

// HelmChart applies overlays to "manifest"-style gitrepos and transforms the
// manifest into a helm chart tgz
func HelmChart(name string, m *manifest.Manifest, options fleet.BundleDeploymentOptions) (io.Reader, error) {
	var (
		style = bundlereader.DetermineStyle(m, options)
		err   error
	)

	if style.IsRawYAML() {
		var overlays []string
		if options.YAML != nil {
			overlays = options.YAML.Overlays
		}
		m, err = patch.Process(m, overlays)
		if err != nil {
			return nil, err
		}
	}

	m, err = process(name, m, style)
	if err != nil {
		return nil, err
	}

	return m.ToTarGZ()
}

// process filters the manifests resources and adds a Chart.yaml if missing
func process(name string, m *manifest.Manifest, style bundlereader.Style) (*manifest.Manifest, error) {
	newManifest := toChart(m, style)
	if !style.HasChartYAML {
		return addChartYAML(name, m, newManifest)
	}
	return newManifest, nil
}

func move(m *manifest.Manifest, from, to string) (result []fleet.BundleResource) {
	if from == "." {
		from = ""
	} else if from != "" {
		from += "/"
	}
	for _, resource := range m.Resources {
		if strings.HasPrefix(resource.Name, from) {
			resource.Name = to + strings.TrimPrefix(resource.Name, from)
			result = append(result, resource)
		}
	}
	return result
}

// manifests returns a filtered list of BundleResources
// It also treats the 'templates/' directory as a special case.
func manifests(m *manifest.Manifest) (result []fleet.BundleResource) {
	var ignorePrefix []string
	for _, resource := range m.Resources {
		if fleetyaml.IsFleetYamlSuffix(resource.Name) ||
			strings.HasSuffix(resource.Name, "/Chart.yaml") {
			ignorePrefix = append(ignorePrefix, filepath.Dir(resource.Name)+"/")
		}
	}

outer:
	for _, resource := range m.Resources {
		if fleetyaml.IsFleetYaml(resource.Name) {
			continue
		}
		if !strings.HasSuffix(resource.Name, ".yaml") &&
			!strings.HasSuffix(resource.Name, ".json") &&
			!strings.HasSuffix(resource.Name, ".yml") {
			continue
		}
		for _, prefix := range ignorePrefix {
			if strings.HasPrefix(resource.Name, prefix) {
				continue outer
			}
		}
		if strings.HasPrefix(resource.Name, "templates/") {
			resource.Name = "chart/" + resource.Name
		} else {
			resource.Name = rawyaml.YAMLPrefix + resource.Name
		}
		result = append(result, resource)
	}

	return result
}

func toChart(m *manifest.Manifest, style bundlereader.Style) *manifest.Manifest {
	var (
		resources []fleet.BundleResource
	)

	if style.ChartPath != "" {
		resources = move(m, filepath.Dir(style.ChartPath), "chart/")
	} else if style.IsRawYAML() {
		resources = manifests(m)
	}

	return &manifest.Manifest{
		Resources: resources,
		Commit:    m.Commit,
	}
}

func addChartYAML(name string, m, newManifest *manifest.Manifest) (*manifest.Manifest, error) {
	manifestID, err := m.ID()
	if err != nil {
		return nil, err
	}

	if newManifest.Commit != "" && len(newManifest.Commit) > 12 {
		manifestID = "git-" + newManifest.Commit[:12]
	}

	_, chartName := kv.RSplit(name, "/")
	metadata := chartv2.Metadata{
		Name:       chartName,
		Version:    "v0.0.0+" + manifestID,
		APIVersion: "v2",
	}

	chart, err := yaml.Marshal(metadata)
	if err != nil {
		return nil, err
	}

	newManifest.Resources = append(newManifest.Resources, fleet.BundleResource{
		Name:    "chart/Chart.yaml",
		Content: string(chart),
	})

	return newManifest, nil
}



================================================
FILE: internal/helmdeployer/render/patch/patch.go
================================================
package patch

import (
	"encoding/json"
	"fmt"
	"path/filepath"
	"sort"
	"strings"

	"github.com/pkg/errors"

	"github.com/rancher/fleet/internal/content"
	"github.com/rancher/fleet/internal/manifest"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"github.com/rancher/wrangler/v3/pkg/patch"

	"sigs.k8s.io/kustomize/kyaml/yaml"
)

var (
	overlayPrefix = "overlays/"
)

func Process(m *manifest.Manifest, overlays []string) (*manifest.Manifest, error) {
	newManifest := &manifest.Manifest{Commit: m.Commit}
	for i, resource := range m.Resources {
		if resource.Name == "" {
			resource.Name = fmt.Sprintf("manifests/file%03d.yaml", i)
		}
		newManifest.Resources = append(newManifest.Resources, resource)
	}
	m = newManifest

	m, err := patchContext(m, overlays)
	if err != nil {
		return nil, err
	}

	newManifest = &manifest.Manifest{Commit: m.Commit}
	for _, resource := range m.Resources {
		if strings.HasPrefix(resource.Name, overlayPrefix) {
			continue
		}
		newManifest.Resources = append(newManifest.Resources, resource)
	}

	sort.Slice(newManifest.Resources, func(i, j int) bool {
		return newManifest.Resources[i].Name < newManifest.Resources[j].Name
	})

	return newManifest, nil
}

func patchContext(m *manifest.Manifest, overlays []string) (*manifest.Manifest, error) {
	result := map[string][]byte{}

	if len(overlays) == 0 {
		return m, nil
	}

	for _, resource := range m.Resources {
		data, err := content.Decode(resource.Content, resource.Encoding)
		if err != nil {
			return nil, err
		}

		result[resource.Name] = data
	}

	if err := patchContent(result, overlays); err != nil {
		return nil, err
	}

	resultManifest := &manifest.Manifest{}
	for name, bytes := range result {
		resultManifest.Resources = append(resultManifest.Resources, fleet.BundleResource{
			Name:    name,
			Content: string(bytes),
		})
	}

	return resultManifest, nil
}

func isPatchFile(name string) (string, bool) {
	base := filepath.Base(name)
	if strings.Contains(base, "_patch.") {
		target := strings.Replace(base, "_patch.", ".", 1)
		return filepath.Join(filepath.Dir(name), target), true
	}
	return "", false
}

func patchContent(content map[string][]byte, overlays []string) error {
	for _, overlay := range overlays {
		prefix := overlayPrefix + overlay + "/"
		for name, bytes := range content {
			if !strings.HasPrefix(name, prefix) {
				continue
			}

			name := strings.TrimPrefix(name, prefix)
			target, ok := isPatchFile(name)
			if !ok {
				content[name] = bytes
				continue
			}

			targetContent, ok := content[target]
			if !ok {
				return fmt.Errorf("failed to find base file %s to patch", target)
			}

			targetContent, err := convertToJSON(targetContent)
			if err != nil {
				return errors.Wrapf(err, "failed to convert %s to json", target)
			}

			bytes, err = convertToJSON(bytes)
			if err != nil {
				return errors.Wrapf(err, "failed to convert %s to json", name)
			}

			newBytes, err := patch.Apply(targetContent, bytes)
			if err != nil {
				return errors.Wrapf(err, "failed to patch %s", target)
			}
			content[target] = newBytes
		}
	}

	return nil
}

func convertToJSON(input []byte) ([]byte, error) {
	var data interface{}
	data = map[string]interface{}{}
	if err := yaml.Unmarshal(input, &data); err != nil {
		data = []interface{}{}
		if err := yaml.Unmarshal(input, &data); err != nil {
			return nil, err
		}
	}
	return json.Marshal(data)
}



================================================
FILE: internal/helmupdater/helmupdater.go
================================================
package helmupdater

import (
	"os"
	"path/filepath"

	"helm.sh/helm/v4/pkg/action"
	"helm.sh/helm/v4/pkg/chart"
	"helm.sh/helm/v4/pkg/chart/v2/loader"
	"helm.sh/helm/v4/pkg/cli"
	"helm.sh/helm/v4/pkg/downloader"
	"helm.sh/helm/v4/pkg/getter"
	"helm.sh/helm/v4/pkg/registry"
)

const (
	ChartYaml = "Chart.yaml"
)

// ChartYAMLExists checks if the provided path is a directory containing a `Chart.yaml` file.
// Returns true if it does, false otherwise or if an error happens when checking `<path>/Chart.yaml`.
func ChartYAMLExists(path string) bool {
	chartPath := filepath.Join(path, ChartYaml)
	if _, err := os.Stat(chartPath); err != nil {
		return false
	}
	return true
}

// UpdateHelmDependencies checks if the helm chart located at the given directory has unmet dependencies and, if so, updates them
func UpdateHelmDependencies(path string) error {
	// load the chart to check if there are unmet dependencies first
	chartRequested, err := loader.Load(path)
	if err != nil {
		return err
	}

	if req := chartRequested.Metadata.Dependencies; req != nil {
		// Convert []*v2.Dependency to []chart.Dependency
		deps := make([]chart.Dependency, len(req))
		for i, d := range req {
			deps[i] = d
		}
		if err := action.CheckDependencies(chartRequested, deps); err != nil {
			settings := cli.New()
			registryClient, err := registry.NewClient(
				registry.ClientOptDebug(settings.Debug),
				registry.ClientOptEnableCache(true),
				registry.ClientOptWriter(os.Stderr),
				registry.ClientOptCredentialsFile(settings.RegistryConfig),
			)
			if err != nil {
				return err
			}
			man := &downloader.Manager{
				Out:              os.Stdout,
				ChartPath:        path,
				Keyring:          "",
				SkipUpdate:       false,
				Getters:          getter.All(settings),
				RepositoryConfig: settings.RegistryConfig,
				RepositoryCache:  settings.RepositoryCache,
				ContentCache:     settings.ContentCache,
				Debug:            settings.Debug,
				RegistryClient:   registryClient,
			}
			if err := man.Update(); err != nil {
				return err
			}
		}
	}
	return nil
}



================================================
FILE: internal/helmupdater/helmupdater_test.go
================================================
package helmupdater_test

import (
	"os"
	"path/filepath"
	"testing"

	"github.com/rancher/fleet/internal/helmupdater"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

type fsNodeSimple struct {
	name     string
	children []fsNodeSimple
	isDir    bool
}

// createDirStruct generates and populates a directory structure which root is node, placing it at basePath.
func createDirStruct(t *testing.T, basePath string, node fsNodeSimple) {
	t.Helper()
	path := filepath.Join(basePath, node.name)

	if !node.isDir {
		_, err := os.Create(path)
		require.NoError(t, err)
		return
	}

	err := os.Mkdir(path, 0777)
	require.NoError(t, err)

	for _, c := range node.children {
		createDirStruct(t, path, c)
	}
}

func TestChartYAMLExists(t *testing.T) {
	cases := []struct {
		name               string
		testDir            string
		directoryStructure fsNodeSimple
		expectedResult     bool
	}{
		{
			name:    "simple case having Chart yaml file in the root",
			testDir: "simpleok",
			directoryStructure: fsNodeSimple{
				name: "Chart.yaml",
			},
			expectedResult: true,
		},
		{
			name:    "Chart.yml file instead of Chart.yaml",
			testDir: "extensionnotyaml",
			directoryStructure: fsNodeSimple{
				name: "Chart.yml",
			},
			expectedResult: false,
		},
		{
			name:    "simple case not having Chart yaml file in the root",
			testDir: "simplenotfound",
			directoryStructure: fsNodeSimple{
				name: "whatever.foo",
			},
			expectedResult: false,
		},
		{
			name:    "Chart.yaml is located in a subdirectory",
			testDir: "subdir",
			directoryStructure: fsNodeSimple{
				isDir: true,
				children: []fsNodeSimple{
					{
						name: "Chart.yaml",
					},
				},
				name: "whatever",
			},
			expectedResult: false,
		},
	}

	base, err := os.MkdirTemp("", "test-fleet")
	require.NoError(t, err)

	defer os.RemoveAll(base)
	for _, c := range cases {
		t.Run(c.name, func(t *testing.T) {
			testDirPath := filepath.Join(base, c.testDir)
			err := os.Mkdir(testDirPath, 0777)
			require.NoError(t, err)
			defer os.RemoveAll(testDirPath)

			createDirStruct(t, testDirPath, c.directoryStructure)
			found := helmupdater.ChartYAMLExists(testDirPath)

			assert.Equal(t, c.expectedResult, found)
		})
	}
}



================================================
FILE: internal/helmvalues/extract.go
================================================
package helmvalues

import (
	"fmt"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

// ExtractOptions extracts the values from options in a bundle deployment.
func ExtractOptions(bd *fleet.BundleDeployment) (string, []byte, []byte, error) {
	var options []byte
	if bd.Spec.Options.Helm != nil && bd.Spec.Options.Helm.Values != nil {
		var err error
		options, err = bd.Spec.Options.Helm.Values.MarshalJSON()
		if err != nil {
			err = fmt.Errorf("failed to marshal values: %w", err)
			return "", []byte{}, []byte{}, err
		}
		if string(options) == "null" || string(options) == "{}" {
			options = []byte{}
		}
	}

	var staged []byte
	if bd.Spec.StagedOptions.Helm != nil && bd.Spec.StagedOptions.Helm.Values != nil {
		var err error
		staged, err = bd.Spec.StagedOptions.Helm.Values.MarshalJSON()
		if err != nil {
			err = fmt.Errorf("failed to marshal staged values: %w", err)
			return "", []byte{}, []byte{}, err
		}
		if string(staged) == "null" || string(staged) == "{}" {
			staged = []byte{}
		}
	}

	var hash string
	if len(options) > 0 || len(staged) > 0 {
		hash = HashOptions(options, staged)
	}

	return hash, options, staged, nil
}

// ClearOptions removes values from the new bundle deployment
func ClearOptions(bd *fleet.BundleDeployment) {
	if bd.Spec.Options.Helm != nil {
		bd.Spec.Options.Helm.Values = nil
	}
	if bd.Spec.StagedOptions.Helm != nil {
		bd.Spec.StagedOptions.Helm.Values = nil
	}
}

// ExtractValues extracts the values from the bundle and returns the values and
// a hash.
func ExtractValues(bundle *fleet.Bundle) (string, map[string][]byte, error) {
	data := map[string][]byte{}
	spec := bundle.Spec

	if spec.Helm != nil && spec.Helm.Values != nil && len(spec.Helm.Values.Data) > 0 {
		v, err := spec.Helm.Values.MarshalJSON()
		if err != nil {
			return "", data, err
		}
		data["values.yaml"] = v
	}

	for _, target := range spec.Targets {
		if target.Helm == nil || target.Helm.Values == nil || len(target.Helm.Values.Data) == 0 {
			continue
		}

		if target.Name == "" {
			return "", data, fmt.Errorf("target name is required")
		}

		v, err := target.Helm.Values.MarshalJSON()
		if err != nil {
			return "", data, err
		}
		data[target.Name] = v
	}

	if len(data) == 0 {
		return "", data, nil
	}

	// Note: we assume json.Marshal is stable for maps
	hash, err := HashValuesSecret(data)
	if err != nil {
		return "", data, err
	}

	return hash, data, nil
}

// ClearValues removes the values from a bundle. It mutates the bundle.
func ClearValues(bundle *fleet.Bundle) {
	if bundle.Spec.Helm != nil {
		bundle.Spec.Helm.Values = nil
		bundle.Spec.Helm.ValuesFiles = nil
	}
	for i := range bundle.Spec.Targets {
		if bundle.Spec.Targets[i].Helm == nil {
			continue
		}
		bundle.Spec.Targets[i].Helm.Values = nil
		bundle.Spec.Targets[i].Helm.ValuesFiles = nil
	}
}



================================================
FILE: internal/helmvalues/extract_test.go
================================================
package helmvalues_test

import (
	"testing"

	"github.com/rancher/fleet/internal/helmvalues"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

func TestExtractValues(t *testing.T) {
	type args struct {
		bundle *fleet.Bundle
	}
	tests := []struct {
		name          string
		args          args
		wantHash      string
		wantData      map[string][]byte
		wantErr       bool
		wantErrString string
	}{
		{
			name: "nil helm options",
			args: args{
				bundle: &fleet.Bundle{Spec: fleet.BundleSpec{
					BundleDeploymentOptions: fleet.BundleDeploymentOptions{
						Helm: nil,
					},
					Targets: []fleet.BundleTarget{
						{
							Name: "target",
							BundleDeploymentOptions: fleet.BundleDeploymentOptions{
								Helm: nil,
							},
						},
					},
				}},
			},
			wantHash: "",
			wantData: map[string][]byte{},
			wantErr:  false,
		},
		{
			name: "nil values",
			args: args{
				bundle: &fleet.Bundle{Spec: fleet.BundleSpec{
					BundleDeploymentOptions: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							Values: nil,
						},
					},
					Targets: []fleet.BundleTarget{
						{
							Name: "target",
							BundleDeploymentOptions: fleet.BundleDeploymentOptions{
								Helm: &fleet.HelmOptions{
									Values: nil,
								},
							},
						},
					},
				}},
			},
			wantHash: "",
			wantData: map[string][]byte{},
			wantErr:  false,
		},
		{
			name: "empty values",
			args: args{
				bundle: &fleet.Bundle{Spec: fleet.BundleSpec{
					BundleDeploymentOptions: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							Values: &fleet.GenericMap{},
						},
					},
					Targets: []fleet.BundleTarget{
						{
							Name: "target",
							BundleDeploymentOptions: fleet.BundleDeploymentOptions{
								Helm: &fleet.HelmOptions{
									Values: &fleet.GenericMap{},
								},
							},
						},
					},
				}},
			},
			wantHash: "",
			wantData: map[string][]byte{},
			wantErr:  false,
		},
		{
			name: "values present",
			args: args{
				bundle: &fleet.Bundle{Spec: fleet.BundleSpec{
					BundleDeploymentOptions: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							Values: &fleet.GenericMap{
								Data: map[string]interface{}{"key": "value"},
							},
						},
					},
					Targets: []fleet.BundleTarget{
						{
							Name: "target",
							BundleDeploymentOptions: fleet.BundleDeploymentOptions{
								Helm: &fleet.HelmOptions{
									Values: &fleet.GenericMap{
										Data: map[string]interface{}{"newkey": "value"},
									},
								},
							},
						},
					},
				}},
			},

			wantHash: "17e05a97acde825d03ca37c2b8fc1aecf3f8f9fde28c21702492c56d4d4a68f1",
			wantData: map[string][]byte{
				"values.yaml": []byte(`{"key":"value"}`),
				"target":      []byte(`{"newkey":"value"}`)},
			wantErr: false,
		},
	}

	for _, tt := range tests {
		h, d, err := helmvalues.ExtractValues(tt.args.bundle)
		if (err != nil) != tt.wantErr {
			t.Errorf("%s: error = %v, wantErr %v", tt.name, err, tt.wantErr)
			return
		}
		if err != nil && err.Error() != tt.wantErrString {
			t.Errorf("%s: error = %s, wantErrString %v", tt.name, err, tt.wantErrString)
			return
		}
		if h != tt.wantHash {
			t.Errorf("%s: hash = %s, want %s", tt.name, h, tt.wantHash)
		}

		if len(d) != len(tt.wantData) {
			t.Errorf("%s: data = %v, want %v", tt.name, d, tt.wantData)
		}
		for k, v := range d {
			if string(v) != string(tt.wantData[k]) {
				t.Errorf("%s: data[%s] = %s, want %s", tt.name, k, v, tt.wantData[k])
			}
		}
	}
}

func TestExtractOptions(t *testing.T) {
	type args struct {
		bd *fleet.BundleDeployment
	}
	var nullMap *fleet.GenericMap
	tests := []struct {
		name          string
		args          args
		wantOptions   []byte
		wantStaged    []byte
		wantHash      string
		wantErr       bool
		wantErrString string
	}{
		{
			name: "nil helm options",
			args: args{
				bd: &fleet.BundleDeployment{Spec: fleet.BundleDeploymentSpec{
					Options: fleet.BundleDeploymentOptions{
						Helm: nil,
					},
				}},
			},
			wantOptions: []byte{},
			wantStaged:  []byte{},
			wantHash:    "",
			wantErr:     false,
		},
		{
			name: "nil values",
			args: args{
				bd: &fleet.BundleDeployment{Spec: fleet.BundleDeploymentSpec{
					Options: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{},
					},
					StagedOptions: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							TakeOwnership: true,
						},
					},
				}},
			},
			wantOptions: []byte{},
			wantStaged:  []byte{},
			wantHash:    "",
			wantErr:     false,
		},
		{
			name: "null values",
			args: args{
				bd: &fleet.BundleDeployment{Spec: fleet.BundleDeploymentSpec{
					Options: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							Values: nullMap,
						},
					},
					StagedOptions: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							TakeOwnership: true,
							Values:        &fleet.GenericMap{},
						},
					},
				}},
			},
			wantOptions: []byte{},
			wantStaged:  []byte{},
			wantHash:    "",
			wantErr:     false,
		},
		{
			name: "empty values",
			args: args{
				bd: &fleet.BundleDeployment{Spec: fleet.BundleDeploymentSpec{
					Options: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							Values: &fleet.GenericMap{Data: map[string]interface{}{}},
						},
					},
					StagedOptions: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							TakeOwnership: true,
							Values:        &fleet.GenericMap{Data: map[string]interface{}{}},
						},
					},
				}},
			},
			wantOptions: []byte(""),
			wantStaged:  []byte(""),
			wantHash:    "",
			wantErr:     false,
		},
		{
			name: "values present",
			args: args{
				bd: &fleet.BundleDeployment{Spec: fleet.BundleDeploymentSpec{
					Options: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							Values: &fleet.GenericMap{
								Data: map[string]interface{}{"key": "value"},
							},
						},
					},
				}},
			},
			wantOptions: []byte(`{"key":"value"}`),
			wantStaged:  []byte{},
			wantHash:    "e43abcf3375244839c012f9633f95862d232a95b00d5bc7348b3098b9fed7f32",
			wantErr:     false,
		},
		{
			name: "values and staged present",
			args: args{
				bd: &fleet.BundleDeployment{Spec: fleet.BundleDeploymentSpec{
					Options: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							Values: &fleet.GenericMap{
								Data: map[string]interface{}{"key": "value"},
							},
						},
					},
					StagedOptions: fleet.BundleDeploymentOptions{
						Helm: &fleet.HelmOptions{
							Values: &fleet.GenericMap{
								Data: map[string]interface{}{"newkey": "value"},
							},
						},
					},
				}},
			},
			wantOptions: []byte(`{"key":"value"}`),
			wantStaged:  []byte(`{"newkey":"value"}`),
			wantHash:    "01c44d8a446abccb870503db292e07cb2b8da135b6fec52b21048bdab8c84a7c",
			wantErr:     false,
		},
	}

	for _, tt := range tests {
		h, o, s, err := helmvalues.ExtractOptions(tt.args.bd)
		if (err != nil) != tt.wantErr {
			t.Errorf("%s: error = %v, wantErr %v", tt.name, err, tt.wantErr)
			return
		}
		if err != nil && err.Error() != tt.wantErrString {
			t.Errorf("%s: error = %s, wantErrString %v", tt.name, err, tt.wantErrString)
			return
		}
		if string(o) != string(tt.wantOptions) {
			t.Errorf("%s: options = %q, want %q", tt.name, o, tt.wantOptions)
		}
		if string(s) != string(tt.wantStaged) {
			t.Errorf("%s: staged = %q, want %q", tt.name, s, tt.wantStaged)
		}
		if h != tt.wantHash {
			t.Errorf("%s: hash = %q, want %q", tt.name, h, tt.wantHash)
		}
	}
}



================================================
FILE: internal/helmvalues/hash.go
================================================
package helmvalues

import (
	"crypto/sha256"
	"encoding/json"
	"fmt"
)

const (
	ValuesKey       = "values"
	StagedValuesKey = "stagedValues"
)

// HashValuesSecret hashes the data of a secret. This is used for the bundle
// values secret created by fleet apply to detect changes and trigger updates.
func HashValuesSecret(data map[string][]byte) (string, error) {
	hasher := sha256.New()
	b, err := json.Marshal(data)
	if err != nil {
		return "", err
	}
	hasher.Write(b)
	return fmt.Sprintf("%x", hasher.Sum(nil)), nil
}

// HashOptions hashes the bytes passed in. This is used to create a hash of the
// bundledeployment's helm options and staged helm options.
func HashOptions(bytes ...[]byte) string {
	hasher := sha256.New()
	for _, b := range bytes {
		hasher.Write(b)
	}
	return fmt.Sprintf("%x", hasher.Sum(nil))
}



================================================
FILE: internal/helmvalues/hash_test.go
================================================
package helmvalues_test

import (
	"testing"

	"github.com/rancher/fleet/internal/helmvalues"
)

func TestHashOptions(t *testing.T) {
	type args struct {
		bytes [][]byte
	}
	tests := []struct {
		name string
		args args
		want string
	}{
		{
			name: "empty",
			args: args{
				bytes: [][]byte{},
			},
			want: "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
		},
		{
			name: "empty string",
			args: args{
				bytes: [][]byte{[]byte("")},
			},
			want: "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
		},
		{
			name: "single word",
			args: args{
				bytes: [][]byte{
					[]byte("test"),
				},
			},
			want: "9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08",
		},
		{
			name: "empty options",
			args: args{
				bytes: [][]byte{
					[]byte(""), []byte(""),
				},
			},
			want: "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
		},
		{
			name: "empty option",
			args: args{
				bytes: [][]byte{
					[]byte("{}"),
				},
			},
			want: "44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a",
		},
		{
			name: "empty options",
			args: args{
				bytes: [][]byte{
					[]byte("{}"), []byte("{}"),
				},
			},
			want: "b51f08b698d88d8027a935d9db649774949f5fb41a0c559bfee6a9a13225c72d",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			if got := helmvalues.HashOptions(tt.args.bytes...); got != tt.want {
				t.Errorf("HashOptions() = %v, want %v", got, tt.want)
			}
		})
	}
}



================================================
FILE: internal/helmvalues/set.go
================================================
package helmvalues

import (
	"fmt"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

// SetValues sets the values in the bundle from the data map. It mutates the bundle.
func SetValues(bundle *fleet.Bundle, data map[string][]byte) error {
	if v, ok := data["values.yaml"]; ok && string(v) != "" {
		gm := fleet.GenericMap{}
		if err := gm.UnmarshalJSON(v); err != nil {
			return fmt.Errorf("failed to unmarshal values.yaml: %w", err)
		}
		if bundle.Spec.Helm == nil {
			bundle.Spec.Helm = &fleet.HelmOptions{}
		}
		bundle.Spec.Helm.Values = &gm
	}

	for i, target := range bundle.Spec.Targets {
		if v, ok := data[target.Name]; ok && string(v) != "" {
			gm := fleet.GenericMap{}
			if err := gm.UnmarshalJSON(v); err != nil {
				return fmt.Errorf("failed to unmarshal values for target %q: %w", target.Name, err)
			}
			if bundle.Spec.Targets[i].Helm == nil {
				bundle.Spec.Targets[i].Helm = &fleet.HelmOptions{}
			}
			bundle.Spec.Targets[i].Helm.Values = &gm
		}
	}

	return nil
}

// SetOptions sets the values in the options of the bundle deployment from the
// data map. It mutates the bundle deployment.
// It sets the staged options, however they are not used by the agent.
func SetOptions(bd *fleet.BundleDeployment, data map[string][]byte) error {
	if v, ok := data[ValuesKey]; ok && string(v) != "" {
		gm := fleet.GenericMap{}
		if err := gm.UnmarshalJSON(v); err != nil {
			return fmt.Errorf("failed to unmarshal values: %w", err)
		}
		if bd.Spec.Options.Helm == nil {
			bd.Spec.Options.Helm = &fleet.HelmOptions{}
		}
		bd.Spec.Options.Helm.Values = &gm
	}

	if v, ok := data[StagedValuesKey]; ok && string(v) != "" {
		gm := fleet.GenericMap{}
		if err := gm.UnmarshalJSON(v); err != nil {
			return fmt.Errorf("failed to unmarshal values: %w", err)
		}
		if bd.Spec.StagedOptions.Helm == nil {
			bd.Spec.StagedOptions.Helm = &fleet.HelmOptions{}
		}
		bd.Spec.StagedOptions.Helm.Values = &gm
	}

	return nil
}



================================================
FILE: internal/manifest/lookup.go
================================================
package manifest

import (
	"context"

	"github.com/rancher/fleet/internal/content"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func NewLookup() *Lookup {
	return &Lookup{}
}

type Lookup struct {
}

func (l *Lookup) Get(ctx context.Context, client client.Reader, id string) (*Manifest, error) {
	c := &fleet.Content{}
	err := client.Get(ctx, types.NamespacedName{Name: id}, c)
	if err != nil {
		return nil, err
	}

	data, err := content.GUnzip(c.Content)
	if err != nil {
		return nil, err
	}
	return FromJSON(data, c.SHA256Sum)
}



================================================
FILE: internal/manifest/manifest.go
================================================
// Package manifest manages content resources, which contain all the resources for a deployed bundle.
//
// Content resources are not namespaced.
package manifest

import (
	"bytes"
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"io"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

type Manifest struct {
	Commit    string                 `json:"-"`
	Resources []fleet.BundleResource `json:"resources,omitempty"`
	shasum    string
	raw       []byte
}

func New(resources []fleet.BundleResource) *Manifest {
	return &Manifest{
		Resources: resources,
	}
}

func FromBundle(bundle *fleet.Bundle) *Manifest {
	return &Manifest{
		Resources: bundle.Spec.Resources,
		shasum:    bundle.Status.ResourcesSHA256Sum,
	}
}

func FromJSON(data []byte, expectedSHAsum string) (*Manifest, error) {
	var m Manifest
	if err := json.NewDecoder(bytes.NewBuffer(data)).Decode(&m); err != nil {
		return nil, err
	}
	m.raw = data
	// Writing all the data to the hasher to avoid unprocessed data issues.
	// See full details in #3807
	h := sha256.New()
	h.Write(data)
	m.shasum = hex.EncodeToString(h.Sum(nil))

	if expectedSHAsum != "" && expectedSHAsum != m.shasum {
		return nil, fmt.Errorf("content does not match hash got %s, expected %s", m.shasum, expectedSHAsum)
	}

	return &m, nil
}

// encodeManifest serializes the provided Manifest and returns the byte array and its sha256sum
func encodeManifest(m *Manifest) ([]byte, string, error) {
	var buf bytes.Buffer
	h := sha256.New()
	out := io.MultiWriter(&buf, h)

	if err := json.NewEncoder(out).Encode(m); err != nil {
		return nil, "", err
	}

	return buf.Bytes(), hex.EncodeToString(h.Sum(nil)), nil
}

func (m *Manifest) load() error {
	data, shasum, err := encodeManifest(m)
	if err != nil {
		return err
	}
	m.raw = data
	m.shasum = shasum
	return nil
}

// Content retrieves the JSON serialization of the bundle resources
func (m *Manifest) Content() ([]byte, error) {
	if m.raw == nil {
		if err := m.load(); err != nil {
			return nil, err
		}
	}
	return m.raw, nil
}

// ResetSHASum removes stored data about calculated SHASum, forcing a recalculation on the next call to SHASum()
func (m *Manifest) ResetSHASum() {
	m.shasum = ""
}

// SHASum returns the SHA256 sum of the JSON serialization. If necessary it
// loads the content resource and caches its shasum and raw data.
func (m *Manifest) SHASum() (string, error) {
	if m.shasum == "" {
		if err := m.load(); err != nil {
			return "", err
		}
	}
	return m.shasum, nil
}

// ID returns the name of the Content resource produced from this Manifest.
// If necessary it loads the content resource and caches its shasum and
// raw data.
func (m *Manifest) ID() (string, error) {
	shasum, err := m.SHASum()
	if err != nil {
		return "", err
	}
	return ToSHA256ID(shasum), nil
}

// ToSHA256ID generates a valid Kubernetes name (max length of 64) from a provided SHA256 sum
func ToSHA256ID(shasum string) string {
	return ("s-" + shasum)[:63]
}



================================================
FILE: internal/manifest/output.go
================================================
package manifest

import (
	"archive/tar"
	"bytes"
	"compress/gzip"
	"io"
	"time"

	"github.com/rancher/fleet/internal/content"
)

func (m *Manifest) ToTarGZ() (io.Reader, error) {
	buf := &bytes.Buffer{}
	gz := gzip.NewWriter(buf)
	w := tar.NewWriter(gz)

	for _, resource := range m.Resources {
		bytes, err := content.Decode(resource.Content, resource.Encoding)
		if err != nil {
			return nil, err
		}

		if err := w.WriteHeader(&tar.Header{
			Name:     resource.Name,
			Mode:     0644,
			Typeflag: tar.TypeReg,
			ModTime:  time.Unix(0, 0),
			Size:     int64(len(bytes)),
		}); err != nil {
			return nil, err
		}
		_, err = w.Write(bytes)
		if err != nil {
			return nil, err
		}
	}

	if err := w.Close(); err != nil {
		return nil, err
	}

	return buf, gz.Close()
}



================================================
FILE: internal/manifest/store.go
================================================
package manifest

import (
	"context"
	"fmt"

	"github.com/rancher/fleet/internal/cmd/controller/errorutil"
	"github.com/rancher/fleet/internal/content"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

func NewStore(client client.Client) *ContentStore {
	return &ContentStore{
		Client: client,
	}
}

type ContentStore struct {
	Client client.Client
}

// Store stores the manifest as a content resource.
// It copies the resources from the bundle to the content resource.
func (c *ContentStore) Store(ctx context.Context, manifest *Manifest) error {
	id, err := manifest.ID()
	if err != nil {
		return err
	}

	if err := c.Client.Get(ctx, types.NamespacedName{Name: id}, &fleet.Content{}); err != nil && !apierrors.IsNotFound(err) {
		return fmt.Errorf("%w: %w", errorutil.ErrRetryable, err)
	} else if err == nil {
		return nil
	}

	return c.createContents(ctx, id, manifest)
}

func (c *ContentStore) createContents(ctx context.Context, id string, manifest *Manifest) error {
	data, err := manifest.Content()
	if err != nil {
		return err
	}
	digest, err := manifest.SHASum()
	if err != nil {
		return err
	}

	// Contents do not exist in the cluster
	compressed, err := content.Gzip(data)
	if err != nil {
		return err
	}

	err = c.Client.Create(ctx, &fleet.Content{
		ObjectMeta: metav1.ObjectMeta{
			Name: id,
		},
		Content:   compressed,
		SHA256Sum: digest,
	})
	err = client.IgnoreAlreadyExists(err)
	if err != nil {
		err = fmt.Errorf("%w: %w", errorutil.ErrRetryable, err)
	}

	return err
}



================================================
FILE: internal/manifest/store_test.go
================================================
package manifest_test

import (
	"bytes"
	"context"
	"strings"
	"testing"

	"go.uber.org/mock/gomock"

	apierrors "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/types"

	"github.com/rancher/fleet/internal/manifest"
	"github.com/rancher/fleet/internal/mocks"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

func testManifest(t *testing.T, data string) *manifest.Manifest {
	t.Helper()

	m, err := manifest.FromJSON([]byte(data), "")
	if err != nil {
		t.Fatal(err)
	}
	if c, err := m.Content(); err != nil {
		t.Fatal(err)
	} else if !bytes.Equal([]byte(data), c) {
		t.Fatalf("Created manifest does not match the original payload")
	}

	return m
}

func Test_contentStore_Store(t *testing.T) {
	resources := `{"resources": [{"name": "foo", "content": "bar"}]}`
	checksum := "752ebbb975f52eea5e87950ef2ca5de4055de3c68a17f54d94527d7fd79c21fd"
	type args struct {
		manifest *manifest.Manifest
		cached   bool
	}
	tests := []struct {
		name string
		args args
		want string
	}{
		{
			name: "new manifest",
			args: args{
				manifest: testManifest(t, resources),
			},
			want: manifest.ToSHA256ID(checksum),
		},
		{
			name: "existing manifest",
			args: args{
				manifest: testManifest(t, resources),
				cached:   true,
			},
			want: manifest.ToSHA256ID(checksum),
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			ctrl := gomock.NewController(t)
			client := mocks.NewMockK8sClient(ctrl)

			store := &manifest.ContentStore{client}
			ctx := context.TODO()
			nsn := types.NamespacedName{Name: tt.want}

			if tt.args.cached {
				client.EXPECT().Get(ctx, nsn, gomock.Any()).Return(nil)
				client.EXPECT().Create(ctx, gomock.Any()).Times(0)
			} else {
				client.EXPECT().Get(ctx, nsn, gomock.Any()).Return(apierrors.NewNotFound(fleet.GroupResource("Content"), tt.want))
				client.EXPECT().Create(ctx, &contentMatcher{
					name:      tt.want,
					sha256sum: checksum,
				}).Times(1)
			}

			err := store.Store(ctx, tt.args.manifest)
			if err != nil {
				t.Errorf("Store() error = %v", err)
				return
			}
		})
	}
}

type contentMatcher struct {
	name      string
	sha256sum string
}

func (m contentMatcher) Matches(x interface{}) bool {
	content, ok := x.(*fleet.Content)
	if !ok {
		return false
	}
	if m.name != "" && m.name != content.Name {
		return false
	}
	if m.sha256sum != "" && m.sha256sum != content.SHA256Sum {
		return false
	}
	return true
}

func (m contentMatcher) String() string {
	var s []string
	if m.name != "" {
		s = append(s, "name is "+m.name)
	}
	if m.sha256sum != "" {
		s = append(s, "sha256sum is "+m.sha256sum)
	}
	return strings.Join(s, ";")
}



================================================
FILE: internal/metrics/bundle_metrics.go
================================================
package metrics

import (
	"fmt"

	"github.com/rancher/fleet/internal/cmd/controller/summary"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
)

var (
	bundleSubsystem = "bundle"
	bundleLabels    = []string{"name", "namespace", "commit", "repo", "generation", "state"}
	BundleCollector = CollectorCollection{
		subsystem: bundleSubsystem,
		metrics: map[string]prometheus.Collector{
			"not_ready": promauto.NewGaugeVec(
				prometheus.GaugeOpts{
					Namespace: metricPrefix,
					Subsystem: bundleSubsystem,
					Name:      "not_ready",
					Help:      "Number of deployments for a specific bundle in a not ready state.",
				},
				bundleLabels,
			),
			"wait_applied": promauto.NewGaugeVec(
				prometheus.GaugeOpts{
					Namespace: metricPrefix,
					Subsystem: bundleSubsystem,
					Name:      "wait_applied",
					Help:      "Number of deployments for a specific bundle in a wait applied state.",
				},
				bundleLabels,
			),
			"err_applied": promauto.NewGaugeVec(
				prometheus.GaugeOpts{
					Namespace: metricPrefix,
					Subsystem: bundleSubsystem,
					Name:      "err_applied",
					Help:      "Number of deployments for a specific bundle in an error applied state.",
				},
				bundleLabels,
			),
			"out_of_sync": promauto.NewGaugeVec(
				prometheus.GaugeOpts{
					Namespace: metricPrefix,
					Subsystem: bundleSubsystem,
					Name:      "out_of_sync",
					Help:      "Number of deployments for a specific bundle in an out of sync state.",
				},
				bundleLabels,
			),
			"modified": promauto.NewGaugeVec(
				prometheus.GaugeOpts{
					Namespace: metricPrefix,
					Subsystem: bundleSubsystem,
					Name:      "modified",
					Help:      "Number of deployments for a specific bundle in a modified state.",
				},
				bundleLabels,
			),
			"ready": promauto.NewGaugeVec(
				prometheus.GaugeOpts{
					Namespace: metricPrefix,
					Subsystem: bundleSubsystem,
					Name:      "ready",
					Help:      "Number of deployments for a specific bundle in a ready state.",
				},
				bundleLabels,
			),
			"pending": promauto.NewGaugeVec(
				prometheus.GaugeOpts{
					Namespace: metricPrefix,
					Subsystem: bundleSubsystem,
					Name:      "pending",
					Help:      "Number of deployments for a specific bundle in a pending state.",
				},
				bundleLabels,
			),
			"desired_ready": promauto.NewGaugeVec(
				prometheus.GaugeOpts{
					Namespace: metricPrefix,
					Subsystem: bundleSubsystem,
					Name:      "desired_ready",
					Help:      "Number of deployments that are desired to be ready for a bundle.",
				},
				bundleLabels,
			),
			"state": promauto.NewGaugeVec(
				prometheus.GaugeOpts{
					Namespace: metricPrefix,
					Subsystem: bundleSubsystem,
					Name:      "state",
					Help:      "Shows the state of this bundle based on the state label. A value of 1 is true 0, is false.",
				},
				bundleLabels,
			),
		},
		collector: collectBundleMetrics,
	}
)

func collectBundleMetrics(obj any, metrics map[string]prometheus.Collector) {
	bundle, ok := obj.(*fleet.Bundle)
	if !ok {
		panic("unexpected object type")
	}

	currentState := summary.GetSummaryState(bundle.Status.Summary)
	labels := prometheus.Labels{
		"name":       bundle.Name,
		"namespace":  bundle.Namespace,
		"commit":     bundle.Labels[fleet.CommitLabel],
		"repo":       bundle.Labels[fleet.RepoLabel],
		"generation": fmt.Sprintf("%d", bundle.Generation),
		"state":      string(currentState),
	}

	metrics["not_ready"].(*prometheus.GaugeVec).With(labels).
		Set(float64(bundle.Status.Summary.NotReady))
	metrics["wait_applied"].(*prometheus.GaugeVec).With(labels).
		Set(float64(bundle.Status.Summary.WaitApplied))
	metrics["err_applied"].(*prometheus.GaugeVec).With(labels).
		Set(float64(bundle.Status.Summary.ErrApplied))
	metrics["out_of_sync"].(*prometheus.GaugeVec).With(labels).
		Set(float64(bundle.Status.Summary.OutOfSync))
	metrics["modified"].(*prometheus.GaugeVec).With(labels).
		Set(float64(bundle.Status.Summary.Modified))
	metrics["ready"].(*prometheus.GaugeVec).With(labels).
		Set(float64(bundle.Status.Summary.Ready))
	metrics["pending"].(*prometheus.GaugeVec).With(labels).
		Set(float64(bundle.Status.Summary.Pending))
	metrics["desired_ready"].(*prometheus.GaugeVec).With(labels).
		Set(float64(bundle.Status.Summary.DesiredReady))

	for _, state := range bundleStates {
		labels["state"] = string(state)

		if state == currentState {
			metrics["state"].(*prometheus.GaugeVec).With(labels).Set(1)
		} else {
			metrics["state"].(*prometheus.GaugeVec).With(labels).Set(0)
		}
	}
}



================================================
FILE: internal/metrics/bundledeployment_metrics.go
================================================
package metrics

import (
	"fmt"

	"github.com/rancher/fleet/internal/cmd/controller/summary"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
)

var (
	bundleDeploymentSubsystem = "bundledeployment"
	bundleDeploymentLabels    = []string{
		"name",
		"namespace",
		"cluster_name",
		"cluster_namespace",
		"repo",
		"commit",
		"bundle",
		"bundle_namespace",
		"generation",
		"state",
	}
	BundleDeploymentCollector = CollectorCollection{
		subsystem: bundleDeploymentSubsystem,
		metrics: map[string]prometheus.Collector{
			"state": promauto.NewGaugeVec(
				prometheus.GaugeOpts{
					Namespace: metricPrefix,
					Subsystem: bundleDeploymentSubsystem,
					Name:      "state",
					Help: "Shows the state of this bundle deployment based on the state label. " +
						"A value of 1 is true 0 is false.",
				},
				bundleDeploymentLabels,
			),
		},
		collector: collectBundleDeploymentMetrics,
	}
)

func collectBundleDeploymentMetrics(obj any, metrics map[string]prometheus.Collector) {
	bundleDep, ok := obj.(*fleet.BundleDeployment)
	if !ok {
		panic("unexpected object type")
	}

	currentState := summary.GetDeploymentState(bundleDep)
	labels := prometheus.Labels{
		"name":              bundleDep.Name,
		"namespace":         bundleDep.Namespace,
		"cluster_name":      bundleDep.Labels[fleet.ClusterLabel],
		"cluster_namespace": bundleDep.Labels[fleet.ClusterNamespaceLabel],
		"repo":              bundleDep.Labels[fleet.RepoLabel],
		"commit":            bundleDep.Labels[fleet.CommitLabel],
		"bundle":            bundleDep.Labels[fleet.BundleLabel],
		"bundle_namespace":  bundleDep.Labels[fleet.BundleNamespaceLabel],
		"generation":        fmt.Sprintf("%d", bundleDep.Generation),
		"state":             string(currentState),
	}

	for _, state := range bundleStates {
		labels["state"] = string(state)

		if state == currentState {
			metrics["state"].(*prometheus.GaugeVec).With(labels).Set(1)
		} else {
			metrics["state"].(*prometheus.GaugeVec).With(labels).Set(0)
		}
	}
}



================================================
FILE: internal/metrics/cluster_metrics.go
================================================
package metrics

import (
	"fmt"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
)

var (
	clusterSubsystem = "cluster"
	clusterLabels    = []string{
		"name",
		"namespace",
		// The name as given per "management.cattle.io/cluster-name" label. This
		// may but does not have to be different from `name` label and is added
		// by Rancher.
		"cluster_name",
		"cluster_display_name",
		"generation",
		"state",
	}

	clusterNameLabel        = "management.cattle.io/cluster-name"
	clusterDisplayNameLabel = "management.cattle.io/cluster-display-name"
	clusterStates           = []string{
		string(fleet.NotReady),
		string(fleet.Ready),
		"WaitCheckIn",
	}

	ClusterCollector = CollectorCollection{
		clusterSubsystem,
		clusterMetrics,
		collectClusterMetrics,
	}

	//nolint:dupl // Same pattern as in clustergroup metrics, but not the same definitions.
	clusterMetrics = map[string]prometheus.Collector{
		"desired_ready_git_repos": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterSubsystem,
				Name:      "desired_ready_git_repos",
				Help:      "The desired number of GitRepos to be in a ready state.",
			},
			clusterLabels,
		),
		"ready_git_repos": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterSubsystem,
				Name:      "ready_git_repos",
				Help:      "The number of GitRepos in a ready state.",
			},
			clusterLabels,
		),
		"desired_ready_helm_ops": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterSubsystem,
				Name:      "desired_ready_helm_ops",
				Help:      "The desired number of HelmOps to be in a ready state.",
			},
			clusterLabels,
		),
		"ready_helm_ops": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterSubsystem,
				Name:      "ready_helm_ops",
				Help:      "The number of HelmOps in a ready state.",
			},
			clusterLabels,
		),
		"resources_count_desiredready": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterSubsystem,
				Name:      "resources_count_desiredready",
				Help:      "The number of resources for the given cluster desired to be in the Ready state.",
			},
			clusterLabels,
		),
		"resources_count_missing": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterSubsystem,
				Name:      "resources_count_missing",
				Help:      "The number of resources in the Missing state.",
			},
			clusterLabels,
		),
		"resources_count_modified": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterSubsystem,
				Name:      "resources_count_modified",
				Help:      "The number of resources in the Modified state.",
			},
			clusterLabels,
		),
		"resources_count_notready": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterSubsystem,
				Name:      "resources_count_notready",
				Help:      "The number of resources in the NotReady state.",
			},
			clusterLabels,
		),
		"resources_count_orphaned": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterSubsystem,
				Name:      "resources_count_orphaned",
				Help:      "The number of resources in the Orphaned state.",
			},
			clusterLabels,
		),
		"resources_count_ready": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterSubsystem,
				Name:      "resources_count_ready",
				Help:      "The number of resources in the Ready state.",
			},
			clusterLabels,
		),
		"resources_count_unknown": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterSubsystem,
				Name:      "resources_count_unknown",
				Help:      "The number of resources in the Unknown state.",
			},
			clusterLabels,
		),
		"resources_count_waitapplied": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterSubsystem,
				Name:      "resources_count_waitapplied",
				Help:      "The number of resources in the WaitApplied state.",
			},
			clusterLabels,
		),
		"state": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterSubsystem,
				Name:      "state",
				Help:      "The current state of a given cluster",
			},
			clusterLabels,
		),
	}
)

func collectClusterMetrics(obj any, metrics map[string]prometheus.Collector) {
	cluster, ok := obj.(*fleet.Cluster)
	if !ok {
		panic("unexpected object type")
	}

	labels := prometheus.Labels{
		"name":                 cluster.Name,
		"namespace":            cluster.Namespace,
		"cluster_name":         cluster.Labels[clusterNameLabel],
		"cluster_display_name": cluster.Labels[clusterDisplayNameLabel],
		"generation":           fmt.Sprintf("%d", cluster.Generation),
		"state":                cluster.Status.Display.State,
	}

	metrics["desired_ready_git_repos"].(*prometheus.GaugeVec).
		With(labels).Set(float64(cluster.Status.DesiredReadyGitRepos))
	metrics["ready_git_repos"].(*prometheus.GaugeVec).
		With(labels).Set(float64(cluster.Status.ReadyGitRepos))
	metrics["desired_ready_helm_ops"].(*prometheus.GaugeVec).
		With(labels).Set(float64(cluster.Status.DesiredReadyHelmOps))
	metrics["ready_helm_ops"].(*prometheus.GaugeVec).
		With(labels).Set(float64(cluster.Status.ReadyHelmOps))
	metrics["resources_count_desiredready"].(*prometheus.GaugeVec).
		With(labels).Set(float64(cluster.Status.ResourceCounts.DesiredReady))
	metrics["resources_count_missing"].(*prometheus.GaugeVec).
		With(labels).Set(float64(cluster.Status.ResourceCounts.Missing))
	metrics["resources_count_modified"].(*prometheus.GaugeVec).
		With(labels).Set(float64(cluster.Status.ResourceCounts.Modified))
	metrics["resources_count_notready"].(*prometheus.GaugeVec).
		With(labels).Set(float64(cluster.Status.ResourceCounts.NotReady))
	metrics["resources_count_orphaned"].(*prometheus.GaugeVec).
		With(labels).Set(float64(cluster.Status.ResourceCounts.Orphaned))
	metrics["resources_count_ready"].(*prometheus.GaugeVec).
		With(labels).Set(float64(cluster.Status.ResourceCounts.Ready))
	metrics["resources_count_unknown"].(*prometheus.GaugeVec).
		With(labels).Set(float64(cluster.Status.ResourceCounts.Unknown))
	metrics["resources_count_waitapplied"].(*prometheus.GaugeVec).
		With(labels).Set(float64(cluster.Status.ResourceCounts.WaitApplied))

	for _, state := range clusterStates {
		labels["state"] = state

		if state == cluster.Status.Display.State {
			metrics["state"].(*prometheus.GaugeVec).With(labels).Set(1)
		} else {
			metrics["state"].(*prometheus.GaugeVec).With(labels).Set(0)
		}
	}
}



================================================
FILE: internal/metrics/clustergroup_metrics.go
================================================
package metrics

import (
	"fmt"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"
)

var (
	clusterGroupSubsystem = "cluster_group"
	clusterGroupLabels    = []string{"name", "namespace", "generation", "state"}
	clusterGroupStates    = []string{
		string(fleet.NotReady),
		string(fleet.Ready),
	}
	ClusterGroupCollector = CollectorCollection{
		clusterGroupSubsystem,
		clusterGroupMetrics,
		collectClusterGroupMetrics,
	}
	//nolint:dupl // Same pattern as in cluster metrics, but not the same definitions.
	clusterGroupMetrics = map[string]prometheus.Collector{
		"cluster_count": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterGroupSubsystem,
				Name:      "cluster_count",
				Help:      "The count of clusters in this cluster group.",
			},
			clusterGroupLabels,
		),
		"non_ready_cluster_count": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterGroupSubsystem,
				Name:      "non_ready_cluster_count",
				Help:      "The count of non ready clusters in this cluster group.",
			},
			clusterGroupLabels,
		),
		"resource_count_desired_ready": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterGroupSubsystem,
				Name:      "resource_count_desired_ready",
				Help:      "The count of resources that are desired to be in the Ready state.",
			},
			clusterGroupLabels,
		),
		"resource_count_missing": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterGroupSubsystem,
				Name:      "resource_count_missing",
				Help:      "The count of resources that are in a Missing state.",
			},
			clusterGroupLabels,
		),
		"resource_count_modified": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterGroupSubsystem,
				Name:      "resource_count_modified",
				Help:      "The count of resources that are in a Modified state.",
			},
			clusterGroupLabels,
		),
		"resource_count_notready": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterGroupSubsystem,
				Name:      "resource_count_notready",
				Help:      "The count of resources that are in a NotReady state.",
			},
			clusterGroupLabels,
		),
		"resource_count_orphaned": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterGroupSubsystem,
				Name:      "resource_count_orphaned",
				Help:      "The count of resources that are in an Orphaned state.",
			},
			clusterGroupLabels,
		),
		"resource_count_ready": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterGroupSubsystem,
				Name:      "resource_count_ready",
				Help:      "The count of resources that are in a Ready state.",
			},
			clusterGroupLabels,
		),
		"resource_count_unknown": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterGroupSubsystem,
				Name:      "resource_count_unknown",
				Help:      "The count of resources that are in an Unknown state.",
			},
			clusterGroupLabels,
		),
		"resource_count_waitapplied": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterGroupSubsystem,
				Name:      "resource_count_waitapplied",
				Help:      "The count of resources that are in a WaitApplied state.",
			},
			clusterGroupLabels,
		),
		"bundle_desired_ready": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterGroupSubsystem,
				Name:      "bundle_desired_ready",
				Help:      "The count of bundles that are desired to be in a Ready state.",
			},
			clusterGroupLabels,
		),
		"bundle_ready": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterGroupSubsystem,
				Name:      "bundle_ready",
				Help:      "The count of bundles that are in a Ready state in the Cluster Group.",
			},
			clusterGroupLabels,
		),
		"state": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: clusterGroupSubsystem,
				Name:      "state",
				Help:      "The current state of a given cluster group.",
			},
			clusterGroupLabels,
		),
	}
)

func collectClusterGroupMetrics(obj any, metrics map[string]prometheus.Collector) {
	clusterGroup, ok := obj.(*fleet.ClusterGroup)
	if !ok {
		panic("unexpected object type")
	}

	labels := prometheus.Labels{
		"name":       clusterGroup.Name,
		"namespace":  clusterGroup.Namespace,
		"generation": fmt.Sprintf("%d", clusterGroup.Generation),
		"state":      clusterGroup.Status.Display.State,
	}

	metrics["cluster_count"].(*prometheus.GaugeVec).With(labels).
		Set(float64(clusterGroup.Status.ClusterCount))
	metrics["non_ready_cluster_count"].(*prometheus.GaugeVec).With(labels).
		Set(float64(clusterGroup.Status.NonReadyClusterCount))
	metrics["resource_count_desired_ready"].(*prometheus.GaugeVec).With(labels).
		Set(float64(clusterGroup.Status.ResourceCounts.DesiredReady))
	metrics["resource_count_missing"].(*prometheus.GaugeVec).With(labels).
		Set(float64(clusterGroup.Status.ResourceCounts.Missing))
	metrics["resource_count_modified"].(*prometheus.GaugeVec).With(labels).
		Set(float64(clusterGroup.Status.ResourceCounts.Modified))
	metrics["resource_count_notready"].(*prometheus.GaugeVec).With(labels).
		Set(float64(clusterGroup.Status.ResourceCounts.NotReady))
	metrics["resource_count_orphaned"].(*prometheus.GaugeVec).With(labels).
		Set(float64(clusterGroup.Status.ResourceCounts.Orphaned))
	metrics["resource_count_ready"].(*prometheus.GaugeVec).With(labels).
		Set(float64(clusterGroup.Status.ResourceCounts.Ready))
	metrics["resource_count_unknown"].(*prometheus.GaugeVec).With(labels).
		Set(float64(clusterGroup.Status.ResourceCounts.Unknown))
	metrics["resource_count_waitapplied"].(*prometheus.GaugeVec).With(labels).
		Set(float64(clusterGroup.Status.ResourceCounts.WaitApplied))
	metrics["bundle_desired_ready"].(*prometheus.GaugeVec).With(labels).
		Set(float64(clusterGroup.Status.Summary.DesiredReady))
	metrics["bundle_ready"].(*prometheus.GaugeVec).With(labels).
		Set(float64(clusterGroup.Status.Summary.Ready))

	for _, state := range clusterGroupStates {
		labels["state"] = state

		if state == clusterGroup.Status.Display.State {
			metrics["state"].(*prometheus.GaugeVec).With(labels).Set(1)
		} else {
			metrics["state"].(*prometheus.GaugeVec).With(labels).Set(0)
		}
	}
}



================================================
FILE: internal/metrics/gitrepo_metrics.go
================================================
package metrics

import (
	"strings"

	"github.com/prometheus/client_golang/prometheus"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

var (
	gitRepoSubsystem = "gitrepo"
	gitRepoLabels    = []string{"name", "namespace", "repo", "branch", "paths"}
	GitRepoCollector = CollectorCollection{
		gitRepoSubsystem,
		gitRepoMetrics,
		collectGitRepoMetrics,
	}
	gitRepoMetrics        = getStatusMetrics(gitRepoSubsystem, gitRepoLabels)
	collectGitRepoMetrics = func(
		obj any,
		metrics map[string]prometheus.Collector,
	) {
		gitrepo, ok := obj.(*fleet.GitRepo)
		if !ok {
			panic("unexpected object type")
		}

		labels := prometheus.Labels{
			"name":      gitrepo.Name,
			"namespace": gitrepo.Namespace,
			"repo":      gitrepo.Spec.Repo,
			"branch":    gitrepo.Spec.Branch,
			"paths":     strings.Join(gitrepo.Spec.Paths, ";"),
		}

		metrics["desired_ready_clusters"].(*prometheus.GaugeVec).
			With(labels).Set(float64(gitrepo.Status.DesiredReadyClusters))
		metrics["ready_clusters"].(*prometheus.GaugeVec).
			With(labels).Set(float64(gitrepo.Status.ReadyClusters))
		metrics["resources_missing"].(*prometheus.GaugeVec).
			With(labels).Set(float64(gitrepo.Status.ResourceCounts.Missing))
		metrics["resources_modified"].(*prometheus.GaugeVec).
			With(labels).Set(float64(gitrepo.Status.ResourceCounts.Modified))
		metrics["resources_not_ready"].(*prometheus.GaugeVec).
			With(labels).Set(float64(gitrepo.Status.ResourceCounts.NotReady))
		metrics["resources_orphaned"].(*prometheus.GaugeVec).
			With(labels).Set(float64(gitrepo.Status.ResourceCounts.Orphaned))
		metrics["resources_desired_ready"].(*prometheus.GaugeVec).
			With(labels).Set(float64(gitrepo.Status.ResourceCounts.DesiredReady))
		metrics["resources_ready"].(*prometheus.GaugeVec).
			With(labels).Set(float64(gitrepo.Status.ResourceCounts.Ready))
		metrics["resources_unknown"].(*prometheus.GaugeVec).
			With(labels).Set(float64(gitrepo.Status.ResourceCounts.Unknown))
		metrics["resources_wait_applied"].(*prometheus.GaugeVec).
			With(labels).Set(float64(gitrepo.Status.ResourceCounts.WaitApplied))
	}
)



================================================
FILE: internal/metrics/helm_metrics.go
================================================
package metrics

import (
	"github.com/prometheus/client_golang/prometheus"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

var (
	helmSubsystem = "helmop"
	helmLabels    = []string{"name", "namespace", "repo", "chart", "version"}
	HelmCollector = CollectorCollection{
		helmSubsystem,
		helmMetrics,
		collectHelmMetrics,
	}
	helmMetrics        = getStatusMetrics(helmSubsystem, helmLabels)
	collectHelmMetrics = func(
		obj any,
		metrics map[string]prometheus.Collector,
	) {
		helm, ok := obj.(*fleet.HelmOp)
		if !ok {
			panic("unexpected object type")
		}

		labels := prometheus.Labels{
			"name":      helm.Name,
			"namespace": helm.Namespace,
			"repo":      helm.Spec.Helm.Repo,
			"chart":     helm.Spec.Helm.Chart,
			"version":   helm.Status.Version,
		}

		metrics["desired_ready_clusters"].(*prometheus.GaugeVec).
			With(labels).Set(float64(helm.Status.DesiredReadyClusters))
		metrics["ready_clusters"].(*prometheus.GaugeVec).
			With(labels).Set(float64(helm.Status.ReadyClusters))
		metrics["resources_missing"].(*prometheus.GaugeVec).
			With(labels).Set(float64(helm.Status.ResourceCounts.Missing))
		metrics["resources_modified"].(*prometheus.GaugeVec).
			With(labels).Set(float64(helm.Status.ResourceCounts.Modified))
		metrics["resources_not_ready"].(*prometheus.GaugeVec).
			With(labels).Set(float64(helm.Status.ResourceCounts.NotReady))
		metrics["resources_orphaned"].(*prometheus.GaugeVec).
			With(labels).Set(float64(helm.Status.ResourceCounts.Orphaned))
		metrics["resources_desired_ready"].(*prometheus.GaugeVec).
			With(labels).Set(float64(helm.Status.ResourceCounts.DesiredReady))
		metrics["resources_ready"].(*prometheus.GaugeVec).
			With(labels).Set(float64(helm.Status.ResourceCounts.Ready))
		metrics["resources_unknown"].(*prometheus.GaugeVec).
			With(labels).Set(float64(helm.Status.ResourceCounts.Unknown))
		metrics["resources_wait_applied"].(*prometheus.GaugeVec).
			With(labels).Set(float64(helm.Status.ResourceCounts.WaitApplied))
	}
)



================================================
FILE: internal/metrics/metrics.go
================================================
package metrics

import (
	"context"
	"errors"

	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promauto"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/metrics"
)

const (
	metricPrefix = "fleet"
)

var (
	bundleStates = []fleet.BundleState{
		fleet.Ready,
		fleet.NotReady,
		fleet.Pending,
		fleet.OutOfSync,
		fleet.Modified,
		fleet.WaitApplied,
		fleet.ErrApplied,
	}

	objMetrics = []prometheus.Collector{}
)

func registerObjMetrics() {
	for _, metric := range objMetrics {
		metrics.Registry.MustRegister(metric)
	}
}

func RegisterMetrics() {
	GitRepoCollector.Register()
	ClusterCollector.Register()
	ClusterGroupCollector.Register()
	BundleCollector.Register()
	BundleDeploymentCollector.Register()

	registerObjMetrics()
}

func RegisterGitOptsMetrics() {
	GitRepoCollector.Register()

	registerObjMetrics()
}

func RegisterHelmOpsMetrics() {
	HelmCollector.Register()

	registerObjMetrics()
}

// CollectorCollection implements the generic methods `Delete` and `Register`
// for a collection of Prometheus collectors. It is used to manage the lifecycle
// of a collection of Prometheus collectors.
type CollectorCollection struct {
	subsystem string
	metrics   map[string]prometheus.Collector
	collector func(obj any, metrics map[string]prometheus.Collector)
}

// Collect collects the metrics for the given object. It deletes the metrics for
// the object if they already exist and then collects the metrics for the
// object.
//
// The metrics need to be deleted because the values of the metrics may have
// changed and this would create a new instance of those metrics, keeping the
// old one around. Metrics are deleted by their name and namespace label values.
func (c *CollectorCollection) Collect(ctx context.Context, obj metav1.ObjectMetaAccessor) {
	logger := log.FromContext(ctx).WithName("metrics")
	defer func() {
		if r := recover(); r != nil {
			logger.Error(errors.New("error collecting metrics"), "observed panic", "panic", r)
		}
	}()
	c.Delete(obj.GetObjectMeta().GetName(), obj.GetObjectMeta().GetNamespace())
	c.collector(obj, c.metrics)
}

// Delete deletes the metric with the given name and namespace labels. It
// returns the number of metrics deleted. It does a DeletePartialMatch on the
// metric with the given name and namespace labels.
func (c *CollectorCollection) Delete(name, namespace string) (deleted int) {
	identityLabels := prometheus.Labels{
		"name":      name,
		"namespace": namespace,
	}
	for _, collector := range c.metrics {
		switch metric := collector.(type) {
		case *prometheus.MetricVec:
			deleted += metric.DeletePartialMatch(identityLabels)
		case *prometheus.CounterVec:
			deleted += metric.DeletePartialMatch(identityLabels)
		case *prometheus.GaugeVec:
			deleted += metric.DeletePartialMatch(identityLabels)
		default:
			panic("unexpected metric type")
		}
	}

	return deleted
}

func (c *CollectorCollection) Register() {
	for _, metric := range c.metrics {
		metrics.Registry.MustRegister(metric)
	}
}

func getStatusMetrics(subsystem string, labels []string) map[string]prometheus.Collector {
	return map[string]prometheus.Collector{
		"resources_desired_ready": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: subsystem,
				Name:      "resources_desired_ready",
				Help:      "The count of resources that are desired to be in a Ready state.",
			},
			labels,
		),
		"resources_missing": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: subsystem,
				Name:      "resources_missing",
				Help:      "The count of resources that are in a Missing state.",
			},
			labels,
		),
		"resources_modified": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: subsystem,
				Name:      "resources_modified",
				Help:      "The count of resources that are in a Modified state.",
			},
			labels,
		),
		"resources_not_ready": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: subsystem,
				Name:      "resources_not_ready",
				Help:      "The count of resources that are in a NotReady state.",
			},
			labels,
		),
		"resources_orphaned": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: subsystem,
				Name:      "resources_orphaned",
				Help:      "The count of resources that are in an Orphaned state.",
			},
			labels,
		),
		"resources_ready": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: subsystem,
				Name:      "resources_ready",
				Help:      "The count of resources that are in a Ready state.",
			},
			labels,
		),
		"resources_unknown": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: subsystem,
				Name:      "resources_unknown",
				Help:      "The count of resources that are in an Unknown state.",
			},
			labels,
		),
		"resources_wait_applied": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: subsystem,
				Name:      "resources_wait_applied",
				Help:      "The count of resources that are in a WaitApplied state.",
			},
			labels,
		),
		"desired_ready_clusters": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: subsystem,
				Name:      "desired_ready_clusters",
				Help:      "The amount of clusters desired to be in a ready state.",
			},
			labels,
		),
		"ready_clusters": promauto.NewGaugeVec(
			prometheus.GaugeOpts{
				Namespace: metricPrefix,
				Subsystem: subsystem,
				Name:      "ready_clusters",
				Help:      "The count of clusters in a Ready state.",
			},
			labels,
		),
	}
}

// ObjCounter creates and registers a new CounterVec metric with the given name and help
// text. The returned CounterVec embeds the CounterVec from the prometheus package and adds a method
// to increment the counter for a given object. The labels of the metric are determined from the
// name and the namespace of the given object.
func ObjCounter(name, help string) (c ObjCounterVec) {
	labels := []string{"name", "namespace"}

	counterVec := promauto.NewCounterVec(
		prometheus.CounterOpts{
			Namespace: metricPrefix,
			Name:      name,
			Help:      help,
		},
		labels,
	)

	objMetrics = append(objMetrics, counterVec)

	return ObjCounterVec{
		counterVec: counterVec,
		labels:     labels,
	}
}

// ObjCounterVec is a wrapper around prometheus.CounterVec that adds a method to increment the
// counter for a given metav1 object. The labels of the metric are determined from the name and the
type ObjCounterVec struct {
	counterVec *prometheus.CounterVec
	labels     []string
}

func (m *ObjCounterVec) Inc(obj metav1.Object) {
	m.counterVec.WithLabelValues(obj.GetName(), obj.GetNamespace()).Inc()
}

func (m *ObjCounterVec) DeleteByReq(req ctrl.Request) bool {
	return m.counterVec.DeleteLabelValues(req.Name, req.Namespace)
}

var BucketsLatency = []float64{.1, .2, .5, 1, 2, 5, 10, 30}

func ObjHistogram(name, help string, buckets []float64) (h ObjHistogramVec) {
	histogram := promauto.NewHistogramVec(
		prometheus.HistogramOpts{
			Namespace: metricPrefix,
			Name:      name,
			Help:      help,
			Buckets:   buckets,
		},
		[]string{"name", "namespace"},
	)

	objMetrics = append(objMetrics, histogram)

	return ObjHistogramVec{
		histogram: histogram,
		labels:    []string{"name", "namespace"},
	}
}

type ObjHistogramVec struct {
	histogram *prometheus.HistogramVec
	labels    []string
}

func (m *ObjHistogramVec) Observe(obj metav1.Object, value float64) {
	m.histogram.WithLabelValues(obj.GetName(), obj.GetNamespace()).Observe(value)
}

func (m *ObjHistogramVec) DeleteByReq(req ctrl.Request) bool {
	return m.histogram.DeleteLabelValues(req.Name, req.Namespace)
}

func ObjGauge(name, help string) (g ObjGaugeVec) {
	gauge := promauto.NewGaugeVec(
		prometheus.GaugeOpts{
			Namespace: metricPrefix,
			Name:      name,
			Help:      help,
		},
		[]string{"name", "namespace"},
	)

	objMetrics = append(objMetrics, gauge)

	return ObjGaugeVec{
		gauge:  gauge,
		labels: []string{"name", "namespace"},
	}
}

type ObjGaugeVec struct {
	gauge  *prometheus.GaugeVec
	labels []string
}

func (m *ObjGaugeVec) Set(obj metav1.Object, value float64) {
	m.gauge.WithLabelValues(obj.GetName(), obj.GetNamespace()).Set(value)
}

func (m *ObjGaugeVec) Delete(obj metav1.Object) bool {
	return m.gauge.DeleteLabelValues(obj.GetName(), obj.GetNamespace())
}



================================================
FILE: internal/mocks/client_mock.go
================================================
// Code generated by MockGen. DO NOT EDIT.
// Source: sigs.k8s.io/controller-runtime/pkg/client (interfaces: Client,SubResourceWriter)
//
// Generated by this command:
//
//	mockgen --build_flags=--mod=mod -destination=internal/mocks/client_mock.go -package=mocks -mock_names=Client=MockK8sClient,SubResourceWriter=MockSubResourceWriter sigs.k8s.io/controller-runtime/pkg/client Client,SubResourceWriter
//

// Package mocks is a generated GoMock package.
package mocks

import (
	context "context"
	reflect "reflect"

	gomock "go.uber.org/mock/gomock"
	meta "k8s.io/apimachinery/pkg/api/meta"
	runtime "k8s.io/apimachinery/pkg/runtime"
	schema "k8s.io/apimachinery/pkg/runtime/schema"
	client "sigs.k8s.io/controller-runtime/pkg/client"
)

// MockK8sClient is a mock of Client interface.
type MockK8sClient struct {
	ctrl     *gomock.Controller
	recorder *MockK8sClientMockRecorder
	isgomock struct{}
}

// MockK8sClientMockRecorder is the mock recorder for MockK8sClient.
type MockK8sClientMockRecorder struct {
	mock *MockK8sClient
}

// NewMockK8sClient creates a new mock instance.
func NewMockK8sClient(ctrl *gomock.Controller) *MockK8sClient {
	mock := &MockK8sClient{ctrl: ctrl}
	mock.recorder = &MockK8sClientMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockK8sClient) EXPECT() *MockK8sClientMockRecorder {
	return m.recorder
}

// Apply mocks base method.
func (m *MockK8sClient) Apply(ctx context.Context, obj runtime.ApplyConfiguration, opts ...client.ApplyOption) error {
	m.ctrl.T.Helper()
	varargs := []any{ctx, obj}
	for _, a := range opts {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "Apply", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// Apply indicates an expected call of Apply.
func (mr *MockK8sClientMockRecorder) Apply(ctx, obj any, opts ...any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]any{ctx, obj}, opts...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Apply", reflect.TypeOf((*MockK8sClient)(nil).Apply), varargs...)
}

// Create mocks base method.
func (m *MockK8sClient) Create(ctx context.Context, obj client.Object, opts ...client.CreateOption) error {
	m.ctrl.T.Helper()
	varargs := []any{ctx, obj}
	for _, a := range opts {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "Create", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// Create indicates an expected call of Create.
func (mr *MockK8sClientMockRecorder) Create(ctx, obj any, opts ...any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]any{ctx, obj}, opts...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Create", reflect.TypeOf((*MockK8sClient)(nil).Create), varargs...)
}

// Delete mocks base method.
func (m *MockK8sClient) Delete(ctx context.Context, obj client.Object, opts ...client.DeleteOption) error {
	m.ctrl.T.Helper()
	varargs := []any{ctx, obj}
	for _, a := range opts {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "Delete", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// Delete indicates an expected call of Delete.
func (mr *MockK8sClientMockRecorder) Delete(ctx, obj any, opts ...any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]any{ctx, obj}, opts...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Delete", reflect.TypeOf((*MockK8sClient)(nil).Delete), varargs...)
}

// DeleteAllOf mocks base method.
func (m *MockK8sClient) DeleteAllOf(ctx context.Context, obj client.Object, opts ...client.DeleteAllOfOption) error {
	m.ctrl.T.Helper()
	varargs := []any{ctx, obj}
	for _, a := range opts {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "DeleteAllOf", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// DeleteAllOf indicates an expected call of DeleteAllOf.
func (mr *MockK8sClientMockRecorder) DeleteAllOf(ctx, obj any, opts ...any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]any{ctx, obj}, opts...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "DeleteAllOf", reflect.TypeOf((*MockK8sClient)(nil).DeleteAllOf), varargs...)
}

// Get mocks base method.
func (m *MockK8sClient) Get(ctx context.Context, key client.ObjectKey, obj client.Object, opts ...client.GetOption) error {
	m.ctrl.T.Helper()
	varargs := []any{ctx, key, obj}
	for _, a := range opts {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "Get", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// Get indicates an expected call of Get.
func (mr *MockK8sClientMockRecorder) Get(ctx, key, obj any, opts ...any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]any{ctx, key, obj}, opts...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Get", reflect.TypeOf((*MockK8sClient)(nil).Get), varargs...)
}

// GroupVersionKindFor mocks base method.
func (m *MockK8sClient) GroupVersionKindFor(obj runtime.Object) (schema.GroupVersionKind, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "GroupVersionKindFor", obj)
	ret0, _ := ret[0].(schema.GroupVersionKind)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// GroupVersionKindFor indicates an expected call of GroupVersionKindFor.
func (mr *MockK8sClientMockRecorder) GroupVersionKindFor(obj any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "GroupVersionKindFor", reflect.TypeOf((*MockK8sClient)(nil).GroupVersionKindFor), obj)
}

// IsObjectNamespaced mocks base method.
func (m *MockK8sClient) IsObjectNamespaced(obj runtime.Object) (bool, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "IsObjectNamespaced", obj)
	ret0, _ := ret[0].(bool)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// IsObjectNamespaced indicates an expected call of IsObjectNamespaced.
func (mr *MockK8sClientMockRecorder) IsObjectNamespaced(obj any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "IsObjectNamespaced", reflect.TypeOf((*MockK8sClient)(nil).IsObjectNamespaced), obj)
}

// List mocks base method.
func (m *MockK8sClient) List(ctx context.Context, list client.ObjectList, opts ...client.ListOption) error {
	m.ctrl.T.Helper()
	varargs := []any{ctx, list}
	for _, a := range opts {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "List", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// List indicates an expected call of List.
func (mr *MockK8sClientMockRecorder) List(ctx, list any, opts ...any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]any{ctx, list}, opts...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "List", reflect.TypeOf((*MockK8sClient)(nil).List), varargs...)
}

// Patch mocks base method.
func (m *MockK8sClient) Patch(ctx context.Context, obj client.Object, patch client.Patch, opts ...client.PatchOption) error {
	m.ctrl.T.Helper()
	varargs := []any{ctx, obj, patch}
	for _, a := range opts {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "Patch", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// Patch indicates an expected call of Patch.
func (mr *MockK8sClientMockRecorder) Patch(ctx, obj, patch any, opts ...any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]any{ctx, obj, patch}, opts...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Patch", reflect.TypeOf((*MockK8sClient)(nil).Patch), varargs...)
}

// RESTMapper mocks base method.
func (m *MockK8sClient) RESTMapper() meta.RESTMapper {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "RESTMapper")
	ret0, _ := ret[0].(meta.RESTMapper)
	return ret0
}

// RESTMapper indicates an expected call of RESTMapper.
func (mr *MockK8sClientMockRecorder) RESTMapper() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "RESTMapper", reflect.TypeOf((*MockK8sClient)(nil).RESTMapper))
}

// Scheme mocks base method.
func (m *MockK8sClient) Scheme() *runtime.Scheme {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Scheme")
	ret0, _ := ret[0].(*runtime.Scheme)
	return ret0
}

// Scheme indicates an expected call of Scheme.
func (mr *MockK8sClientMockRecorder) Scheme() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Scheme", reflect.TypeOf((*MockK8sClient)(nil).Scheme))
}

// Status mocks base method.
func (m *MockK8sClient) Status() client.SubResourceWriter {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Status")
	ret0, _ := ret[0].(client.SubResourceWriter)
	return ret0
}

// Status indicates an expected call of Status.
func (mr *MockK8sClientMockRecorder) Status() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Status", reflect.TypeOf((*MockK8sClient)(nil).Status))
}

// SubResource mocks base method.
func (m *MockK8sClient) SubResource(subResource string) client.SubResourceClient {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "SubResource", subResource)
	ret0, _ := ret[0].(client.SubResourceClient)
	return ret0
}

// SubResource indicates an expected call of SubResource.
func (mr *MockK8sClientMockRecorder) SubResource(subResource any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "SubResource", reflect.TypeOf((*MockK8sClient)(nil).SubResource), subResource)
}

// Update mocks base method.
func (m *MockK8sClient) Update(ctx context.Context, obj client.Object, opts ...client.UpdateOption) error {
	m.ctrl.T.Helper()
	varargs := []any{ctx, obj}
	for _, a := range opts {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "Update", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// Update indicates an expected call of Update.
func (mr *MockK8sClientMockRecorder) Update(ctx, obj any, opts ...any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]any{ctx, obj}, opts...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Update", reflect.TypeOf((*MockK8sClient)(nil).Update), varargs...)
}

// MockSubResourceWriter is a mock of SubResourceWriter interface.
type MockSubResourceWriter struct {
	ctrl     *gomock.Controller
	recorder *MockSubResourceWriterMockRecorder
	isgomock struct{}
}

// MockSubResourceWriterMockRecorder is the mock recorder for MockSubResourceWriter.
type MockSubResourceWriterMockRecorder struct {
	mock *MockSubResourceWriter
}

// NewMockSubResourceWriter creates a new mock instance.
func NewMockSubResourceWriter(ctrl *gomock.Controller) *MockSubResourceWriter {
	mock := &MockSubResourceWriter{ctrl: ctrl}
	mock.recorder = &MockSubResourceWriterMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockSubResourceWriter) EXPECT() *MockSubResourceWriterMockRecorder {
	return m.recorder
}

// Create mocks base method.
func (m *MockSubResourceWriter) Create(ctx context.Context, obj, subResource client.Object, opts ...client.SubResourceCreateOption) error {
	m.ctrl.T.Helper()
	varargs := []any{ctx, obj, subResource}
	for _, a := range opts {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "Create", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// Create indicates an expected call of Create.
func (mr *MockSubResourceWriterMockRecorder) Create(ctx, obj, subResource any, opts ...any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]any{ctx, obj, subResource}, opts...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Create", reflect.TypeOf((*MockSubResourceWriter)(nil).Create), varargs...)
}

// Patch mocks base method.
func (m *MockSubResourceWriter) Patch(ctx context.Context, obj client.Object, patch client.Patch, opts ...client.SubResourcePatchOption) error {
	m.ctrl.T.Helper()
	varargs := []any{ctx, obj, patch}
	for _, a := range opts {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "Patch", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// Patch indicates an expected call of Patch.
func (mr *MockSubResourceWriterMockRecorder) Patch(ctx, obj, patch any, opts ...any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]any{ctx, obj, patch}, opts...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Patch", reflect.TypeOf((*MockSubResourceWriter)(nil).Patch), varargs...)
}

// Update mocks base method.
func (m *MockSubResourceWriter) Update(ctx context.Context, obj client.Object, opts ...client.SubResourceUpdateOption) error {
	m.ctrl.T.Helper()
	varargs := []any{ctx, obj}
	for _, a := range opts {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "Update", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// Update indicates an expected call of Update.
func (mr *MockSubResourceWriterMockRecorder) Update(ctx, obj any, opts ...any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]any{ctx, obj}, opts...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Update", reflect.TypeOf((*MockSubResourceWriter)(nil).Update), varargs...)
}



================================================
FILE: internal/mocks/eventrecorder_mock.go
================================================
// Code generated by MockGen. DO NOT EDIT.
// Source: k8s.io/client-go/tools/record (interfaces: EventRecorder)

// Package mocks is a generated GoMock package.
package mocks

import (
	reflect "reflect"

	gomock "go.uber.org/mock/gomock"
	runtime "k8s.io/apimachinery/pkg/runtime"
)

// MockEventRecorder is a mock of EventRecorder interface.
type MockEventRecorder struct {
	ctrl     *gomock.Controller
	recorder *MockEventRecorderMockRecorder
}

// MockEventRecorderMockRecorder is the mock recorder for MockEventRecorder.
type MockEventRecorderMockRecorder struct {
	mock *MockEventRecorder
}

// NewMockEventRecorder creates a new mock instance.
func NewMockEventRecorder(ctrl *gomock.Controller) *MockEventRecorder {
	mock := &MockEventRecorder{ctrl: ctrl}
	mock.recorder = &MockEventRecorderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockEventRecorder) EXPECT() *MockEventRecorderMockRecorder {
	return m.recorder
}

// AnnotatedEventf mocks base method.
func (m *MockEventRecorder) AnnotatedEventf(arg0 runtime.Object, arg1 map[string]string, arg2, arg3, arg4 string, arg5 ...interface{}) {
	m.ctrl.T.Helper()
	varargs := []interface{}{arg0, arg1, arg2, arg3, arg4}
	for _, a := range arg5 {
		varargs = append(varargs, a)
	}
	m.ctrl.Call(m, "AnnotatedEventf", varargs...)
}

// AnnotatedEventf indicates an expected call of AnnotatedEventf.
func (mr *MockEventRecorderMockRecorder) AnnotatedEventf(arg0, arg1, arg2, arg3, arg4 interface{}, arg5 ...interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]interface{}{arg0, arg1, arg2, arg3, arg4}, arg5...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AnnotatedEventf", reflect.TypeOf((*MockEventRecorder)(nil).AnnotatedEventf), varargs...)
}

// Event mocks base method.
func (m *MockEventRecorder) Event(arg0 runtime.Object, arg1, arg2, arg3 string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "Event", arg0, arg1, arg2, arg3)
}

// Event indicates an expected call of Event.
func (mr *MockEventRecorderMockRecorder) Event(arg0, arg1, arg2, arg3 interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Event", reflect.TypeOf((*MockEventRecorder)(nil).Event), arg0, arg1, arg2, arg3)
}

// Eventf mocks base method.
func (m *MockEventRecorder) Eventf(arg0 runtime.Object, arg1, arg2, arg3 string, arg4 ...interface{}) {
	m.ctrl.T.Helper()
	varargs := []interface{}{arg0, arg1, arg2, arg3}
	for _, a := range arg4 {
		varargs = append(varargs, a)
	}
	m.ctrl.Call(m, "Eventf", varargs...)
}

// Eventf indicates an expected call of Eventf.
func (mr *MockEventRecorderMockRecorder) Eventf(arg0, arg1, arg2, arg3 interface{}, arg4 ...interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]interface{}{arg0, arg1, arg2, arg3}, arg4...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Eventf", reflect.TypeOf((*MockEventRecorder)(nil).Eventf), varargs...)
}



================================================
FILE: internal/mocks/helm_deployer_mock.go
================================================
// Code generated by MockGen. DO NOT EDIT.
// Source: github.com/rancher/fleet/internal/cmd/agent/deployer/cleanup (interfaces: HelmDeployer)
//
// Generated by this command:
//
//	mockgen --build_flags=--mod=mod -destination=../../../../mocks/helm_deployer_mock.go -package=mocks github.com/rancher/fleet/internal/cmd/agent/deployer/cleanup HelmDeployer
//

// Package mocks is a generated GoMock package.
package mocks

import (
	context "context"
	reflect "reflect"

	helmdeployer "github.com/rancher/fleet/internal/helmdeployer"
	gomock "go.uber.org/mock/gomock"
)

// MockHelmDeployer is a mock of HelmDeployer interface.
type MockHelmDeployer struct {
	ctrl     *gomock.Controller
	recorder *MockHelmDeployerMockRecorder
	isgomock struct{}
}

// MockHelmDeployerMockRecorder is the mock recorder for MockHelmDeployer.
type MockHelmDeployerMockRecorder struct {
	mock *MockHelmDeployer
}

// NewMockHelmDeployer creates a new mock instance.
func NewMockHelmDeployer(ctrl *gomock.Controller) *MockHelmDeployer {
	mock := &MockHelmDeployer{ctrl: ctrl}
	mock.recorder = &MockHelmDeployerMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockHelmDeployer) EXPECT() *MockHelmDeployerMockRecorder {
	return m.recorder
}

// Delete mocks base method.
func (m *MockHelmDeployer) Delete(ctx context.Context, name string) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Delete", ctx, name)
	ret0, _ := ret[0].(error)
	return ret0
}

// Delete indicates an expected call of Delete.
func (mr *MockHelmDeployerMockRecorder) Delete(ctx, name any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Delete", reflect.TypeOf((*MockHelmDeployer)(nil).Delete), ctx, name)
}

// DeleteRelease mocks base method.
func (m *MockHelmDeployer) DeleteRelease(ctx context.Context, deployed helmdeployer.DeployedBundle) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "DeleteRelease", ctx, deployed)
	ret0, _ := ret[0].(error)
	return ret0
}

// DeleteRelease indicates an expected call of DeleteRelease.
func (mr *MockHelmDeployerMockRecorder) DeleteRelease(ctx, deployed any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "DeleteRelease", reflect.TypeOf((*MockHelmDeployer)(nil).DeleteRelease), ctx, deployed)
}

// ListDeployments mocks base method.
func (m *MockHelmDeployer) ListDeployments(list helmdeployer.ListAction) ([]helmdeployer.DeployedBundle, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "ListDeployments", list)
	ret0, _ := ret[0].([]helmdeployer.DeployedBundle)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// ListDeployments indicates an expected call of ListDeployments.
func (mr *MockHelmDeployerMockRecorder) ListDeployments(list any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "ListDeployments", reflect.TypeOf((*MockHelmDeployer)(nil).ListDeployments), list)
}

// NewListAction mocks base method.
func (m *MockHelmDeployer) NewListAction() helmdeployer.ListAction {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "NewListAction")
	ret0, _ := ret[0].(helmdeployer.ListAction)
	return ret0
}

// NewListAction indicates an expected call of NewListAction.
func (mr *MockHelmDeployerMockRecorder) NewListAction() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "NewListAction", reflect.TypeOf((*MockHelmDeployer)(nil).NewListAction))
}



================================================
FILE: internal/mocks/manifest_store_mock.go
================================================
// Code generated by MockGen. DO NOT EDIT.
// Source: github.com/rancher/fleet/internal/cmd/controller/reconciler (interfaces: Store)
//
// Generated by this command:
//
//	mockgen --build_flags=--mod=mod -destination=../../../mocks/manifest_store_mock.go -package=mocks github.com/rancher/fleet/internal/cmd/controller/reconciler Store
//

// Package mocks is a generated GoMock package.
package mocks

import (
	context "context"
	reflect "reflect"

	manifest "github.com/rancher/fleet/internal/manifest"
	gomock "go.uber.org/mock/gomock"
)

// MockStore is a mock of Store interface.
type MockStore struct {
	ctrl     *gomock.Controller
	recorder *MockStoreMockRecorder
}

// MockStoreMockRecorder is the mock recorder for MockStore.
type MockStoreMockRecorder struct {
	mock *MockStore
}

// NewMockStore creates a new mock instance.
func NewMockStore(ctrl *gomock.Controller) *MockStore {
	mock := &MockStore{ctrl: ctrl}
	mock.recorder = &MockStoreMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockStore) EXPECT() *MockStoreMockRecorder {
	return m.recorder
}

// Store mocks base method.
func (m *MockStore) Store(arg0 context.Context, arg1 *manifest.Manifest) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Store", arg0, arg1)
	ret0, _ := ret[0].(error)
	return ret0
}

// Store indicates an expected call of Store.
func (mr *MockStoreMockRecorder) Store(arg0, arg1 any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Store", reflect.TypeOf((*MockStore)(nil).Store), arg0, arg1)
}



================================================
FILE: internal/mocks/oci_client_mock.go
================================================
// Code generated by MockGen. DO NOT EDIT.
// Source: oras.land/oras-go/v2/registry/remote (interfaces: Client)
//
// Generated by this command:
//
//	mockgen --build_flags=--mod=mod -destination=../mocks/oci_client_mock.go -package=mocks -mock_names=Client=MockOCIClient oras.land/oras-go/v2/registry/remote Client
//

// Package mocks is a generated GoMock package.
package mocks

import (
	http "net/http"
	reflect "reflect"

	gomock "go.uber.org/mock/gomock"
)

// MockOCIClient is a mock of Client interface.
type MockOCIClient struct {
	ctrl     *gomock.Controller
	recorder *MockOCIClientMockRecorder
	isgomock struct{}
}

// MockOCIClientMockRecorder is the mock recorder for MockOCIClient.
type MockOCIClientMockRecorder struct {
	mock *MockOCIClient
}

// NewMockOCIClient creates a new mock instance.
func NewMockOCIClient(ctrl *gomock.Controller) *MockOCIClient {
	mock := &MockOCIClient{ctrl: ctrl}
	mock.recorder = &MockOCIClientMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockOCIClient) EXPECT() *MockOCIClientMockRecorder {
	return m.recorder
}

// Do mocks base method.
func (m *MockOCIClient) Do(arg0 *http.Request) (*http.Response, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Do", arg0)
	ret0, _ := ret[0].(*http.Response)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// Do indicates an expected call of Do.
func (mr *MockOCIClientMockRecorder) Do(arg0 any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Do", reflect.TypeOf((*MockOCIClient)(nil).Do), arg0)
}



================================================
FILE: internal/mocks/orastarget_mock.go
================================================
// Code generated by MockGen. DO NOT EDIT.
// Source: oras.land/oras-go/v2 (interfaces: Target)

// Package mocks is a generated GoMock package.
package mocks

import (
	context "context"
	io "io"
	reflect "reflect"

	v1 "github.com/opencontainers/image-spec/specs-go/v1"
	gomock "go.uber.org/mock/gomock"
)

// MockTarget is a mock of Target interface.
type MockTarget struct {
	ctrl     *gomock.Controller
	recorder *MockTargetMockRecorder
}

// MockTargetMockRecorder is the mock recorder for MockTarget.
type MockTargetMockRecorder struct {
	mock *MockTarget
}

// NewMockTarget creates a new mock instance.
func NewMockTarget(ctrl *gomock.Controller) *MockTarget {
	mock := &MockTarget{ctrl: ctrl}
	mock.recorder = &MockTargetMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockTarget) EXPECT() *MockTargetMockRecorder {
	return m.recorder
}

// Exists mocks base method.
func (m *MockTarget) Exists(arg0 context.Context, arg1 v1.Descriptor) (bool, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Exists", arg0, arg1)
	ret0, _ := ret[0].(bool)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// Exists indicates an expected call of Exists.
func (mr *MockTargetMockRecorder) Exists(arg0, arg1 interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Exists", reflect.TypeOf((*MockTarget)(nil).Exists), arg0, arg1)
}

// Fetch mocks base method.
func (m *MockTarget) Fetch(arg0 context.Context, arg1 v1.Descriptor) (io.ReadCloser, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Fetch", arg0, arg1)
	ret0, _ := ret[0].(io.ReadCloser)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// Fetch indicates an expected call of Fetch.
func (mr *MockTargetMockRecorder) Fetch(arg0, arg1 interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Fetch", reflect.TypeOf((*MockTarget)(nil).Fetch), arg0, arg1)
}

// Push mocks base method.
func (m *MockTarget) Push(arg0 context.Context, arg1 v1.Descriptor, arg2 io.Reader) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Push", arg0, arg1, arg2)
	ret0, _ := ret[0].(error)
	return ret0
}

// Push indicates an expected call of Push.
func (mr *MockTargetMockRecorder) Push(arg0, arg1, arg2 interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Push", reflect.TypeOf((*MockTarget)(nil).Push), arg0, arg1, arg2)
}

// Resolve mocks base method.
func (m *MockTarget) Resolve(arg0 context.Context, arg1 string) (v1.Descriptor, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Resolve", arg0, arg1)
	ret0, _ := ret[0].(v1.Descriptor)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// Resolve indicates an expected call of Resolve.
func (mr *MockTargetMockRecorder) Resolve(arg0, arg1 interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Resolve", reflect.TypeOf((*MockTarget)(nil).Resolve), arg0, arg1)
}

// Tag mocks base method.
func (m *MockTarget) Tag(arg0 context.Context, arg1 v1.Descriptor, arg2 string) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Tag", arg0, arg1, arg2)
	ret0, _ := ret[0].(error)
	return ret0
}

// Tag indicates an expected call of Tag.
func (mr *MockTargetMockRecorder) Tag(arg0, arg1, arg2 interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Tag", reflect.TypeOf((*MockTarget)(nil).Tag), arg0, arg1, arg2)
}



================================================
FILE: internal/mocks/reader_mock.go
================================================
// Code generated by MockGen. DO NOT EDIT.
// Source: sigs.k8s.io/controller-runtime/pkg/client (interfaces: Reader)

// Package mocks is a generated GoMock package.
package mocks

import (
	context "context"
	reflect "reflect"

	gomock "go.uber.org/mock/gomock"
	types "k8s.io/apimachinery/pkg/types"
	client "sigs.k8s.io/controller-runtime/pkg/client"
)

// MockReader is a mock of Reader interface.
type MockReader struct {
	ctrl     *gomock.Controller
	recorder *MockReaderMockRecorder
}

// MockReaderMockRecorder is the mock recorder for MockReader.
type MockReaderMockRecorder struct {
	mock *MockReader
}

// NewMockReader creates a new mock instance.
func NewMockReader(ctrl *gomock.Controller) *MockReader {
	mock := &MockReader{ctrl: ctrl}
	mock.recorder = &MockReaderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockReader) EXPECT() *MockReaderMockRecorder {
	return m.recorder
}

// Get mocks base method.
func (m *MockReader) Get(arg0 context.Context, arg1 types.NamespacedName, arg2 client.Object, arg3 ...client.GetOption) error {
	m.ctrl.T.Helper()
	varargs := []interface{}{arg0, arg1, arg2}
	for _, a := range arg3 {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "Get", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// Get indicates an expected call of Get.
func (mr *MockReaderMockRecorder) Get(arg0, arg1, arg2 interface{}, arg3 ...interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]interface{}{arg0, arg1, arg2}, arg3...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Get", reflect.TypeOf((*MockReader)(nil).Get), varargs...)
}

// List mocks base method.
func (m *MockReader) List(arg0 context.Context, arg1 client.ObjectList, arg2 ...client.ListOption) error {
	m.ctrl.T.Helper()
	varargs := []interface{}{arg0, arg1}
	for _, a := range arg2 {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "List", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// List indicates an expected call of List.
func (mr *MockReaderMockRecorder) List(arg0, arg1 interface{}, arg2 ...interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]interface{}{arg0, arg1}, arg2...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "List", reflect.TypeOf((*MockReader)(nil).List), varargs...)
}



================================================
FILE: internal/mocks/scheduler_mock.go
================================================
// Code generated by MockGen. DO NOT EDIT.
// Source: github.com/reugn/go-quartz/quartz (interfaces: Scheduler,ScheduledJob)
//
// Generated by this command:
//
//	mockgen --build_flags=--mod=mod -destination=../../../../mocks/scheduler_mock.go -package=mocks github.com/reugn/go-quartz/quartz Scheduler,ScheduledJob
//

// Package mocks is a generated GoMock package.
package mocks

import (
	context "context"
	reflect "reflect"

	quartz "github.com/reugn/go-quartz/quartz"
	gomock "go.uber.org/mock/gomock"
)

// MockScheduler is a mock of Scheduler interface.
type MockScheduler struct {
	ctrl     *gomock.Controller
	recorder *MockSchedulerMockRecorder
	isgomock struct{}
}

// MockSchedulerMockRecorder is the mock recorder for MockScheduler.
type MockSchedulerMockRecorder struct {
	mock *MockScheduler
}

// NewMockScheduler creates a new mock instance.
func NewMockScheduler(ctrl *gomock.Controller) *MockScheduler {
	mock := &MockScheduler{ctrl: ctrl}
	mock.recorder = &MockSchedulerMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockScheduler) EXPECT() *MockSchedulerMockRecorder {
	return m.recorder
}

// Clear mocks base method.
func (m *MockScheduler) Clear() error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Clear")
	ret0, _ := ret[0].(error)
	return ret0
}

// Clear indicates an expected call of Clear.
func (mr *MockSchedulerMockRecorder) Clear() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Clear", reflect.TypeOf((*MockScheduler)(nil).Clear))
}

// DeleteJob mocks base method.
func (m *MockScheduler) DeleteJob(jobKey *quartz.JobKey) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "DeleteJob", jobKey)
	ret0, _ := ret[0].(error)
	return ret0
}

// DeleteJob indicates an expected call of DeleteJob.
func (mr *MockSchedulerMockRecorder) DeleteJob(jobKey any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "DeleteJob", reflect.TypeOf((*MockScheduler)(nil).DeleteJob), jobKey)
}

// GetJobKeys mocks base method.
func (m *MockScheduler) GetJobKeys(arg0 ...quartz.Matcher[quartz.ScheduledJob]) ([]*quartz.JobKey, error) {
	m.ctrl.T.Helper()
	varargs := []any{}
	for _, a := range arg0 {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "GetJobKeys", varargs...)
	ret0, _ := ret[0].([]*quartz.JobKey)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// GetJobKeys indicates an expected call of GetJobKeys.
func (mr *MockSchedulerMockRecorder) GetJobKeys(arg0 ...any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "GetJobKeys", reflect.TypeOf((*MockScheduler)(nil).GetJobKeys), arg0...)
}

// GetScheduledJob mocks base method.
func (m *MockScheduler) GetScheduledJob(jobKey *quartz.JobKey) (quartz.ScheduledJob, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "GetScheduledJob", jobKey)
	ret0, _ := ret[0].(quartz.ScheduledJob)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// GetScheduledJob indicates an expected call of GetScheduledJob.
func (mr *MockSchedulerMockRecorder) GetScheduledJob(jobKey any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "GetScheduledJob", reflect.TypeOf((*MockScheduler)(nil).GetScheduledJob), jobKey)
}

// IsStarted mocks base method.
func (m *MockScheduler) IsStarted() bool {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "IsStarted")
	ret0, _ := ret[0].(bool)
	return ret0
}

// IsStarted indicates an expected call of IsStarted.
func (mr *MockSchedulerMockRecorder) IsStarted() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "IsStarted", reflect.TypeOf((*MockScheduler)(nil).IsStarted))
}

// PauseJob mocks base method.
func (m *MockScheduler) PauseJob(jobKey *quartz.JobKey) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "PauseJob", jobKey)
	ret0, _ := ret[0].(error)
	return ret0
}

// PauseJob indicates an expected call of PauseJob.
func (mr *MockSchedulerMockRecorder) PauseJob(jobKey any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "PauseJob", reflect.TypeOf((*MockScheduler)(nil).PauseJob), jobKey)
}

// ResumeJob mocks base method.
func (m *MockScheduler) ResumeJob(jobKey *quartz.JobKey) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "ResumeJob", jobKey)
	ret0, _ := ret[0].(error)
	return ret0
}

// ResumeJob indicates an expected call of ResumeJob.
func (mr *MockSchedulerMockRecorder) ResumeJob(jobKey any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "ResumeJob", reflect.TypeOf((*MockScheduler)(nil).ResumeJob), jobKey)
}

// ScheduleJob mocks base method.
func (m *MockScheduler) ScheduleJob(jobDetail *quartz.JobDetail, trigger quartz.Trigger) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "ScheduleJob", jobDetail, trigger)
	ret0, _ := ret[0].(error)
	return ret0
}

// ScheduleJob indicates an expected call of ScheduleJob.
func (mr *MockSchedulerMockRecorder) ScheduleJob(jobDetail, trigger any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "ScheduleJob", reflect.TypeOf((*MockScheduler)(nil).ScheduleJob), jobDetail, trigger)
}

// Start mocks base method.
func (m *MockScheduler) Start(arg0 context.Context) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "Start", arg0)
}

// Start indicates an expected call of Start.
func (mr *MockSchedulerMockRecorder) Start(arg0 any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Start", reflect.TypeOf((*MockScheduler)(nil).Start), arg0)
}

// Stop mocks base method.
func (m *MockScheduler) Stop() {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "Stop")
}

// Stop indicates an expected call of Stop.
func (mr *MockSchedulerMockRecorder) Stop() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Stop", reflect.TypeOf((*MockScheduler)(nil).Stop))
}

// Wait mocks base method.
func (m *MockScheduler) Wait(arg0 context.Context) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "Wait", arg0)
}

// Wait indicates an expected call of Wait.
func (mr *MockSchedulerMockRecorder) Wait(arg0 any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Wait", reflect.TypeOf((*MockScheduler)(nil).Wait), arg0)
}

// MockScheduledJob is a mock of ScheduledJob interface.
type MockScheduledJob struct {
	ctrl     *gomock.Controller
	recorder *MockScheduledJobMockRecorder
	isgomock struct{}
}

// MockScheduledJobMockRecorder is the mock recorder for MockScheduledJob.
type MockScheduledJobMockRecorder struct {
	mock *MockScheduledJob
}

// NewMockScheduledJob creates a new mock instance.
func NewMockScheduledJob(ctrl *gomock.Controller) *MockScheduledJob {
	mock := &MockScheduledJob{ctrl: ctrl}
	mock.recorder = &MockScheduledJobMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockScheduledJob) EXPECT() *MockScheduledJobMockRecorder {
	return m.recorder
}

// JobDetail mocks base method.
func (m *MockScheduledJob) JobDetail() *quartz.JobDetail {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "JobDetail")
	ret0, _ := ret[0].(*quartz.JobDetail)
	return ret0
}

// JobDetail indicates an expected call of JobDetail.
func (mr *MockScheduledJobMockRecorder) JobDetail() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "JobDetail", reflect.TypeOf((*MockScheduledJob)(nil).JobDetail))
}

// NextRunTime mocks base method.
func (m *MockScheduledJob) NextRunTime() int64 {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "NextRunTime")
	ret0, _ := ret[0].(int64)
	return ret0
}

// NextRunTime indicates an expected call of NextRunTime.
func (mr *MockScheduledJobMockRecorder) NextRunTime() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "NextRunTime", reflect.TypeOf((*MockScheduledJob)(nil).NextRunTime))
}

// Trigger mocks base method.
func (m *MockScheduledJob) Trigger() quartz.Trigger {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Trigger")
	ret0, _ := ret[0].(quartz.Trigger)
	return ret0
}

// Trigger indicates an expected call of Trigger.
func (mr *MockScheduledJobMockRecorder) Trigger() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Trigger", reflect.TypeOf((*MockScheduledJob)(nil).Trigger))
}



================================================
FILE: internal/mocks/status_writer_mock.go
================================================
// Code generated by MockGen. DO NOT EDIT.
// Source: sigs.k8s.io/controller-runtime/pkg/client (interfaces: SubResourceWriter)
// Generated by this command:
//
//	mockgen --build_flags=--mod=mod -destination=internal/mocks/status_writer_mock.go -package=mocks -mock_names=SubResourceWriter=MockStatusWriter sigs.k8s.io/controller-runtime/pkg/client SubResourceWriter
//
// Package mocks is a generated GoMock package.
package mocks

import (
	context "context"
	reflect "reflect"

	gomock "go.uber.org/mock/gomock"
	client "sigs.k8s.io/controller-runtime/pkg/client"
)

// MockStatusWriter is a mock of SubResourceWriter interface.
type MockStatusWriter struct {
	ctrl     *gomock.Controller
	recorder *MockStatusWriterMockRecorder
}

// MockStatusWriterMockRecorder is the mock recorder for MockStatusWriter.
type MockStatusWriterMockRecorder struct {
	mock *MockStatusWriter
}

// NewMockStatusWriter creates a new mock instance.
func NewMockStatusWriter(ctrl *gomock.Controller) *MockStatusWriter {
	mock := &MockStatusWriter{ctrl: ctrl}
	mock.recorder = &MockStatusWriterMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockStatusWriter) EXPECT() *MockStatusWriterMockRecorder {
	return m.recorder
}

// Create mocks base method.
func (m *MockStatusWriter) Create(arg0 context.Context, arg1, arg2 client.Object, arg3 ...client.SubResourceCreateOption) error {
	m.ctrl.T.Helper()
	varargs := []interface{}{arg0, arg1, arg2}
	for _, a := range arg3 {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "Create", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// Create indicates an expected call of Create.
func (mr *MockStatusWriterMockRecorder) Create(arg0, arg1, arg2 interface{}, arg3 ...interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]interface{}{arg0, arg1, arg2}, arg3...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Create", reflect.TypeOf((*MockStatusWriter)(nil).Create), varargs...)
}

// Patch mocks base method.
func (m *MockStatusWriter) Patch(arg0 context.Context, arg1 client.Object, arg2 client.Patch, arg3 ...client.SubResourcePatchOption) error {
	m.ctrl.T.Helper()
	varargs := []interface{}{arg0, arg1, arg2}
	for _, a := range arg3 {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "Patch", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// Patch indicates an expected call of Patch.
func (mr *MockStatusWriterMockRecorder) Patch(arg0, arg1, arg2 interface{}, arg3 ...interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]interface{}{arg0, arg1, arg2}, arg3...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Patch", reflect.TypeOf((*MockStatusWriter)(nil).Patch), varargs...)
}

// Update mocks base method.
func (m *MockStatusWriter) Update(arg0 context.Context, arg1 client.Object, arg2 ...client.SubResourceUpdateOption) error {
	m.ctrl.T.Helper()
	varargs := []interface{}{arg0, arg1}
	for _, a := range arg2 {
		varargs = append(varargs, a)
	}
	ret := m.ctrl.Call(m, "Update", varargs...)
	ret0, _ := ret[0].(error)
	return ret0
}

// Update indicates an expected call of Update.
func (mr *MockStatusWriterMockRecorder) Update(arg0, arg1 interface{}, arg2 ...interface{}) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	varargs := append([]interface{}{arg0, arg1}, arg2...)
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Update", reflect.TypeOf((*MockStatusWriter)(nil).Update), varargs...)
}



================================================
FILE: internal/mocks/target_builder_mock.go
================================================
// Code generated by MockGen. DO NOT EDIT.
// Source: github.com/rancher/fleet/internal/cmd/controller/reconciler (interfaces: TargetBuilder)
//
// Generated by this command:
//
//	mockgen --build_flags=--mod=mod -destination=../../../mocks/target_builder_mock.go -package=mocks github.com/rancher/fleet/internal/cmd/controller/reconciler TargetBuilder
//

// Package mocks is a generated GoMock package.
package mocks

import (
	context "context"
	reflect "reflect"

	target "github.com/rancher/fleet/internal/cmd/controller/target"
	v1alpha1 "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	gomock "go.uber.org/mock/gomock"
)

// MockTargetBuilder is a mock of TargetBuilder interface.
type MockTargetBuilder struct {
	ctrl     *gomock.Controller
	recorder *MockTargetBuilderMockRecorder
}

// MockTargetBuilderMockRecorder is the mock recorder for MockTargetBuilder.
type MockTargetBuilderMockRecorder struct {
	mock *MockTargetBuilder
}

// NewMockTargetBuilder creates a new mock instance.
func NewMockTargetBuilder(ctrl *gomock.Controller) *MockTargetBuilder {
	mock := &MockTargetBuilder{ctrl: ctrl}
	mock.recorder = &MockTargetBuilderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockTargetBuilder) EXPECT() *MockTargetBuilderMockRecorder {
	return m.recorder
}

// Targets mocks base method.
func (m *MockTargetBuilder) Targets(arg0 context.Context, arg1 *v1alpha1.Bundle, arg2 string) ([]*target.Target, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Targets", arg0, arg1, arg2)
	ret0, _ := ret[0].([]*target.Target)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// Targets indicates an expected call of Targets.
func (mr *MockTargetBuilderMockRecorder) Targets(arg0, arg1, arg2 any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Targets", reflect.TypeOf((*MockTargetBuilder)(nil).Targets), arg0, arg1, arg2)
}



================================================
FILE: internal/names/keyhash.go
================================================
package names

import (
	"crypto/sha256"
	"encoding/hex"
)

// KeyHash returns the first 12 hex characters of the hash of the first 100 chars
// of the input string
func KeyHash(s string) string {
	if len(s) > 100 {
		s = s[:100]
	}
	d := sha256.Sum256([]byte(s))
	return hex.EncodeToString(d[:])[:12]
}



================================================
FILE: internal/names/name.go
================================================
// Package names provides functions for truncating and hashing strings and for generating valid k8s resource names.
package names

import (
	"crypto/md5" //nolint:gosec // Non-crypto use
	"encoding/hex"
	"fmt"
	"regexp"
	"strings"

	"github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/sirupsen/logrus"
)

var (
	disallowedChars = regexp.MustCompile(`[^a-zA-Z0-9-]+`)
	helmReleaseName = regexp.MustCompile(`^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$`)
	dnsLabelSafe    = regexp.MustCompile(`^[a-z0-9]([-a-z0-9]*[a-z0-9])?$`)
	multiDash       = regexp.MustCompile("-+")
)

// Limit the length of a string to count characters. If the string's length is
// greater than count, it will be truncated and a separator will be appended to
// the end, followed by a hash.
// If the last character of the truncated string is the separator, then the
// separator itself is omitted. This prevents the result from containing two
// consecutive separators. In such a case, the result will be [count -1]
// characters long.
// If count is too small to include the shortened hash the string is simply
// truncated.
func Limit(s string, count int) string {
	if len(s) <= count {
		return s
	}

	const hexLen int = 5
	separator := "-"

	if count <= hexLen+len(separator) {
		return s[:count]
	}

	nbCharsBeforeTrim := count - hexLen - len(separator)

	// If the last character of the truncated string is the separator, include it instead of the separator.
	if string(s[nbCharsBeforeTrim-1]) == separator {
		separator = ""
	}

	return fmt.Sprintf("%s%s%s", s[:nbCharsBeforeTrim], separator, Hex(s, hexLen))
}

// Hex returns a hex-encoded hash of the string and truncates it to length.
// Warning: truncating the 32 character hash makes collisions more likely.
func Hex(s string, length int) string {
	h := md5.Sum([]byte(s)) //nolint:gosec // Non-crypto use
	d := hex.EncodeToString(h[:])
	return d[:length]
}

// HelmReleaseName uses the provided string to create a unique name. The
// resulting name is DNS label safe (RFC1123) and complies with Helm's regex
// for release names.
func HelmReleaseName(str string) string {
	needHex := false
	bak := str

	str = strings.ReplaceAll(str, "/", "-")

	// avoid collision from different case
	if str != strings.ToLower(str) {
		needHex = true
	}

	// avoid collision from disallowed characters
	if disallowedChars.MatchString(str) {
		needHex = true
	}

	if needHex {
		// append checksum before cleaning up the string
		str = fmt.Sprintf("%s-%s", str, Hex(str, 8))
	}

	// clean up new name
	str = strings.ToLower(str)
	str = disallowedChars.ReplaceAllLiteralString(str, "-")
	str = multiDash.ReplaceAllString(str, "-")
	str = strings.TrimLeft(str, "-")
	str = strings.TrimRight(str, "-")

	// shorten name to 53 characters, the limit for helm release names
	if helmReleaseName.MatchString(str) && dnsLabelSafe.MatchString(str) {
		short := Limit(str, v1alpha1.MaxHelmReleaseNameLen)
		if short != str {
			logrus.Debugf("shorten bundle name %v to %v", str, short)
		}
		return short
	}

	// if the string ends up empty or otherwise invalid, fall back to just
	// a checksum of the original input
	logrus.Debugf("couldn't derive a valid bundle name, using checksum instead for '%s'", str)
	return Hex(bak, 24)
}



================================================
FILE: internal/names/name_test.go
================================================
package names_test

import (
	"fmt"

	"github.com/rancher/fleet/internal/names"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

var _ = Describe("Name", func() {
	type test struct {
		arg    string
		result string
		n      int
	}

	const (
		str50 = "1234567890" + "1234567890" + "1234567890" + "1234567890" + "1234567890"
		str53 = str50 + "123"
		str63 = str50 + "1234567890" + "123"
	)

	Context("Limit", func() {
		tests := []test{
			{arg: "1234567", n: 5, result: "12345"},
			{arg: "1234567", n: 6, result: "123456"},
			{arg: "1234567", n: 7, result: "1234567"},
			{arg: "1234567", n: 8, result: "1234567"},
			{arg: "12345678", n: 8, result: "12345678"},
			{arg: "12345678", n: 7, result: "1-25d55"},
			{arg: "123456789", n: 8, result: "12-25f9e"},
			{arg: "1-3456789", n: 8, result: "1-9b657"}, // no double dash in the result
		}

		It("matches expected results", func() {
			for _, t := range tests {
				r := names.Limit(t.arg, t.n)
				Expect(r).To(Equal(t.result), fmt.Sprintf("%#v", t))
			}
		})
	})

	Context("HelmReleaseName", func() {
		tests := []test{
			{arg: str53, result: str53},
			{arg: str53 + "a", result: "12345678901234567890123456789012345678901234567-fdba4"},
			{arg: str63 + "a", result: "12345678901234567890123456789012345678901234567-eb12d"},
			{arg: "long-name-test-shortpath-with@char", result: "long-name-test-shortpath-with-char-031bab5e"},
			{arg: "long-name-test-shortpath-with+char", result: "long-name-test-shortpath-with-char-21c88393"},
			{arg: "long-name-test-0.App_ ", result: "long-name-test-0-app-5bf6b3fb"},
			{arg: "long-name-test--App_-_12.factor", result: "long-name-test-app-12-factor-0efbac37"},
			{arg: "bundle.name.example.com", result: "bundle-name-example-com-645ef821"},
			// no double dash in the result
			{arg: str53[0:46] + "-1234567", result: "1234567890123456789012345678901234567890123456-d0bce"},
		}

		It("matches expected results", func() {
			for _, t := range tests {
				r := names.HelmReleaseName(t.arg)
				Expect(r).To(Equal(t.result), fmt.Sprintf("%#v", t))
			}
		})
	})
})



================================================
FILE: internal/names/safeconcat.go
================================================
package names

import (
	"crypto/sha256"
	"encoding/hex"
	"strings"
)

// SafeConcatName concatenates the given strings and ensures the returned name is under 64 characters
// by cutting the string off at 57 characters and setting the last 6 with an encoded version of the concatenated string.
func SafeConcatName(name ...string) string {
	fullPath := strings.Join(name, "-")
	if len(fullPath) < 64 {
		return fullPath
	}
	digest := sha256.Sum256([]byte(fullPath))
	// since we cut the string in the middle, the last char may not be compatible with what is expected in k8s
	// we are checking and if necessary removing the last char
	c := fullPath[56]
	if 'a' <= c && c <= 'z' || '0' <= c && c <= '9' {
		return fullPath[0:57] + "-" + hex.EncodeToString(digest[0:])[0:5]
	}

	return fullPath[0:56] + "-" + hex.EncodeToString(digest[0:])[0:6]
}



================================================
FILE: internal/names/safeconcat_test.go
================================================
package names

import (
	"testing"
)

const (
	string32 = "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"
	string63 = "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"
	string64 = "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"
)

func TestSafeConcatName(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name   string
		input  []string
		output string
	}{
		{
			name:   "empty input",
			output: "",
		},
		{
			name:   "single string",
			input:  []string{string63},
			output: string63,
		},
		{
			name:   "single long string",
			input:  []string{string64},
			output: "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-ffe05",
		},
		{
			name:   "concatenate strings",
			input:  []string{"first", "second", "third"},
			output: "first-second-third",
		},
		{
			name:   "concatenate past 64 characters",
			input:  []string{string32, string32},
			output: "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-aaaaaaaaaaaaaaaaaaaaaaaa-da5ed",
		},
		{
			name:   "last character after truncation is not alphanumeric",
			input:  []string{"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-aaaaaaa"},
			output: "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-768c62",
		},
		{
			name:   "last characters after truncation aren't alphanumeric",
			input:  []string{"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa--aaaaaaa"},
			output: "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa--9e8cfe",
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			if got := SafeConcatName(tt.input...); got != tt.output {
				t.Errorf("SafeConcatName() = %v, want %v", got, tt.output)
			}
		})
	}
}



================================================
FILE: internal/names/suite_test.go
================================================
package names_test

import (
	"testing"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

func TestNames(t *testing.T) {
	RegisterFailHandler(Fail)
	RunSpecs(t, "Name Suite")
}



================================================
FILE: internal/namespaces/namespaces.go
================================================
package namespaces

import "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

// GetDeploymentNS returns the target namespace for a Fleet deployment.
// A non-empty TargetNamespace in the provided bundle deployment options `o` has precedence over a non-empty
// DefaultNamespace in those same options.
// If no namespace is specified in options, GetDeploymentNS returns the provided defaultNS.
func GetDeploymentNS(defaultNS string, o v1alpha1.BundleDeploymentOptions) string {
	if o.TargetNamespace != "" {
		return o.TargetNamespace
	}

	if o.DefaultNamespace != "" {
		return o.DefaultNamespace
	}

	return defaultNS
}



================================================
FILE: internal/ocistorage/ociwrapper.go
================================================
package ocistorage

import (
	"bytes"
	"context"
	"crypto/tls"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"os"
	"path/filepath"
	"strconv"
	"strings"

	"github.com/opencontainers/go-digest"
	ocispec "github.com/opencontainers/image-spec/specs-go/v1"
	"github.com/rancher/fleet/internal/manifest"
	"oras.land/oras-go/v2"
	"oras.land/oras-go/v2/content"
	"oras.land/oras-go/v2/content/memory"
	"oras.land/oras-go/v2/registry/remote"
	"oras.land/oras-go/v2/registry/remote/auth"
	"oras.land/oras-go/v2/registry/remote/retry"
)

const (
	fileType     = "application/fleet.file"
	artifactType = "application/fleet.manifest"

	OCIStorageFlag = "OCI_STORAGE"
)

type OCIOpts struct {
	Reference       string
	Username        string
	Password        string
	AgentUsername   string
	AgentPassword   string
	BasicHTTP       bool
	InsecureSkipTLS bool
}

type OrasOps interface {
	PackManifest(ctx context.Context, pusher content.Pusher, packManifestVersion oras.PackManifestVersion, artifactType string, opts oras.PackManifestOptions) (ocispec.Descriptor, error)
	Copy(ctx context.Context, src oras.ReadOnlyTarget, srcRef string, dst oras.Target, dstRef string, opts oras.CopyOptions) (ocispec.Descriptor, error)
	NewStore() oras.Target
}

type OrasOperator struct{}

func (o *OrasOperator) NewStore() oras.Target {
	return memory.New()
}

func (o *OrasOperator) PackManifest(ctx context.Context, pusher content.Pusher, packManifestVersion oras.PackManifestVersion, artifactType string, opts oras.PackManifestOptions) (ocispec.Descriptor, error) {
	return oras.PackManifest(ctx, pusher, packManifestVersion, artifactType, opts)
}

func (o *OrasOperator) Copy(ctx context.Context, src oras.ReadOnlyTarget, srcRef string, dst oras.Target, dstRef string, opts oras.CopyOptions) (ocispec.Descriptor, error) {
	return oras.Copy(ctx, src, srcRef, dst, dstRef, opts)
}

type OCIWrapper struct {
	oci OrasOps
}

func NewOCIWrapper() *OCIWrapper {
	return &OCIWrapper{
		oci: &OrasOperator{},
	}
}

func getHTTPClient(insecureSkipTLS bool) *http.Client {
	if !insecureSkipTLS {
		return retry.DefaultClient
	}
	return &http.Client{
		Transport: &http.Transport{
			TLSClientConfig: &tls.Config{InsecureSkipVerify: true}, // #nosec G402
		},
	}
}

func getAuthClient(opts OCIOpts) *auth.Client {
	client := &auth.Client{
		Client: getHTTPClient(opts.InsecureSkipTLS),
		Cache:  auth.NewCache(),
	}
	if opts.Username != "" {
		cred := auth.Credential{
			Username: opts.Username,
			Password: opts.Password,
		}
		client.Credential = func(ctx context.Context, s string) (auth.Credential, error) {
			return cred, nil
		}
	}
	return client
}

func newOCIRepository(id string, opts OCIOpts) (*remote.Repository, error) {
	repo, err := remote.NewRepository(join(opts.Reference, id))
	if err != nil {
		return nil, err
	}
	repo.PlainHTTP = opts.BasicHTTP
	repo.Client = getAuthClient(opts)
	return repo, nil
}

// join cleans and joins the elements with slash. We avoid filepath.Join, since
// it uses backslashes on Windows.
func join(elem ...string) string {
	for i, e := range elem {
		if e != "" {
			return filepath.Clean(strings.Join(elem[i:], "/"))
		}
	}
	return ""
}

func getDataFromDescriptor(ctx context.Context, store oras.Target, desc ocispec.Descriptor) ([]byte, error) {
	rc, err := store.Fetch(ctx, desc)
	if err != nil {
		return nil, err
	}
	data, err := io.ReadAll(rc)
	if err != nil {
		return nil, err
	}
	err = rc.Close()
	if err != nil {
		return nil, err
	}
	return data, nil
}

func checkIDAnnotation(desc ocispec.Descriptor, id string) error {
	if len(desc.Annotations) != 1 {
		return fmt.Errorf("expecting 1 Annotation in layer descriptor. Found %d", len(desc.Annotations))
	}
	idFound, ok := desc.Annotations["id"]
	if !ok || idFound != id {
		return fmt.Errorf("could not find expected id in Descriptor's annotations")
	}
	return nil
}

func (o *OCIWrapper) pushFile(ctx context.Context, opts OCIOpts, reader io.Reader, desc ocispec.Descriptor, id string) error {
	s := o.oci.NewStore()
	err := s.Push(ctx, desc, reader)
	if err != nil {
		return err
	}

	fileDescriptors := make([]ocispec.Descriptor, 0, 1)
	fileDescriptors = append(fileDescriptors, desc)
	ociOpts := oras.PackManifestOptions{
		Layers: fileDescriptors,
	}
	manifestDescriptor, err := o.oci.PackManifest(ctx, s, oras.PackManifestVersion1_1, artifactType, ociOpts)
	if err != nil {
		return err
	}
	tag := "latest"
	err = s.Tag(ctx, manifestDescriptor, tag)
	if err != nil {
		return err
	}
	repo, err := newOCIRepository(id, opts)
	if err != nil {
		return err
	}

	_, err = o.oci.Copy(ctx, s, tag, repo, tag, oras.DefaultCopyOptions)
	return err
}

func (o *OCIWrapper) pullFile(ctx context.Context, opts OCIOpts, id string) ([]byte, error) {
	s := o.oci.NewStore()

	// use the agent credentials (read only) if present
	if opts.AgentUsername != "" {
		opts.Username = opts.AgentUsername
	}
	if opts.AgentPassword != "" {
		opts.Password = opts.AgentPassword
	}

	// copy from remote OCI registry to local memory store
	repo, err := newOCIRepository(id, opts)
	if err != nil {
		return nil, err
	}
	tag := "latest"
	_, err = o.oci.Copy(ctx, repo, tag, s, tag, oras.DefaultCopyOptions)
	if err != nil {
		return nil, err
	}

	// access the root node
	rootDesc, err := s.Resolve(ctx, tag)
	if err != nil {
		return nil, err
	}

	// fetch the root node of the manifest
	rootData, err := getDataFromDescriptor(ctx, s, rootDesc)
	if err != nil {
		return nil, err
	}

	// unmarshall the root node in order to access the layers
	var root struct {
		MediaType string `json:"mediaType"`
		Layers    []ocispec.Descriptor
	}
	if err := json.Unmarshal(rootData, &root); err != nil {
		return nil, err
	}
	if len(root.Layers) != 1 {
		return nil, fmt.Errorf("expected 1 layer in OCI manifest, %d found", len(root.Layers))
	}
	// get the layer descriptor and fetch from the store
	desc := root.Layers[0]
	// when pushing we add the id of the manifest to the annotations
	// it should match
	if err := checkIDAnnotation(desc, id); err != nil {
		return nil, err
	}

	// return the data for the layer (which is the original fleet manifest)
	return getDataFromDescriptor(ctx, s, desc)
}

// PushManifest creates and pushes an OCI manifest to a remote OCI registry with the
// contents of the given fleet manifest.
// The OCI manifest will be named after the given id.
func (o *OCIWrapper) PushManifest(ctx context.Context, opts OCIOpts, id string, m *manifest.Manifest) error {
	data, err := m.Content()
	if err != nil {
		return err
	}
	desc := ocispec.Descriptor{
		MediaType: fileType,
		Digest:    digest.FromBytes(data),
		Size:      int64(len(data)),
		Annotations: map[string]string{
			"id": id,
		},
	}
	return o.pushFile(ctx, opts, bytes.NewReader(data), desc, id)
}

// PullManifest pulls the OCI manifest identified by the given id from a remote OCI registry
// and fills and returns a fleet manifest with the contents.
func (o *OCIWrapper) PullManifest(ctx context.Context, opts OCIOpts, id string) (*manifest.Manifest, error) {
	data, err := o.pullFile(ctx, opts, id)
	if err != nil {
		return nil, err
	}

	return manifest.FromJSON(data, "")
}

// DeleteManifest deletes the OCI manifest identified by the given id and "latest" tag from a remote OCI registry.
func (o *OCIWrapper) DeleteManifest(ctx context.Context, opts OCIOpts, id string) error {
	repo, err := newOCIRepository(id, opts)
	if err != nil {
		return fmt.Errorf("failed to create repository for %s: %w", id, err)
	}

	tag := "latest"
	desc, err := repo.Resolve(ctx, tag)
	if err != nil {
		return fmt.Errorf("failed to resolve tag '%s' for artifact '%s': %w", tag, id, err)
	}

	return repo.Delete(ctx, desc)
}

// OCIIsEnabled returns true if the OCI_STORAGE env variable is not set or
// if it's set to true
func OCIIsEnabled() bool {
	if v, ok := os.LookupEnv(OCIStorageFlag); ok {
		value, err := strconv.ParseBool(v)
		if err != nil {
			// if the env variable is set to a non valid value, return true
			// as OCI Storage is enabled by default.
			return true
		}
		return value
	}
	// if not defined, return true.
	// OCI Storage is enabled by default.
	return true
}



================================================
FILE: internal/ocistorage/ociwrapper_test.go
================================================
package ocistorage

import (
	"bytes"
	"context"
	"crypto/tls"
	"fmt"
	"net/http"
	"os"

	"github.com/opencontainers/go-digest"
	"go.uber.org/mock/gomock"

	ocispec "github.com/opencontainers/image-spec/specs-go/v1"
	"oras.land/oras-go/v2"
	"oras.land/oras-go/v2/content"
	orasmemory "oras.land/oras-go/v2/content/memory"
	"oras.land/oras-go/v2/registry/remote/auth"
	"oras.land/oras-go/v2/registry/remote/retry"

	"github.com/rancher/fleet/internal/manifest"
	"github.com/rancher/fleet/internal/mocks"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

type MockOrasOperator struct {
	ReturnOrasStore  bool
	Target           *mocks.MockTarget
	CopyMock         func(ctx context.Context, src oras.ReadOnlyTarget, srcRef string, dst oras.Target, dstRef string, opts oras.CopyOptions) (ocispec.Descriptor, error)
	PackManifestMock func(ctx context.Context, pusher content.Pusher, packManifestVersion oras.PackManifestVersion, artifactType string, opts oras.PackManifestOptions) (ocispec.Descriptor, error)
}

func (m *MockOrasOperator) PackManifest(ctx context.Context, pusher content.Pusher, packManifestVersion oras.PackManifestVersion, artifactType string, opts oras.PackManifestOptions) (ocispec.Descriptor, error) {
	if m.PackManifestMock != nil {
		return m.PackManifestMock(ctx, pusher, packManifestVersion, artifactType, opts)
	}
	return oras.PackManifest(ctx, pusher, packManifestVersion, artifactType, opts)
}

func (m *MockOrasOperator) Copy(ctx context.Context, src oras.ReadOnlyTarget, srcRef string, dst oras.Target, dstRef string, opts oras.CopyOptions) (ocispec.Descriptor, error) {
	if m.CopyMock != nil {
		return m.CopyMock(ctx, src, srcRef, dst, dstRef, opts)
	}
	return oras.Copy(ctx, src, srcRef, dst, dstRef, opts)
}

func (m *MockOrasOperator) NewStore() oras.Target {
	if m.ReturnOrasStore {
		return orasmemory.New()
	}
	return m.Target
}

func NewMockOrasOperator(ctrl *gomock.Controller, returnOrasStore bool) *MockOrasOperator {
	return &MockOrasOperator{
		ReturnOrasStore: returnOrasStore,
		Target:          mocks.NewMockTarget(ctrl),
	}
}

var _ = Describe("OCIUtils tests", func() {
	var (
		ctrl *gomock.Controller
	)

	BeforeEach(func() {
		ctrl = gomock.NewController(GinkgoT())
	})

	It("returns an error when can't push to the store", func() {
		orasOperatorMock := NewMockOrasOperator(ctrl, false)
		orasOperatorMock.Target.EXPECT().Push(gomock.Any(), gomock.Any(), gomock.Any()).Return(fmt.Errorf("TEST ERROR")).Times(1)

		opts := OCIOpts{
			Reference: "test.com",
		}
		manifest := &manifest.Manifest{
			Commit: "123456",
			Resources: []fleet.BundleResource{
				{
					Name:     "resource1",
					Content:  "Content1",
					Encoding: "encoding1",
				},
			},
		}
		oci := &OCIWrapper{
			oci: orasOperatorMock,
		}
		err := oci.PushManifest(context.Background(), opts, "123", manifest)
		Expect(err.Error()).To(Equal("TEST ERROR"))
	})
	It("returns an error if oras fails packing the manifest", func() {
		orasOperatorMock := NewMockOrasOperator(ctrl, true)
		orasOperatorMock.PackManifestMock = func(_ context.Context,
			_ content.Pusher,
			_ oras.PackManifestVersion,
			_ string,
			_ oras.PackManifestOptions) (ocispec.Descriptor, error) {
			return ocispec.Descriptor{}, fmt.Errorf("ERROR PACKING")
		}
		oci := &OCIWrapper{
			oci: orasOperatorMock,
		}
		opts := OCIOpts{
			Reference: "test.com",
		}
		manifest := &manifest.Manifest{
			Commit: "123456",
			Resources: []fleet.BundleResource{
				{
					Name:     "resource1",
					Content:  "Content1",
					Encoding: "encoding1",
				},
			},
		}
		err := oci.PushManifest(context.Background(), opts, "123", manifest)
		Expect(err.Error()).To(Equal("ERROR PACKING"))
	})
	It("returns an OCI repository with the expected values when using basic HTTP", func() {
		opts := OCIOpts{
			Reference: "test.com",
			BasicHTTP: true,
		}
		repo, err := newOCIRepository("1234", opts)
		Expect(err).ToNot(HaveOccurred())
		Expect(repo.PlainHTTP).To(BeTrue())

		opts.BasicHTTP = false
		repo, err = newOCIRepository("1234", opts)
		Expect(err).ToNot(HaveOccurred())
		Expect(repo.PlainHTTP).To(BeFalse())
	})
	It("return the expected tls client", func() {
		client := getHTTPClient(true)
		expected := &http.Client{
			Transport: &http.Transport{
				TLSClientConfig: &tls.Config{InsecureSkipVerify: true},
			},
		}
		Expect(client).To(Equal(expected))

		client = getHTTPClient(false)
		Expect(client).To(Equal(retry.DefaultClient))
	})
	It("return the expected credentials", func() {
		opts := OCIOpts{
			Reference: "test.com",
			BasicHTTP: true,
		}
		client := getAuthClient(opts)
		Expect(client.Credential).To(BeNil())

		opts.Username = "user"
		client = getAuthClient(opts)
		Expect(client.Credential).ToNot(BeNil())
		cred, err := client.Credential(context.Background(), "test")
		Expect(err).ToNot(HaveOccurred())
		Expect(cred).To(Equal(auth.Credential{
			Username: "user",
			Password: "",
		}))

		opts.Password = "pass"
		client = getAuthClient(opts)
		Expect(client.Credential).ToNot(BeNil())
		cred, err = client.Credential(context.Background(), "test")
		Expect(err).ToNot(HaveOccurred())
		Expect(cred).To(Equal(auth.Credential{
			Username: "user",
			Password: "pass",
		}))
	})
	It("returns an error when using an empty OCI registry reference", func() {
		opts := OCIOpts{
			Reference: "",
		}
		manifest := &manifest.Manifest{
			Commit: "123456",
			Resources: []fleet.BundleResource{
				{
					Name:     "resource1",
					Content:  "Content1",
					Encoding: "encoding1",
				},
			},
		}
		oci := NewOCIWrapper()
		err := oci.PushManifest(context.Background(), opts, "123", manifest)
		Expect(err.Error()).To(Equal("invalid reference: missing registry or repository"))
	})
	It("returns an error if the OCI manifest does not have the expected annotation id", func() {
		orasOperatorMock := NewMockOrasOperator(ctrl, true)
		orasOperatorMock.CopyMock = func(_ context.Context,
			_ oras.ReadOnlyTarget,
			_ string,
			target oras.Target,
			_ string,
			opts oras.CopyOptions) (ocispec.Descriptor, error) {
			// fill the store with data
			data := []byte("This is test")
			desc := ocispec.Descriptor{
				MediaType: "application/octet-stream",
				Digest:    digest.FromBytes(data),
				Size:      int64(len(data)),
				Annotations: map[string]string{
					"id": "example_id", // this is the annotation id that is not the expected one
				},
			}

			ctx := context.Background()
			err := target.Push(ctx, desc, bytes.NewReader(data))
			Expect(err).ToNot(HaveOccurred())

			fileDescriptors := make([]ocispec.Descriptor, 0, 1)
			fileDescriptors = append(fileDescriptors, desc)
			ociOpts := oras.PackManifestOptions{
				Layers: fileDescriptors,
			}

			manifestDescriptor, err := oras.PackManifest(ctx, target, oras.PackManifestVersion1_1, artifactType, ociOpts)
			Expect(err).ToNot(HaveOccurred())

			tag := "latest"
			err = target.Tag(ctx, manifestDescriptor, tag)
			Expect(err).ToNot(HaveOccurred())

			return ocispec.Descriptor{}, nil
		}

		opts := OCIOpts{
			Reference: "test.com",
		}
		oci := &OCIWrapper{
			oci: orasOperatorMock,
		}
		_, err := oci.PullManifest(context.Background(), opts, "s-123456")
		// s-123456 != example_id so PullManifest should return an error
		Expect(err.Error()).To(Equal("could not find expected id in Descriptor's annotations"))
	})
	It("returns an error if the OCI manifest does not have the expected tag", func() {
		orasOperatorMock := NewMockOrasOperator(ctrl, true)
		orasOperatorMock.CopyMock = func(_ context.Context, _ oras.ReadOnlyTarget, _ string, target oras.Target, _ string, opts oras.CopyOptions) (ocispec.Descriptor, error) {
			// fill the store with data
			data := []byte("This is test")
			desc := ocispec.Descriptor{
				MediaType: "application/octet-stream",
				Digest:    digest.FromBytes(data),
				Size:      int64(len(data)),
				Annotations: map[string]string{
					"id": "example_id", // this is the annotation id that is not the expected one
				},
			}

			ctx := context.Background()
			err := target.Push(ctx, desc, bytes.NewReader(data))
			Expect(err).ToNot(HaveOccurred())

			fileDescriptors := make([]ocispec.Descriptor, 0, 1)
			fileDescriptors = append(fileDescriptors, desc)
			ociOpts := oras.PackManifestOptions{
				Layers: fileDescriptors,
			}
			manifestDescriptor, err := oras.PackManifest(ctx, target, oras.PackManifestVersion1_1, artifactType, ociOpts)
			Expect(err).ToNot(HaveOccurred())

			tag := "this_tag_is_not_expected"
			err = target.Tag(ctx, manifestDescriptor, tag)
			Expect(err).ToNot(HaveOccurred())

			return ocispec.Descriptor{}, nil
		}

		opts := OCIOpts{
			Reference: "test.com",
		}
		oci := &OCIWrapper{
			oci: orasOperatorMock,
		}
		_, err := oci.PullManifest(context.Background(), opts, "s-123456")
		Expect(err.Error()).To(ContainSubstring("not found"))
	})
	It("returns an error if the OCI manifest is empty", func() {
		orasOperatorMock := NewMockOrasOperator(ctrl, true)
		orasOperatorMock.CopyMock = func(_ context.Context, _ oras.ReadOnlyTarget, _ string, target oras.Target, _ string, opts oras.CopyOptions) (ocispec.Descriptor, error) {
			// fill the store with an empty manifest
			ctx := context.Background()

			fileDescriptors := make([]ocispec.Descriptor, 0)
			ociOpts := oras.PackManifestOptions{
				Layers: fileDescriptors,
			}
			manifestDescriptor, err := oras.PackManifest(ctx, target, oras.PackManifestVersion1_1, artifactType, ociOpts)
			Expect(err).ToNot(HaveOccurred())

			tag := "latest"
			err = target.Tag(ctx, manifestDescriptor, tag)
			Expect(err).ToNot(HaveOccurred())

			return ocispec.Descriptor{}, nil
		}

		opts := OCIOpts{
			Reference: "test.com",
		}
		oci := &OCIWrapper{
			oci: orasOperatorMock,
		}
		_, err := oci.PullManifest(context.Background(), opts, "s-123456")
		Expect(err.Error()).To(Equal("expecting 1 Annotation in layer descriptor. Found 0"))
	})
})

var _ = Describe("OCIUtils flag tests", func() {
	var envBeforeTest string

	BeforeEach(func() {
		envBeforeTest = os.Getenv(OCIStorageFlag)
	})

	AfterEach(func() {
		if envBeforeTest != "" {
			// set the value it had before the test
			Expect(os.Setenv(OCIStorageFlag, envBeforeTest)).ToNot(HaveOccurred())
		} else {
			Expect(os.Unsetenv(OCIStorageFlag)).ToNot(HaveOccurred())
		}
	})

	DescribeTable("Check value returned is the expected one",
		func(value string, expected bool) {
			if value == "unset" {
				Expect(os.Unsetenv(OCIStorageFlag)).ToNot(HaveOccurred())
			} else {
				Expect(os.Setenv(OCIStorageFlag, value)).ToNot(HaveOccurred())
			}
			result := OCIIsEnabled()
			Expect(result).To(Equal(expected))
		},
		Entry("When setting to True", "True", true),
		Entry("When setting to true", "true", true),
		Entry("When setting to TRUE", "TRUE", true),
		Entry("When setting to tRue", "tRue", true), // true because OCI storage is enabled by default.
		Entry("When setting to false", "false", false),
		Entry("When setting to whatever", "whatever", true), // true because OCI storage is enabled by default.
		Entry("When not setting the value", "unset", true),  // true because OCI storage is enabled by default.
	)
})



================================================
FILE: internal/ocistorage/secret.go
================================================
package ocistorage

import (
	"context"
	"fmt"
	"strconv"

	corev1 "k8s.io/api/core/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"

	"github.com/rancher/fleet/internal/config"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

const (
	OCISecretUsername      = "username"
	OCISecretPassword      = "password"
	OCISecretAgentUsername = "agentUsername"
	OCISecretAgentPassword = "agentPassword"
	OCISecretReference     = "reference"
	OCISecretBasicHTTP     = "basicHTTP"
	OCISecretInsecure      = "insecure"
)

// ReadOptsFromSecret reads the secret identified by the given NamespacedName and
// returns an OCIOpts structure filled with the information obtained from that secret.
func ReadOptsFromSecret(ctx context.Context, c client.Reader, ns client.ObjectKey) (OCIOpts, error) {
	// if no secret was specified, fallback to the default one
	if ns.Name == "" {
		ns.Name = config.DefaultOCIStorageSecretName
	}

	opts := OCIOpts{}
	var secret corev1.Secret
	err := c.Get(ctx, ns, &secret)
	if err != nil {
		return OCIOpts{}, err
	}

	if secret.Type != fleet.SecretTypeOCIStorage {
		return OCIOpts{}, fmt.Errorf("unexpected secret type: got %q, want %q", secret.Type, fleet.SecretTypeOCIStorage)
	}

	// Fill the values from the secret.
	// Only Reference is strictly required.
	opts.Reference, err = getStringValueFromSecret(secret.Data, OCISecretReference, true)
	if err != nil {
		return OCIOpts{}, err
	}

	opts.Username, err = getStringValueFromSecret(secret.Data, OCISecretUsername, false)
	if err != nil {
		return OCIOpts{}, err
	}

	opts.Password, err = getStringValueFromSecret(secret.Data, OCISecretPassword, false)
	if err != nil {
		return OCIOpts{}, err
	}

	opts.AgentUsername, err = getStringValueFromSecret(secret.Data, OCISecretAgentUsername, false)
	if err != nil {
		return OCIOpts{}, err
	}

	opts.AgentPassword, err = getStringValueFromSecret(secret.Data, OCISecretAgentPassword, false)
	if err != nil {
		return OCIOpts{}, err
	}

	opts.BasicHTTP, err = getBoolValueFromSecret(secret.Data, OCISecretBasicHTTP, false)
	if err != nil {
		return OCIOpts{}, err
	}

	opts.InsecureSkipTLS, err = getBoolValueFromSecret(secret.Data, OCISecretInsecure, false)
	if err != nil {
		return OCIOpts{}, err
	}

	return opts, nil
}

func getStringValueFromSecret(data map[string][]byte, key string, required bool) (string, error) {
	value, ok := data[key]
	if !ok {
		if !required {
			return "", nil
		}
		return "", fmt.Errorf("key %q not found in secret", key)
	}
	return string(value), nil
}

func getBoolValueFromSecret(data map[string][]byte, key string, required bool) (bool, error) {
	value, ok := data[key]
	if !ok {
		if !required {
			return false, nil
		}
		return false, fmt.Errorf("key %q not found in secret", key)
	}
	valueStr := string(value)
	boolValue, err := strconv.ParseBool(valueStr)
	if err != nil {
		return false, fmt.Errorf("failed to parse %q as bool: %w", valueStr, err)
	}

	return boolValue, nil
}



================================================
FILE: internal/ocistorage/secret_test.go
================================================
package ocistorage

import (
	"context"
	"errors"
	"fmt"

	"github.com/rancher/fleet/internal/config"
	"github.com/rancher/fleet/internal/mocks"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"sigs.k8s.io/controller-runtime/pkg/client"

	"go.uber.org/mock/gomock"
	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/apimachinery/pkg/types"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

var _ = Describe("OCIOpts loaded from secret", func() {
	var (
		ctrl                   *gomock.Controller
		secretGetErrorMessage  string
		secretGetNotFoundError bool
		mockClient             *mocks.MockK8sClient

		secretName string
		secretData map[string][]byte
		secretType string
	)

	JustBeforeEach(func() {
		ctrl = gomock.NewController(GinkgoT())
		mockClient = mocks.NewMockK8sClient(ctrl)
		ns := types.NamespacedName{Name: secretName, Namespace: "test"}
		getSecretFromMockK8sClient(
			mockClient,
			ns,
			secretData,
			secretType,
			secretGetNotFoundError,
			secretGetErrorMessage,
		)
	})

	When("the given oci storage secret exists with all fields set", func() {
		BeforeEach(func() {
			secretName = "test"
			secretData = map[string][]byte{
				OCISecretUsername:      []byte("username"),
				OCISecretPassword:      []byte("password"),
				OCISecretAgentUsername: []byte("agentUsername"),
				OCISecretAgentPassword: []byte("agentPassword"),
				OCISecretReference:     []byte("reference"),
				OCISecretBasicHTTP:     []byte("true"),
				OCISecretInsecure:      []byte("true"),
			}
			secretType = fleet.SecretTypeOCIStorage
			secretGetErrorMessage = ""
			secretGetNotFoundError = false
		})
		It("returns the expected OCIOpts from the data in the secret", func() {
			ns := client.ObjectKey{Name: secretName, Namespace: "test"}
			opts, err := ReadOptsFromSecret(context.TODO(), mockClient, ns)
			Expect(err).ToNot(HaveOccurred())
			Expect(opts.Reference).To(Equal(string(secretData[OCISecretReference])))
			Expect(opts.Username).To(Equal(string(secretData[OCISecretUsername])))
			Expect(opts.Password).To(Equal(string(secretData[OCISecretPassword])))
			Expect(opts.AgentUsername).To(Equal(string(secretData[OCISecretAgentUsername])))
			Expect(opts.AgentPassword).To(Equal(string(secretData[OCISecretAgentPassword])))
			Expect(opts.BasicHTTP).To(BeTrue())
			Expect(opts.InsecureSkipTLS).To(BeTrue())
		})
	})

	When("the secret name is not set, but a default secret exists", func() {
		BeforeEach(func() {
			secretName = ""
			secretData = map[string][]byte{
				OCISecretUsername:      []byte("username"),
				OCISecretPassword:      []byte("password"),
				OCISecretAgentUsername: []byte("agentUsername"),
				OCISecretAgentPassword: []byte("agentPassword"),
				OCISecretReference:     []byte("reference"),
				OCISecretBasicHTTP:     []byte("true"),
				OCISecretInsecure:      []byte("true"),
			}
			secretType = fleet.SecretTypeOCIStorage
			secretGetErrorMessage = ""
			secretGetNotFoundError = false
		})
		It("returns the expected OCIOpts from the data in the secret", func() {
			ns := client.ObjectKey{Name: secretName, Namespace: "test"}
			opts, err := ReadOptsFromSecret(context.TODO(), mockClient, ns)
			Expect(err).ToNot(HaveOccurred())
			Expect(opts.Reference).To(Equal(string(secretData[OCISecretReference])))
			Expect(opts.Username).To(Equal(string(secretData[OCISecretUsername])))
			Expect(opts.Password).To(Equal(string(secretData[OCISecretPassword])))
			Expect(opts.AgentUsername).To(Equal(string(secretData[OCISecretAgentUsername])))
			Expect(opts.AgentPassword).To(Equal(string(secretData[OCISecretAgentPassword])))
			Expect(opts.BasicHTTP).To(BeTrue())
			Expect(opts.InsecureSkipTLS).To(BeTrue())
		})
	})

	When("the given oci storage secret exists with all the non-required fields are not set", func() {
		BeforeEach(func() {
			secretName = "test"
			secretData = map[string][]byte{
				OCISecretReference: []byte("reference"),
			}

			secretType = fleet.SecretTypeOCIStorage
			secretGetErrorMessage = ""
			secretGetNotFoundError = false
		})
		It("returns the expected OCIOpts from the data in the secret", func() {
			ns := client.ObjectKey{Name: secretName, Namespace: "test"}
			opts, err := ReadOptsFromSecret(context.TODO(), mockClient, ns)
			Expect(err).ToNot(HaveOccurred())
			Expect(opts.Reference).To(Equal(string(secretData[OCISecretReference])))
			Expect(opts.Username).To(BeEmpty())
			Expect(opts.Password).To(BeEmpty())
			Expect(opts.AgentUsername).To(BeEmpty())
			Expect(opts.AgentPassword).To(BeEmpty())
			Expect(opts.BasicHTTP).To(BeFalse())
			Expect(opts.InsecureSkipTLS).To(BeFalse())
		})
	})

	When("the given oci storage secret exists and reference is not set", func() {
		BeforeEach(func() {
			secretName = "test"
			secretData = map[string][]byte{
				OCISecretUsername:      []byte("username"),
				OCISecretPassword:      []byte("password"),
				OCISecretAgentUsername: []byte("agentUsername"),
				OCISecretAgentPassword: []byte("agentPassword"),
				OCISecretBasicHTTP:     []byte("true"),
				OCISecretInsecure:      []byte("true"),
			}

			secretType = fleet.SecretTypeOCIStorage
			secretGetErrorMessage = ""
			secretGetNotFoundError = false
		})
		It("returns an error complaining about reference not being set", func() {
			ns := client.ObjectKey{Name: secretName, Namespace: "test"}
			_, err := ReadOptsFromSecret(context.TODO(), mockClient, ns)
			Expect(err).To(HaveOccurred())
			Expect(err.Error()).To(Equal("key \"reference\" not found in secret"))
		})
	})

	When("the given oci storage secret does not exist", func() {
		BeforeEach(func() {
			secretName = "test"
			secretGetNotFoundError = true
			secretGetErrorMessage = ""
		})
		It("returns an error complaining about a secret not being found", func() {
			ns := client.ObjectKey{Name: secretName, Namespace: "test"}
			_, err := ReadOptsFromSecret(context.TODO(), mockClient, ns)
			Expect(err).To(HaveOccurred())
			Expect(apierrors.IsNotFound(err)).To(BeTrue())
		})
	})

	When("the given oci storage secret exists but the type is not the expected one", func() {
		BeforeEach(func() {
			secretName = "test"
			secretType = "party-like-its-1999"
			secretGetNotFoundError = false
			secretGetErrorMessage = ""
		})
		It("returns an error complaining about wrong type", func() {
			ns := client.ObjectKey{Name: secretName, Namespace: "test"}
			_, err := ReadOptsFromSecret(context.TODO(), mockClient, ns)
			Expect(err).To(HaveOccurred())
			Expect(err.Error()).To(Equal(fmt.Sprintf("unexpected secret type: got %q, want %q", secretType, fleet.SecretTypeOCIStorage)))
		})
	})

	When("there is an error when getting the secret", func() {
		BeforeEach(func() {
			secretName = "test"
			secretGetNotFoundError = false
			secretGetErrorMessage = "SOME ERROR"
		})
		It("returns the error", func() {
			ns := client.ObjectKey{Name: secretName, Namespace: "test"}
			_, err := ReadOptsFromSecret(context.TODO(), mockClient, ns)
			Expect(err).To(HaveOccurred())
			Expect(err.Error()).To(Equal(secretGetErrorMessage))
		})
	})
})

func getSecretFromMockK8sClient(
	mockClient *mocks.MockK8sClient,
	ns types.NamespacedName,
	data map[string][]byte,
	secretType string,
	wantNotFound bool,
	wantErrorMessage string) {
	switch {
	case wantErrorMessage != "":
		mockClient.EXPECT().Get(gomock.Any(), ns, gomock.Any()).DoAndReturn(
			func(_ context.Context, _ types.NamespacedName, secret *corev1.Secret, _ ...interface{}) error {
				return errors.New(wantErrorMessage)
			},
		)
	case wantNotFound:
		mockClient.EXPECT().Get(gomock.Any(), ns, gomock.Any()).DoAndReturn(
			func(_ context.Context, _ types.NamespacedName, secret *corev1.Secret, _ ...interface{}) error {
				return apierrors.NewNotFound(schema.GroupResource{}, "TEST ERROR")
			},
		)
	case ns.Name == "":
		// verify that when the name is not set it uses the default secret name.
		mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any()).DoAndReturn(
			func(_ context.Context, key types.NamespacedName, secret *corev1.Secret, _ ...interface{}) error {
				Expect(key.Name).To(Equal(config.DefaultOCIStorageSecretName))
				secret.Data = data
				secret.Type = corev1.SecretType(secretType)
				return nil
			},
		)
	default:
		mockClient.EXPECT().Get(gomock.Any(), gomock.Any(), gomock.Any()).DoAndReturn(
			func(_ context.Context, key types.NamespacedName, secret *corev1.Secret, _ ...interface{}) error {
				Expect(ns.Name).To(Equal(key.Name))
				secret.Data = data
				secret.Type = corev1.SecretType(secretType)
				return nil
			},
		)
	}
}



================================================
FILE: internal/ocistorage/suite_test.go
================================================
package ocistorage_test

import (
	"testing"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

const (
	timeout = 30 * time.Second
)

func TestFleet(t *testing.T) {
	RegisterFailHandler(Fail)
	RunSpecs(t, "OCI Utils Suite")
}

var _ = BeforeSuite(func() {
	SetDefaultEventuallyTimeout(timeout)
})



================================================
FILE: internal/registration/secret.go
================================================
package registration

import (
	"crypto/sha256"
	"encoding/hex"
)

func SecretName(clientID, clientRandom string) string {
	d := sha256.New()
	d.Write([]byte(clientID))
	d.Write([]byte(clientRandom))
	return ("c-" + hex.EncodeToString(d.Sum(nil)))[:63]
}



================================================
FILE: internal/resourcestatus/resourcekey.go
================================================
package resourcestatus

import (
	"cmp"
	"maps"
	"slices"
	"sort"
	"strings"

	"github.com/rancher/fleet/internal/cmd/controller/summary"
	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
)

func SetResources(list []fleet.BundleDeployment, status *fleet.StatusBase) {
	byCluster := fromResources(list)
	status.Resources = aggregateResourceStatesClustersMap(byCluster)
	status.ResourceCounts = sumResourceCounts(list)
	status.PerClusterResourceCounts = resourceCountsPerCluster(list)
}

func SetClusterResources(list []fleet.BundleDeployment, cluster *fleet.Cluster) {
	cluster.Status.ResourceCounts = sumResourceCounts(list)
}

func key(resource fleet.Resource) string {
	return resource.Type + "/" + resource.ID
}

func resourceCountsPerCluster(items []fleet.BundleDeployment) map[string]*fleet.ResourceCounts {
	res := make(map[string]*fleet.ResourceCounts)
	for _, bd := range items {
		clusterID := bd.Labels[fleet.ClusterNamespaceLabel] + "/" + bd.Labels[fleet.ClusterLabel]
		if _, ok := res[clusterID]; !ok {
			res[clusterID] = &fleet.ResourceCounts{}
		}
		summary.IncrementResourceCounts(res[clusterID], bd.Status.ResourceCounts)
	}
	return res
}

type resourceStateEntry struct {
	state      string
	clusterID  string
	incomplete bool
}

type resourceStatesByResourceKey map[fleet.ResourceKey][]resourceStateEntry

func clusterID(bd fleet.BundleDeployment) string {
	return bd.Labels[fleet.ClusterNamespaceLabel] + "/" + bd.Labels[fleet.ClusterLabel]
}

// fromResources inspects a list of BundleDeployments and returns a list of per-cluster states per resource keys.
// It also returns a list of errors messages produced that may have occurred during processing
func fromResources(items []fleet.BundleDeployment) resourceStatesByResourceKey {
	// ensure cluster order is stable, so we do not trigger reconciles
	sort.Slice(items, func(i, j int) bool {
		return clusterID(items[i]) < clusterID(items[j])
	})
	resources := make(resourceStatesByResourceKey)
	for _, bd := range items {
		for key, entry := range bundleDeploymentResources(bd) {
			resources[key] = append(resources[key], entry)
		}
	}

	return resources
}

func resourceId(namespace, name string) string {
	if namespace != "" {
		return namespace + "/" + name
	}
	return name
}

func toType(apiVersion, kind string) string {
	group := strings.Split(apiVersion, "/")[0]
	if group == "v1" {
		group = ""
	} else if len(group) > 0 {
		group += "."
	}
	return group + strings.ToLower(kind)
}

// resourceDefaultState calculates the state for items in the status.Resources list.
// This default state may be replaced individually for each resource with the information from NonReadyStatus and ModifiedStatus fields.
func resourcesDefaultState(bd *fleet.BundleDeployment) string {
	switch bdState := summary.GetDeploymentState(bd); bdState {
	// NotReady and Modified BD states are inferred from resource statuses, so it's incorrect to use that to calculate resource states
	case fleet.NotReady, fleet.Modified:
		if bd.Status.IncompleteState {
			return "Unknown"
		} else {
			return string(fleet.Ready)
		}
	default:
		return string(bdState)
	}
}

func bundleDeploymentResources(bd fleet.BundleDeployment) map[fleet.ResourceKey]resourceStateEntry {
	clusterID := bd.Labels[fleet.ClusterNamespaceLabel] + "/" + bd.Labels[fleet.ClusterLabel]
	incomplete := bd.Status.IncompleteState
	defaultState := resourcesDefaultState(&bd)

	resources := make(map[fleet.ResourceKey]resourceStateEntry, len(bd.Status.Resources))
	for _, bdResource := range bd.Status.Resources {
		resourceKey := fleet.ResourceKey{
			Kind:       bdResource.Kind,
			APIVersion: bdResource.APIVersion,
			Name:       bdResource.Name,
			Namespace:  bdResource.Namespace,
		}
		resources[resourceKey] = resourceStateEntry{
			state:      defaultState,
			clusterID:  clusterID,
			incomplete: incomplete,
		}
	}

	for _, nonReady := range bd.Status.NonReadyStatus {
		resourceKey := fleet.ResourceKey{
			Kind:       nonReady.Kind,
			APIVersion: nonReady.APIVersion,
			Namespace:  nonReady.Namespace,
			Name:       nonReady.Name,
		}
		resources[resourceKey] = resourceStateEntry{
			state:      nonReady.Summary.State,
			clusterID:  clusterID,
			incomplete: incomplete,
		}
	}

	for _, modified := range bd.Status.ModifiedStatus {
		key := fleet.ResourceKey{
			Kind:       modified.Kind,
			APIVersion: modified.APIVersion,
			Namespace:  modified.Namespace,
			Name:       modified.Name,
		}
		state := "Modified"
		if modified.Delete {
			state = "Orphaned"
		} else if modified.Create {
			state = "Missing"
		}
		resources[key] = resourceStateEntry{
			state:      state,
			clusterID:  clusterID,
			incomplete: incomplete,
		}
	}

	return resources
}

func aggregateResourceStatesClustersMap(resourceKeyStates resourceStatesByResourceKey) []fleet.Resource {
	// sort to make the result stable in case of truncation
	keys := slices.SortedFunc(maps.Keys(resourceKeyStates), func(a, b fleet.ResourceKey) int {
		return cmp.Compare(
			(b.Kind + b.APIVersion + " " + b.Namespace + "/" + b.Name),
			(a.Kind + a.APIVersion + " " + a.Namespace + "/" + a.Name),
		)
	})

	size := 0
	result := make([]fleet.Resource, 0, len(resourceKeyStates))
	for _, resourceKey := range keys {
		entries := resourceKeyStates[resourceKey]
		resource := &fleet.Resource{
			Kind:       resourceKey.Kind,
			APIVersion: resourceKey.APIVersion,
			Namespace:  resourceKey.Namespace,
			Name:       resourceKey.Name,
			State:      "Ready",
			Type:       toType(resourceKey.APIVersion, resourceKey.Kind),
			ID:         resourceId(resourceKey.Namespace, resourceKey.Name),
		}

		for _, entry := range entries {
			if entry.incomplete {
				resource.IncompleteState = true
			}

			size += len(entry.clusterID)
			if size < 1024*1024 { // 1mb
				appendToPerClusterState(&resource.PerClusterState, entry.state, entry.clusterID)
			}

			// top-level state is set from first non "Ready" per-cluster state
			if resource.State == "Ready" {
				resource.State = entry.state
			}
		}

		result = append(result, *resource)
	}

	// sort to make result easier to read
	sort.Slice(result, func(i, j int) bool {
		return key(result[i]) < key(result[j])
	})

	return result
}

func appendToPerClusterState(states *fleet.PerClusterState, state, clusterID string) {
	switch state {
	case "Ready":
		states.Ready = append(states.Ready, clusterID)
	case "WaitApplied":
		states.WaitApplied = append(states.WaitApplied, clusterID)
	case "Pending":
		states.Pending = append(states.Pending, clusterID)
	case "Modified":
		states.Modified = append(states.Modified, clusterID)
	case "NotReady", "updating", "error":
		// `updating` comes from checkTransitioning summarizer in the
		// fleet-agent. When the `Available` condition is set to false, the
		// state is set to `updating`, but we treat it as nonReady for per
		// cluster states.
		states.NotReady = append(states.NotReady, clusterID)
	case "Orphaned":
		states.Orphaned = append(states.Orphaned, clusterID)
	case "Missing":
		states.Missing = append(states.Missing, clusterID)
	case "Unknown":
		states.Unknown = append(states.Unknown, clusterID)
	default:
		// ignore
	}
}

func sumResourceCounts(items []fleet.BundleDeployment) fleet.ResourceCounts {
	var res fleet.ResourceCounts
	for _, bd := range items {
		summary.IncrementResourceCounts(&res, bd.Status.ResourceCounts)
	}
	return res
}



================================================
FILE: internal/resourcestatus/resourcekey_test.go
================================================
package resourcestatus

import (
	"encoding/json"
	"fmt"
	"slices"
	"testing"

	"github.com/stretchr/testify/require"

	"github.com/stretchr/testify/assert"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"

	fleet "github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1"
	"github.com/rancher/fleet/pkg/apis/fleet.cattle.io/v1alpha1/summary"
)

func TestSetResources(t *testing.T) {
	list := []fleet.BundleDeployment{
		{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "bd1",
				Namespace: "ns1-cluster1-ns",
				Labels: map[string]string{
					fleet.ClusterLabel:          "cluster1",
					fleet.ClusterNamespaceLabel: "c-ns1",
				},
			},
			Spec: fleet.BundleDeploymentSpec{
				DeploymentID: "id2",
			},
			Status: fleet.BundleDeploymentStatus{
				Ready:               false,
				NonModified:         true,
				AppliedDeploymentID: "id1",
				Resources: []fleet.BundleDeploymentResource{
					{
						Kind:       "Deployment",
						APIVersion: "v1",
						Name:       "web",
						Namespace:  "default",
					},
					{
						// extra service for one cluster
						Kind:       "Service",
						APIVersion: "v1",
						Name:       "web-svc",
						Namespace:  "default",
					},
				},
				ResourceCounts: fleet.ResourceCounts{
					DesiredReady: 2,
					WaitApplied:  2,
				},
			},
		},
		{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "bd1",
				Namespace: "ns1-cluster2-ns",
				Labels: map[string]string{
					fleet.ClusterLabel:          "cluster2",
					fleet.ClusterNamespaceLabel: "c-ns1",
				},
			},
			Status: fleet.BundleDeploymentStatus{
				Ready:       true,
				NonModified: true,
				Resources: []fleet.BundleDeploymentResource{
					{
						Kind:       "Deployment",
						APIVersion: "v1",
						Name:       "web",
						Namespace:  "default",
					},
				},
				ResourceCounts: fleet.ResourceCounts{
					DesiredReady: 1,
					Ready:        1,
				},
			},
		},
		{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "bd2",
				Namespace: "ns1-cluster2-ns",
				Labels: map[string]string{
					fleet.ClusterLabel:          "cluster2",
					fleet.ClusterNamespaceLabel: "c-ns1",
				},
			},
			Status: fleet.BundleDeploymentStatus{
				Ready:       true,
				NonModified: true,
				Resources: []fleet.BundleDeploymentResource{
					{
						Kind:       "ConfigMap",
						APIVersion: "v1",
						Name:       "cm-web",
						Namespace:  "default",
					},
				},
				ResourceCounts: fleet.ResourceCounts{
					DesiredReady: 1,
					Ready:        1,
				},
			},
		},
		{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "bd1",
				Namespace: "ns2-cluster1",
				Labels: map[string]string{
					fleet.ClusterLabel:          "cluster1",
					fleet.ClusterNamespaceLabel: "c-ns2",
				},
			},
			Spec: fleet.BundleDeploymentSpec{
				DeploymentID: "id2",
			},
			Status: fleet.BundleDeploymentStatus{
				Ready:               false,
				NonModified:         true,
				AppliedDeploymentID: "id1",
				NonReadyStatus: []fleet.NonReadyStatus{
					{
						Kind:       "Deployment",
						APIVersion: "v1",
						Name:       "web",
						Namespace:  "default",
						Summary: summary.Summary{
							State:         "Pending",
							Error:         true,
							Transitioning: true,
							Message:       []string{"message1", "message2"},
						},
					},
				},
				Resources: []fleet.BundleDeploymentResource{
					{
						Kind:       "Deployment",
						APIVersion: "v1",
						Name:       "web",
						Namespace:  "default",
					},
				},
				ResourceCounts: fleet.ResourceCounts{
					DesiredReady: 1,
					NotReady:     1,
				},
			},
		},
	}

	var status fleet.GitRepoStatus
	SetResources(list, &status.StatusBase)

	assert.Len(t, status.Resources, 3)
	assert.Contains(t, status.Resources, fleet.Resource{
		APIVersion: "v1",
		Kind:       "Deployment",
		Type:       "deployment",
		ID:         "default/web",

		Namespace: "default",
		Name:      "web",

		IncompleteState: false,
		State:           "WaitApplied",
		Error:           false,
		Transitioning:   false,
		Message:         "",
		PerClusterState: fleet.PerClusterState{
			Ready:       []string{"c-ns1/cluster2"},
			WaitApplied: []string{"c-ns1/cluster1"},
			Pending:     []string{"c-ns2/cluster1"},
		},
	})
	assert.Contains(t, status.Resources, fleet.Resource{
		APIVersion: "v1",
		Kind:       "Service",
		Type:       "service",
		ID:         "default/web-svc",

		Namespace: "default",
		Name:      "web-svc",

		IncompleteState: false,
		State:           "WaitApplied",
		Error:           false,
		Transitioning:   false,
		Message:         "",
		PerClusterState: fleet.PerClusterState{
			WaitApplied: []string{"c-ns1/cluster1"},
		},
	})

	assert.Equal(t, fleet.ResourceCounts{
		Ready:        2,
		DesiredReady: 5,
		WaitApplied:  2,
		NotReady:     1,
	}, status.ResourceCounts)

	assert.Equal(t, map[string]*fleet.ResourceCounts{
		"c-ns1/cluster1": {
			DesiredReady: 2,
			WaitApplied:  2,
		},
		"c-ns1/cluster2": {
			DesiredReady: 2,
			Ready:        2,
		},
		"c-ns2/cluster1": {
			DesiredReady: 1,
			NotReady:     1,
		},
	}, status.PerClusterResourceCounts)

}

func TestPerClusterState(t *testing.T) {
	bundleDeploymentWithState := func(state string) fleet.BundleDeployment {
		return fleet.BundleDeployment{
			ObjectMeta: metav1.ObjectMeta{
				Name:      "bd1",
				Namespace: "ns1-cluster1",
				Labels: map[string]string{
					fleet.ClusterLabel:          "cluster",
					fleet.ClusterNamespaceLabel: "namespace",
				},
			},
			Spec: fleet.BundleDeploymentSpec{
				DeploymentID: "bd1",
			},
			Status: fleet.BundleDeploymentStatus{
				AppliedDeploymentID: "bd1",
				NonReadyStatus: []fleet.NonReadyStatus{
					{
						Kind:       "Deployment",
						APIVersion: "v1",
						Namespace:  "default",
						Name:       "web",
						Summary: summary.Summary{
							State: state,
						},
					},
				},
			},
		}
	}

	tests := []struct {
		name              string
		bundleDeployments []fleet.BundleDeployment
		expectedStatus    fleet.StatusBase
	}{
		{
			name:              "if the state of the resource is error, then it should report it as NotReady",
			bundleDeployments: []fleet.BundleDeployment{bundleDeploymentWithState("error")},
			expectedStatus: fleet.StatusBase{
				Resources: []fleet.Resource{
					{
						Namespace: "default",
						Name:      "web",
						PerClusterState: fleet.PerClusterState{
							NotReady: []string{"namespace/cluster"},
						},
					},
				},
			},
		},
		{
			name:              "if the state of the resource is updating, then it should report it as NotReady",
			bundleDeployments: []fleet.BundleDeployment{bundleDeploymentWithState("updating")},
			expectedStatus: fleet.StatusBase{
				Resources: []fleet.Resource{
					{
						Namespace: "default",
						Name:      "web",
						PerClusterState: fleet.PerClusterState{
							NotReady: []string{"namespace/cluster"},
						},
					},
				},
			},
		},
		{
			name:              "if the state of the resource is unknown, then it should ignore the state",
			bundleDeployments: []fleet.BundleDeployment{bundleDeploymentWithState("")},
			expectedStatus: fleet.StatusBase{
				Resources: []fleet.Resource{
					{
						Namespace:       "default",
						Name:            "web",
						PerClusterState: fleet.PerClusterState{},
					},
				},
			},
		},
		{
			name:              "if the state of the resource is NotReady, then it should report it as NotReady",
			bundleDeployments: []fleet.BundleDeployment{bundleDeploymentWithState("NotReady")},
			expectedStatus: fleet.StatusBase{
				Resources: []fleet.Resource{
					{
						Namespace: "default",
						Name:      "web",
						PerClusterState: fleet.PerClusterState{
							NotReady: []string{"namespace/cluster"},
						},
					},
				},
			},
		},
	}
	for _, test := range tests {
		t.Run(test.name, func(t *testing.T) {
			var status fleet.GitRepoStatus
			SetResources(test.bundleDeployments, &status.StatusBase)

			assert.Equal(t, test.expectedStatus.Resources[0].PerClusterState, status.StatusBase.Resources[0].PerClusterState,
				"Expected resources to match for bundle deployments: %v", test.bundleDeployments,
			)
		})
	}
}

func TestPerClusterStateTruncation(t *testing.T) {
	percluster := func(b, c int) fleet.BundleDeployment {
		workload := fmt.Sprintf("workload%02d", b)
		bd := fleet.BundleDeployment{
			ObjectMeta: metav1.ObjectMeta{
				Name:      fmt.Sprintf("bundlename%d", b),
				Namespace: fmt.Sprintf("ns-cluster%d", c),
				Labels: map[string]string{
					fleet.ClusterLabel:          fmt.Sprintf("d0-k3k-downstream%04d-downstream%04d", c, c),
					fleet.ClusterNamespaceLabel: "fleet-default",
				},
			},
			Spec: fleet.BundleDeploymentSpec{
				DeploymentID: "fakeid",
			},
			Status: fleet.BundleDeploymentStatus{
				AppliedDeploymentID: "fakeid",
				Resources: []fleet.BundleDeploymentResource{
					{Kind: "ConfigMap", APIVersion: "v1", Namespace: workload, Name: "cm-web"},
					{Kind: "Deployment", APIVersion: "v1", Namespace: workload, Name: "web"},
					{Kind: "Service", APIVersion: "v1", Namespace: workload, Name: "web-svc"},
				},
				ModifiedStatus: []fleet.ModifiedStatus{
					{Kind: "Secret", APIVersion: "v1", Namespace: workload, Name: "cm-creds", Create: true},
				},
				NonReadyStatus: []fleet.NonReadyStatus{
					{Kind: "Deployment", APIVersion: "v1", Namespace: workload, Name: "db", Summary: summary.Summary{State: "NotReady"}},
				},
			},
		}
		return bd
	}
	// we are not comparing the whole struct
	sizeOf := func(res []fleet.Resource) int {
		size := 0
		for _, r := range res {
			for _, s := range r.PerClusterState.Ready {
				size += len(s)
			}
			for _, s := range r.PerClusterState.NotReady {
				size += len(s)
			}
			for _, s := range r.PerClusterState.Missing {
				size += len(s)
			}
		}
		return size
	}

	n := 0
	maxBundle := 50
	maxCluster := 800
	var items = make([]fleet.BundleDeployment, maxBundle*maxCluster)
	for c := range maxCluster {
		for b := range maxBundle {
			items[n] = percluster(b, c)
			n++
		}
	}

	// different order should produce the same truncation
	ritems := slices.Clone(items)
	slices.Reverse(ritems)

	var status fleet.GitRepoStatus
	SetResources(items, &status.StatusBase)

	assert.Less(t, sizeOf(status.Resources), 1024*1024, "resources should be truncated to be less than 1MB")

	js, err := json.Marshal(status.Resources)
	require.NoError(t, err)

	// and the truncation is stable
	SetResources(items, &status.StatusBase)
	js2, err := json.Marshal(status.Resources)
	require.NoError(t, err)
	// avoid the long diff from assert.Equal
	assert.Equal(t, string(js), string(js2), "truncation should produce stable json for the same input")

	SetResources(ritems, &status.StatusBase)
	js2, err = json.Marshal(status.Resources)
	require.NoError(t, err)
	assert.Equal(t, string(js), string(js2), "truncation should produce stable json, when items are in a different order")
}



================================================
FILE: internal/ssh/knownhosts.go
================================================
package ssh

import (
	"context"
	"errors"
	"fmt"
	"os"

	gossh "github.com/go-git/go-git/v5/plumbing/transport/ssh"
	"golang.org/x/crypto/ssh"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/controller-runtime/pkg/client"

	"github.com/rancher/fleet/internal/config"
)

const (
	KnownHostsConfigMap = "known-hosts" // XXX: is this the name we want?
	KnownHostsEnvVar    = "FLEET_KNOWN_HOSTS"
)

type KnownHosts struct {
	EnforceHostKeyChecks bool
}

// Get looks for SSH known hosts information in the following locations, in decreasing order of precedence:
// * secret referenced by secretName, in namespace ns
// * `gitcredential` secret, in namespace ns, if secretName is empty
// * config map in Fleet controller namespace
// It returns found known_hosts data, if any, and any error that may have happened in the process (eg. missing fallback,
// Fleet-wide known hosts config map)
// Possible returned errors include a failure to enforce strict host key checks, if those are enabled but no known_hosts
// data is found.
func (s KnownHosts) Get(ctx context.Context, c client.Client, ns string, secretName string) (string, error) {
	if ns == "" {
		return "", errors.New("empty namespace provided for secret search")
	}

	if secretName == "" {
		secretName = config.DefaultGitCredentialsSecretName
	}

	var secret corev1.Secret
	err := c.Get(ctx, types.NamespacedName{
		Namespace: ns,
		Name:      secretName,
	}, &secret)

	if client.IgnoreNotFound(err) != nil {
		return "", err
	}

	return s.GetWithSecret(ctx, c, &secret)
}

// GetWithSecret looks for SSH known hosts information in the injected secret, then in a config map in the Fleet
// controller namespace, returning data from the first source it finds.
// It returns found known_hosts data, if any, and any error that may have happened in the process (eg. missing fallback,
// Fleet-wide known hosts config map)
// Possible returned errors include a failure to enforce strict host key checks, if those are enabled but no known_hosts
// data is found.
func (s KnownHosts) GetWithSecret(ctx context.Context, c client.Client, secret *corev1.Secret) (string, error) {
	if secret != nil {
		kh, ok := secret.Data["known_hosts"]
		if ok && len(kh) > 0 {
			return string(kh), nil
		}
	}

	var cm corev1.ConfigMap
	err := c.Get(ctx, types.NamespacedName{
		Namespace: config.DefaultNamespace,
		Name:      KnownHostsConfigMap,
	}, &cm)

	if client.IgnoreNotFound(err) != nil {
		return "", err
	}

	if err != nil { // The config map should exist as part of any Fleet deployment
		return "", fmt.Errorf(
			"config map %q should exist in namespace %q; this Fleet deployment is incomplete",
			KnownHostsConfigMap,
			config.DefaultNamespace,
		)
	}

	kh, ok := cm.Data["known_hosts"]
	if ok && len(kh) > 0 {
		return kh, nil
	}

	if s.EnforceHostKeyChecks {
		return "", errors.New("strict host key checks are enforced, but no known_hosts data was found")
	}

	return "", nil
}

func (s KnownHosts) IsStrict() bool {
	return s.EnforceHostKeyChecks
}

// CreateKnownHostsCallBack creates a callback function for host key checks based on the provided knownHosts.
func CreateKnownHostsCallBack(knownHosts []byte) (ssh.HostKeyCallback, error) {
	f, err := os.CreateTemp("", "known_hosts")
	if err != nil {
		return nil, err
	}
	defer os.RemoveAll(f.Name())
	defer f.Close()

	if _, err := f.Write(knownHosts); err != nil {
		return nil, err
	}

	return gossh.NewKnownHostsCallback(f.Name())
}



================================================
FILE: internal/ssh/knownhosts_test.go
================================================
package ssh_test

import (
	"context"
	"strings"
	"testing"

	"go.uber.org/mock/gomock"

	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/apimachinery/pkg/types"

	"github.com/rancher/fleet/internal/mocks"
	"github.com/rancher/fleet/internal/ssh"
)

func TestGetKnownHosts(t *testing.T) {
	tests := map[string]struct {
		isStrict       bool
		secret         *corev1.Secret
		fallbackSecret *corev1.Secret
		configMap      *corev1.ConfigMap
		ns             string
		secretName     string
		expectedData   string
		expectedErr    string
	}{
		"no secret, no config map": {
			ns:           "foo",
			secretName:   "bar",
			expectedData: "",
			expectedErr:  "deployment is incomplete",
		},
		"secret exists but does not contain known_hosts data, no config map": {
			secret: &corev1.Secret{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "foo",
					Name:      "bar",
				},
			},
			ns:           "foo",
			secretName:   "bar",
			expectedData: "",
			expectedErr:  "deployment is incomplete",
		},
		"secret exists with known_hosts data, no config map": {
			secret: &corev1.Secret{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "foo",
					Name:      "bar",
				},
				Data: map[string][]byte{
					"known_hosts": []byte("somedata"),
				},
			},
			ns:           "foo",
			secretName:   "bar",
			expectedData: "somedata",
		},
		"secret exists without known_hosts data, but config map exists": {
			secret: &corev1.Secret{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "foo",
					Name:      "bar",
				},
			},
			configMap: &corev1.ConfigMap{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "cattle-fleet-system",
					Name:      "known-hosts",
				},
				Data: map[string]string{
					"known_hosts": "somedata",
				},
			},
			ns:           "foo",
			secretName:   "bar",
			expectedData: "somedata",
		},
		"secret does not exist, but config map exists": {
			configMap: &corev1.ConfigMap{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "cattle-fleet-system",
					Name:      "known-hosts",
				},
				Data: map[string]string{
					"known_hosts": "somedata",
				},
			},
			ns:           "foo",
			secretName:   "bar",
			expectedData: "somedata",
		},
		"both secret and config map exist": { // secret has precedence
			secret: &corev1.Secret{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "foo",
					Name:      "bar",
				},
				Data: map[string][]byte{
					"known_hosts": []byte("somedata_from_secret"),
				},
			},
			configMap: &corev1.ConfigMap{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "cattle-fleet-system",
					Name:      "known-hosts",
				},
				Data: map[string]string{
					"known_hosts": "somedata_from_configmap",
				},
			},
			ns:           "foo",
			secretName:   "bar",
			expectedData: "somedata_from_secret",
		},
		"secret, fallback secret and config map exist": { // secret still has precedence
			secret: &corev1.Secret{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "foo",
					Name:      "bar",
				},
				Data: map[string][]byte{
					"known_hosts": []byte("somedata_from_secret"),
				},
			},
			fallbackSecret: &corev1.Secret{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "foo",
					Name:      "gitcredential",
				},
				Data: map[string][]byte{
					"known_hosts": []byte("somedata_from_gitcredential"),
				},
			},
			configMap: &corev1.ConfigMap{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "cattle-fleet-system",
					Name:      "known-hosts",
				},
				Data: map[string]string{
					"known_hosts": "somedata_from_configmap",
				},
			},
			ns:           "foo",
			secretName:   "bar",
			expectedData: "somedata_from_secret",
		},
		"empty namespace": {
			ns:           "",
			secretName:   "bar",
			expectedData: "",
			expectedErr:  "empty namespace",
		},
		"empty secret name, no config map": {
			ns:           "foo",
			secretName:   "",
			expectedData: "",
			expectedErr:  "deployment is incomplete",
		},
		"empty secret name, but gitcredential secret exists": {
			fallbackSecret: &corev1.Secret{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "foo",
					Name:      "gitcredential",
				},
				Data: map[string][]byte{
					"known_hosts": []byte("somedata"),
				},
			},
			ns:           "foo",
			secretName:   "",
			expectedData: "somedata",
		},
		"empty secret name, but both gitcredential secret and config map exist": {
			fallbackSecret: &corev1.Secret{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "foo",
					Name:      "gitcredential",
				},
				Data: map[string][]byte{
					"known_hosts": []byte("somedata_gitcredential"),
				},
			},
			ns: "foo",
			configMap: &corev1.ConfigMap{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "cattle-fleet-system",
					Name:      "known-hosts",
				},
				Data: map[string]string{
					"known_hosts": "somedata_configmap",
				},
			},
			secretName:   "",
			expectedData: "somedata_gitcredential",
		},
		"empty secret name, but config map exists": {
			configMap: &corev1.ConfigMap{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "cattle-fleet-system",
					Name:      "known-hosts",
				},
				Data: map[string]string{
					"known_hosts": "somedata",
				},
			},
			ns:           "foo",
			secretName:   "",
			expectedData: "somedata",
		},
		"empty data, without enforcement": {
			secret: &corev1.Secret{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "foo",
					Name:      "bar",
				},
				Data: map[string][]byte{
					"known_hosts": []byte(""),
				},
			},
			configMap: &corev1.ConfigMap{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "cattle-fleet-system",
					Name:      "known-hosts",
				},
				Data: map[string]string{
					"known_hosts": "",
				},
			},
			ns:           "foo",
			secretName:   "bar",
			expectedData: "",
			expectedErr:  "",
		},
		"empty data, with enforcement": {
			isStrict: true,
			secret: &corev1.Secret{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "foo",
					Name:      "bar",
				},
				Data: map[string][]byte{
					"known_hosts": []byte(""),
				},
			},
			configMap: &corev1.ConfigMap{
				ObjectMeta: metav1.ObjectMeta{
					Namespace: "cattle-fleet-system",
					Name:      "known-hosts",
				},
				Data: map[string]string{
					"known_hosts": "",
				},
			},
			ns:           "foo",
			secretName:   "bar",
			expectedData: "",
			expectedErr:  "strict host key checks are enforced",
		},
	}

	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			mockCtrl := gomock.NewController(t)
			defer mockCtrl.Finish()

			client := mocks.NewMockK8sClient(mockCtrl)

			nsn := types.NamespacedName{
				Namespace: test.ns,
				Name:      test.secretName,
			}

			client.EXPECT().Get(gomock.Any(), nsn, secretPointerMatcher{}, gomock.Any()).MaxTimes(1).DoAndReturn(
				func(ctx context.Context, req types.NamespacedName, secret *corev1.Secret, opts ...interface{}) error {
					if test.secret == nil {
						return apierrors.NewNotFound(schema.GroupResource{}, "TEST ERROR")
					}

					secret.ObjectMeta = test.secret.ObjectMeta
					secret.Data = test.secret.Data

					return nil
				},
			)

			nsn.Name = "gitcredential"

			client.EXPECT().Get(gomock.Any(), nsn, secretPointerMatcher{}, gomock.Any()).MaxTimes(1).DoAndReturn(
				func(ctx context.Context, req types.NamespacedName, secret *corev1.Secret, opts ...interface{}) error {
					if test.fallbackSecret == nil {
						return apierrors.NewNotFound(schema.GroupResource{}, "TEST ERROR")
					}

					secret.ObjectMeta = test.fallbackSecret.ObjectMeta
					secret.Data = test.fallbackSecret.Data

					return nil
				},
			)

			client.EXPECT().Get(gomock.Any(), gomock.Any(), configMapPointerMatcher{}, gomock.Any()).MaxTimes(1).DoAndReturn(
				func(ctx context.Context, req types.NamespacedName, c *corev1.ConfigMap, opts ...interface{}) error {
					if test.configMap == nil {
						return apierrors.NewNotFound(schema.GroupResource{}, "TEST ERROR")
					}

					c.ObjectMeta = test.configMap.ObjectMeta
					c.Data = test.configMap.Data

					return nil
				},
			)

			getter := ssh.KnownHosts{EnforceHostKeyChecks: test.isStrict}
			data, err := getter.Get(context.TODO(), client, test.ns, test.secretName)

			if (err == nil && test.expectedErr != "") || (err != nil && test.expectedErr == "") {
				t.Errorf("expected error to match %q, got %v", test.expectedErr, err)
			}
			if err != nil && !strings.Contains(err.Error(), test.expectedErr) {
				t.Errorf("expected error to match %q, got %v", test.expectedErr, err)
			}
			if data != test.expectedData {
				t.Errorf("expected data %q, got %q", test.expectedData, data)
			}
		})
	}
}

type configMapPointerMatcher struct{}

func (m configMapPointerMatcher) Matches(x interface{}) bool {
	_, ok := x.(*corev1.ConfigMap)
	return ok
}

func (m configMapPointerMatcher) String() string {
	return ""
}

type secretPointerMatcher struct{}

func (m secretPointerMatcher) Matches(x interface{}) bool {
	_, ok := x.(*corev1.Secret)
	return ok
}

func (m secretPointerMatcher) String() string {
	return ""
}



================================================
FILE: internal/ssh/url.go
================================================
package ssh

import (
	"strings"

	giturls "github.com/rancher/fleet/pkg/git-urls"
)

// Is checks if the provided string s is a valid SSH URL, returning a boolean.
func Is(s string) bool {
	url, err := giturls.Parse(s)
	if err != nil {
		return false
	}

	return strings.HasSuffix(url.Scheme, "ssh")
}



================================================
FILE: internal/ssh/url_test.go
================================================
package ssh_test

import (
	"testing"

	"github.com/rancher/fleet/internal/ssh"
)

func TestIs(t *testing.T) {
	tests := map[string]struct {
		url       string
		expectSSH bool
	}{
		"http": {
			url:       "http://foo/bar",
			expectSSH: false,
		},
		"ftp": {
			url:       "ftp://foo/bar",
			expectSSH: false,
		},
		"http with @": {
			url:       "http://fleet-ci:foo@git-service.fleet-local.svc.cluster.local:8080/repo",
			expectSSH: false,
		},
		"simple ssh": {
			url:       "ssh://foo/bar",
			expectSSH: true,
		},
		"git ssh with @": {
			url:       "git@github.com:foo/bar.git",
			expectSSH: true,
		},
		"git+ssh": {
			url:       "git+ssh://foo/bar.git",
			expectSSH: true,
		},
		"invalid with ssh": {
			url:       "sshfoo://foo/bar.git",
			expectSSH: false,
		},
	}

	for name, test := range tests {
		t.Run(name, func(t *testing.T) {
			isSSH := ssh.Is(test.url)

			if isSSH != test.expectSSH {
				t.Errorf("expected SSH match to be %t, got %t", test.expectSSH, isSSH)
			}
		})
	}
}



================================================
FILE: package/Dockerfile
================================================
ARG BUILD_ENV=dapper
ARG ARCH

FROM --platform=linux/$ARCH registry.suse.com/bci/bci-base:15.7 AS base
COPY package/log.sh /usr/bin/
RUN zypper rm -y container-suseconnect && \
    zypper ar --priority=500 https://download.opensuse.org/repositories/Virtualization:containers/5.5/Virtualization:containers.repo && \
    zypper --gpg-auto-import-keys ref && \
    zypper -n update && \
    zypper -n install --no-recommends openssh-clients tini git-core && \
    zypper -n clean -a && \
    rm -fr /var/log/zypp* /usr/share/doc

FROM base AS copy_dapper
ONBUILD ARG ARCH
ONBUILD COPY bin/fleetcontroller-linux-$ARCH /usr/bin/fleetcontroller
ONBUILD COPY bin/fleet-linux-$ARCH /usr/bin/fleet

FROM base AS copy_buildx
ONBUILD ARG TARGETARCH
ONBUILD COPY bin/fleetcontroller-linux-$TARGETARCH /usr/bin/fleetcontroller
ONBUILD COPY bin/fleet-linux-$TARGETARCH /usr/bin/fleet

FROM base AS copy_goreleaser
ONBUILD ARG ARCH
ONBUILD COPY fleetcontroller-linux-$ARCH /usr/bin/fleetcontroller
ONBUILD COPY fleet-linux-$ARCH /usr/bin/fleet

FROM copy_${BUILD_ENV}
RUN useradd -u 1000 user
USER 1000
ENTRYPOINT ["tini", "--"]
CMD ["fleetcontroller"]



================================================
FILE: package/Dockerfile.agent
================================================
ARG BUILD_ENV=dapper
ARG ARCH

FROM --platform=linux/$ARCH registry.suse.com/bci/bci-busybox:15.7 AS base

FROM base AS copy_dapper
ONBUILD ARG ARCH
ONBUILD COPY bin/fleetagent-linux-$ARCH /usr/bin/fleetagent

FROM base AS copy_buildx
ONBUILD ARG TARGETARCH
ONBUILD COPY bin/fleetagent-linux-$TARGETARCH /usr/bin/fleetagent

FROM base AS copy_goreleaser
ONBUILD ARG ARCH
ONBUILD COPY fleetagent-linux-$ARCH /usr/bin/fleetagent

FROM copy_${BUILD_ENV}
USER 1000
CMD ["fleetagent"]



================================================
FILE: package/log.sh
================================================
#!/bin/bash
set -o pipefail
env
"$@" 2>&1 | tee /dev/termination-log


